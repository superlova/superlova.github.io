<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Superlova</title>
  
  <subtitle>Welcome...</subtitle>
  <link href="https://superlova.github.io/atom.xml" rel="self"/>
  
  <link href="https://superlova.github.io/"/>
  <updated>2022-11-20T17:34:50.656Z</updated>
  <id>https://superlova.github.io/</id>
  
  <author>
    <name>Superlova</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【学习笔记】通过文字检索视频片段：VCED——Streamlit前端简介</title>
    <link href="https://superlova.github.io/2022/11/21/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED%E2%80%94%E2%80%94Streamlit%E5%89%8D%E7%AB%AF%E7%AE%80%E4%BB%8B/"/>
    <id>https://superlova.github.io/2022/11/21/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED%E2%80%94%E2%80%94Streamlit%E5%89%8D%E7%AB%AF%E7%AE%80%E4%BB%8B/</id>
    <published>2022-11-20T17:12:30.000Z</published>
    <updated>2022-11-20T17:34:50.656Z</updated>
    
    <content type="html"><![CDATA[<p>第四篇：介绍一个 Web 应用程序框架Streamlit。</p><!--more---><p>Streamlit 是一个基于 Python 的 Web 应用程序框架。Streamlit 基于 Python，开发者无需学习其他就可以搭建一个较为完整的系统。因此此次教程，VCED项目就通过 Streamlit + Jina 构建了一套系统。</p><h2 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h2><pre><code>pip install streamlit</code></pre><h2 id="2-使用-Streamlit-构建-Web-app"><a href="#2-使用-Streamlit-构建-Web-app" class="headerlink" title="2. 使用 Streamlit 构建 Web app"></a>2. 使用 Streamlit 构建 Web app</h2><pre><code class="lang-py">import pandas as pdimport numpy as npimport streamlit as stst.set_page_config(page_title=&quot;Hello Streamlit&quot;)st.title(&#39;This is your first Streamlit page!&#39;)  # 标题st.markdown(&#39;Streamlit is **_really_ cool**.&#39;)  # markdowncode = &#39;&#39;&#39;def hello():     print(&quot;Hello, Streamlit!&quot;)&#39;&#39;&#39;st.code(code, language=&#39;python&#39;)  # codedf = pd.DataFrame(    np.random.randn(50, 20),    columns=(&#39;col %d&#39; % i for i in range(20)))st.dataframe(df)  # dataframest.latex(r&#39;&#39;&#39;     a + ar + a r^2 + a r^3 + \cdots + a r^&#123;n-1&#125; =     \sum_&#123;k=0&#125;^&#123;n-1&#125; ar^k =     a \left(\frac&#123;1-r^&#123;n&#125;&#125;&#123;1-r&#125;\right)     &#39;&#39;&#39;)  # latex</code></pre><p>按照以上代码实现的网页如下：</p><p><img src="/2022/11/21/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED%E2%80%94%E2%80%94Streamlit%E5%89%8D%E7%AB%AF%E7%AE%80%E4%BB%8B/streamlit_demo.png" alt></p><h2 id="使用-Streamlit-复现-VCED-项目的前端页面"><a href="#使用-Streamlit-复现-VCED-项目的前端页面" class="headerlink" title="使用 Streamlit 复现 VCED 项目的前端页面"></a>使用 Streamlit 复现 VCED 项目的前端页面</h2><p>VCED项目实现的流程有：上传视频、输入描述、限定搜索个数、搜索按钮等功能。</p><pre><code class="lang-py"># 导入需要的包import streamlit as stfrom jina import Client, DocumentArray, Documentimport jsonimport osimport timeimport uuidVIDEO_PATH = f&quot;&#123;os.getcwd()&#125;/data&quot;# 视频存储的路径if not os.path.exists(VIDEO_PATH):    os.mkdir(VIDEO_PATH)# 视频剪辑后存储的路径if not os.path.exists(VIDEO_PATH + &quot;/videos/&quot;):    os.mkdir(VIDEO_PATH + &quot;/videos&quot;)# GRPC 监听的端口port = 45679# 创建 Jina 客户端c = Client(host=f&quot;grpc://localhost:&#123;port&#125;&quot;)# 设置标签栏st.set_page_config(page_title=&quot;VCED&quot;, page_icon=&quot;🔍&quot;)# 设置标题st.title(&#39;Welcome to VCED!&#39;)# 视频上传组件uploaded_file = st.file_uploader(&quot;Choose a video&quot;)video_name = None  # name of the video# 判断视频是否上传成功if uploaded_file is not None:    # preview, delete and download the video    video_bytes = uploaded_file.read()    st.video(video_bytes)    # save file to disk for later process    video_name = uploaded_file.name    with open(f&quot;&#123;VIDEO_PATH&#125;/&#123;video_name&#125;&quot;, mode=&#39;wb&#39;) as f:        f.write(video_bytes)  # save video to diskvideo_file_path = f&quot;&#123;VIDEO_PATH&#125;/&#123;video_name&#125;&quot;uid = uuid.uuid1()# 文本输入框text_prompt = st.text_input(    &quot;Description&quot;, placeholder=&quot;please input the description&quot;, help=&#39;The description of clips from the video&#39;)# top k 输入框topn_value = st.text_input(    &quot;Top N&quot;, placeholder=&quot;please input an integer&quot;, help=&#39;The number of results. By default, n equals 1&#39;)# 根据秒数还原 例如 10829s 转换为 03:04:05def getTime(t: int):    m,s = divmod(t, 60)    h, m = divmod(m, 60)    t_str = &quot;%02d:%02d:%02d&quot; % (h, m, s)    print (t_str)    return t_str# 根据传入的时间戳位置对视频进行截取def cutVideo(start_t: str, length: int, input: str, output: str):    &quot;&quot;&quot;    start_t: 起始位置    length: 持续时长    input: 视频输入位置    output: 视频输出位置    &quot;&quot;&quot;    os.system(f&#39;ffmpeg -ss &#123;start_t&#125; -i &#123;input&#125; -t &#123;length&#125; -c:v copy -c:a copy -y &#123;output&#125;&#39;)# 与后端交互部分def search_clip(uid, uri, text_prompt, topn_value):    video = DocumentArray([Document(uri=uri, id=str(uid) + uploaded_file.name)])    t1 = time.time()    c.post(&#39;/index&#39;, inputs=video) # 首先将上传的视频进行处理    text = DocumentArray([Document(text=text_prompt)])    print(topn_value)    resp = c.post(&#39;/search&#39;, inputs=text, parameters=&#123;&quot;uid&quot;: str(uid), &quot;maxCount&quot;:int(topn_value)&#125;) # 其次根据传入的文本对视频片段进行搜索    data = [&#123;&quot;text&quot;: doc.text,&quot;matches&quot;: doc.matches.to_dict()&#125; for doc in resp] # 得到每个文本对应的相似视频片段起始位置列表    return json.dumps(data)# searchsearch_button = st.button(&quot;Search&quot;)if search_button: # 判断是否点击搜索按钮    if uploaded_file is not None: # 判断是否上传视频文件        if text_prompt == None or text_prompt == &quot;&quot;: # 判断是否输入查询文本            st.warning(&#39;Please input the description first!&#39;)        else:            if topn_value == None or topn_value == &quot;&quot;: # 如果没有输入 top k 则默认设置为1                topn_value = 1            with st.spinner(&quot;Processing...&quot;):                result = search_clip(uid, video_file_path, text_prompt, topn_value)                 result = json.loads(result) # 解析得到的结果                for i in range(len(result)):                    matchLen = len(result[i][&#39;matches&#39;])                    for j in range(matchLen):                        print(j)                        left = result[i][&#39;matches&#39;][j][&#39;tags&#39;][&#39;leftIndex&#39;] # 视频片段的开始位置                        right = result[i][&#39;matches&#39;][j][&#39;tags&#39;][&#39;rightIndex&#39;] # 视频片段的结束位置                        print(left)                        print(right)                        start_t = getTime(left) # 将其转换为标准时间                        output = VIDEO_PATH + &quot;/videos/clip&quot; + str(j) +&quot;.mp4&quot;                        cutVideo(start_t,right-left, video_file_path, output) # 对视频进行切分                        st.video(output) #将视频显示到前端界面                st.success(&quot;Done!&quot;)    else:        st.warning(&#39;Please upload video first!&#39;)</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;第四篇：介绍一个 Web 应用程序框架Streamlit。&lt;/p&gt;
&lt;!--more---&gt;
&lt;p&gt;Streamlit 是一个基于 Python 的 Web 应用程序框架。Streamlit 基于 Python，开发者无需学习其他就可以搭建一个较为完整的系统。因此此次教程，</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="VCED" scheme="https://superlova.github.io/tags/VCED/"/>
    
    <category term="multimodal" scheme="https://superlova.github.io/tags/multimodal/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】Ernie 模型</title>
    <link href="https://superlova.github.io/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ernie-%E6%A8%A1%E5%9E%8B/"/>
    <id>https://superlova.github.io/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ernie-%E6%A8%A1%E5%9E%8B/</id>
    <published>2022-11-20T08:42:47.000Z</published>
    <updated>2022-11-20T17:38:52.854Z</updated>
    
    <content type="html"><![CDATA[<!--more---><p>文心大模型ERNIE是百度发布的产业级知识增强大模型。</p><p><a href="https://github.com/PaddlePaddle/ERNIE">https://github.com/PaddlePaddle/ERNIE</a></p><h2 id="Roadmap"><a href="#Roadmap" class="headerlink" title="Roadmap"></a>Roadmap</h2><div class="table-container"><table><thead><tr><th>时间</th><th>发布模型</th></tr></thead><tbody><tr><td>2019.3.16</td><td>ERNIE 1.0</td></tr><tr><td>2019.7.7</td><td>ERNIE 2.0</td></tr><tr><td>2021.5.20</td><td>多粒度语言知识模型 ERNIE-Gram、超长文本双向建模预训练模型ERNIE-Doc ，以及其他图文多模态模型</td></tr><tr><td>2021.12.3</td><td>多语言 ERNIE-M</td></tr><tr><td>2022.5.20</td><td>ERNIE 3.0 系列: 110M参数通用模型ERNIE 3.0 Base、280M参数重量级通用模型ERNIE 3.0 XBase、74M轻量级通用模型ERNIE 3.0 Medium</td></tr></tbody></table></div><h2 id="Ernie-1-0"><a href="#Ernie-1-0" class="headerlink" title="Ernie 1.0"></a>Ernie 1.0</h2><p>Ernie: Enhanced representation through knowledge integration</p><p>相较于 BERT，Ernie 1.0 在模型结构上没有改动，都是采用若干 Transformer 的 Encoder 结构。</p><p>Ernie 1.0 的改动在预训练任务。</p><h3 id="1-Knowledge-Masking"><a href="#1-Knowledge-Masking" class="headerlink" title="1. Knowledge Masking"></a>1. Knowledge Masking</h3><p>BERT 的预训练采用 masked language-model（MLM）的方法，即在训练的时候随即从输入预料上mask掉一些单词，然后通过的上下文预测这些单词，该任务非常像我们在中学时期经常做的完形填空；以及Next Sentence Prediction（NSP）的方法，即判断连个句子是否是具有前后顺承关系的两句话。</p><p>ERNIE提出了<code>Knowledge Masking</code>的策略，其包含三个级别：ERNIE将Knowledge分成了三个类别：<code>token级别(Basic-Level)</code>、<code>短语级别(Phrase-Level)</code> 和 <code>实体级别(Entity-Level)</code>。通过对这三个级别的对象进行Masking，提高模型对字词、短语的知识理解。</p><p>Basic-Level Masking 就是最基础的 masking 任务，和 BERT 一样，随机选取某些单词进行 mask，令模型来预测这些 mask 是什么词。</p><p>Phrase-Level Masking 是对语句中的短语进行 masking，如 a series of；</p><p>Entity-Level Masking 是对语句中的实体词进行 masking，如人名 J. K. Rowling。</p><p><img src="/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ernie-%E6%A8%A1%E5%9E%8B/knowledge_masking.png" alt></p><h3 id="2-Dialogue-Language-Model"><a href="#2-Dialogue-Language-Model" class="headerlink" title="2. Dialogue Language Model"></a>2. Dialogue Language Model</h3><p>在 Bert 已有的预训练任务中，加入了 Dialogue Language Model 任务，使 ERNIE 学习到对话中的隐含关系，增加模型的语义表达能力。</p><p>在生成预训练数据的时候，有一定几率用另外的句子替代里面的问题和答案，所以模型还要预测是否是真实的问答对。</p><p><img src="/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ernie-%E6%A8%A1%E5%9E%8B/dialogue_language_model.png" alt></p><p>DLM 中，不再构建如同 “[CLS] + Sentence_A + [SEP] + Sentence_B + [SEP]” 的句子对，</p><p>而是如同 “[CLS] + Query + [SEP] + Response_A + [SEP] + Response_B + [SEP]” 的对话三元组；</p><p>是否上下文连续的二分类训练目标转为预测该对话是否真实 (real/fake)。</p><p>三元组随机地采用 QRQ、QRR、QQR 其中一种构建形式，上面的例子便是其中的 QRR。</p><p>为了给处理后的数据添加噪声，部分的 Query 和 Response 使用不相关的语句进行随机替换，以强制模型学习对话中的语境关联。</p><p>在训练一般语料时，ERNIE 采用 Knowledge Masking Strategies 改造后的 Masked LM；而在训练对话型语料时，ERNIE 采用 DLM；两者交替使用。</p><h3 id="3-添加数据"><a href="#3-添加数据" class="headerlink" title="3. 添加数据"></a>3. 添加数据</h3><p>Ernie 还采用多个来自不同源头的数据，比如百度贴吧，百度新闻，维基百科等等帮助模型训练。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://blog.csdn.net/u011150266/article/details/116479149">https://blog.csdn.net/u011150266/article/details/116479149</a></p><p><a href="https://www.cnblogs.com/shona/p/11820492.html">https://www.cnblogs.com/shona/p/11820492.html</a></p><h2 id="Ernie-2-0"><a href="#Ernie-2-0" class="headerlink" title="Ernie 2.0"></a>Ernie 2.0</h2><p>Ernie 2.0: A continual pre-training framework for language understanding</p><h3 id="1-总体改动"><a href="#1-总体改动" class="headerlink" title="1. 总体改动"></a>1. 总体改动</h3><p>ERNIE 2.0 将 1.0 版本中的功能特性全部予以保留，并在此基础上做更为丰富的扩展和延伸。</p><p>Ernie 2.0 添加了很多其他的自监督学习方法进行训练。该思想与 ALBERT（SOP）、SpanBERT（SBO）类似。</p><p>但是一味堆叠任务可能导致各种任务都学不好，Ernie 2.0 提出了一个持续学习的框架，模型可以持续添加任务但又不降低之前任务的精度。</p><p>目标是能够更好更有效地获得词法lexical，句法syntactic，语义semantic上的表达。</p><p><img src="/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ernie-%E6%A8%A1%E5%9E%8B/ernie2_0_structure.png" alt></p><h3 id="2-添加预训练任务"><a href="#2-添加预训练任务" class="headerlink" title="2. 添加预训练任务"></a>2. 添加预训练任务</h3><p><strong>1）Word-aware Pretraining Tasks</strong>，捕捉词汇 (lexical) 级别的信息</p><p>Knowledge Masking Task：沿袭 ERNIE 1.0 中的 Knowledge Masking Strategies，预测被 Mask 的对象。</p><p>Capitalization Prediction Task：预测对象是否大小写 (cased/uncased)；ERNIE 2.0 认为大写开头的词汇具备特定的含义，例如人名、地名、机构名等，这将对命名实体识别一类的下游任务有所帮助。</p><p>Token-Document Relation Prediction Task：预测对象是否在文档中其他文段有出现；正案例通常包含文档的关键词以及语言通用词，因此添加这一任务有助于模型将更多的注意力放在这一类词汇上。</p><p><strong>2）Structure-aware Pretraining Tasks</strong>，捕捉语料中语法 (syntactic) 级别的信息</p><p>Sentence Recording Task：针对文档中的每一个段落，以句子为单位划分为 1~m 段，而后对整个文档所有文段进行打乱排列，对每一个文段预测原始位置，成为 k 分类问题。</p><p>Sentence Distance Task：取代 Next Sentence Prediction，预测输入句子对的相对距离；三分类问题，0 代表两个句子在同一篇文档且距离相近，1 代表两个句子在同一片文档但距离较远，2 代表两个句子在不同文档。</p><p><strong>3）Semantic-aware Pretraining Tasks</strong>，提取语义 (semantic) 类的信息</p><p>Discourse Relation Task：预测两个句子之间的语法及修辞关联。</p><p><img src="/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ernie-%E6%A8%A1%E5%9E%8B/discourse_relation_task.png" alt></p><p>IR Relevance Task：专门为百度搜索引擎日志设计的任务，预测 Query-Title 对的相关性；三分类问题，0、1、2 分别代表强相关、弱相关以及不相关；这将有助于标题自动生成以及文本摘要类任务。</p><h3 id="3-预训练框架"><a href="#3-预训练框架" class="headerlink" title="3. 预训练框架"></a>3. 预训练框架</h3><p>任务叠加，不是一次性进行的（Multi-task learning），而是持续学习(Continual Pre-training)，所以必须避免模型在学了新的任务之后，忘记旧的任务，即在旧的任务上loss变高。</p><p>为此，ERNIE 2.0 首次引入 连续预训练机制 —— 以串行的方式进行多任务学习。</p><p>3、Sequential Multi-task Learning：ERNIE 2.0中新提出的方法，每当有新任务出现时，使用先前学习的参数来初始化模型，并同时训练新引入的任务和原始任务。这样解决了前两种方法的问题，可以随时引入新任务，并保留先前学到的知识。</p><p><img src="/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ernie-%E6%A8%A1%E5%9E%8B/ernie2_0_framework.png" alt></p><h3 id="参考-1"><a href="#参考-1" class="headerlink" title="参考"></a>参考</h3><p><a href="https://blog.csdn.net/qq_35386727/article/details/105947650">https://blog.csdn.net/qq_35386727/article/details/105947650</a></p><p><a href="https://zhuanlan.zhihu.com/p/460151166">https://zhuanlan.zhihu.com/p/460151166</a></p><p><a href="https://blog.csdn.net/weixin_43269174/article/details/98437096">https://blog.csdn.net/weixin_43269174/article/details/98437096</a></p><p><a href="https://www.jiqizhixin.com/articles/2020-04-28-3">https://www.jiqizhixin.com/articles/2020-04-28-3</a></p><p><a href="https://aijishu.com/a/1060000000199742">https://aijishu.com/a/1060000000199742</a></p><h2 id="Ernie-gram-和-Ernie-doc"><a href="#Ernie-gram-和-Ernie-doc" class="headerlink" title="Ernie gram 和 Ernie doc"></a>Ernie gram 和 Ernie doc</h2><p>ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding</p><p>ERNIE-Doc: A Retrospective Long-Document Modeling Transformer¶</p><h3 id="1-Ernie-Gram-的改进"><a href="#1-Ernie-Gram-的改进" class="headerlink" title="1. Ernie-Gram 的改进"></a>1. Ernie-Gram 的改进</h3><p>很简单，即在 MLM 任务中，把最小 mask 单位从字粒度变成 n-gram 粒度。这样做的理由是原有的 masking 粒度太细了，切分的时候容易破坏语义完整性。</p><p>因此 Ernie-Gram 直接去预测一个n-gram词，而不是预测一系列连续的token，从而保证n-gram词的语义完整性。</p><p>ERNIE-Gram主要提出了两种融合方式：Explictly N-gram MLM 和 Comprehensive N-gram Prediction。</p><p>此外，Ernie-Gram 还使用了RTD预训练任务，来识别每个token是否是生成的。</p><h3 id="2-Ernie-Doc-的改进"><a href="#2-Ernie-Doc-的改进" class="headerlink" title="2. Ernie-Doc 的改进"></a>2. Ernie-Doc 的改进</h3><p>Ernie-Doc 可以对更长的输入序列进行建模。</p><h3 id="参考-2"><a href="#参考-2" class="headerlink" title="参考"></a>参考</h3><p><a href="https://zhuanlan.zhihu.com/p/376000666">https://zhuanlan.zhihu.com/p/376000666</a></p><p><a href="https://blog.csdn.net/jokerxsy/article/details/116482035">https://blog.csdn.net/jokerxsy/article/details/116482035</a></p><p><a href="https://www.sohu.com/a/468508378_129720">https://www.sohu.com/a/468508378_129720</a></p><p><a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/pretrain_model/ERNIE-Gram.html">https://paddlepedia.readthedocs.io/en/latest/tutorials/pretrain_model/ERNIE-Gram.html</a></p><h2 id="Ernie-3-0"><a href="#Ernie-3-0" class="headerlink" title="Ernie 3.0"></a>Ernie 3.0</h2><p>ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation</p><p>ERNIE 3.0同时结合了将自回归和自编码网络，从而模型在文本生成和语言理解任务表现均很好。另外，ERNiE 3.0在预训练阶段中引入了知识图谱数据。</p><h3 id="1-网络结构改进"><a href="#1-网络结构改进" class="headerlink" title="1. 网络结构改进"></a>1. 网络结构改进</h3><p>ERNIE 3.0设计了上下两层网络结构：Universal Representation Module 和 Task-specific Representation Module。</p><p><img src="/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ernie-%E6%A8%A1%E5%9E%8B/ernie3_0_structure.png" alt></p><p>在模型底层结构上共享参数，用于获取词汇、句法等通用的抽象特征；在顶层对不同任务使用独立参数，用于获取特定任务的特征。</p><p>ERNIE 3.0认为不同的任务模式依赖的自然语言的底层特征是相同的，比如词法和句法信息，然而不同的任务模式需要的上层具体的特征是不同的。自然语言理解的任务往往倾向于学习语义连贯性，然而自然语义生成任务却期望能够看见更长的上下文信息。</p><p>由于自回归模型在文本生成任务上表现更好，自编码模型在语言理解任务上表现更好。因此，ERNIE 3.0在上层使用了两个网络，一个用于聚焦自然语言理解，一个用于聚焦自然语言生成任务。不同任务适配更合适的网络，能够提高模型在相应任务上的表现。</p><p>在fine-tuning阶段，可以固定Universal Representation Module，只微调Task-specific Representation Module参数，提高训练效率。</p><p><strong>1）Universal Representation Module</strong></p><p>使用Transformer-XL作为骨干网络，Transformer-XL允许模型使用记忆循环机制建模更长的文本序列依赖。</p><p>Transformer-XL 在普通 transformer 基础上增加了一个 recurrence memory module 用于建模长文本，避免了普通 transformer 中由于输入长度限制，不得不对长文本进行切分从而导致损失长距离依赖的问题。</p><p><strong>2）Task-specific Representation Module</strong></p><p>同样使用了Transformer-XL作为骨干网络，但使用了base size模型。</p><h3 id="2-引入新的预训练任务"><a href="#2-引入新的预训练任务" class="headerlink" title="2. 引入新的预训练任务"></a>2. 引入新的预训练任务</h3><p>除了 Ernie 2.0 引入的很多个预训练任务之外，Ernie 3.0 还增加了 universal knowledge-text prediction（UKTP）任务</p><p>给定一个三元组\<head, relation, tail\>和一个句子，ERNIE 3.0会mask掉三元组中的实体关系relation，或者句子中的单词word，然后让模型去预测这些内容。</head,></p><p>模型在预测三元组中的关系时，需要找到对应句子中的head和tail词并确定他们之间的语义关系；而模型在预测句子中的词时，则需要同时考虑句子中的依存信息 (dependency information) 和三元组中的逻辑关系 (logical relationship)。</p><p>这个任务之所以有效，是因为我们假设：如果一个句子中同时出现head和tail两个实体，则这个句子能够表达这两个实体的关系。</p><p><img src="/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ernie-%E6%A8%A1%E5%9E%8B/universal_knowledge_text_pred.png" alt></p><h3 id="3-预训练-Trick"><a href="#3-预训练-Trick" class="headerlink" title="3. 预训练 Trick"></a>3. 预训练 Trick</h3><p>为了加快预训练过程中的模型收敛速度，本文提出了逐渐增加训练正则因子 (training regularization factor) 的方法。</p><p>具体来说就是在训练过程中逐步且同时增加输入序列长度、batch size、学习率和dropout rate。</p><p>在一般的transformer训练中都会使用warm-up策略逐步增加学习率，本文提出了将batch size等其他因子同时增加的策略。</p><h3 id="参考-3"><a href="#参考-3" class="headerlink" title="参考"></a>参考</h3><p><a href="https://alexyxwang.com/2021/12/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ERNIE-3-0-Large-scale-Knowledge-Enhanced-Pre-training-for-Language-Understanding-and-Generation/">https://alexyxwang.com/2021/12/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ERNIE-3-0-Large-scale-Knowledge-Enhanced-Pre-training-for-Language-Understanding-and-Generation/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;!--more---&gt;
&lt;p&gt;文心大模型ERNIE是百度发布的产业级知识增强大模型。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/ERNIE&quot;&gt;https://github.com/PaddlePaddle/ERNIE&lt;/a&gt;</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="BERT" scheme="https://superlova.github.io/tags/BERT/"/>
    
    <category term="Ernie" scheme="https://superlova.github.io/tags/Ernie/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】NLP数据增强方法记录</title>
    <link href="https://superlova.github.io/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95/"/>
    <id>https://superlova.github.io/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95/</id>
    <published>2022-11-20T08:41:35.000Z</published>
    <updated>2022-11-20T11:56:37.363Z</updated>
    
    <content type="html"><![CDATA[<!--more---><h2 id="1-基于规则的数据增强"><a href="#1-基于规则的数据增强" class="headerlink" title="1. 基于规则的数据增强"></a>1. 基于规则的数据增强</h2><p><a href="https://mp.weixin.qq.com/s?__biz=Mzk0NzMwNjU5Nw==&amp;mid=2247484450&amp;idx=1&amp;sn=953dd0856ee087d52a9c229f68281eb3&amp;chksm=c379ad28f40e243eade7677e16fb4146a257c32c39d8b364952c061bdba0af02b6a49a8d5cb0&amp;scene=178&amp;cur_album_id=2592122537619554305#rd">https://mp.weixin.qq.com/s?__biz=Mzk0NzMwNjU5Nw==&amp;mid=2247484450&amp;idx=1&amp;sn=953dd0856ee087d52a9c229f68281eb3&amp;chksm=c379ad28f40e243eade7677e16fb4146a257c32c39d8b364952c061bdba0af02b6a49a8d5cb0&amp;scene=178&amp;cur_album_id=2592122537619554305#rd</a></p><p>对于同一个句子，可以采用如下的数据增强方法：</p><ul><li>随机删除一个词</li><li>随机选择一个词，用它的同义词替换</li><li>随机选择两个词，然后交换它们的位置</li><li>随机选择一个词，然后随机选择一个它的近义词，然后随机插入句子的任意位置</li></ul><p>重点是 “同义词替换”，在问题中选择一个不是停止词的词，并用一个随机选择的同义词来替换它。我们可以使用nltk WordNet来产生同义词，生成与原问题等价的新问题。这样就得到了一条新训练数据。</p><p>或者我们识别实体后，通过反义词进行替换，则得到负例。</p><p>先使用规则的方法令可回答和不可回答的问题平衡；在二者之间达到平衡之后，数据增强即可结束。</p><h2 id="2-使用模型生成数据"><a href="#2-使用模型生成数据" class="headerlink" title="2. 使用模型生成数据"></a>2. 使用模型生成数据</h2><p>我们希望得到一个生成模型，将每个篇章与对应的问题以及答案，输入生成模型，模型输出新的问题，这个问题与篇章很相关；或者直接输出新的问题和新的答案（这个比较困难）。</p><p>UniLM 模型：</p><p><a href="https://xv44586.github.io/2020/08/22/qa-augmentation/">https://xv44586.github.io/2020/08/22/qa-augmentation/</a></p><p>GPT-2模型：</p><p><a href="https://zhuanlan.zhihu.com/p/146382050">https://zhuanlan.zhihu.com/p/146382050</a></p><p>Pair-to-Sequence 模型：</p><p><a href="https://zhuanlan.zhihu.com/p/74514486">https://zhuanlan.zhihu.com/p/74514486</a></p><h2 id="3-使用对抗训练方法扰动输入数据"><a href="#3-使用对抗训练方法扰动输入数据" class="headerlink" title="3. 使用对抗训练方法扰动输入数据"></a>3. 使用对抗训练方法扰动输入数据</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;!--more---&gt;
&lt;h2 id=&quot;1-基于规则的数据增强&quot;&gt;&lt;a href=&quot;#1-基于规则的数据增强&quot; class=&quot;headerlink&quot; title=&quot;1. 基于规则的数据增强&quot;&gt;&lt;/a&gt;1. 基于规则的数据增强&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://mp</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="NLP" scheme="https://superlova.github.io/tags/NLP/"/>
    
    <category term="Data Augmentation" scheme="https://superlova.github.io/tags/Data-Augmentation/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】百度搜索竞赛——问答抽取赛道</title>
    <link href="https://superlova.github.io/2022/11/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E7%99%BE%E5%BA%A6%E6%90%9C%E7%B4%A2%E7%AB%9E%E8%B5%9B%E2%80%94%E2%80%94%E9%97%AE%E7%AD%94%E6%8A%BD%E5%8F%96%E8%B5%9B%E9%81%93/"/>
    <id>https://superlova.github.io/2022/11/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E7%99%BE%E5%BA%A6%E6%90%9C%E7%B4%A2%E7%AB%9E%E8%B5%9B%E2%80%94%E2%80%94%E9%97%AE%E7%AD%94%E6%8A%BD%E5%8F%96%E8%B5%9B%E9%81%93/</id>
    <published>2022-11-20T07:58:37.000Z</published>
    <updated>2022-11-20T16:59:39.980Z</updated>
    
    <content type="html"><![CDATA[<!--more---><p>11 月至 12 月 15 作品提交</p><p>给看课程的同学算力，100 小时；fork 项目、创建项目，提升用户等级，从而获取免费时长；相当多实际项目；其他比赛；其他项目的 baseline paddleNLP 在 github</p><p>baseline：<br>赛道 1 的 baseline 5043272 4960090<br>赛道 2 的 baseline TensorRT 5007642</p><p>序列标注任务的思路、QA 的思路</p><h2 id="思路-1：使用抽取式问答思路"><a href="#思路-1：使用抽取式问答思路" class="headerlink" title="思路 1：使用抽取式问答思路"></a>思路 1：使用抽取式问答思路</h2><h3 id="baseline1-水哥"><a href="#baseline1-水哥" class="headerlink" title="baseline1: 水哥"></a>baseline1: 水哥</h3><p><a href="https://aistudio.baidu.com/aistudio/projectdetail/5029712?forkThirdPart=1">https://aistudio.baidu.com/aistudio/projectdetail/5029712?forkThirdPart=1</a></p><p>notebook逻辑</p><pre><code>1. 按照。分句，需要统计还可以按照什么进行分句切割2. 删除文档中存在单句超长的文档，该部分可以提升，可以看一些 case 决定是由于分词不佳还是数据质量低3. 对输入数据进行编码，3.1 问题+答案长度超过 512 的会被跳过3.2 调用 tokenizer.encode 接口，参数 1 是 query，参数 2 是 sentence3.3 确认 start_pos 和 end_pos产出的数据为&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;seq_len&#39;, &#39;attention_mask&#39;, &#39;position_ids&#39;, &#39;start_positions&#39;, &#39;end_positions&#39;4. 数据批量打包，使用 batch_size = 6，这里可以使用其他的 batch_size5. 设置 SGD 优化器，learning rate = 0.0005，这里可以使用其他的优化器以及learning rate损失函数设置为 CrossEntropyLoss6. 开始训练7. 导出结果</code></pre><p>步骤：</p><ol><li>先跑通 Fine tune 中文预训练模型</li><li>调试一些超参数，batch_size/learning rate/epoch/等</li><li>应用学习率的一些 trick，adam/学习率衰减等</li><li>提升数据预处理的效果，不同切分方法、不同建模目标、数据增强等；</li><li>尝试使用其他预训练模型，做模型调研并修改；</li><li>提升数据后处理的效果，模型融合等；</li></ol><p>会议提到的其他思想</p><p>数据整理手段：</p><ol><li>。分段</li><li>滑动窗口</li></ol><p>ErnieForQuestionAnswering<br>ErnieTokenizer<br>Ernie 结构</p><p>预处理方法调研<br>不同模型、不同框架调研<br>模型融合调研</p><p>SEO 的思路应用在数据增强<br>混合精度<br>warm up</p><p>optimizer<br>xgboost/lightgbm<br>bert 参数个数</p><p>水哥的优化思路：<br>1、他的方案只使用了 5000 条训练数据，后续可以使用全量数据；<br>2、数据预处理的改进，分割 sentences 的方式、正负样本比例的采样方式、句子长度改小一点方便训练等；<br>3、预训练模型，使用更 large 的模型；<br>4、集成模型，使用多折数据，或者不同文本处理方法来训练不同模型，然后集成；<br>5、标题信息可能会有用，不同类别的帖子中包含答案的概率不同；把标题、domain 等信息作为树模型的输入进行分类；<br>6、不同的建模方式，可以使用 query-answer 匹配的思路（nsp）来做？本思路是传统的 QA 方式进行建模的，提供 start 和 end<br>7、水哥的效果再 0.4~0.46，官方基线会更好，因为官方用到了更大的模型以及更多的数据；<br>8、必须用 paddle 开发。</p><h3 id="baseline2"><a href="#baseline2" class="headerlink" title="baseline2:"></a>baseline2:</h3><p><a href="https://aistudio.baidu.com/aistudio/projectdetail/4960090">https://aistudio.baidu.com/aistudio/projectdetail/4960090</a></p><h3 id="baseline3"><a href="#baseline3" class="headerlink" title="baseline3:"></a>baseline3:</h3><p><a href="https://aistudio.baidu.com/aistudio/projectdetail/5043272">https://aistudio.baidu.com/aistudio/projectdetail/5043272</a></p><p>默认回答：【】详情请看视频回答】，重复越高可能问题越大<br>长度过短：Noanswer 越可能<br>Noanswer：数据占 1/3，可以把 haveanswer 作为二分类，或者多任务学习方法</p><p>多片段抽取详见参考资料，框架应该支持；无答案？<br>官方的 baseline 优化的比较底层<br>对抗训练</p><h2 id="思路-2：使用序列标注思路"><a href="#思路-2：使用序列标注思路" class="headerlink" title="思路 2：使用序列标注思路"></a>思路 2：使用序列标注思路</h2><h3 id="baseline4"><a href="#baseline4" class="headerlink" title="baseline4:"></a>baseline4:</h3><p><a href="https://aistudio.baidu.com/aistudio/projectdetail/5030168">https://aistudio.baidu.com/aistudio/projectdetail/5030168</a></p><h2 id="对齐"><a href="#对齐" class="headerlink" title="对齐"></a>对齐</h2><p>滑窗要提前处理</p><p>没有答案就少滑窗，答案短就划得快，答案长过滑动窗口的大小，则以滑动窗口的上下标为start、end</p><p>focal loss 损失函数</p><p>标注 0-1 改造成高斯分布</p><p>分层任务，先预测是否存在答案，然后预测答案的开始和结束</p><p>分词按照 [，。【】、] 等等</p><p>查看水哥的代码，检验 badcase；answer list 有好多个答案的情况，他们如何对答案进行建模的？</p><p>序列标注问题</p><p>doc_stride 小的情况下，正负样本如何决定？如果 window 内只有部分答案，能算作正样本还是负样本？</p><p>CRF</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>算力如何获取？大家算力充足吗</p><ol><li>公开代码？代码仓库</li><li>时间表；分工</li></ol><p>关键时间节点：<br>调研模型、挖掘 baseline 思想、数据预处理，数据增强结束<br>提交第一版模型，预计分数<br>不同数据训练出来不同的模型，多个高水平模型进行投票集成</p><p><strong>11 月至 12 月 15 作品提交</strong></p><p>11 月 21 日——11 月 27 日</p><p>11 月 28 日——12 月 4 日</p><p>12 月 5 日——12 月 11 日</p><p>12 月 12 日——12 月 15 日</p>]]></content>
    
    
      
      
    <summary type="html">&lt;!--more---&gt;
&lt;p&gt;11 月至 12 月 15 作品提交&lt;/p&gt;
&lt;p&gt;给看课程的同学算力，100 小时；fork 项目、创建项目，提升用户等级，从而获取免费时长；相当多实际项目；其他比赛；其他项目的 baseline paddleNLP 在 github&lt;/p&gt;
&lt;</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Extractive Question Answering" scheme="https://superlova.github.io/tags/Extractive-Question-Answering/"/>
    
    <category term="Competition" scheme="https://superlova.github.io/tags/Competition/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】李宏毅ML-HW7-使用BERT解决答案提取问题</title>
    <link href="https://superlova.github.io/2022/11/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-HW7-%E4%BD%BF%E7%94%A8BERT%E8%A7%A3%E5%86%B3%E7%AD%94%E6%A1%88%E6%8F%90%E5%8F%96%E9%97%AE%E9%A2%98/"/>
    <id>https://superlova.github.io/2022/11/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-HW7-%E4%BD%BF%E7%94%A8BERT%E8%A7%A3%E5%86%B3%E7%AD%94%E6%A1%88%E6%8F%90%E5%8F%96%E9%97%AE%E9%A2%98/</id>
    <published>2022-11-19T10:24:36.000Z</published>
    <updated>2022-11-20T07:58:47.244Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇会试着完成作业 7，以及分析问答模型的建模方法和优化方法。</p><!--more---><p>本篇博客记录课程作业 7 的做法。</p><h2 id="一、任务目标"><a href="#一、任务目标" class="headerlink" title="一、任务目标"></a>一、任务目标</h2><p>抽取式问答（Extractive Question Answering）任务是指：从一段文本中提取对应问题的答案。</p><p>一般而言，模型的输入为文本（Document）和问题（Question），输出答案在文本中的位置。如果模型认为文本中没有答案，则输出一个特定的占位符，比如“无答案（NoAnswer）”。</p><p><img src="/2022/11/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-HW7-%E4%BD%BF%E7%94%A8BERT%E8%A7%A3%E5%86%B3%E7%AD%94%E6%A1%88%E6%8F%90%E5%8F%96%E9%97%AE%E9%A2%98/extractive_question_answering.png" alt></p><p>本次作业的 baseline 使用了 BERT 作为预训练模型。具体地，使用 [SEP] 的这个特殊单词，将 Query 和 Document 一起作为输入。然后在 Bert 中获取良好的 embedding(词向量)，然后将这个embedding的结果接入一个分类器，分别得到答案在文章中位置的id和结束位置的id。</p><p>因为数据集中的答案是可以直接在文章中抽取出来，所以得到答案起始位置的id和结束位置的id可以直接抽取出正确的答案。</p><h2 id="二、数据分析"><a href="#二、数据分析" class="headerlink" title="二、数据分析"></a>二、数据分析</h2><p>本次数据集是繁体中文的阅读理解数据集。类似的数据集还有很多，比如 <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD 数据集</a>、<a href="https://github.com/pluto-junzeng/ChineseSquad">中文 SQuAD</a></p><p>首先我们来分析下数据集。从 <a href="https://www.kaggle.com/competitions/ml2021-spring-hw7/data">kaggle 的对应链接</a> 就能下载本次作业所需的数据集。</p><p>数据集由三个文件组成，分别是训练集，测试集和验证集。每个文件都是一段 json 文本，可以由以下 python 代码读取：</p><pre><code class="lang-py"># 将 训练集 读入内存，并整理成 dict 的数据结构def load_file(filename):    with open(filename, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f:        data = json.loads(f.read())    return datatrain_file = &quot;../data/hw7_train.json&quot;test_file = &quot;../data/hw7_test.json&quot;dev_file = &quot;../data/hw7_dev.json&quot;train_data = load_file(train_file)test_data = load_file(test_file)dev_data = load_file(dev_file)print(&quot;Train Data Num: &#123;0&#125;\nTest Data Num: &#123;1&#125;\nDev Data Num: &#123;2&#125;&quot;.format(len(train_data[&quot;questions&quot;]), len(test_data[&quot;questions&quot;]), len(dev_data[&quot;questions&quot;])))</code></pre><p>以训练数据为例，数据被分成了两部分：questions 和 paragraphs 。</p><pre><code class="lang-py">train_data.keys()# [OUT]: dict_keys([&#39;questions&#39;, &#39;paragraphs&#39;])train_data[&quot;questions&quot;][0]# [OUT]: #  &#123;&#39;id&#39;: 0,#  &#39;paragraph_id&#39;: 3884,#  &#39;question_text&#39;: &#39;羅馬教皇利奧三世在800年正式加冕誰為羅馬人的皇帝?&#39;,#  &#39;answer_text&#39;: &#39;查理大帝&#39;,#  &#39;answer_start&#39;: 141,#  &#39;answer_end&#39;: 144&#125;train_data[&quot;paragraphs&quot;][3884]#[OUT]: &#39;利奧三世開創的伊蘇里亞王朝在8世紀末期走上了末路...&#39;</code></pre><p>questions 部分储存了问题的编号，以及对应的文章储存在 paragraphs 中的下标。由于 questions 和 paragraphs 的数目不相等，有多个问题对应一篇文章，因此 questions 需要指定查找的文章。</p><p>questions 部分还储存有问题的文本，以及答案的文本，以及答案在文档中的开始位置和起始位置。如果要提取 paragraphs 的答案，直接使用 <code>train_data[&quot;paragraphs&quot;][3884][answer_start:answer_end+1]</code> 即可。</p><p>paragraphs 部分则是纯文本。</p><p>训练集、测试集和验证集的大小如下所示：</p><pre><code>Train Data Question Num: 26936Test Data Question Num: 3493Dev Data Question Num: 3524Train Data Para Num: 8014Test Data Para Num: 1000Dev Data Para Num: 1000</code></pre><p>测试集 test_data 则没有答案，answer 相关字段全部为 None 。</p><h2 id="三、输出格式要求和得分标准"><a href="#三、输出格式要求和得分标准" class="headerlink" title="三、输出格式要求和得分标准"></a>三、输出格式要求和得分标准</h2><p>如果想要验证自己的模型效果，则提交到 kaggle 系统中进行自动评判即可。需要使用模型给出 test 测试集的所有推理结果，并将其整理成 <code>ID,Answer</code> 的 csv 文本，然后上传。</p><p><img src="/2022/11/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-HW7-%E4%BD%BF%E7%94%A8BERT%E8%A7%A3%E5%86%B3%E7%AD%94%E6%A1%88%E6%8F%90%E5%8F%96%E9%97%AE%E9%A2%98/output_standard.png" alt></p><p>如果最终成绩能达到 0.84 以上，则说明成绩已经非常优秀。</p><h2 id="四、上分技巧"><a href="#四、上分技巧" class="headerlink" title="四、上分技巧"></a>四、上分技巧</h2><h3 id="1-使用滑动窗口截取多段文本解决长文本问题"><a href="#1-使用滑动窗口截取多段文本解决长文本问题" class="headerlink" title="1. 使用滑动窗口截取多段文本解决长文本问题"></a>1. 使用滑动窗口截取多段文本解决长文本问题</h3><p>模型的输入长度为 问题长度 + 篇章长度 + 3（特殊字符个数）。对于 BERT 模型而言，最长输入长度为 512。这是由 self-attention 的计算量限制的。</p><p>下面是对训练集的 paragraphs 部分的长度直方图：</p><p><img src="/2022/11/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-HW7-%E4%BD%BF%E7%94%A8BERT%E8%A7%A3%E5%86%B3%E7%AD%94%E6%A1%88%E6%8F%90%E5%8F%96%E9%97%AE%E9%A2%98/length_of_paragraphs.png" alt></p><p>因此使用整篇文章作为输入是不现实的。</p><p>但是如果假定文章的大部分信息对回答问题没有帮助，我们可以只使用局部的信息进行训练。</p><p>具体做法是：训练时，在篇章的答案附近，仅截取一个固定长度的窗口内的所有文本，该窗口尽可能大，比如 32 个字符。</p><p>推理时，由于我们不知道答案到底在哪里，因此在篇章上滑动窗口切分多段文本，令模型在每段文本中寻找答案，最后 total_score 评分最高的就是最终的输出。</p><p><img src="/2022/11/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-HW7-%E4%BD%BF%E7%94%A8BERT%E8%A7%A3%E5%86%B3%E7%AD%94%E6%A1%88%E6%8F%90%E5%8F%96%E9%97%AE%E9%A2%98/example_of_sliding_window.png" alt></p><p>以上图的两个窗口举例， window_1 的答案中 start_position 的模型评分为 0.5， end_score 为 0.4，加起来为 0.9，低于 window_2 的 1.0，因此最终选择 window_2 的 start_pos 和 end_pos。</p><h3 id="2-动态调整学习率"><a href="#2-动态调整学习率" class="headerlink" title="2. 动态调整学习率"></a>2. 动态调整学习率</h3><p>手工调整学习率：</p><p><img src="/2022/11/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-HW7-%E4%BD%BF%E7%94%A8BERT%E8%A7%A3%E5%86%B3%E7%AD%94%E6%A1%88%E6%8F%90%E5%8F%96%E9%97%AE%E9%A2%98/learning_rate_decay.png" alt></p><p><a href="https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup">https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup</a></p><p><a href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate">https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate</a></p><h3 id="3-修改滑动窗口的步进字符数"><a href="#3-修改滑动窗口的步进字符数" class="headerlink" title="3. 修改滑动窗口的步进字符数"></a>3. 修改滑动窗口的步进字符数</h3><p>doc_stride 指滑动窗口每次移动的字符个数。</p><p>实际使用模型做测试时，如果答案在窗口附近，横跨两个窗口的话，答案会被截断，模型会看不到正确答案。为了防止答案被截断，滑动窗口的步进个数必须比滑动窗口本身要小，下一个截取的字符串要与上一个有一定重合，即 overlapping window 。</p><p>应该减小 doc_stride ，才能做到重叠窗口的效果。默认的 doc_stride 为 150，我们可以减半为 75，以减少答案被截断的概率。</p><h3 id="4-滑动窗口切分位置的优化"><a href="#4-滑动窗口切分位置的优化" class="headerlink" title="4. 滑动窗口切分位置的优化"></a>4. 滑动窗口切分位置的优化</h3><p>baseline 中默认将答案的中点作为切分窗口的中点，导致切分出来的训练集大部分都是 <code>无用文本+答案+无用文本</code> ，模型学习到了很多无用文本。</p><p>实际上答案并不总在窗口的中间，在测试阶段窗口的划分是随机的，答案可能出现在窗口的前半部分或者后半部分。因此我们需要对训练集的窗口切分方式做优化，比如引入随机性。</p><h3 id="5-使用其他的预训练模型"><a href="#5-使用其他的预训练模型" class="headerlink" title="5. 使用其他的预训练模型"></a>5. 使用其他的预训练模型</h3><p><a href="https://huggingface.co/models">https://huggingface.co/models</a></p><h3 id="6-模型后处理"><a href="#6-模型后处理" class="headerlink" title="6. 模型后处理"></a>6. 模型后处理</h3><p>我们可以通过添加规则，来规避模型的显著错误，比如 start_pos &gt; end_pos 的情况显然是不可能的。</p><h3 id="7-自动混合精度"><a href="#7-自动混合精度" class="headerlink" title="7. 自动混合精度"></a>7. 自动混合精度</h3><p>Pytorch 训练模型默认使用 float32 精度的浮点数。自动混合精度（Automatic mixed precision）能够允许你使用半精度浮点数 float16 来加速模型的训练过程。这个操作只在 T4/V100 等 GPU 上有用。</p><p><img src="/2022/11/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-HW7-%E4%BD%BF%E7%94%A8BERT%E8%A7%A3%E5%86%B3%E7%AD%94%E6%A1%88%E6%8F%90%E5%8F%96%E9%97%AE%E9%A2%98/automatic_mixed_precision.png" alt></p><p><a href="https://huggingface.co/docs/accelerate/index">https://huggingface.co/docs/accelerate/index</a></p><p><a href="https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/">https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/</a></p><h3 id="8-梯度累加"><a href="#8-梯度累加" class="headerlink" title="8. 梯度累加"></a>8. 梯度累加</h3><p>当我们希望使用更大的 batch_size 但是显存不够时，可以使用梯度累加（Gradient Accumulation）的方法来节约显存。</p><p><a href="https://kozodoi.me/python/deep%20learning/pytorch/tutorial/2021/02/19/gradient-accumulation.html">https://kozodoi.me/python/deep%20learning/pytorch/tutorial/2021/02/19/gradient-accumulation.html</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇会试着完成作业 7，以及分析问答模型的建模方法和优化方法。&lt;/p&gt;
&lt;!--more---&gt;
&lt;p&gt;本篇博客记录课程作业 7 的做法。&lt;/p&gt;
&lt;h2 id=&quot;一、任务目标&quot;&gt;&lt;a href=&quot;#一、任务目标&quot; class=&quot;</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
    <category term="BERT" scheme="https://superlova.github.io/tags/BERT/"/>
    
    <category term="Extraction-based Question Answering" scheme="https://superlova.github.io/tags/Extraction-based-Question-Answering/"/>
    
    <category term="homework" scheme="https://superlova.github.io/tags/homework/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】通过文字检索视频片段：VCED——跨模态模型介绍</title>
    <link href="https://superlova.github.io/2022/11/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED%E2%80%94%E2%80%94%E8%B7%A8%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/"/>
    <id>https://superlova.github.io/2022/11/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED%E2%80%94%E2%80%94%E8%B7%A8%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/</id>
    <published>2022-11-18T16:58:52.000Z</published>
    <updated>2022-11-18T18:34:19.697Z</updated>
    
    <content type="html"><![CDATA[<p>第三篇：跨模态模型介绍</p><!--more---><h2 id="一、CLIP-模型"><a href="#一、CLIP-模型" class="headerlink" title="一、CLIP 模型"></a>一、CLIP 模型</h2><p>CLIP模型将原有的图像标签替换为图像的文本描述信息，来监督视觉任务的训练，在下游任务中得到了较好的zero-shot的结果。</p><p>该模型将图像分类问题转换为图文匹配问题，在下游任务（以图像分类为例）的实现中，我们需要先基于图像标签集合，构造 text prompt 并将其通过 clip 的 text encoder 获得文本编码向量，之后，将图片通过 image encoder 获得图像编码向量。</p><p>对于每张图片，我们需要计算它与所有 text prompt 之间的距离（通过计算图像编码向量与文本编码向量之间的余弦相似度或点积得到），选择距离最近的 text prompt 所对应的标签作为该张图片的分类标签。这样的转换使得模型在 zero-shot 场景下可以获得与监督学习比肩的效果，除此之外，clip 模型可广泛应用到图像检索，视频理解，图像生成等其他领域。接下来我们对 CLIP 模型进行简单的介绍。</p><h3 id="1-CLIP-原理"><a href="#1-CLIP-原理" class="headerlink" title="1. CLIP 原理"></a>1. CLIP 原理</h3><p>CLIP将图像的分类任务转化为了图文匹配的任务，将图像信息和语义信息映射到同一多模态语义空间下，再使用对比学习的方法进行训练。</p><p>在预训练阶段，模型的数据来源为从互联网上搜集的4亿个（图像，文本）对。假设batch中有N个（图像，文本）对，那么我们便可以得到N个文本向量，记为：t1,t2,…,tn,以及N个图像向量，记为img1,img2,…,imgn。我们需要做的就是让t(i)与img(i)之间的语义距离尽可能接近，而与其他的img之间的距离尽可能拉远。如果我们将其看为一个矩阵，其中横轴方向为文本，纵轴方向为图像，且文本、图像的标号均按照次序排列，那么我们就可以得到一个N * N的方阵，方阵的每个元素(i,j)的位置为t(i)与img(j)之间的语义相似度，相似度可通过余弦或点积的方式获得。我们的优化目标就是让对角线上的元素的值尽可能的大，而其余部分的值尽可能地小。</p><p>在zero shot阶段，我们可以根据上下文语义建立prompt模板，并将标签集合映射到该模板中，得到prompt text模板。比方说，现在我们需要做对(大象，冰箱，蚂蚁)的三分类任务，prompt模板为 “这是一张有关{类别}的照片”，将分类标签映射到prompt模板后可以得到集合：{“这是一张有关大象的照片”“这是一张有关冰箱的照片”“这是一张有关蚂蚁的照片”}。对集合中的文本，通过clip的text encoder之后，便可以得到三个类别对应的文本特征向量，之后，对于每一张需要分类的图片，我们只需要比较该图片的特征向量与三个类别对应的文本特征向量之间的语义距离，并选择最近的那一条文本所对应的标签作为图像的分类结果。 具体细节可参考论文《Learning transferable visual models from natural language supervision》。</p><h3 id="2-其他多模态模型"><a href="#2-其他多模态模型" class="headerlink" title="2. 其他多模态模型"></a>2. 其他多模态模型</h3><p>在图文生成领域，广为人知的模型有VAE，GAN等。最近大火的diffusion model则使用了一种非常有趣的思想来做生成任务。</p><h2 id="二、多模态与跨模态的应用：新的交互方式"><a href="#二、多模态与跨模态的应用：新的交互方式" class="headerlink" title="二、多模态与跨模态的应用：新的交互方式"></a>二、多模态与跨模态的应用：新的交互方式</h2><p>多模态应用允许我们通过利用每种方式的优势来结合不同的模态。例如，我们可以在对话中同时使用口语和书面语言，以确保我们相互理解。我们还可以使用图片或视频等作为视觉辅助工具，来帮助解释仅用文字难以描述的事物。</p><p>跨模态应用针对的是来自不同模态（如视觉和听觉）的输入和输出。它通过使用一种感官的信息来增强另一种感官，使用户体验比传统应用更上一层楼。比如说，我们可以通过触摸的方式来帮助我们理解在触觉地图或盲文文本中看到的内容。我们还可以使用声音来帮助我们定位环境中的事物，一般通过声纳或雷达来完成。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://medium.com/jina-ai/multimodal-and-crossmodal-applications-the-new-way-to-interact-d73d3e932990">https://medium.com/jina-ai/multimodal-and-crossmodal-applications-the-new-way-to-interact-d73d3e932990</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;第三篇：跨模态模型介绍&lt;/p&gt;
&lt;!--more---&gt;
&lt;h2 id=&quot;一、CLIP-模型&quot;&gt;&lt;a href=&quot;#一、CLIP-模型&quot; class=&quot;headerlink&quot; title=&quot;一、CLIP 模型&quot;&gt;&lt;/a&gt;一、CLIP 模型&lt;/h2&gt;&lt;p&gt;CLIP模型将原有的</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="VCED" scheme="https://superlova.github.io/tags/VCED/"/>
    
    <category term="multimodal" scheme="https://superlova.github.io/tags/multimodal/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】李宏毅ML-Note8-自监督学习、BERT</title>
    <link href="https://superlova.github.io/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/"/>
    <id>https://superlova.github.io/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/</id>
    <published>2022-11-17T08:09:44.000Z</published>
    <updated>2022-11-18T12:48:10.804Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：自监督学习、BERT、GPT。</p><!--more---><h2 id="二、自监督学习"><a href="#二、自监督学习" class="headerlink" title="二、自监督学习"></a>二、自监督学习</h2><p>自监督学习由 Yann LeCun 在 2019 年首次提出，模型会从无标签的数据中采取某些方法构建标签，然后进行学习。由于自监督学习不需要人为对数据进行标签，因此自监督学习可以看做无监督学习的一种。</p><p>自监督学习常用于训练大规模的语言模型，比如 BERT、GPT 等。由于这些模型的参数量非常大，因此训练模型所需的数据量也非常大，甚至大到人工标注无法完成的程度。</p><p>BERT 的训练数据包含整个维基百科的无标签号文本的大语料库中（足足有25亿字！） 和图书语料库（有8亿字）；使用自监督学习能更高效、更直接地从数据中学到知识，避免人工标注带来的成本。</p><p>下面介绍几个 BERT 使用到的自监督学习方法。</p><h3 id="1-Masking-Input"><a href="#1-Masking-Input" class="headerlink" title="1. Masking Input"></a>1. Masking Input</h3><p>BERT 的输入是一个句子，句子由若干 token 组成。Masking Input 训练任务就是随机 Mask 句子中的词，来构建 {填空题, 答案} 这样的数据。</p><p>Mask 方法有很多，可以直接替换为 [MSK] 这种全新的特殊符号，或者替换为任意一个其他字。</p><p>到底用哪一种也是随机决定的。在原文中，80%的数据是由 [MSK] 特殊符号替换方法来生成的，10%的数据是以其他词进行替换生成的，还有 10%的数据是保持原样生成的。</p><p>这个过程 BERT 会预测 [MSK] 对应的字是哪一个，把盖住的部分对应的输出向量做线性变换，做 softmax 产出一个分布。由此，变成了一个监督学习的分类任务。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/masking_input.png" alt></p><p>训练方法：与 ground truth 进行最小化交叉熵。</p><h3 id="2-Next-Sentence-Prediction"><a href="#2-Next-Sentence-Prediction" class="headerlink" title="2. Next Sentence Prediction"></a>2. Next Sentence Prediction</h3><p>选择句子A和B作为预训练样本：B有50%的可能是A的下一句，也有50%的可能是来自语料库的随机句子。</p><pre><code>[cls] w1 w2 [sep] w3 w4 w5</code></pre><p>只取 cls 对应的输出，然后预测 yes or no，判断这两个句子是否相接。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/next_sentence_pred.png" alt></p><p>这一招不太有用，RoBERTa 舍弃了该方法。可能是这个任务太简单了，学不到太有用的东西。</p><h3 id="3-Sentence-Order-Prediction"><a href="#3-Sentence-Order-Prediction" class="headerlink" title="3. Sentence Order Prediction"></a>3. Sentence Order Prediction</h3><p>ALBERT 的训练方法，判断两句的顺序或者逆序。</p><h3 id="4-如何评估学习的效果？"><a href="#4-如何评估学习的效果？" class="headerlink" title="4. 如何评估学习的效果？"></a>4. 如何评估学习的效果？</h3><p>评估方法：GLUE（General Language Understanding Evaluation）</p><p>有九大任务，评估预训练模型的好坏。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/glue_score_9_missions.png" alt></p><h2 id="三、如何使用-BERT"><a href="#三、如何使用-BERT" class="headerlink" title="三、如何使用 BERT"></a>三、如何使用 BERT</h2><p>预训练模型完成训练后，需要将模型学习到的知识迁移到其他领域。这个迁移的过程叫做微调（fine-tuning）。微调需要一定的标注数据，进行监督学习。根据下游任务不同，所需要的标注数据不同。</p><h3 id="1-文本分类："><a href="#1-文本分类：" class="headerlink" title="1. 文本分类："></a>1. 文本分类：</h3><p>文本分类以一串文本为输入，输出为一个概率分布，表明该串文本被分成各个类别的概率。</p><p>INPUT:</p><pre><code>[cls] w1 w2 w3</code></pre><p>OUTPUT:<br>看 [cls] 的输出分布，然后与真实标注进行对比，最小交叉熵。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/bert_classification.png" alt></p><p>训练时，linear 线性变换矩阵的参数是随机初始化的，预训练的参数是直接下载的。</p><p>fine tuning 时，所有部分的参数都会梯度下降进行优化。</p><h3 id="2-序列标注"><a href="#2-序列标注" class="headerlink" title="2. 序列标注"></a>2. 序列标注</h3><p>输入一个序列，输出另一个序列。</p><pre><code>[cls] w1 w2 w3 w4</code></pre><p>BERT 以一个 [cls] 开头的序列为输入，对于每个 token，都会产出向量，并转化为预测该 token 的词性。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/pos_tagging.png" alt></p><h3 id="3-语义推理"><a href="#3-语义推理" class="headerlink" title="3. 语义推理"></a>3. 语义推理</h3><p>Natural Language Inference ，输入两个句子，输出两个句子的逻辑关系，多分类问题。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/natural_language_inference.png" alt></p><p>可以扩展到评论立场判断，输入原文+评论，输出读者对此的态度，支持反对还是中立。</p><h3 id="4-问答系统"><a href="#4-问答系统" class="headerlink" title="4. 问答系统"></a>4. 问答系统</h3><p>给定一篇文章+一个问题，机器产出答案在原文中的位置，分别是起始节点和终止节点。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/extraction_based_question_answering.png" alt></p><p>我们需要将输入整理成 query + document，输出两个位置，start_pos 和 end_pos ，在原文中 [start_pos, end_pos] 内的位置就是模型认为的答案。如果 start_pos 和 end_pos 都为 0，则模型认为没有答案。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/extraction_based_qa_input_output.png" alt></p><p>在这里唯一需要从头开始训练的模块是如上图所示的<font color="Orange">橙色</font>和<font color="Blue">蓝色</font>向量。</p><p>首先把橙色向量跟每个 document 的 N 个词向量对应的输出向量做点积，算出 N 个数值，然后过一下 softmax 得到概率分布。取最高的值为 start_pos。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/extraction_based_qa_input_outout_2.png" alt></p><p>然后把蓝色向量跟每个 document 的 N 个词向量错对应的输出向量做点积，算出 N 个数值，同样取最大后决定 end_pos。</p><p>这个过程类比 self-attention，就好像我们要训练 query 向量对应的 Q 矩阵。</p><p>实际上 BERT 的输入序列一般为 512，而一整篇文章往往巨长无比，我们一般每次只拿一小段一小段 document 进行训练，只有一个有正例。</p><h3 id="———"><a href="#———" class="headerlink" title="———"></a>———</h3><p>Bert 损失函数组成</p><p>第一部分是来自 Mask-LM 的单词级别分类任务；<br>另一部分是句子级别的分类任务；</p><p>所用的损失函数叫做负对数似然函数</p><p>BERT 则是通过「transformer 编码器」模块构建的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：自监督学习、BERT、GPT。&lt;/p&gt;
&lt;!--more---&gt;
&lt;h2 id=&quot;二、自监督学习&quot;&gt;&lt;a href=&quot;#二、自监督学习&quot; class=&quot;headerlink&quot; title=&quot;二、自监督学习&quot;&gt;&lt;</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
    <category term="Self-Supervised Learning" scheme="https://superlova.github.io/tags/Self-Supervised-Learning/"/>
    
    <category term="BERT" scheme="https://superlova.github.io/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】通过文字检索视频片段：VCED之Jina</title>
    <link href="https://superlova.github.io/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED%E4%B9%8BJina/"/>
    <id>https://superlova.github.io/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED%E4%B9%8BJina/</id>
    <published>2022-11-16T16:33:03.000Z</published>
    <updated>2022-11-16T18:04:05.359Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第二篇：核心组件-Jina-介绍"><a href="#第二篇：核心组件-Jina-介绍" class="headerlink" title="第二篇：核心组件 Jina 介绍"></a>第二篇：核心组件 Jina 介绍</h1><!--more---><p>Jina 是一个能够将非结构化数据例如图像，文档视频等，转换为向量数据的工具。利用该工具，可以快速实现多模态的检索任务。另外，Jina 也是一家新的公司，目前正在优化中。他们的 <a href="https://github.com/jina-ai/jina">GitHub Repo</a>。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED%E4%B9%8BJina/jina%20introduction.png" alt></p><p>其实 Jina 公司提供了包括向量化、服务化到实际部署的全部工具，可以支持包括 PDF 检索、视频检索在内的很多检索操作。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED%E4%B9%8BJina/jina_全家桶.png" alt></p><h2 id="1-Jina-安装"><a href="#1-Jina-安装" class="headerlink" title="1. Jina 安装"></a>1. Jina 安装</h2><p>在 windows 系统上的安装教程：</p><p><a href="https://blog.csdn.net/Jina_AI/article/details/122820646">https://blog.csdn.net/Jina_AI/article/details/122820646</a></p><p>本文将记录在 mac 上安装 jina 的过程。</p><p>首先确保自己的 Python 版本在 3.7 及以上，然后通过下列方式安装 jina：</p><pre><code class="lang-sh"># via pypipip install jina# via condaconda install jina -c conda-forge# via dockerdocker pull jinaai/jina:latest</code></pre><p>使用 pip 安装 jina 时，提示：</p><pre><code>lz4/_version.c:32:10: fatal error: &#39;Python.h&#39; file not foundERROR: Failed building wheel for lz4Successfully built jina docarray grpcio jcloud python-multipartFailed to build lz4ERROR: Could not build wheels for lz4, which is required to install pyproject.toml-based projects</code></pre><p>原因：系统中没有 Python.h，是因为没有安装 python 的开发版，即 Python-devel 这个包。</p><h2 id="2-Jina-的基本组件"><a href="#2-Jina-的基本组件" class="headerlink" title="2. Jina 的基本组件"></a>2. Jina 的基本组件</h2><p>详细的文档可以参考<a href="https://docs.jina.ai/fundamentals/architecture-overview/">这里</a>。</p><p>Document、Executor 和 Flow 是 Jina 的三个基本概念，分别代表数据类型，算法单元，和工作流。</p><p>简单来说，Document 是一种数据类型的定义方式，Flow 则是负责 Document 在整个架构间的传输，最后 Executor 则实现具体的算法功能。</p><p>比如下面这个简单的 Demo，客户端发起一个请求给服务器端，服务器端处存在定义好的 Flow，Flow 则会把不同的 Executor 串联起来。这里有两个已经定义好的 Executor，分别执行将字符串末尾添加特定字符的操作。</p><p>服务器端代码：</p><pre><code class="lang-py">from jina import DocumentArray, Executor, Flow, requestsclass FooExec(Executor):    @requests    async def add_text(self, docs: DocumentArray, **kwargs):        for d in docs:            d.text += &#39;hello, world!&#39;class BarExec(Executor):    @requests    async def add_text(self, docs: DocumentArray, **kwargs):        for d in docs:            d.text += &#39;goodbye!&#39;f = Flow(port=12345).add(uses=FooExec, replicas=3).add(uses=BarExec, replicas=2)with f:    f.block()</code></pre><p>客户端代码：</p><pre><code class="lang-py">from jina import Client, DocumentArrayc = Client(port=12345)r = c.post(&#39;/&#39;, DocumentArray.empty(2))print(r.texts)</code></pre><p>运行逻辑动图为：</p><p><img src="https://docs.jina.ai/_images/arch-overview.svg" alt="jina_overview"></p><p>返回结果为：</p><pre><code>[&#39;hello, world!goodbye!&#39;, &#39;hello, world!goodbye!&#39;]</code></pre><h2 id="3-启动-jina-示例"><a href="#3-启动-jina-示例" class="headerlink" title="3. 启动 jina 示例"></a>3. 启动 jina 示例</h2><p>安装完毕后，新建 toy.yml 作为 gRPC 服务的配置文件：</p><pre><code class="lang-yaml"># toy.ymljtype: Flowwith:  port: 51000  protocol: grpcexecutors:- uses: FooExecutor  name: foo  py_modules:    - test.py- uses: BarExecutor  name: bar  py_modules:    - test.py</code></pre><p>然后定义 test.py ，定义若干 Executor 的处理逻辑：</p><p>test.py</p><pre><code class="lang-py"># 创建 test.py 文件与 YAML 文件在同一目录下# 导入 document、executor 和 flow 以及 requests 装饰器from jina import DocumentArray, Executor, requests, Document# 编写 FooExecutor 与 BarExecutor 类，类中定义了函数 foo 和 bar# 该函数从网络请求接收 DocumentArray (先暂时不需要理解它是什么)，并在其内容后面附加 &quot;foo was here&quot; 与 &quot;bar was here&quot;class FooExecutor(Executor):    @requests # 用于指定路由，类似网页访问 /index 和 /login 会被路由到不同的方法上是用样的概念，关于 request 下面会再进行详细介绍    def foo(self, docs: DocumentArray, **kwargs):        docs.append(Document(text=&#39;foo was here&#39;))class BarExecutor(Executor):    @requests    def bar(self, docs: DocumentArray, **kwargs):        docs.append(Document(text=&#39;bar was here&#39;))</code></pre><p>然后使用该指令启动服务：</p><pre><code class="lang-sh">jina flow --uses toy.yml</code></pre><p>启动服务之后放着别动，我们新建一个 shell 窗口；然后新建 client.py，用于储存客户端请求消息的逻辑：</p><p>client.py</p><pre><code class="lang-py"># 从 Jina 中导入连接的客户端与 Documentfrom jina import Client, Documentc = Client(host=&#39;grpc://0.0.0.0:51000&#39;)  # 如果运行提示失败，可尝试使用localhostresult = c.post(&#39;/&#39;, Document()) # 将一个空的 Document 传到服务端执行print(result.texts)</code></pre><p>随后启动客户端：</p><pre><code class="lang-sh">python client.py</code></pre><p>最终会打印出一个 “[‘’, ‘foo was here’, ‘bar was here’]” 字符串。</p><h2 id="4-DocArray-简介"><a href="#4-DocArray-简介" class="headerlink" title="4. DocArray 简介"></a>4. DocArray 简介</h2><p>DocArray 也是一个工具包，它被整合在 Jina 中，作为 Jina 的重要组成部分，方便实现跨模态应用。关于 DocArray 的其他资料可以参考<a href="https://docarray.jina.ai/">这里</a>。</p><p>DocArray 类比 Pandas，其基本数据类型为 Document，并整合了多种操作 Document 的方法。DocArray 对数据采用分层结构存储。</p><blockquote><p>可以利用 DocArray 实现在第一层存入该画面的视频，第二层存入该视频的不同镜头，第三层可以是视频的某一帧，也可以存储台台词等等，这使得你可以通过台词去搜索到视频，也可以通过视频定位某几帧画面，这样搜索的颗粒度，结构的多样性和结果的丰富度，都比传统文本检索好很多。</p></blockquote><h2 id="5-通过-DocArray-导入任意模态的数据"><a href="#5-通过-DocArray-导入任意模态的数据" class="headerlink" title="5. 通过 DocArray 导入任意模态的数据"></a>5. 通过 DocArray 导入任意模态的数据</h2><p>项目代码参考<a href="https://github.com/datawhalechina/vced/tree/main/code/jina_demo">这里</a>。有将图片、文本、视频分别导入 Jina 的实例。</p><h3 id="5-1-文本数据导入"><a href="#5-1-文本数据导入" class="headerlink" title="5.1 文本数据导入"></a>5.1 文本数据导入</h3><p><strong>创建文本</strong></p><pre><code class="lang-py">from jina import Document  # 导包# 创建简单的文本数据d = Document(text=&#39;hello, world.&#39;)print(d.text)  # 通过text获取文本数据# 如果文本数据很大，或者自URI，可以先定义URI，然后将文本加载到文档中d = Document(uri=&#39;https://www.w3.org/History/19921103-hypertext/hypertext/README.html&#39;)d.load_uri_to_text()print(d.text)# 支持多语言d = Document(text=&#39;👋    नमस्ते दुनिया!    你好世界！こんにちは世界！    Привет мир!&#39;)print(d.text)</code></pre><p><strong>切割文本</strong></p><pre><code class="lang-py">from jina import Document  # 导包d = Document(text=&#39;👋    नमस्ते दुनिया!    你好世界！こんにちは世界！    Привет мир!&#39;)d.chunks.extend([Document(text=c) for c in d.text.split(&#39;!&#39;)])  # 按&#39;!&#39;分割d.summary()</code></pre><p><strong>文本匹配</strong></p><pre><code class="lang-py">from jina import Document, DocumentArrayd = Document(    uri=&#39;https://www.gutenberg.org/files/1342/1342-0.txt&#39;).load_uri_to_text()  # 链接是傲慢与偏见的电子书，此处将电子书内容加载到 Document 中da = DocumentArray(Document(text=s.strip()) for s in d.text.split(&#39;\n&#39;) if s.strip())  # 按照换行进行分割字符串da.apply(lambda d: d.embed_feature_hashing())q = (    Document(text=&#39;she entered the room&#39;)  # 要匹配的文本    .embed_feature_hashing()  # 通过 hash 方法进行特征编码    .match(da, limit=5, exclude_self=True, metric=&#39;jaccard&#39;, use_scipy=True)  # 找到五个与输入的文本最相似的句子)print(q.matches[:, (&#39;text&#39;, &#39;scores__jaccard&#39;)])  # 输出对应的文本与 jaccard 相似性分数# 输出结果：# [[&#39;staircase, than she entered the breakfast-room, and congratulated&#39;, &#39;of the room.&#39;,#   &#39;She entered the room with an air more than usually ungracious,&#39;,#   &#39;entered the breakfast-room, where Mrs. Bennet was alone, than she&#39;, &#39;those in the room.&#39;],#  [&#123;&#39;value&#39;: 0.6&#125;, &#123;&#39;value&#39;: 0.6666666666666666&#125;, &#123;&#39;value&#39;: 0.6666666666666666&#125;, &#123;&#39;value&#39;: 0.6666666666666666&#125;,#   &#123;&#39;value&#39;: 0.7142857142857143&#125;]]</code></pre><h3 id="5-2-从影片导入"><a href="#5-2-从影片导入" class="headerlink" title="5.2 从影片导入"></a>5.2 从影片导入</h3><pre><code class="lang-py"># 视频需要依赖 av 包# pip install avfrom jina import Documentd = Document(uri=&#39;cat.mp4&#39;)d.load_uri_to_video_tensor()# 相较于图像，视频是一个 4 维数组，第一维表示视频帧 id 或是视频的时间，剩下的三维则和图像一致。print(d.tensor.shape)  # (31, 1080, 1920, 3)# 使用 append 方法将 Document 放入 chunk 中for b in d.tensor:    d.chunks.append(Document(tensor=b))d.chunks.plot_image_sprites(&#39;mov.png&#39;)</code></pre><p>还有许多其他的操作方法，有待后续进一步发掘和使用。总体感觉还是很不错的。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://u84gxokzmi.feishu.cn/docx/doxcn30HXXLbqFrsyR6bL5A6o1g">https://u84gxokzmi.feishu.cn/docx/doxcn30HXXLbqFrsyR6bL5A6o1g</a></p><p><a href="https://github.com/datawhalechina/vced">https://github.com/datawhalechina/vced</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;第二篇：核心组件-Jina-介绍&quot;&gt;&lt;a href=&quot;#第二篇：核心组件-Jina-介绍&quot; class=&quot;headerlink&quot; title=&quot;第二篇：核心组件 Jina 介绍&quot;&gt;&lt;/a&gt;第二篇：核心组件 Jina 介绍&lt;/h1&gt;&lt;!--more---&gt;
&lt;p&gt;J</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="VCED" scheme="https://superlova.github.io/tags/VCED/"/>
    
    <category term="multimodal" scheme="https://superlova.github.io/tags/multimodal/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】通过文字检索视频片段：VCED</title>
    <link href="https://superlova.github.io/2022/11/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED/"/>
    <id>https://superlova.github.io/2022/11/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED/</id>
    <published>2022-11-14T14:12:07.000Z</published>
    <updated>2022-11-16T17:42:55.721Z</updated>
    
    <content type="html"><![CDATA[<p>第一篇：环境部署</p><!--more---><p>VCED: Video Clip Extraction by description, 可以通过你的文字描述来自动识别视频中相符合的片段进行视频剪辑。基于跨模态搜索与向量检索技术搭建。</p><p>本项目参考自 <a href="https://github.com/datawhalechina/vced">Datawhale 的 VCED 学习教程</a>。</p><p>环境为 Mac Monterey, Apple M1 Pro 芯片，内存 16GB。</p><p>首先需要安装 docker，在 mac 上安装 docker 只需去官网下载客户端：</p><p><a href="https://docs.docker.com/desktop/install/mac-install/">https://docs.docker.com/desktop/install/mac-install/</a></p><p><img src="/2022/11/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED/mac安装docker.png" alt></p><p>注意，在 mac 上安装 docker 需要提前安装 rosetta。</p><p>安装 docker 完成后，为了方便下载镜像，我们先修改下源。</p><p><img src="/2022/11/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED/mac修改源.png" alt></p><p>在如图所示的文本框内按照 json 格式添加如下文本：</p><pre><code class="lang-json">&quot;registry-mirrors&quot;: [    &quot;https://hub-mirror.c.163.com&quot;,    &quot;https://mirror.baidubce.com&quot;  ]</code></pre><p>之后使用现有的镜像文件即可部署：</p><pre><code class="lang-sh">docker pull nil01/vceddocker run -itd -p 8501:8501 -p 45679:45679 --name vced_arm nil01/vced</code></pre><p>最大的文件有 2GB，需要等待一会儿下载。部署完成后，访问 <code>localhost:8501</code> 即可。</p><p><img src="/2022/11/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED/浏览器交互页面.png" alt></p><p>启动 docker 进程之后，输入指令 <code>docker ps</code> 查看运行中的 container，输入 <code>docker ps &lt;CONTAINER ID&gt;</code> 即可结束该 container。</p><p><img src="/2022/11/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED/docker操作.png" alt></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://github.com/datawhalechina/vced/blob/main/README.md">https://github.com/datawhalechina/vced/blob/main/README.md</a></p><p><a href="https://docs.jina.ai/get-started/install/windows/">https://docs.jina.ai/get-started/install/windows/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;第一篇：环境部署&lt;/p&gt;
&lt;!--more---&gt;
&lt;p&gt;VCED: Video Clip Extraction by description, 可以通过你的文字描述来自动识别视频中相符合的片段进行视频剪辑。基于跨模态搜索与向量检索技术搭建。&lt;/p&gt;
&lt;p&gt;本项目参考自 &lt;</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="VCED" scheme="https://superlova.github.io/tags/VCED/"/>
    
    <category term="multimodal" scheme="https://superlova.github.io/tags/multimodal/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】李宏毅ML-Note7-Transformer</title>
    <link href="https://superlova.github.io/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/"/>
    <id>https://superlova.github.io/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/</id>
    <published>2022-10-29T12:05:45.000Z</published>
    <updated>2022-10-29T19:59:51.366Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：Transformer。<br><!--more---></p><h2 id="一、Seq2Seq"><a href="#一、Seq2Seq" class="headerlink" title="一、Seq2Seq"></a>一、Seq2Seq</h2><p>Transformer是一类Seq2seq结构的模型，输入长度为N的序列，输出长度为M的序列。其中M的长度是由模型决定的。诸如文本生成、语音合成、机器翻译等任务都需要应用seq2seq模型。</p><p>很多任务可以被转化为seq2seq任务，但不一定是最优解。</p><p>seq2seq模型由一个编码器（Encodeer）和一个解码器（Decoder）组成，基础架构如下：</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-1.png" alt></p><p>seq2seq模型是一种架构，只规定了输入输出，并没有规定必须使用哪种具体的模型。</p><h2 id="二、Encoder"><a href="#二、Encoder" class="headerlink" title="二、Encoder"></a>二、Encoder</h2><p>如图所示是Transformer的Encoder：</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-2.png" alt></p><p>Transformer 的 Encoder 是由若干的 Block 构成的，每个 Block 包含一个 multihead self-attention 结构和一个全连接网络等，这些 Block 串联起来组成 Encoder。</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-3.png" alt></p><p>需要关注两个细节：</p><ol><li>self-attention 层的向量输出后，需要经过一步残差运算，和原来的输入相加；</li><li>残差运算后的向量需要 LayerNormalization 操作；</li><li>normalized 之后的向量需要输入内部的全连接网络，输出的向量同样要与这一步骤的输入进行残差；</li><li>在输出前同样需要经过 normalization。</li></ol><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-4.png" alt></p><p>实际上，在具体实现时，上面的操作顺序是可以改变的，有若干文章已经在讨论 LayerNormalization 的执行时机对最终性能的影响。</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-layer-norm.png" alt></p><h2 id="三、Decoder"><a href="#三、Decoder" class="headerlink" title="三、Decoder"></a>三、Decoder</h2><p>Decoder负责以Encoder的输出为知识，生成一系列输出向量。</p><h3 id="3-1-Decoder-的执行步骤"><a href="#3-1-Decoder-的执行步骤" class="headerlink" title="3.1 Decoder 的执行步骤"></a>3.1 Decoder 的执行步骤</h3><p>Transformer 的 Decoder 是一个自回归（AutoRegressive）模型，即使用自身以前的产出来预测将来的产出。</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-decoder-example-1.png" alt></p><p>在这里我们以语音转文字任务为例，以一串音频为输入（在图示的例子中，是“机器学习”四个字），Encoder 首先学习适当的语义向量，并将其输入 Decoder 中，Decoder 执行如下操作：</p><ol><li>Decoder 先以 [start] 符号作为输入，通过 Encoder 提供的知识，来预测该符号对应的输出是什么；</li><li>Decoder 产出的预测向量是一个长度为单词表大小的向量，向量的每个分量代表输出为该词时的概率值；</li><li>紧接着 Decoder 会以该输出为输入，来预测输出；</li><li>由此循环往复，直至 Decoder 认为模型的输出为特殊符号 [end]为止。</li></ol><h3 id="3-2-Decoder-的具体架构"><a href="#3-2-Decoder-的具体架构" class="headerlink" title="3.2 Decoder 的具体架构"></a>3.2 Decoder 的具体架构</h3><p>Transformer 的 Decoder 与 Encoder 架构十分类似：</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-decoder-structure.png" alt></p><ol><li>接纳模型输入的 attention 层是 masked self-attention 层，它的特点是，在考虑某个输入向量$x_n$的输出时，self-attention 会综合所有的上下文信息，而 masked self-attention 只会考虑已经出现过的 $x_1~x_n$ 范围内的所有向量，而不会考虑它的下文；<br> 这样做是自然的，因为 Decoder 是以过去的输出预测将来的输出的自回归模型，它无法参考尚未生成的下文内容；</li><li>经过 masked self-attention 的编码后的输出向量，会被当做下一层 self-attention 的 query 查询向量，然后与 encoder 得到的输出向量计算相关性；也就是说，query 来自 Decoder 的输入，而 key 和 value 都来自 Encoder 层。这一步骤叫做 Cross Attention。<br> 在 Decoder 中加入 Cross Attention ，是希望模型能够正确地利用 Encoder 提供的知识，并根据现有的输入，自动生成合适的输出表示。要完成这一步，Encoder 提供的先验知识与 Decoder 提供的查询词都是必不可少的。</li></ol><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-decoder-cross-attention.png" alt></p><p>Transformer 的 Encoder 与 Decoder 的详细架构如图：</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-encoder-decoder.png" alt></p><p>需要注意，原文中 Transformer 的多个 Decoder 结构使用的 Encoder 信息都来自于最后一个 Encoder，这样做是自然地，毕竟我们认为最后一层学习到的知识最抽象，泛化能力最强；</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-encoder-decoder-2.png" alt></p><p>事实上并不总是这样，也有一些文章在研究令不同时期的 Encoder 产出的向量送给不同时期的 Decoder，并取得了一定的效果：</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-cross-attention-various.png" alt></p><h3 id="3-3-非自回归-Decoder"><a href="#3-3-非自回归-Decoder" class="headerlink" title="3.3 非自回归 Decoder"></a>3.3 非自回归 Decoder</h3><p>存在一些 None AutoRegressive Decoder，这些 Decoder 不是以自身之前的输出预测之后的输出，而是一次性给出整个输出序列。</p><p>自回归问题是通过在输出集合中添加特殊符号 [end] 来解决序列不停止的问题的，但是 非自回归 Decoder 一次性输出整个序列，它只能预先设定一个序列长度值，并依此长度进行输出。这种类型的 Decoder 当然也会输出 [end] ，但 Decoder 并不会根据此来终止后续字符的生成。最终的结果会截断 [end] 之后的字符。</p><p>部分非自回归 Decoder 可以通过另外训练一个分类器，通过学习输入序列长度与输出序列长度之间的关系来动态产生序列的长度。</p><p>非自回归 Decoder 的优点在于可并行化，快速；而且输出序列的长度是人为可控的；</p><p>缺点在于准确率等相对不如自回归 Decoder。</p><h2 id="四、Transformer-的训练过程"><a href="#四、Transformer-的训练过程" class="headerlink" title="四、Transformer 的训练过程"></a>四、Transformer 的训练过程</h2><p>其实就是训练一个分类问题。假设问题为一段语音，问题的答案为“我爱中国”四个字，则 Encoder 会学习该语音的向量表示并输出给 Decoder，Decoder 则会分别输入 <code>BOS、我、爱、中、国</code>，作为五个训练样本；他们的正确答案为<code>我、爱、中、国、EOS</code>，每个字对应一个 one-hot 向量。最后通过优化交叉熵损失函数，按照正常的梯度下降算法进行优化即可。</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-training.png" alt></p><p>这种以正确答案为输入的训练方法称作 Teacher Forcing。</p><h2 id="五、Transformer-训练技巧"><a href="#五、Transformer-训练技巧" class="headerlink" title="五、Transformer 训练技巧"></a>五、Transformer 训练技巧</h2><h3 id="5-1-Copy-Mechanism"><a href="#5-1-Copy-Mechanism" class="headerlink" title="5.1 Copy Mechanism"></a>5.1 Copy Mechanism</h3><p>对于一些问答等问题，当输入中包含一些实体时，模型可以直接把该实体复制到输出序列中进行原样输出，称之为“复制机制”，这需要保证输入信息到输出信息之间的畅通。</p><p>实现 Copy Mechanism 的网络结构有指针网络等。</p><h3 id="5-2-Guided-Attention"><a href="#5-2-Guided-Attention" class="headerlink" title="5.2 Guided Attention"></a>5.2 Guided Attention</h3><p>通过人为限制注意力的学习方向、学习重点等，提升模型的学习能力。比如规定语音合成任务的注意力只能从左往右学习等。</p><h3 id="5-3-Beam-Search"><a href="#5-3-Beam-Search" class="headerlink" title="5.3 Beam Search"></a>5.3 Beam Search</h3><p>Beam Search 是一种帮助找到最优解的方法，其思想类似于动态规划，是为了解决 Decoder 在贪心地选择最优输出时，错过了次优输出，导致后续输出全错的问题。</p><p>Beam Search 并不总一定能找到最优解，而且一味追寻最优解，在一些需要发挥创意的任务中，反而不好，比如新闻生成任务等。</p><h3 id="5-4-优化评估指标"><a href="#5-4-优化评估指标" class="headerlink" title="5.4 优化评估指标"></a>5.4 优化评估指标</h3><p>一般而言 Decoder 的损失函数都是交叉熵，这是因为交叉熵方便求导好训练；而评估模型时使用的往往是 BLEU。但如果我们使用 BLEU 这种相对科学一点的评估函数来做损失函数的话，可能需要借助强化学习（Reinforcement Learning, RL）的思想，强行把 BLEU 移植到模型中。</p><h3 id="5-5-scheduled-sampling"><a href="#5-5-scheduled-sampling" class="headerlink" title="5.5 scheduled sampling"></a>5.5 scheduled sampling</h3><p>Decoder 在训练过程中，训练样本永远都是正确答案，缺乏一些负样本，告诉 Decoder 你不应该将预测值预测为该值。为了做到这一点，我们需要在训练时随机在输入序列中添加噪音，但这一举措可能会损失训练的并行化能力。</p><h2 id="六、总结和问答"><a href="#六、总结和问答" class="headerlink" title="六、总结和问答"></a>六、总结和问答</h2><h3 id="为什么要加入残差模块？"><a href="#为什么要加入残差模块？" class="headerlink" title="为什么要加入残差模块？"></a>为什么要加入残差模块？</h3><h3 id="为什么要加入-LayerNormalization-模块？"><a href="#为什么要加入-LayerNormalization-模块？" class="headerlink" title="为什么要加入 LayerNormalization 模块？"></a>为什么要加入 LayerNormalization 模块？</h3><h3 id="BatchNormalization-和-LayerNormalization-的区别？"><a href="#BatchNormalization-和-LayerNormalization-的区别？" class="headerlink" title="BatchNormalization 和 LayerNormalization 的区别？"></a>BatchNormalization 和 LayerNormalization 的区别？</h3><h3 id="Transformer-使用到的几种-Mask"><a href="#Transformer-使用到的几种-Mask" class="headerlink" title="Transformer 使用到的几种 Mask"></a>Transformer 使用到的几种 Mask</h3><h3 id="前馈神经网络在-Transformer-中的作用"><a href="#前馈神经网络在-Transformer-中的作用" class="headerlink" title="前馈神经网络在 Transformer 中的作用"></a>前馈神经网络在 Transformer 中的作用</h3><h3 id="Gelu-的作用"><a href="#Gelu-的作用" class="headerlink" title="Gelu 的作用"></a>Gelu 的作用</h3><h2 id="七、transformer-的代码实现"><a href="#七、transformer-的代码实现" class="headerlink" title="七、transformer 的代码实现"></a>七、transformer 的代码实现</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：Transformer。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h2 id=&quot;一、Seq2Seq&quot;&gt;&lt;a href=&quot;#一、Seq2Seq&quot; class=&quot;headerlink&quot; title=&quot;一、Seq2</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
    <category term="Self-Attention" scheme="https://superlova.github.io/tags/Self-Attention/"/>
    
    <category term="Transformer" scheme="https://superlova.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】李宏毅ML-Note6-注意力机制</title>
    <link href="https://superlova.github.io/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <id>https://superlova.github.io/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</id>
    <published>2022-10-23T13:57:40.000Z</published>
    <updated>2022-10-29T19:40:57.261Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：自注意力机制。<br><!--more---></p><h2 id="一、输入从图像到序列"><a href="#一、输入从图像到序列" class="headerlink" title="一、输入从图像到序列"></a>一、输入从图像到序列</h2><p>图像分类任务的输入是固定的，比如都是28*28像素的黑白图片等等。但是一些输入和输出不是定长序列的任务，比如机器翻译、语音转文字等任务，传统模型在这些任务上的表现不好。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/不同输出对应不同任务.png" alt></p><p>比如词性标注任务，模型的输入是N个向量，输出是N个分类标签；比如文本分类任务，模型的输入是N个向量，输出只是一个分类标签；比如文本翻译任务，模型的输入是N个向量，输出可能是N’个向量，此时N’是未知的，需要机器自己进行判断和学习；这种任务叫做 sequences to sequences (seq2seq) 。</p><p>这些任务的输入特点是，都有多个向量作为一个输入，且向量的相对位置会影响输入的含义。</p><p>以词性标注任务举例，该任务的输入和输出向量个数都为N。我们首先把一句话切分为N个词，然后利用一些编码方式（比如 one-hot 或者 word2vec）将其变成N个定长向量。我们将这N个向量作为一个输入序列，输入到模型中，寄希望于模型能够把每个词的词性标注出来。所以这其实是一个分类任务。</p><p>但是当我们输入句子”I saw a saw.“时，前后两个saw的词性是不同的，如果模型只以词汇向量为输入，不考虑上下文的话，是无法得知这件事的。</p><p>为此，前人们做了以下改进，试图使用传统模型解决这个问题：</p><ol><li><p>通过 N-gram 模型，将两三个词打包成一个新词，”I saw a saw“经过3-gram模型的编码，会产生如下输入序列：</p><pre><code> I saw a saw -&gt; [i saw a], [saw a saw]</code></pre><p> 由此，模型能够看到的上下文就扩展到了三个单词。但是该种方法会快速扩大单词量，增加计算负担，且无法把距离较远的上下文也加入进来。</p></li><li><p>使用TextCNN</p><p> TextCNN 是把图像领域大货成功的CNN模型的经验移植到了NLP领域的成果，它使用一维卷积核。但究其本质还是受限于卷积核的大小。</p></li></ol><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/向量之间的相对位置.png" alt></p><p>有没有一种方法能够考虑输入向量的全部上下文呢？ self-attention 可以做到。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention与FC的区别.png" alt></p><p>self-attention 接受N个输入向量，输出N个输出向量。输出的向量不仅保留了原来向量的语义信息，还额外添加了该向量上下文的信息。</p><p>有了这个全新的向量，我们便不必使用诸如 n-gram 等特征工程手段了，self-attention 就能够产生非常好的稠密向量，该向量考虑的上下文范围是整篇文档。</p><p>self-attention 适合编码整篇文档的信息，产出向量后可以接一个全连接网络来进行具体的分类；也可以像之前的卷积网络一样，多层 self-attention + FC全连接进行堆叠。这实际上就是Transformer的基本思想。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/transformer的简化结构.png" alt></p><p>self-attention 是怎么做到的呢？</p><h2 id="二、self-attention-的实现原理"><a href="#二、self-attention-的实现原理" class="headerlink" title="二、self-attention 的实现原理"></a>二、self-attention 的实现原理</h2><p>self-attention 的输入是N个向量组成的输入序列，输出也是N个向量组成的序列。区别在于，输入的向量本身不包含任何与上下文有关的信息，但是与输入向量对应的输出向量会包含一定的上下文信息。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention的抽象结构.png" alt></p><p>如果要产生b1向量，其实是考虑了a1/a2/a3/a4全部四个向量后的结果。</p><p>具体地，首先我们需要考虑a1向量与其他向量a2/a3/a4之间的相关性，两个向量之间的相关性是一个标量，我们用$\alpha$来表示：</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理1_相关性.png" alt></p><p>相关性是如何计算得到的？有很多做法可以实现相关性的计算。需要注意的是这里的<strong>相关性</strong>并非矩阵向量空间中的相似性，而是一个<strong>需要机器从数据中学习</strong>的参数。下图是两种不同的相关性的计算方式。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理2_相关性的计算方法.png" alt></p><p>上图左边是一种常用的向量相关性计算法，首先把两个向量各自乘以一个参数矩阵，其中输入向量会得到对应的查询向量 query，上下文向量会得到对应的关键向量 key，然后我们把query和key进行点积，得到的就是两个向量的相关性。参数矩阵是可以通过数据进行学习的。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理3_相关性计算方法2.png" alt></p><p>由此我们推广开来，每个输入向量都可以计算属于自己的查询向量query，以及其他输入向量的关键向量key。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理4_相关性计算方法3.png" alt></p><p>输入向量还需要与自己计算相关性，这样凑齐4个相关性数值。由此我们就得到了输入向量a1与其他（包含自己的）四个向量之间的相关程度。</p><p>接下来我们需要做的，就是把四个输入向量都与该相关性进行乘积，然后相加，就得到了包含上下文相关信息的输出向量b1。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理5_相关性计算方法4.png" alt></p><p>再分别对a2/a3/a4向量如法炮制，就可以分别得到四个输出向量。</p><p>需要注意，输入向量最后参与计算时，为了将其转化为稠密向量，也需要乘以一个矩阵后才能参与计算。</p><h2 id="三、self-attention-的并行计算"><a href="#三、self-attention-的并行计算" class="headerlink" title="三、self-attention 的并行计算"></a>三、self-attention 的并行计算</h2><p>向量的转化操作都是矩阵乘法，这意味着对向量进行转化的操作是可以并行计算的。</p><ol><li>通过参数学习，将输入向量（一般是稀疏的）转化为query/key/value（稠密向量），然后将向量拼合形成矩阵，得到矩阵Q、K、V：</li></ol><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention的并行计算1.png" alt></p><ol><li>矩阵Q与K做点乘，得到相关性系数矩阵A</li></ol><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention-parallel-2.png" alt></p><ol><li>相关性矩阵A经过softmax进行概率归一化后，与矩阵V进行点乘，得到输出向量B</li></ol><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention-parallel-3.png" alt></p><p><strong>self-attention将输入向量转化为输出向量的过程到此结束。整个过程总结如下</strong>：</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention-parallel-summary.png" alt></p><h2 id="四、多头注意力-Multihead-Self-attention"><a href="#四、多头注意力-Multihead-Self-attention" class="headerlink" title="四、多头注意力 Multihead Self-attention"></a>四、多头注意力 Multihead Self-attention</h2><p>多头注意力机制，就是同时学习多种相关性的机制。</p><p>这样做的理由是：语言是存在同义词、一词多义等现象的，即便是同一句话，在不同语境下阐述也会产生不同的效果。如果只算一种相关性，模型便无法掌握一词多义等能力，因此我们需要学习多个相关性矩阵A。</p><p>如果想要学习多个相关性矩阵A，</p><ol><li>就必须有多个输入矩阵Q/K/V与之对应，每多一个A，就要多学习一类参数矩阵；</li><li>会产生多组输出矩阵B，需要再学习一个参数矩阵，将一系列的输出矩阵合并起来；</li></ol><p>多头注意力机制在原有注意力机制的基础上，把原有的query向量再多乘n个参数矩阵，将query复制为n个子向量；key和value向量亦是如此：</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/multi-head-self-attention-1.png" alt></p><p>query向量的第一个复制体会与各个key向量的第一个复制体进行计算相关性，生成相关性矩阵A1，第二个复制体会与各个key向量的第二个复制体计算相关性，生成相关性矩阵A2，以此类推。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/multi-head-self-attention-2.png" alt></p><p>由此会产出n份不同的输出向量，他们分别是考虑n种不同相关性下计算出来的输出向量。使用一个参数矩阵将他们合并起来：</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/multi-head-self-attention-3.png" alt></p><p>这就是多头注意力机制的设计思路。如果要计算N种相关性，就叫做N头注意力机制。</p><h2 id="五、位置编码"><a href="#五、位置编码" class="headerlink" title="五、位置编码"></a>五、位置编码</h2><p>上面的机制只介绍了 self-attention 如何把输入向量和它的上下文信息进行编码，但是忽略了上下文也是有位置关系的，理论上离当前位置更近的上下文应该获得更多注意力。因此我们需要额外添加一个能够表示位置的参数。</p><p>做法也很简单，只需要在输入层为每个向量添加一个额外的位置向量即可。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/positional-encoding-1.png" alt></p><p>这个位置向量可以是你自己定义的，也可以作为参数放到网络中进行学习。总之它理论上应该能够表示位置的远近关系。</p><h2 id="六、self-attention-的具体应用"><a href="#六、self-attention-的具体应用" class="headerlink" title="六、self-attention 的具体应用"></a>六、self-attention 的具体应用</h2><p>上面我们是以文本为例介绍的self-attention，实际上有很多其他任务也可以用self-attention来解决（虽然不见得是最佳方案）。</p><p>语音识别任务里，通过将一段音频按照一定时间窗口进行切分，也可以得到一个包含很多向量的输入序列；则处理方法与文本任务大同小异，只不过语音分割时产生的输入序列会非常长，此时需要对计算过程加以优化。例子：Truncate Self-Attention</p><p>图像识别任务里，每个图像按照像素进行分割，并逐行遍历，也可以形成一个序列，这个序列里的每个输入向量是(R,G,B)的三维向量。例子：Self-Attention GAN</p><h2 id="七、其他变种-self-attention"><a href="#七、其他变种-self-attention" class="headerlink" title="七、其他变种 self-attention"></a>七、其他变种 self-attention</h2><h2 id="八、总结和提问"><a href="#八、总结和提问" class="headerlink" title="八、总结和提问"></a>八、总结和提问</h2><h3 id="1-Self-Attention-的核心思想？"><a href="#1-Self-Attention-的核心思想？" class="headerlink" title="1. Self-Attention 的核心思想？"></a>1. Self-Attention 的核心思想？</h3><p>通过综合考虑输入词与上下文之间的相关关系，得到包含上下文语义信息的输出向量。</p><h3 id="2-Self-attention-是如何计算的？"><a href="#2-Self-attention-是如何计算的？" class="headerlink" title="2. Self-attention 是如何计算的？"></a>2. Self-attention 是如何计算的？</h3><script type="math/tex; mode=display">\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V</script><p>其中Q、K、V都是从输入向量学习而来的稠密向量组成的矩阵，它们不包含上下文的语义信息；</p><ul><li>Q：查询向量，目标字作为 Query；</li><li>K：键向量，其上下文的各个字作为 Key；</li><li>V：值向量，上下文各个字的 Value；</li></ul><p>$d_k$是Q/K/V的维度。这样做是为了降低分量之间的方差，防止输入向量的维度过高导致点击出来的结果过大，使softmax出来的结果出现一些极端情况（比如只有一个分量是0.99999，其他分量都是0），进而导致训练困难的现象。</p><h3 id="3-Self-Attention-和-CNN-的异同？"><a href="#3-Self-Attention-和-CNN-的异同？" class="headerlink" title="3. Self-Attention 和 CNN 的异同？"></a>3. Self-Attention 和 CNN 的异同？</h3><p>CNN是一类特殊的Attention，即将注意力聚焦于感受野（卷积核）中的一种self-attention网络结构。</p><p>而self-attention的感受野范围是整个序列，可以自行学习哪些是需要重点关注的。</p><p>这也就意味着self-attention相较于CNN而言更复杂、参数更多，需要更多数据进行训练。</p><p>有讨论二者关系的论文：</p><blockquote><p>On the Relationship between Self-Attention and Convolutional Layers</p></blockquote><p>从所需的数据量和准确率比较上，可以辅证这一点：</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention-versus-cnn.png" alt></p><p>那就是在数据量较小的情况下，CNN作为简单模型可以表现得更好；但是数据量充足时，self-attention具有更高的上限。</p><h3 id="4-Self-Attention-与-RNN-的比较？"><a href="#4-Self-Attention-与-RNN-的比较？" class="headerlink" title="4. Self-Attention 与 RNN 的比较？"></a>4. Self-Attention 与 RNN 的比较？</h3><p>RNN虽然也能够学习长距离依赖关系，但是它的结构和运作方式（要想计算b2，必须先计算b1，存在先后依赖关系）决定了它难以并行化，这就导致它的训练过程非常缓慢。</p><p>另外RNN在输入序列过长时，也存在梯度消失或者梯度爆炸的问题，导致无法记忆长距离信息。</p><p>Self-Attention的内部有大量矩阵乘法运算，可以被GPU优化的很快。</p><h3 id="5-MultiHead-Attention-的生效原理？"><a href="#5-MultiHead-Attention-的生效原理？" class="headerlink" title="5. MultiHead Attention 的生效原理？"></a>5. MultiHead Attention 的生效原理？</h3><p>借鉴了CNN中同一卷积层内使用多个卷积核的思想。类似于CNN中通过多通道机制进行特征选择。</p><p>Transformer中使用切头(split)的方法，是为了在不增加复杂度（$O(n^2 d)$）的前提下享受类似CNN中“不同卷积核”的优势。</p><p>在每个头的计算过程中，彼此之间相互独立，参数不共享，仅在最后将结果拼接起来，这样可以允许模型在不同的表示子空间里学习到相关的信息。</p><p>最后整合多个向量，把他们降维到一个向量的长度，这个过程也是在学习”到底哪个头学到的知识是有效的”。</p><h3 id="6-位置编码的计算方法？"><a href="#6-位置编码的计算方法？" class="headerlink" title="6. 位置编码的计算方法？"></a>6. 位置编码的计算方法？</h3><p>Attention is all you need 论文中的位置编码的实现方式如下：</p><script type="math/tex; mode=display">\text{PosEncoding}_{(\text{pos}, 2i)}=\sin{(\frac{\text{pos}}{10000^{2i/d_{model}}})} \\\text{PosEncoding}_{(\text{pos}, 2i+1)}=\cos{(\frac{\text{pos}}{10000^{2i/d_{model}}})}</script><p>其中pos表示词在句子中的位置，i则表示向量的分量。这种计算方法无需学习任何参数。</p><p>在BERT论文中，采用的位置编码就变成了Embedding的方法自动学习得到。</p><h2 id="附、self-attention-的代码实现"><a href="#附、self-attention-的代码实现" class="headerlink" title="附、self-attention 的代码实现"></a>附、self-attention 的代码实现</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：自注意力机制。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h2 id=&quot;一、输入从图像到序列&quot;&gt;&lt;a href=&quot;#一、输入从图像到序列&quot; class=&quot;headerlink&quot; title=&quot;一、输入从图像到序</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
    <category term="Self-Attention" scheme="https://superlova.github.io/tags/Self-Attention/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】李宏毅2019ML-Note5-卷积神经网络</title>
    <link href="https://superlova.github.io/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://superlova.github.io/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2022-10-22T13:48:58.000Z</published>
    <updated>2022-10-22T18:46:52.357Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：卷积神经网络。<br><!--more---></p><h2 id="为什么我们要用CNN？"><a href="#为什么我们要用CNN？" class="headerlink" title="为什么我们要用CNN？"></a>为什么我们要用CNN？</h2><p>要回答这个问题，首先要说明为什么全连接网络不适合做图像识别任务。</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/为什么要用CNN_1.png" alt></p><p>以图像为输入，会导致输入序列特别长。假设输入是一张<code>100*100</code>像素的图片，每个像素有<code>R/G/B</code>三个分量，要想将它整理为输入序列，每张图片的维度为<code>100*100*3=30000</code>。</p><p>输入层的参数过多，会导致中间层的参数过多，网络不仅训练代价提升，还有过拟合的风险。实际上图像分类任务用不着那么多参数，原因有三：</p><ol><li>每种特征所需的仅仅是图片的一小部分</li></ol><p>从神经网络的可解释性上来说，网络的第一层学习到的是最简单的特征分类，比如一张输入图片”有没有绿色出现，有没有黄色出现，有没有斜的条纹“等等；第二层会学到更复杂一些的特征组合，越往后学到的特征越抽象。</p><p>但是对于一个检测”是否有鸟嘴“的分类器来说，不需要看整张图片就可以知道这件事；<strong>输入序列可以从一整张图片优化为原图的一小部分</strong>。</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/为什么要用CNN_2.png" alt></p><ol><li>识别同一特征的分类器的部分参数可以共享</li></ol><p>对于检测鸟嘴的分类器而言，鸟嘴出现在图片的不同位置根本无所谓。我们不需要训练两组参数，来分别检测到底是左下方的鸟嘴还是右上方的鸟嘴。这意味着<strong>做同一特征分类的参数可以共享</strong>。</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/为什么要用CNN_3_参数共享.png" alt></p><ol><li><strong>输入序列的部分信息可以略过</strong></li></ol><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/为什么要用CNN_4.png" alt></p><p>对图片进行采样，得到的像素更低的图片，并不会影响人对这张图片的理解。</p><h2 id="CNN-架构"><a href="#CNN-架构" class="headerlink" title="CNN 架构"></a>CNN 架构</h2><p>卷积神经网络有两个基本单元：卷积层(Convolution Layers)和池化层(MaxPooling Layers)，可以实现上面三点性能优化。</p><p>一个完整的卷积神经网络大概长这样：</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CNN架构_1.png" alt></p><p>卷积层：是一种利用卷积操作提取图像的局部特征的网络结构。通过选择卷积核，不断执行卷积操作，得到原图像的特征图；后续的网络根据该特征图为输入，继续提取特征。</p><p>池化层：有最大池化和平均池化两种方式，最大池化是取滑动窗口内最大的元素作为输出。</p><h2 id="CNN-卷积层与全连接层的联系"><a href="#CNN-卷积层与全连接层的联系" class="headerlink" title="CNN 卷积层与全连接层的联系"></a>CNN 卷积层与全连接层的联系</h2><p>卷积核中的权值每次滑动计算时只是局部连接，且在卷积列中的神经元共享参数——计算局部信息，而全连接层神经元的权值与所有输入相连——计算全局信息。</p><p>卷积层的作用是从输入数据中采集关键数据内容。全连接层在深度卷积神经网络中的作用是将前面经过多次卷积后高度抽象的特征进行整合。</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CNN卷积层与全连接层的关系.png" alt></p><h2 id="用Keras实现CNN网络"><a href="#用Keras实现CNN网络" class="headerlink" title="用Keras实现CNN网络"></a>用Keras实现CNN网络</h2><p>以<a href="https://www.tensorflow.org/tutorials/images/cnn">tensorflow的图像分类教程</a>为例，实践使用keras搭建卷积模型的方法：</p><p>先导入相关包</p><pre><code class="lang-py">import tensorflow as tffrom tensorflow.keras import datasets, layers, modelsimport matplotlib.pyplot as plt</code></pre><p>然后下载并准备 CIFAR10 数据集</p><pre><code class="lang-py">(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()# 归一化像素值到 0 ~ 1train_images, test_images = train_images / 255.0, test_images / 255.0</code></pre><p>验证数据</p><pre><code class="lang-py">class_names = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;,               &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;]plt.figure(figsize=(10,10))for i in range(25):    plt.subplot(5,5,i+1)    plt.xticks([])    plt.yticks([])    plt.grid(False)    plt.imshow(train_images[i])    # The CIFAR labels happen to be arrays,     # which is why you need the extra index    plt.xlabel(class_names[train_labels[i][0]])plt.show()</code></pre><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CIFAR10样例.png" alt></p><p>接下来开始搭建网络：</p><pre><code class="lang-py">model = models.Sequential()model.add(layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(32, 32, 3)))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;))</code></pre><p>卷积网络被封装为<code>Conv2D</code>，其中第一个参数为卷积核个数，你可以理解为隐藏层神经元个数；我们这里定义为32个，它会提取32种不同类别的特征。</p><p>第二个参数是(3,3)，代表卷积核的大小是<code>3*3</code>的，这与李老师课堂上的示例一致；</p><p>第三个参数代表我们选择激活函数为<code>relu</code>；</p><p>第四个参数是只有在临近输入层的卷积层才会有的，代表输入向量的维度。这里的图像是彩色的，不但有长宽，还有通道数(channels)为3，因此输入向量的大小为(32,32,3)。</p><p>在搭建完第一层Conv2D后，紧接着会搭建最大池化层，池化层filter为<code>2*2</code>。后续的操作也是重复的，都是一层卷积、一层池化。到目前为止，模型结构如下：</p><pre><code class="lang-py">model.summary()</code></pre><pre><code>Model: &quot;sequential&quot;_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= conv2d (Conv2D)             (None, 30, 30, 32)        896        max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0          )                                                                conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496      max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0          2D)                                                              conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     =================================================================Total params: 56,320Trainable params: 56,320Non-trainable params: 0_________________________________________________________________</code></pre><p>我们可以看到，第一层Conv2D的输出向量的大小变成了(30,30,32)，这里32是我们之前定义的卷积核个数，<code>30*30</code>则是特征图大小。每次卷积操作都会导致宽度和高度的收缩。</p><p>参数量为896，这是如何计算来的呢？首先每个卷积核是<code>3*3</code>，同时图像有3个channel，因此每个卷积核的参数个数为<code>3*3*3</code>，再加上一个偏置量，就是28个参数。一共32个卷积核，最后的参数量即为<code>[(height * width * channel) + 1] * filter</code>。</p><p>卷积层只是特征提取器，最后为了完成分类操作，我们需要在卷积层后面拼接全连接层。不用太多，几层就够了：</p><pre><code class="lang-py">model.add(layers.Flatten())model.add(layers.Dense(64, activation=&#39;relu&#39;))model.add(layers.Dense(10))model.summary()</code></pre><pre><code>Model: &quot;sequential&quot;_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= conv2d (Conv2D)             (None, 30, 30, 32)        896        max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0          )                                                                conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496      max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0          2D)                                                              conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928      flatten (Flatten)           (None, 1024)              0          dense (Dense)               (None, 64)                65600      dense_1 (Dense)             (None, 10)                650       =================================================================Total params: 122,570Trainable params: 122,570Non-trainable params: 0_________________________________________________________________</code></pre><p>卷积层的输出在输入两个 Dense 层之前需要被展平（Flatten）为形状为 (1024) 的向量。</p><p>仅添加了两层全连接层，参数量就多了65600+650个，可以看到全连接层真的比CNN多很多冗余参数。</p><p>最后编译并训练一下：</p><pre><code class="lang-py">model.compile(optimizer=&#39;adam&#39;,              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),              metrics=[&#39;accuracy&#39;])history = model.fit(train_images, train_labels, epochs=10,                     validation_data=(test_images, test_labels))plt.plot(history.history[&#39;accuracy&#39;], label=&#39;accuracy&#39;)plt.plot(history.history[&#39;val_accuracy&#39;], label = &#39;val_accuracy&#39;)plt.xlabel(&#39;Epoch&#39;)plt.ylabel(&#39;Accuracy&#39;)plt.ylim([0.5, 1])plt.legend(loc=&#39;lower right&#39;)plt.show()test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)</code></pre><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/训练结果.png" alt></p><p>准确率只有不到70%，可以改进的地方还有很多。不过对于一个简单模型而言，在10分类任务上能达到如此成绩已是不容易了。</p><h2 id="CNN学到了什么？"><a href="#CNN学到了什么？" class="headerlink" title="CNN学到了什么？"></a>CNN学到了什么？</h2><p>分析第一层Conv到底学到了什么还是很容易的，因为每个卷积核实际上是在计算小范围内是否包含指定特征。但是后续Conv会根据该特征图计算更抽象的特征，我们该如何分析？</p><p>可以采用这种分析方法，首先固定参数，然后通过梯度下降法去找能够让某个神经元的输出值最大的输入图片。</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/生成最能激活某个神经元的图片.png" alt></p><p>上面就是12个filter对应的结果，这些图片的特征是某种纹路在图上不断的重复。因此实际上神经元会对这种不断重复的花纹做出最大的反应。</p><p>当我们以同样的分析方法分析卷积网络最后的全连接层（这里汇聚了卷积层提取出来的各路特征），得到的图像为：</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/全连接_生成最能激活某个神经元的图片.png" alt></p><p>发现最能激活某个神经元的图片，不再是那些纹路单一的图像了，而是带有某种花纹，似乎有某些含义。</p><p>但是当我们用同样的方法分析输出层时，得到的图像理论上应该是确切的数字，实际上确实乱码一样无法被辨认的图片：</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/输出层_生成最能激活某个神经元的图片.png" alt></p><p>人类虽然无法辨识，但是机器十分确定地告诉我们，左上角的图片就是0，右下角就是8。这种现象很有意思。</p><p>这种思想被应用于对抗样本攻击之中，可以生成一些令模型误判的诡异图片，并将其改造原本的数据集，就可以令模型犯错。</p><p>关于文本领域对抗样本的讨论在<a href="https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/">这里</a></p><p>Deep Dream则利用该思想，将CNN变成一个图像生成器，夸大化原有的输入图像：</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/DeepDream夸大的图像.png" alt></p><p>右边有一只熊，这个熊原来是一个石头(对机器来说，这个石头有点像熊，它就会强化这件事情，所以它就真的变成了一只熊)。</p><h2 id="CNN的其他应用场景"><a href="#CNN的其他应用场景" class="headerlink" title="CNN的其他应用场景"></a>CNN的其他应用场景</h2><p>CNN可以应用于游戏领域，比如围棋</p><p>CNN可以应用于自然语言处理领域，比如文本分类、语音识别</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：卷积神经网络。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h2 id=&quot;为什么我们要用CNN？&quot;&gt;&lt;a href=&quot;#为什么我们要用CNN？&quot; class=&quot;headerlink&quot; title=&quot;为什么我们要用</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
    <category term="Convolutional Neural Network" scheme="https://superlova.github.io/tags/Convolutional-Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】李宏毅ML-Note4-深度学习优化</title>
    <link href="https://superlova.github.io/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/"/>
    <id>https://superlova.github.io/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/</id>
    <published>2022-10-19T11:03:40.000Z</published>
    <updated>2022-10-19T14:21:27.005Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：深度学习算法的优化。<br><!--more---></p><p>神经网络训练不好怎么办？为什么loss不下降？这里讨论的训练不好，是指在训练过程中的loss始终降不下来。从数学优化的角度，此时可能陷入了局部最小值或者鞍点等微分为零的点。</p><h2 id="1-优化陷入鞍点-Saddle-Point-或局部极小值-Local-Minima"><a href="#1-优化陷入鞍点-Saddle-Point-或局部极小值-Local-Minima" class="headerlink" title="1. 优化陷入鞍点 (Saddle Point) 或局部极小值 (Local Minima)"></a>1. 优化陷入鞍点 (Saddle Point) 或局部极小值 (Local Minima)</h2><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/陷入鞍点或者局部最小点.png" alt></p><p>鞍点是指在一个方向上的gradient=0，但是在其他方向的gradient不为零，说明loss仍有继续优化的空间。</p><p>局部最小点则是所有方向的gradient均为零，此时无论往哪个方向走都无法令loss继续变小。这种情况比较棘手。</p><p>如何确认到底是鞍点还是局部极值？</p><p>这里有个结论：如果该点处的二阶偏导数矩阵（海塞矩阵）是正定的，即该海塞矩阵的所有特征值都大于零，说明该点附近的所有值均大于该点，即该点为局部最小值。</p><p>如果海塞矩阵的特征值全为负，则说明这里是局部最大值。</p><p>其他情况下，如果海塞矩阵的特征值有正有负，说明这里是个鞍点。</p><p>求解该点处的二阶偏导数不但可以帮助判断是否为鞍点，还可以帮助我们选择优化的方向。如果该处是一个鞍点，则我们就选择一个海塞矩阵特征值为负值的特征向量，朝着这个特征向量的方向进行移动，就能够最快速走出鞍点。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/快速走出鞍点.png" alt></p><p>实际上求海塞矩阵是一个计算量比较大的操作，除非有必要，否则这种方法其实很少使用。事实上，很少有优化算法具备把损失函数给优化到局部最小值或者鞍点的程度，在此之前我们会受到其他问题的困扰，比如batch没有选择好、优化方法并不够优秀等。</p><h2 id="2-batch没有选择好"><a href="#2-batch没有选择好" class="headerlink" title="2. batch没有选择好"></a>2. batch没有选择好</h2><p>在使用随机梯度下降算法时，我们会定义一个batch大小，并把训练集分割成batch大小的一个个子集，每次梯度下降只在子集上进行遍历，每次训练完一个子集的内容叫做一个epoch。</p><p>batch的大小自然会影响网络训练的速度以及优化效果。小batch可以加速训练，缺点是小batch随机性更大，没办法代表整体的数据集。极端情况下，batch=1时，甚至会出现loss在原地不断震荡的情况。</p><p>然而batch过大则失去了意义，因为这会导致训练速度变慢。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/batch大小的权衡.png" alt></p><p>如果引入并行计算（即GPU运算），那么batch_size在一定程度内都可以很快。但是不同GPU支持的batch_size是有极限的（取决于显存）。</p><p>batch_size 对最终的优化效果有影响吗？是有的。大的batch_size可能会导致优化阶段产生问题。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/batch_size对最终优化效果的影响.png" alt></p><p>这种问题是怎么产生的呢？有一种解释是这样的，当我们试图以一个比较大的batch_size训练模型时，可能会卡在客观存在的局部极值点；但是如果我们使用比较小的batch，相当于人为引入了更多的随机性。w在batch_A上是局部极值点，但是在batch_B上就不一定了。</p><p>另外，在testing的时候，选择不同的batch也会让accuracy不一样。如果大的Batch对应的Testing结果差 ，代表 Overfitting 。</p><p>最后总结一下，batch_size可能造成的影响：</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/batch_size的选择总结.png" alt></p><h2 id="3-虽然-gradient-没有降低到0，但是-loss-已经不再-变小-了（没有选择合适的优化算法）"><a href="#3-虽然-gradient-没有降低到0，但是-loss-已经不再-变小-了（没有选择合适的优化算法）" class="headerlink" title="3. 虽然 gradient 没有降低到0，但是 loss 已经不再 变小 了（没有选择合适的优化算法）"></a>3. 虽然 gradient 没有降低到0，但是 loss 已经不再 变小 了（没有选择合适的优化算法）</h2><p>多数情况下，不会有真正陷入 critical point 的机会，而是在 critical point 附近徘徊。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/训练卡住了并不一定意味着gradient很小.png" alt></p><p>因此需要我们调整学习率。每个不同的参数应该在训练的不同阶段拥有不同的学习率。这种通过改变学习率来优化算法的训练速度的方法，在Note2中已有介绍。</p><p>常用的自适应改变参数学习率的算法有Adagrad算法、RMSProp算法等。</p><p>一种方法是使用带有momentum的优化算法。动量法在Note2里已有介绍，我们将梯度下降的优化过程比喻成小球滚落山坡的过程。但是梯度下降的优化过程忽略了小球本身具有动量，与现实中有一定区别。如果我们将梯度下降算法添加上动量的模拟，则小球在陷入局部最优解时，足够高的动量能帮助小球“冲”出局部最优解。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/动量法模拟.png" alt></p><p>使用动量法的优化算法有Adam等。实际上，Adam是Adagrad和Momentum动量法的结合。</p><p>此外，从经验上看，学习率一开始要保持大一些，来保证收敛到最优点附近；然后要减小学习率来避免震荡。那么我们自然可以想到，学习率跟随训练轮次变大逐渐变小。这就是<strong>学习率衰减</strong>。</p><p>在刚开始训练时，由于参数是随机初始化的，梯度也往往比较大，再加上比较大的初始学习率，会使得训练不稳定。因此我们希望刚开始几轮迭代的学习率较小，等梯度下降到一定程度时再恢复学习率。这种方法称之为<strong>学习率预热</strong>。等预热完毕后在进行学习率衰减。</p><p>利用这两个思想，改进Adam后，就是优化算法RAdam。</p><h2 id="4-损失函数也会有影响"><a href="#4-损失函数也会有影响" class="headerlink" title="4. 损失函数也会有影响"></a>4. 损失函数也会有影响</h2><p>分类问题的损失函数可以选择MSE和Cross-entropy。现在我们更多选择交叉熵。为什么MSE不行呢？</p><p>MSE在loss大的地方非常平坦，梯度趋近于零，最后stuck卡住！难以优化。</p><p>Cross-entropy ：左上角有斜率，可以透过梯度，一路往右下角走。更易收敛。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/MSE和Cross-entropy.png" alt></p><h2 id="5-也许需要-normalization"><a href="#5-也许需要-normalization" class="headerlink" title="5. 也许需要 normalization"></a>5. 也许需要 normalization</h2><p>一般而言，样本特征由于来源以及度量单位不同，他们的尺度 (Scale) 即取值范围也不同。对于某些机器学习模型，会对那些尺度更大的数据更敏感。因此对于尺度敏感的模型，必须先对样本进行预处理，将各个维度的特征转换到相同的取值区间内，并且消除不同特征之间的相关性，才能取得理想的效果。</p><p>归一化 (Normalization) 方法泛指把数据特征转换为相同尺度的方法，比如把数据特征映射到 [0,1] 区间内，或者直接映射为均值为0、方差为1的正态分布。</p><p><strong>最小最大值归一化</strong>：将每个特征缩放到 [0,1] 或者 [-1,1] 之间。假设我们有 N 哥样本 ${x^{(n)}}^{N}_{n=1}$，对每一维特征x，归一化后的特征为：</p><script type="math/tex; mode=display">\hat{x}^{(n)}=\frac{x^{(n)}-\min_n(x^{(n)})}{\max_n(x^{(n)})-\min_n(x^{(n)})}</script><p><strong>标准化</strong>：将每维特征都调整到标准正态分布：</p><script type="math/tex; mode=display">\hat{x}^{(n)}=\frac{x^{(n)}-\mu}{\sigma}</script><p>除此之外，还有Batch-Normalization和Layer-Normalization的算法。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：深度学习算法的优化。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;p&gt;神经网络训练不好怎么办？为什么loss不下降？这里讨论的训练不好，是指在训练过程中的loss始终降不下来。从数学优化的角度，此时可能陷入了局部</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
    <category term="Deep Learning" scheme="https://superlova.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】李宏毅2019ML-Note3-深度学习介绍和反向传播</title>
    <link href="https://superlova.github.io/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <id>https://superlova.github.io/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</id>
    <published>2022-10-16T10:32:13.000Z</published>
    <updated>2022-10-16T13:56:17.399Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：深度学习算法介绍、反向传播机制。<br><!--more---></p><h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><p>深度学习大家已经很熟悉了。深度学习技术属于一类机器学习技术。使用深度学习解决问题的过程同样需要三步骤：</p><p>Step1：选择深度学习模型：神经网络（Neural network），神经网络有各种不同类型，比如前馈神经网络、卷积神经网络等；<br>Step2：模型评估（Goodness of function）；<br>Step3：选择最优函数（Pick best function），不同的网络结构需要定义不同的参数学习方法，比如反向传播算法；</p><p>接下来逐一介绍三个步骤的具体内容。</p><h2 id="第一步：选择神经网络：从前馈网络开始"><a href="#第一步：选择神经网络：从前馈网络开始" class="headerlink" title="第一步：选择神经网络：从前馈网络开始"></a>第一步：选择神经网络：从前馈网络开始</h2><p>前馈（feedforward）也可以称为前向，从信号流向来理解就是输入信号进入网络后，信号流动是单向的，即信号从前一层流向后一层，一直到输出层，其中任意两层之间的连接并没有反馈（feedback），亦即信号没有从后一层又返回到前一层。</p><p>下图是一个由一层输入层、N层隐藏层和一层输出层构成的、每层包含M个神经元的全连接网络，神经元的激活函数为sigmoid：</p><p><img src="/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/前馈网络.png" alt></p><p>为什么叫全链接呢？因为layer1与layer2之间两两都有连接，所以叫做Fully Connect；</p><p>为什么叫前馈呢？因为现在传递的方向是由后往前传，所以叫做Feedforward。</p><p>当隐藏层N非常大时，我们就说这个网络非常深。现如今的网络结构动辄几百层，随之带来很大的计算开销。一个一个计算神经元的输入输出是不现实的，一种加速方法是将神经网络的信息传递过程具象化为矩阵运算。</p><p>假设下图的前馈网络：</p><p><img src="/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/前馈网络-矩阵计算演示.png" alt></p><p>计算方法就是：sigmoid（权重w【黄色】 * 输入【蓝色】+ 偏移量b【绿色】）= 输出</p><p>如果有很多层，就进行嵌套</p><p><img src="/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/很多层的全连接网络的矩阵计算.png" alt></p><p>所以整个神经网络运算就相当于一连串的矩阵运算。矩阵运算是可以被GPU并行加速的，因此深度学习模型才能够在如今这个算力大爆发的年代大放异彩。</p><p><strong>深度学习的本质是利用隐藏层学习到了输入特征的有效表示方法，代替了之前依靠数据科学家的经验的特征工程。</strong></p><h2 id="第二步：模型评估方法：选择损失函数"><a href="#第二步：模型评估方法：选择损失函数" class="headerlink" title="第二步：模型评估方法：选择损失函数"></a>第二步：模型评估方法：选择损失函数</h2><p>对于模型的评估，我们一般采用损失函数来反应模型的好差，所以对于神经网络来说，我们采用交叉熵（cross entropy）函数来对$y$和$\hat{y}$的损失进行计算，接下来我们就是调整参数，让交叉熵越小越好。</p><h2 id="第三步：选择最优模型：梯度下降，以及用反向传播算法优化计算"><a href="#第三步：选择最优模型：梯度下降，以及用反向传播算法优化计算" class="headerlink" title="第三步：选择最优模型：梯度下降，以及用反向传播算法优化计算"></a>第三步：选择最优模型：梯度下降，以及用反向传播算法优化计算</h2><p>梯度下降算法在之前的笔记已经讲过，神经网络的梯度下降算法也是相同的。</p><p><img src="/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/梯度下降.png" alt></p><p>梯度下降的具体过程：</p><p><img src="/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/梯度下降的过程.png" alt></p><ol><li>给到$\theta$(weight and bias)</li><li>先选择一个初始的 $\theta^0$，计算 $\theta^0$ 的损失函数（Loss Function）设一个参数的偏微分</li><li>计算完这个向量（vector）偏微分，然后就可以去更新的你 $\theta$</li><li>百万级别的参数（millions of parameters）</li><li>反向传播（Backpropagation）是一个比较有效率的算法，让你计算梯度（Gradient） 的向量（Vector）时，可以有效率的计算出来</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：深度学习算法介绍、反向传播机制。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h1 id=&quot;深度学习&quot;&gt;&lt;a href=&quot;#深度学习&quot; class=&quot;headerlink&quot; title=&quot;深度学习&quot;&gt;&lt;/a&gt;深度</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
    <category term="Backpropagation" scheme="https://superlova.github.io/tags/Backpropagation/"/>
    
    <category term="Deep Learning" scheme="https://superlova.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】李宏毅2019ML课程-Note2</title>
    <link href="https://superlova.github.io/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/"/>
    <id>https://superlova.github.io/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/</id>
    <published>2022-10-13T02:52:43.000Z</published>
    <updated>2022-10-15T02:44:09.974Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：偏差和方差、模型选择、梯度下降算法。<br><!--more---></p><h1 id="一、误差的来源：偏差与方差"><a href="#一、误差的来源：偏差与方差" class="headerlink" title="一、误差的来源：偏差与方差"></a>一、误差的来源：偏差与方差</h1><p>对学习算法除了通过实验估计其泛化性能，人们往往还希望能了解为什么具有这样的性能。误差来源于偏差 (bias) 和方差 (Variance)以及不可避免的噪声。</p><h2 id="1-偏差和方差的概念和来源"><a href="#1-偏差和方差的概念和来源" class="headerlink" title="1. 偏差和方差的概念和来源"></a>1. 偏差和方差的概念和来源</h2><p><strong>偏差</strong>度量了预测值与真实值的偏离程度，对应的是学习算法本身的拟合能力；<strong>方差</strong>度量了数据扰动对模型的影响，对应的是模型的稳定性；<strong>噪声</strong>则是对应问题对应的难度。</p><p>上面的结论说明，模型的性能是由模型能力、数据的充分性以及问题本身的难度决定的。由于噪音是问题本身的特性，不好解决，因此要想提升模型的性能，就需要采取措施降低偏差和方差。</p><h2 id="4-偏差和方差分解：重新考虑欠拟合、过拟合问题"><a href="#4-偏差和方差分解：重新考虑欠拟合、过拟合问题" class="headerlink" title="4. 偏差和方差分解：重新考虑欠拟合、过拟合问题"></a>4. 偏差和方差分解：重新考虑欠拟合、过拟合问题</h2><p>为了避免过拟合，我们经常会在模型的拟合能力和复杂度之间进行权衡。拟合能力强的模型一般复杂度会比较高，容易导致过拟合。相反，如果限制模型的复杂度，降低其拟合能力，有可能导致欠拟合。因此，如何在模型的拟合能力与复杂度之间取得平衡，对机器学习算法来说十分重要。</p><p>偏差-方差分解为我们提供了一个很好用的分析工具。数学推导过程比较复杂，结论为：</p><p><strong>最小化期望错误等价于最小化偏差和方差之和。</strong></p><p>下图给出了机器学习模型四种偏差和方差的组合情况。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/偏差和方差.png" alt></p><p>每个图的中心店为最优模型$f(x)$，蓝点为不同训练集上得到的模型$f^*(x)$。左上角的图是一种理想情况，方差和偏差都比较低；右上角为高偏差低方差的情况，表示模型的泛化能力很好，但是拟合能力不足，类似于我们之前的线性模型；左下角是低偏差高方差的情况，表示模型的拟合能力很好，但是泛化能力较差。当训练数据比较少的时候往往会出现这种情况，我们一般把他称之为过拟合；右下角为高偏差高方差的情况，是最差的情况，等于没训练。</p><p>方差一般会随着训练样本的增加而减小。当样本比较多，方差比较小，这是可以选择能力强的模型来减少偏差；当训练集比较有限时，最优的偏差和方差往往无法兼顾，我们称之为“偏差-方差窘境”。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/偏差方差窘境.png" alt></p><p>偏差-方差窘境说的是，模型复杂度增加后，虽然你和能力变强导致偏差减少，但是方差会增大，过拟合现象会导致性能下降。</p><p>偏差-方差分解给机器学习模型提供了一种分析途径，能够指导我们解决过拟合和欠拟合的问题，但是实际操作中很难直接衡量。下面是一些通用的方法。</p><pre><code>欠拟合现象：模型在训练集上的错误率很高，此时说明模型偏差较大。欠拟合解决：1. 增加数据特征2. 提高模型复杂度3. 减小正则化系数</code></pre><pre><code>过拟合现象：模型在训练集上的错误率较低，但是在测试集上较高，此时说明模型方差较大。过拟合解决：1. 降低模型复杂度；2. 加大正则化系数；3. 引入先验知识，比如数据清洗，剔除无用特征等；4. 使用更多数据进行训练，但这个往往成本最高。</code></pre><p>此外，还有一种降低方差的方法：集成模型，即通过多个高方差模型进行平均，来降低方差。这背后的原理在介绍集成学习时会介绍。</p><h1 id="二、模型选择方法"><a href="#二、模型选择方法" class="headerlink" title="二、模型选择方法"></a>二、模型选择方法</h1><p>前面讨论了机器学习的三个步骤，以及如何改进模型的性能。那么我们如何正确评估模型呢？</p><p>之前的例子中，我们把数据集分成两部分，Training Set、Test Set。一般比例控制在4:1左右。我们在训练结束后，使用测试集进行评估，并判断训练效果。</p><p>这样做其实存在问题。如果我们真的使用测试集来指导模型的选择、参数的改进等各个步骤，那算不算模型正在学习测试集的内容呢？就好像学生做完考卷后，老师虽然不会直接告诉学生正确答案，但是会不断地给学生机会，告诉你：你这里错了，那里错了。这算不算是另一种泄题呢？</p><p>上面的数据泄露现象是经常发生的。为了避免数据泄露，一般可以通过把数据分成三部分：训练集 (Training Set)、验证集 (Validation Set) 和测试集 (Test Set)。模型参数的改进、模型选择等过程，只使用验证集；最后评估模型时使用测试集。</p><p>但是这样又会带来另一个问题：本来数据就匮乏，又分割出两大部分不能使用，可供学习的数据就更少了。<strong>k折交叉验证</strong>可以解决这个问题。</p><h2 id="2-1-交叉验证"><a href="#2-1-交叉验证" class="headerlink" title="2.1 交叉验证"></a>2.1 交叉验证</h2><p>交叉验证 (Cross-Validation) 是一种评估泛化性能的统计学方法。它比单次划分训练集和测试集的方法更加稳定。最常用的交叉验证方法是<strong>k折交叉验证</strong>。</p><p>首先把数据集分成大致相等的k个部分，每一部分叫做一折；接下来训练k个模型，第一个模型以第1折数据作为测试集，其他作为训练集；第二个模型以第2折数据作为测试集，其他作为训练集……最后我们得到了k个模型，以及对应的k个精度值。模型的最终精度就是这k个精度值的平均值。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/交叉验证.png" alt></p><p>交叉验证的优点：</p><ol><li>帮助更好评估模型的真实泛化性能，防止数据集分割时的随机性影响评估准确率；</li><li>所有数据都有机会被训练，提高数据利用率；</li></ol><p>交叉验证的缺点：</p><ul><li>显著增加了计算成本，需要训练k个模型，是原来的k倍。</li></ul><h1 id="三、梯度下降算法详解，以及优化方法"><a href="#三、梯度下降算法详解，以及优化方法" class="headerlink" title="三、梯度下降算法详解，以及优化方法"></a>三、梯度下降算法详解，以及优化方法</h1><p>目前，机器学习中参数学习的主要方式是通过梯度下降法来寻找一组最小化损失函数的参数。再具体视线中，梯度下降法可以分为：批量梯度下降、随机梯度下降以及小批量梯度下降三种形式。</p><p>本节还会介绍一些梯度下降算法的变种，他们大多改善了以下两部分的内容：1) 调整学习率，使优化更稳定；2) 梯度估计修正，提升训练速度。</p><h2 id="3-1-梯度下降算法的类别"><a href="#3-1-梯度下降算法的类别" class="headerlink" title="3.1 梯度下降算法的类别"></a>3.1 梯度下降算法的类别</h2><h3 id="3-1-1-批量梯度下降"><a href="#3-1-1-批量梯度下降" class="headerlink" title="3.1.1 批量梯度下降"></a>3.1.1 批量梯度下降</h3><p>最传统的梯度下降算法，上篇笔记已经介绍。</p><p>梯度下降的具体过程是这样的：</p><ol><li>选择一个初始$\theta_0$</li><li>计算该位置下$\theta$对L的微分，这个微分对应函数在此处下降最快的方向；</li><li>将$\theta_0$朝着这个方向移动一小步$ \eta$</li><li>在新的位置开始新一轮迭代计算，直到$w$不再变化为止。</li></ol><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/批量梯度下降法.png" alt></p><p>其中$\eta$叫做学习率，它控制了每次梯度下降迭代的优化幅度。$\eta$越大，学习得越快。然而$\eta$并不是越大越好。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/批量梯度下降法的梯度深谷.png" alt></p><p>如果$\eta$过大，优化过程很可能会跨越最低点，从而永远达不到最优解；如果$\eta$过小，则会造成训练缓慢，甚至陷入局部最优解。因此调整学习率$\eta$是优化梯度下降算法的一大思路。</p><p>梯度下降算法的另一个问题是，当训练数据很大时（这很常见），梯度下降算法会花费很长时间去遍历整个训练集。大多数情况下，我们不必遍历所有训练集。</p><h3 id="3-1-2-随机梯度下降"><a href="#3-1-2-随机梯度下降" class="headerlink" title="3.1.2 随机梯度下降"></a>3.1.2 随机梯度下降</h3><p>在训练机器学习模型时，训练数据的规模比较大的情况下，批量梯度下降的每次梯度迭代都要遍历整个数据集，这会造成计算资源的浪费。为了解决批量梯度下降算法导致的训练缓慢的问题，随机梯度下降算法应运而生。其思想是通过随机选取少量训练输入样本来计算$\nabla L_x$，进而估算$\nabla L$。</p><p>更准确地说，随机梯度下降通过随机选取少量的m个训练数据来训练。也叫做小批量梯度下降法 (Mini-Batch Gradient Descent)。</p><p>假设样本数量m足够大，我们期望$\nabla L<em>{X_j}$的平均值大致相等于整个$\nabla L</em>{x}$的平均值，即：</p><script type="math/tex; mode=display">\frac{\sum^m_{j-1}\nabla L_{X_j}}{m}\approx \frac{\sum_x \nabla L_x}{n}=\nabla L</script><p>这里第二个求和符号实在整个训练数据上进行的。交换两边我们得到：</p><script type="math/tex; mode=display">\nabla L \approx \frac{1}{m}\sum^m_{j=1}\nabla L_{X_j}</script><p>证实了我们可以进通过计算随机选取的小批量数据的梯度来估算整体的梯度。</p><p>影响小批量梯度下降的主要因素有：1）批大小m，2）学习率$\eta$，3）梯度估计方法。</p><h2 id="3-2-批量大小选择"><a href="#3-2-批量大小选择" class="headerlink" title="3.2 批量大小选择"></a>3.2 批量大小选择</h2><p>直观来说，批包含的数据越大，方差也就越小，训练越稳定。该种情况下，可以设置一个较大的学习率。学习率越大，需要的批大小就越大。</p><p>另外，根据经验，批越大越可能收敛到“尖锐最小值”；批越小越能收敛到“平坦最小值”。</p><h2 id="3-3-学习率调整"><a href="#3-3-学习率调整" class="headerlink" title="3.3 学习率调整"></a>3.3 学习率调整</h2><p>学习率过大会导致模型不收敛，如果过小会导致收敛太慢。由此，有一些学者根据学习的不同阶段，制定了学习率的不同变化策略。</p><p>从经验上看，学习率一开始要保持大一些，来保证收敛到最优点附近；然后要减小学习率来避免震荡。那么我们自然可以想到，学习率跟随训练轮次变大逐渐变小。这就是<strong>学习率衰减</strong>。</p><p>在刚开始训练时，由于参数是随机初始化的，梯度也往往比较大，再加上比较大的初始学习率，会使得训练不稳定。因此我们希望刚开始几轮迭代的学习率较小，等梯度下降到一定程度时再恢复学习率。这种方法称之为<strong>学习率预热</strong>。等预热完毕后在进行学习率衰减。</p><h3 id="3-3-3-AdaGrad-算法"><a href="#3-3-3-AdaGrad-算法" class="headerlink" title="3.3.3 AdaGrad 算法"></a>3.3.3 AdaGrad 算法</h3><p>AdaGrad算法的做法是，每次迭代时，每个参数的学习率都把它除上之前微分的均方根。</p><p>普通的梯度下降算法采用的参数更新思路：</p><script type="math/tex; mode=display">w^{t+1}\leftarrow w^t-\eta^tg^t \\\eta^t=\frac{\eta^t}{\sqrt{t + 1}}</script><p>则Adagrad是这样更新的：</p><script type="math/tex; mode=display">w^{t+1}\leftarrow w^t-\frac{\eta^t}{\sigma^t}g^t \\g^t =\frac{\partial L(\theta^t)}{\partial w}</script><p>其中$\sigma^t$是之前参数的所有微分的均方根，对于每个参数都是不一样的。</p><p>将Adagrad的式子化简：</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/化简后的adagrad.png" alt></p><p>Adagrad的设计思路为，如果某个参数的偏导数累计比较大，其学习率就会相对较小；相反，如果偏导数累计较小，则学习率相对较大。但是整体上迭代次数越多，学习率越小。</p><p>AdaGrad的缺点是，如果在经过一定次数的迭代，依然没有找到最优点时，由于学习率已经非常小，很难再继续优化了。</p><h3 id="3-3-4-RMSprop-算法"><a href="#3-3-4-RMSprop-算法" class="headerlink" title="3.3.4 RMSprop 算法"></a>3.3.4 RMSprop 算法</h3><p>RMSProp算法可以帮助解决某些情况下Adagrad算法过早衰减的问题。</p><p>RMSProp算法在AdaGrad的基础上，改进了参数偏导数的累计方式。Adagrad算法是计算每个参数梯度平方的累计值：</p><script type="math/tex; mode=display">G_t=\sum^t_{\tau=1}\textbf{g}_{\tau}\odot\textbf{g}_{\tau}</script><p>其中$\textbf{g}_{\tau}\in\mathbb{R}^{|\theta|}$是第$\tau$ 次迭代时的梯度。</p><p>RMSProp算法首先计算每次迭代梯度 $\textbf{g}_t$平方的指数衰减移动平均：</p><script type="math/tex; mode=display">G_t=\beta G_{t-1}+(1-\beta)\textbf{g}_t\odot\textbf{g}_t \\=(1-\beta)\sum^t_{\tau=1}\beta^{t-\tau}\textbf{g}_{\tau}\odot\textbf{g}_{\tau}</script><p>其中$\beta$为衰减率，一般为0.9。</p><p>RMSProp算法的参数更新差值为：</p><script type="math/tex; mode=display">\Delta\theta_t=-\frac{\alpha}{\sqrt{G_t+\epsilon}}\odot\textbf{g}_t</script><p>其中$\alpha$是初始的学习率，比如0.001。</p><p>在迭代过程中，由于每个参数的学习率并不是衰减趋势，因此既可以变小也可以变大。</p><h2 id="3-4-梯度估计"><a href="#3-4-梯度估计" class="headerlink" title="3.4 梯度估计"></a>3.4 梯度估计</h2><p>除了调整学习率之外，还可以进行梯度估计的修正。这样做的原因是，小批量梯度下降选取的样本具有随机性，如果每次选取的样本数量比较小，则损失可能会呈现震荡的方式下降。一般我们可以采取使用最近一段时间内的平均梯度来代替当前时刻的随机梯度的做法来缓解随机性，提升优化速度。</p><h3 id="3-4-1-动量法"><a href="#3-4-1-动量法" class="headerlink" title="3.4.1 动量法"></a>3.4.1 动量法</h3><p>还记得之前我们说过的，梯度下降算法容易陷入局部最优解吗？我们将梯度下降的优化过程比喻成小球滚落山坡的过程。但是梯度下降的优化过程忽略了小球本身具有动量，与现实中有一定区别。如果我们将梯度下降算法添加上动量的模拟，则小球在陷入局部最优解时，足够高的动量能帮助小球“冲”出局部最优解。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/梯度下降算法容易陷入局部最优.png" alt></p><p>在第t次迭代是，计算负梯度的“加权移动平均”作为参数的更新方向：</p><script type="math/tex; mode=display">\Delta\theta_t=\rho\Delta\theta_{t-1}-\alpha\textbf{g}_t=-\alpha\sum^t_{\tau=1}\rho^{t-\tau}\textbf{g}_{\tau}</script><p>其中$\rho$为动量因子，一般为0.9，$\alpha$为学习率。</p><p>这样一来，每个参数的实际更新差值取决于最近一段时间内梯度的加权平均值。当某个参数在最近一段时间内的梯度方向都一致，其真是更新参数的幅度会变大，相当于加速冲刺；当某个参数在最近一段时间内的梯度方向不一致，则参数更新幅度会变小，相当于刹车。从某种角度来说，当前梯度叠加部分上次的梯度，这种做法可以近似看做二阶梯度。</p><h3 id="3-4-3-Adam-算法"><a href="#3-4-3-Adam-算法" class="headerlink" title="3.4.3 Adam 算法"></a>3.4.3 Adam 算法</h3><p>Adam算法是如今最常用的优化算法了，它是RMSProp算法和动量法的结合，不但使用动量作为参数更新方向，而且可以自适应调整学习率。</p><h2 id="3-3-数据预处理"><a href="#3-3-数据预处理" class="headerlink" title="3.3 数据预处理"></a>3.3 数据预处理</h2><p>一般而言，样本特征由于来源以及度量单位不同，他们的尺度 (Scale) 即取值范围也不同。对于某些机器学习模型，会对那些尺度更大的数据更敏感。因此对于尺度敏感的模型，必须先对样本进行预处理，将各个维度的特征转换到相同的取值区间内，并且消除不同特征之间的相关性，才能取得理想的效果。</p><p>归一化 (Normalization) 方法泛指把数据特征转换为相同尺度的方法，比如把数据特征映射到 [0,1] 区间内，或者直接映射为均值为0、方差为1的正态分布。</p><p><strong>最小最大值归一化</strong>：将每个特征缩放到 [0,1] 或者 [-1,1] 之间。假设我们有 N 哥样本 ${x^{(n)}}^{N}_{n=1}$，对每一维特征x，归一化后的特征为：</p><script type="math/tex; mode=display">\hat{x}^{(n)}=\frac{x^{(n)}-\min_n(x^{(n)})}{\max_n(x^{(n)})-\min_n(x^{(n)})}</script><p><strong>标准化</strong>：将每维特征都调整到标准正态分布：</p><script type="math/tex; mode=display">\hat{x}^{(n)}=\frac{x^{(n)}-\mu}{\sigma}</script><h2 id="3-4-梯度下降算法的局限性"><a href="#3-4-梯度下降算法的局限性" class="headerlink" title="3.4 梯度下降算法的局限性"></a>3.4 梯度下降算法的局限性</h2><p>上一篇笔记已经有提到该部分内容：</p><p>首先，<strong>梯度下降算法容易陷入局部最优，找不到全局最优解</strong>。</p><p>如下图所示，当梯度下降算法优化到local minima 时，前面有座高山，它的梯度是正的，优化算法会强迫我们往回走。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/梯度下降算法局部最优.png" alt></p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/梯度下降局部最优2.png" alt></p><p>这个问题在我们的线性回归模型里暂时遇不到，因为线性模型非常简单，是一个凸函数。但当我们试图利用梯度下降算法训练复杂模型，比如神经网络时，就很有可能遭遇该问题。</p><p>有很多方法能够解决梯度下降算法的全局最优解问题，包括调整每次行进的步长$\eta$，使其更有希望跨越“大山”等手段等等。</p><p>其次，梯度下降算法要求目标函数是可微的，这在某些情况下会<strong>产生相当大的计算代价</strong>。假设我们的问题有上百万维，计算二阶偏导数就需要上万亿（百万的平方）次！</p><p>再次，<strong>当训练数据相当多时，梯度下降算法会变得很慢</strong>。在实践中，为了计算梯度$\nabla L$，我们需要为了每个训练样本x单独计算梯度$\nabla L_x$，然后求平均值。这会花费很长时间。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：偏差和方差、模型选择、梯度下降算法。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h1 id=&quot;一、误差的来源：偏差与方差&quot;&gt;&lt;a href=&quot;#一、误差的来源：偏差与方差&quot; class=&quot;headerlink&quot;</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
    <category term="Gradient Descent" scheme="https://superlova.github.io/tags/Gradient-Descent/"/>
    
    <category term="Bias and Variance" scheme="https://superlova.github.io/tags/Bias-and-Variance/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】李宏毅2019ML课程-Note1</title>
    <link href="https://superlova.github.io/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/"/>
    <id>https://superlova.github.io/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/</id>
    <published>2022-10-10T12:57:00.000Z</published>
    <updated>2022-10-12T13:44:37.134Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：机器学习的概念和分类、利用线性模型解决回归问题以及梯度下降。<br><!--more---></p><h3 id="一、机器学习介绍"><a href="#一、机器学习介绍" class="headerlink" title="一、机器学习介绍"></a>一、<a href="https://www.bilibili.com/video/av59538266">机器学习介绍</a></h3><p>说一下我学这门课的初衷吧。其实机器学习的课程在研究生阶段就已经上过了，但是工作之后才发现有一些基础没搞懂或者已经遗忘，因此在学习新知识时存在障碍。恰逢Datawhale给了这次组队学习的机会，我便想与群友们一起把这门基础课程搞定。</p><h4 id="1-概要"><a href="#1-概要" class="headerlink" title="1. 概要"></a>1. 概要</h4><p>本节通俗易懂地介绍了机器学习的概念，介绍了AI的发展历史，以及与传统规则的区别。简单来说，<strong>机器学习</strong>是一种从有限数据中学习规律并对未知数据进行预测的方法。</p><p>刚开始讲了比较多的名词和概念。老师最后的图里很好地总结了本次课的内容：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/机器学习介绍01.png" alt></p><h4 id="首先，机器学习的过程分成三个步骤："><a href="#首先，机器学习的过程分成三个步骤：" class="headerlink" title="首先，机器学习的过程分成三个步骤："></a>首先，机器学习的过程分成三个步骤：</h4><ol><li>根据问题的不同，选择模型（function）；</li><li>根据模型的不同，定义能够度量学习效果的损失函数；</li><li>从有限数据中持续训练模型，使得损失函数最小。</li></ol><h4 id="其次，根据学习所需的数据是否存在标签，机器学习可以分成如下几类（图中蓝色框框的部分）："><a href="#其次，根据学习所需的数据是否存在标签，机器学习可以分成如下几类（图中蓝色框框的部分）：" class="headerlink" title="其次，根据学习所需的数据是否存在标签，机器学习可以分成如下几类（图中蓝色框框的部分）："></a>其次，根据学习所需的数据是否存在标签，机器学习可以分成如下几类（图中蓝色框框的部分）：</h4><ol><li>监督学习，指学习所需的数据是经过人工标注的；</li><li>无监督学习，指学习所需的数据不需人工标注；</li><li>其他学习方法，比如半监督学习、迁移学习、强化学习等。</li></ol><h4 id="在监督学习里，根据想要解决的问题不同，可分为如下几类（图中黄色框框的部分）："><a href="#在监督学习里，根据想要解决的问题不同，可分为如下几类（图中黄色框框的部分）：" class="headerlink" title="在监督学习里，根据想要解决的问题不同，可分为如下几类（图中黄色框框的部分）："></a>在监督学习里，根据想要解决的问题不同，可分为如下几类（图中黄色框框的部分）：</h4><ol><li>分类问题，指我们希望模型给出yes或no这样具体的评价；</li><li>回归问题，指我们希望模型能够给出一个数值，这个数值的大小有现实意义；</li><li>结构化问题，我们希望模型直接输出结构化结果，比如语言翻译模型能够产出一段文本，dall-e能够生成图像，等等。</li></ol><h4 id="分类问题是比较简单的问题，有很多模型可以解决分类问题（图中绿色框框的部分）："><a href="#分类问题是比较简单的问题，有很多模型可以解决分类问题（图中绿色框框的部分）：" class="headerlink" title="分类问题是比较简单的问题，有很多模型可以解决分类问题（图中绿色框框的部分）："></a>分类问题是比较简单的问题，有很多模型可以解决分类问题（图中绿色框框的部分）：</h4><ol><li>线性模型；</li><li>非线性模型，比如深度学习、SVM、决策树等方法。</li></ol><p>后续我们会学到什么是线性模型、什么是非线性模型，以及上面的具体模型的设计细节。</p><h4 id="为什么要学习机器学习这门课程？"><a href="#为什么要学习机器学习这门课程？" class="headerlink" title="为什么要学习机器学习这门课程？"></a>为什么要学习机器学习这门课程？</h4><p><del>当然是为了挣钱了</del></p><p>由于目前还没有一个普适的学习模型，能够解决世界一切可以用机器学习方法解决的问题，因此，我们需要依赖经验和知识，来根据不同的问题，选择不同的学习模型和和损失函数。还记得机器学习的三个步骤吗？选择模型、定义损失函数、训练模型过程的知识，都是能够帮助我们得到更加可靠的机器学习系统的技能。学习这门课程，能够帮助我们成为一名更好的机器学习工程师。</p><h3 id="二、机器学习案例——回归问题"><a href="#二、机器学习案例——回归问题" class="headerlink" title="二、机器学习案例——回归问题"></a>二、<a href="https://www.bilibili.com/video/BV1Ht411g7Ef?p=3">机器学习案例——回归问题</a></h3><p>这部分以一个回归问题为引子，引导我们探索机器学习的三大步骤。回归问题比较普遍，像股票预测、温度预测、房价预测等，都是回归问题。</p><p>机器学习的三大步骤分别为：</p><ol><li>根据问题的不同，选择模型（function）；</li><li>根据模型的不同，定义能够度量学习效果的损失函数；</li><li>从有限数据中持续训练模型，使得损失函数最小。</li></ol><h4 id="第一步，为回归问题选择合适的模型"><a href="#第一步，为回归问题选择合适的模型" class="headerlink" title="第一步，为回归问题选择合适的模型"></a>第一步，为回归问题选择合适的模型</h4><p>这里我们使用线性模型试试水。所谓<strong>线性模型</strong> (Linear Model) 是把输入数据的各种特征，通过线性组合的方式进行预测。</p><script type="math/tex; mode=display">y=b+\Sigma\omega_{i}x_{i}</script><p>其中y是预测值，x是特征本身，w是特征对应的参数。我们的学习目标就是找到正确的w，令预测值y尽可能靠谱。</p><h4 id="第二步，为线性模型选择合适的评估函数"><a href="#第二步，为线性模型选择合适的评估函数" class="headerlink" title="第二步，为线性模型选择合适的评估函数"></a>第二步，为线性模型选择合适的评估函数</h4><p>知错能改，善莫大焉。每当模型预测得到一个值，为了让模型认识到自己的预测值y与真实值$\hat{y}$的差距，我们不妨直接取二者之差，作为计算差距的<strong>损失函数</strong> (Loss Function)。</p><script type="math/tex; mode=display">L(f)=\sum (y-\hat{y})</script><p>这样肯定是有问题的，假设我们有两组数据，一组超过真实值0.5，另一组低于预测值0.5，它们与真实值的差值分别是0.5与-0.5。结果经过我们的计算，损失函数居然为0，也就是没有损失？</p><p>为了避免上述情况的出现，我们选择平方损失函数作为衡量差距的手段：</p><script type="math/tex; mode=display">L(f)=\sum (y-\hat{y})^2</script><p>其实也有其他的损失函数定义方法可以规避第一个损失函数的问题，比如使用绝对值。但这里我们就钦定平方损失函数了，它有很多好处，但是我们先按下不表。</p><h4 id="第三步，进行训练，并利用损失函数指导训练过程"><a href="#第三步，进行训练，并利用损失函数指导训练过程" class="headerlink" title="第三步，进行训练，并利用损失函数指导训练过程"></a>第三步，进行训练，并利用损失函数指导训练过程</h4><p>到目前为止，我们有很多带标签的数据 $(x,\hat{y})$，有线性模型，有损失函数。那我该怎么得到训练好的模型呢？</p><p><strong>最好的模型</strong>到底是什么？对于线性模型来说，其实求解最优的$w$和$b$，从而可以让我们的模型无论输入什么$x$，都能准确得到与真实值相差无几的$y$。</p><p>让我们忘掉w和b的具体含义、晦涩不清的L函数，专心解决这个问题：如何优化$w$和$b$，以便L达到最小？</p><script type="math/tex; mode=display">w^*=arg\min_{w}L(w) \\b^*=arg\min_{b}L(b)</script><p>一种解决问题的方法使用微积分来嗯算，通过计算导数去寻找L的极值点。运气好的话，L的变量不多，该方法看似可行；运气不好的话，我们面对的问题过于复杂、参数过多，问题就不好解决了。</p><p>还有一种笨办法是穷举所有可能的$w$，选择能使L最小的$w^*$即可。这种方法虽然可行，但是没有效率。</p><p>有一种通用的最优化方法，叫做<strong>梯度下降</strong> (Gradient Descent) 方法，专门用于解决这种凸优化问题。使用梯度下降算法的前提是优化目标是可微的。</p><h5 id="1-梯度下降算法简介"><a href="#1-梯度下降算法简介" class="headerlink" title="1. 梯度下降算法简介"></a>1. 梯度下降算法简介</h5><p>恰好，我们的损失函数L是可微的，也就是二阶可导的。如果我们当初选取损失函数时，使用绝对值作为损失函数，处理起来便没有这么便利了。</p><p>梯度下降的具体过程是这样的：</p><ol><li>选择一个初始$w_0$</li><li>计算该位置下w对L的微分，这个微分对应函数在此处下降最快的方向；</li><li>将$w_0$朝着这个方向移动一小步$ \eta$<script type="math/tex; mode=display">w\prime=w_0-\eta\frac{\mathrm{d}L}{\mathrm{d}w}</script></li><li>在新的位置开始新一轮迭代计算，直到$w$不再变化为止。</li></ol><p>不妨将梯度下降算法的求解过程想象为人下山的过程，人会先找到下山最快的方向，然后朝着那个方向走一步，直到抵达最低点。</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/模拟梯度下降.png" alt></p><p>在上图中，每一条线围成的圈就是等高线，代表损失函数的值，颜色约深的区域代表的损失函数越小；红色的箭头代表等高线的法线方向。</p><p>上述过程是求解L在变量w下的最优解的梯度下降过程，但是L还与偏移量b存在对应关系，所以实际上梯度下降在线性模型的具体公式如下：</p><script type="math/tex; mode=display">w_k\rightarrow w'_k=w_k-\eta\frac{\partial C}{\partial w_k} \\b_l\rightarrow b'_l=b_l-\eta\frac{\partial C}{\partial b_l}</script><p>其中，偏微分的具体公式如下：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/线性回归偏微分方程.png" alt></p><h5 id="2-梯度下降算法的局限性"><a href="#2-梯度下降算法的局限性" class="headerlink" title="2. 梯度下降算法的局限性"></a>2. 梯度下降算法的局限性</h5><p>首先，<strong>梯度下降算法容易陷入局部最优，找不到全局最优解</strong>。</p><p>如下图所示，当梯度下降算法优化到local minima 时，前面有座高山，它的梯度是正的，优化算法会强迫我们往回走。</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/梯度下降算法局部最优.png" alt></p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/梯度下降局部最优2.png" alt></p><p>这个问题在我们的线性回归模型里暂时遇不到，因为线性模型非常简单，是一个凸函数。但当我们试图利用梯度下降算法训练复杂模型，比如神经网络时，就很有可能遭遇该问题。</p><p>有很多方法能够解决梯度下降算法的全局最优解问题，包括调整每次行进的步长$\eta$，使其更有希望跨越“大山”等手段等等。</p><p>其次，梯度下降算法要求目标函数是可微的，这在某些情况下会<strong>产生相当大的计算代价</strong>。假设我们的问题有上百万维，计算二阶偏导数就需要上万亿（百万的平方）次！</p><p>再次，<strong>当训练数据相当多时，梯度下降算法会变得很慢</strong>。在实践中，为了计算梯度$\nabla L$，我们需要为了每个训练样本x单独计算梯度$\nabla L_x$，然后求平均值。这会花费很长时间。</p><h4 id="检验：如何检验模型是否训练成功？更加复杂的模型以及对应的风险。"><a href="#检验：如何检验模型是否训练成功？更加复杂的模型以及对应的风险。" class="headerlink" title="检验：如何检验模型是否训练成功？更加复杂的模型以及对应的风险。"></a>检验：如何检验模型是否训练成功？更加复杂的模型以及对应的风险。</h4><p>经过一轮又一轮的优化，我们终于求得了该问题下线性回归模型的最优参数$w$和$b$。接下来该如何检验模型在未知数据上的泛化能力呢？</p><p>我们可以在训练之前，把数据集分成两部分，一部分用作后续的训练，另一部分用作最后的验收。如果在这部分测试数据上的表现与训练过程中的表现一致，那我们就可以验收。</p><p>这种通过划分数据集为训练集和测试集的方法，与其说是一种技术细节，不如说是一种工程实践上的经验。通过训练集和测试集的性能比较，我们可以发现模型存在的潜在问题：</p><p><strong>1. 训练集和测试集上的表现都不太高；</strong></p><p>这种情况我们称之为欠拟合。线性模型由于过于简单，当面对一些较复杂的现实问题时，欠拟合便出现了。你会发现无论如何训练，无论投入多少数据都很难提升模型的性能了。我们需要更加复杂的模型，然后进行上面所说的机器学习三个步骤：选择新模型，选择合适的损失函数，选择优化方法进行训练。</p><p>对于一些现实问题，类似$y=wx+b$这种一次模型确实过于简单了。比如预测房价，可能与房屋面积有关，也可以与房屋面积的平方有关，甚至是三次方、四次方。</p><p>课堂上老师举了一个例子，用一次函数模型预测宝可梦数据集，效果如下：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦1.png" alt></p><p>模型在测试集上的误差平均值为 35.0</p><p>当我们使用更复杂的2次模型时，模型性能有明显好转：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦2.png" alt></p><p>模型的图像从一次函数变为二次函数了，更好拟合了训练集和测试集。测试集平均误差降至 18.4 了。那是不是模型越复杂，性能越好呢？</p><p><strong>2. 训练集上的表现良好，测试集上的表现很差</strong></p><p>当我们持续优化模型复杂度到3次方函数、4次方函数时，测试集平均误差开始停滞，4次方函数的平均误差不降反增：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦3.png" alt></p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦4.png" alt></p><p>训练集平均误差【15.4】【15.3】【14.9】【12.8】<br>测试集平均误差【18.4】【18.1】【28.8】【232.1】</p><p>这种情况称之为过拟合。所谓“过犹不及”，那我们应该怎样选择模型的复杂度，避免过拟合呢？我们可以将模型复杂度与测试集性能之间的关系绘制成图像，通过寻找图像的“拐点”来决定模型的复杂度。</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦性能变化.png" alt></p><h4 id="反思：改进模型性能的其他方法"><a href="#反思：改进模型性能的其他方法" class="headerlink" title="反思：改进模型性能的其他方法"></a>反思：改进模型性能的其他方法</h4><p>上面的宝可梦CP值预测问题，老师之后给出了更多的数据，绘制在坐标图上：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦5.png" alt></p><p>很显然，任何曲线的线性模型都没办法拟合这种数据点。这是哪里出了问题？答案是我们忽略了数据点的其他特征。当我们引入其他特征到模型后，线性模型的表达能力就增强了。下面就是引入了宝可梦种类的特征：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦6.png" alt></p><p>但是当我们继续引入一些无关特征，比如宝可梦的性别、年龄等，过拟合现象再次出现：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦7.png" alt></p><p>这是由于在训练过程中，某些特征的权重过大导致的。一种直观的解释是，模型训练过程也会偷懒！如果数据量不足，模型会自动选取某几个影响力大的特征，赋予较高的权值，然后忽略其他特征，那些被忽略的特征中可能包含更加有用的信息。</p><p>我们希望最终得到的模型不要出现太大的权重，为了防止某些特征的权值过大，限制权重的增长速度，可以使用正则化方法。</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/正则化1.png" alt></p><p>改造后的损失函数，末尾添加上了一个正则化项，这个正则化项的大小仅与模型的参数大小有关。参数越大，正则化项越大。有了正则化项，模型在优化过程就会更倾向于选择参数值更小的模型了。</p><p>在很多应用场景中，并不是 $w$ 越小模型越平滑越好，但是经验值告诉我们 $w$ 越小大部分情况下都是好的。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>机器学习的概念、分类和三个步骤：选择模型、选择损失函数、选择优化方法；</li><li>根据三个步骤，实践解决回归问题：使用线性模型，解决宝可梦CP值预测问题；</li></ol><p>这里的重点是梯度下降算法，后面还会继续学习该算法。</p><ol><li>通过分析例子的优化点，引出模型常用的优化方法以及风险：过拟合、欠拟合，以及他们的解决方案。</li></ol><p>过拟合问题的解决方法有：降低模型复杂度、增加训练样本、增加训练样本的特征、添加正则化项等。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：机器学习的概念和分类、利用线性模型解决回归问题以及梯度下降。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h3 id=&quot;一、机器学习介绍&quot;&gt;&lt;a href=&quot;#一、机器学习介绍&quot; class=&quot;headerlin</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】机器学习编译——Note2</title>
    <link href="https://superlova.github.io/2022/09/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E2%80%94%E2%80%94Note2/"/>
    <id>https://superlova.github.io/2022/09/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E2%80%94%E2%80%94Note2/</id>
    <published>2022-09-13T08:13:40.000Z</published>
    <updated>2022-09-14T06:48:32.068Z</updated>
    
    <content type="html"><![CDATA[<p>陈天奇老师主讲的机器学习编译相关的课程，本节课讨论张量程序抽象以及相关实现。<br><!--more---></p><p>机器学习编译的过程可以看做是对张量函数的变换过程。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;陈天奇老师主讲的机器学习编译相关的课程，本节课讨论张量程序抽象以及相关实现。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;p&gt;机器学习编译的过程可以看做是对张量函数的变换过程。&lt;/p&gt;
</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="Compilation" scheme="https://superlova.github.io/tags/Compilation/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】职业素养</title>
    <link href="https://superlova.github.io/2022/08/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%81%8C%E4%B8%9A%E7%B4%A0%E5%85%BB/"/>
    <id>https://superlova.github.io/2022/08/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%81%8C%E4%B8%9A%E7%B4%A0%E5%85%BB/</id>
    <published>2022-08-01T03:26:14.000Z</published>
    <updated>2022-08-02T13:32:34.818Z</updated>
    
    <content type="html"><![CDATA[<p>有关新人培养手册的一些阅读体会和思考<br><!--more---></p><h2 id="执行任务并漂亮地完成"><a href="#执行任务并漂亮地完成" class="headerlink" title="执行任务并漂亮地完成"></a>执行任务并漂亮地完成</h2><p>分析问题、解决问题是工程师的日常工作，如何对待工作、如何执行工作，反映了一个工程师的基本素质。</p><p>不同级别的工程师适合处理不同难度的任务。根据任务难度，可以分类为：</p><ol><li>任务已经拆解完毕，我只负责某个简单子任务的执行；</li><li>任务已经拆解完毕，我负责各个步骤的执行，或者其中较困难的部分；</li><li>复杂的事情需要自己做拆解然后执行。</li></ol><p>一般我们把简单的执行任务交付给 T5 左右级别工程师去执行，把复杂的任务交给 T7 左右的工程师执行。而 T8 及以上的工程师，我们希望能独立完成任务拆解并执行。T10 以上的工程师，我们希望能够承担一个技术方向上的难题。</p><p>无论任务难度如何，工程师还是要根据结果的完成度来评价。根据结果分类：</p><ul><li>最低评价：不能按时完成任务；</li><li>较低评价：按时完成任务，但是质量较差；</li><li>合格评价：按时完成任务，质量合格。</li></ul><h2 id="独立思考"><a href="#独立思考" class="headerlink" title="独立思考"></a>独立思考</h2><p>什么叫做独立思考？工作是由思考和执行两部分组成的，任务的分析和拆解过程、出现问题后的总结和复盘、对现有技术方案的调研，都算是独立思考。</p><p>比如说，当我负责的模块出现 Bug，或者我的团队负责的模块报警时，进行分析问题、总结和复盘的过程，算是独立思考；<br>比如说，项目的技术方案不够完美或存在缺陷，我对现有技术方案进行调研，算是独立思考；<br>比如说，我对问题进行分析和总结，经常反思是否已经无可挑剔、不会被人挑毛病，算是独立思考。</p><p>独立思考代表着三个工程师宝贵的能力：责任心、事业心和硬实力。一个高绩效的工程师，往往有以下特征：</p><ol><li>工作态度上，对业务负责，从来不会消极怠工，负责的模块出现事故很少；</li><li>做事方法上，规范、及时沟通，事情交给他很放心，不会把自己当成工具人；</li><li>做事效率上，想得清楚做得快，紧急问题得交给他，做事不会反反复复；</li><li>技术实力上，此方向的任何问题都可以咨询他。</li></ol><h2 id="协作沟通"><a href="#协作沟通" class="headerlink" title="协作沟通"></a>协作沟通</h2><p>以上几点是工程师个人的能力评价，但是在公司中免不了要与其他工程师打交道，也就是沟通。</p><p>协作沟通时有一些常见的问题，新手工程师可能不太了解，需要琢磨一下；在此我把一些定律列举出来，方便直接照做：</p><ol><li>收到消息必回复，塑造收到消息及时回复的形象；不仅如此，在自己负责通知时，也要确认相关责任人收到消息。</li><li>沟通时先想好诉求，问对了问题是成功的一半；要有礼貌，先带好称呼，再组织语言，逻辑通顺；要确认下解决问题的时间节点，并且将讨论的共识发到群里进行同步；</li><li>要及时跟进问题，就比如说当你找对方要时间节点时，可能会遇到：“还不知道，需要评估”、“事情太多，不好说”、等等类似的无法给出时间点的情况，这个时候就需要及时跟进。及时跟进的做法不是每过一段时间就问一次，那样很招人烦；正确做法是一起拆分问题，对方一旦完成子任务，就通知自己。</li><li>即便你负责的是一个项目里的一小部分，你也需要知道整个项目的计划节奏。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;有关新人培养手册的一些阅读体会和思考&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h2 id=&quot;执行任务并漂亮地完成&quot;&gt;&lt;a href=&quot;#执行任务并漂亮地完成&quot; class=&quot;headerlink&quot; title=&quot;执行任务并漂亮地完成&quot;&gt;&lt;/a&gt;执行任务并漂亮地完成&lt;/h2</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Career" scheme="https://superlova.github.io/tags/Career/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】如何实现模型的热更新？</title>
    <link href="https://superlova.github.io/2022/07/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%83%AD%E6%9B%B4%E6%96%B0%EF%BC%9F/"/>
    <id>https://superlova.github.io/2022/07/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%83%AD%E6%9B%B4%E6%96%B0%EF%BC%9F/</id>
    <published>2022-07-22T03:11:37.000Z</published>
    <updated>2022-08-02T13:32:34.801Z</updated>
    
    <content type="html"><![CDATA[<p>陈天奇老师主讲的机器学习编译相关的课程，回答了机器学习模型是如何从开发状态到部署状态的。整个过程类似源代码到可执行程序的编译过程，因此得名机器学习编译。据他本人所述，这门课是目前全世界第一门主讲机器学习编译的课程。<br><!--more---></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;陈天奇老师主讲的机器学习编译相关的课程，回答了机器学习模型是如何从开发状态到部署状态的。整个过程类似源代码到可执行程序的编译过程，因此得名机器学习编译。据他本人所述，这门课是目前全世界第一门主讲机器学习编译的课程。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="Compilation" scheme="https://superlova.github.io/tags/Compilation/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】机器学习编译——Note1</title>
    <link href="https://superlova.github.io/2022/07/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E2%80%94%E2%80%94Note1/"/>
    <id>https://superlova.github.io/2022/07/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E2%80%94%E2%80%94Note1/</id>
    <published>2022-07-04T02:36:43.000Z</published>
    <updated>2022-08-02T13:32:34.809Z</updated>
    
    <content type="html"><![CDATA[<p>陈天奇老师主讲的机器学习编译相关的课程，回答了机器学习模型是如何从开发状态到部署状态的。整个过程类似源代码到可执行程序的编译过程，因此得名机器学习编译。据他本人所述，这门课是目前全世界第一门主讲机器学习编译的课程。<br><!--more---></p><p>《机器学习编译》是陈天奇老师的公开课。目前在Bilibili上更新视频。<a href="https://space.bilibili.com/1663273796">这里</a>是陈天奇老师在Bilibili的个人主页，如果对课程感兴趣的小伙伴可以免费观摩。</p><p>另外课件地址在这里：<a href="https://mlc.ai/zh/chapter_introduction/">Machine Learning Compilation</a></p><h1 id="一、什么是机器学习编译？学习这门课的意义是什么？"><a href="#一、什么是机器学习编译？学习这门课的意义是什么？" class="headerlink" title="一、什么是机器学习编译？学习这门课的意义是什么？"></a>一、什么是机器学习编译？学习这门课的意义是什么？</h1><p><strong>机器学习编译</strong> (machine learning compilation, MLC) 是指，将机器学习算法从开发阶段，通过变换和优化算法，使其变成部署状态。</p><p>开发阶段：用 PyTorch、TensorFlow 或 JAX 等通用框架编写的模型描述 + 对应的权重文件。</p><p>部署状态：在考虑支撑代码、内存管理、不同语言的开发接口等因素后，成功执行在设备上的状态。</p><p>该过程类似于把源代码转换为可执行文件的程序编译过程，但是二者有比较大的不同。</p><p>首先是目标不同，机器学习编译的目标有：</p><ul><li>集成打包 + 最小化依赖，即将必要的元素组合在一起以用于部署应用程序；</li><li>利用硬件加速，利用硬件本身的特性进行加速；</li><li>通用优化，以最小化内存使用或提高执行效率为目标转换模型执行方式。</li></ul><p>这些目标没有严格的界限。</p><p>其次，这个过程不一定涉及代码生成；例如将开发状态转化为部署形式，可能只是将抽象的模型定义，转化为对某几个预定义的库函数的调用。</p><p>最后，遇到的挑战和解决方案也大不相同。随着硬件和模型种类的增长，机器学习编译难以表示单一稳定的解决方案。</p><p>那么，我们能够从这门课学习到什么？</p><p>对于在从事机器学习工作工程师，机器学习编译提供了以基础的解决问题的方法和工具。它有助于回答我们可以采用什么方法来特定模型的部署和内存效率，如何将优化模型的单个部分的经验推广到更端到端解决方案等一系列问题。</p><p>对于机器学习科学家，学习机器学习编译可以更深入地了解将模型投入生产所需的步骤。机器学习框架本身隐藏了一些技术复杂性，但是当我们尝试开始部署新模型或将模型部署到框架支持不完善的平台时，仍然会面临巨大的挑战。机器学习编译使机器学习算法科学家有机会了解背后的基本原理，并且知晓为什么我的模型的运行速度不及预期，以及如何来使部署更有效。</p><p>最后，学习 MLC 本身很有趣。借助这套现代机器学习编译工具，我们可以进入机器学习模型从高级、代码优化到裸机的各个阶段。端到端 (end to end) 地了解这里发生的事情并使用它们来解决我们的问题。</p><h1 id="二、机器学习编译的关键要素"><a href="#二、机器学习编译的关键要素" class="headerlink" title="二、机器学习编译的关键要素"></a>二、机器学习编译的关键要素</h1><h2 id="1-张量和张量函数"><a href="#1-张量和张量函数" class="headerlink" title="1. 张量和张量函数"></a>1. 张量和张量函数</h2><p>张量 (Tensor) 是执行中最重要的元素。张量是表示神经网络模型执行的输入、输出和中间结果的多维数组。</p><p>张量函数 (Tensor functions) 指接受张量和输出张量的计算序列。</p><p>下面这张图展示了机器学习编译过程中，两种不同形式的张量函数的变换过程。从左边比较抽象的表示形式，转换为右侧较为具体的表示形式。</p><p><img src="https://mlc.ai/zh/_images/mlc-elem-transform.png" alt="机器学习编译过程中的张量函数变换"></p><p>机器学习编译的过程就是是将上图左侧的内容转换为右侧的过程。在不同的场景中，这个过程可以是手动完成的，也可以使用一些自动转换工具，或两者兼而有之。</p><h2 id="2-抽象和实现"><a href="#2-抽象和实现" class="headerlink" title="2. 抽象和实现"></a>2. 抽象和实现</h2><p>上一部分提到了抽象和实现。对于同样的目标，我们会有不同的 抽象表现，但是不同抽象表示有些细节不同。我们会把更细化的抽象表示称为原有抽象表示的一个具体实现。</p><p>抽象和实现可能是所有计算机系统中最重要的关键字。抽象指定“做什么”，实现提供“如何”做。没有具体的界限。根据我们的看法，for 循环本身可以被视为一种抽象，因为它可以使用 python 解释器实现或编译为本地汇编代码。</p><p>MLC 实际上是在相同或不同抽象下转换和组装张量函数的过程。</p><p>本课程会介绍四种不同形式的抽象表示</p><ul><li>计算图的抽象</li><li>张量程序的抽象</li><li>算子库和运行时的抽象</li><li>硬件层面的抽象</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;陈天奇老师主讲的机器学习编译相关的课程，回答了机器学习模型是如何从开发状态到部署状态的。整个过程类似源代码到可执行程序的编译过程，因此得名机器学习编译。据他本人所述，这门课是目前全世界第一门主讲机器学习编译的课程。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;p&gt;《机器学习编</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="Compilation" scheme="https://superlova.github.io/tags/Compilation/"/>
    
  </entry>
  
</feed>
