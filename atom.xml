<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Superlova</title>
  
  <subtitle>Welcome...</subtitle>
  <link href="https://superlova.github.io/atom.xml" rel="self"/>
  
  <link href="https://superlova.github.io/"/>
  <updated>2022-10-15T01:44:58.532Z</updated>
  <id>https://superlova.github.io/</id>
  
  <author>
    <name>Superlova</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【学习笔记】李宏毅2019ML课程-Note2</title>
    <link href="https://superlova.github.io/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/"/>
    <id>https://superlova.github.io/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/</id>
    <published>2022-10-13T02:52:43.000Z</published>
    <updated>2022-10-15T01:44:58.532Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：偏差和方差、模型选择、梯度下降算法。<br><!--more---></p><h1 id="一、误差的来源：偏差与方差"><a href="#一、误差的来源：偏差与方差" class="headerlink" title="一、误差的来源：偏差与方差"></a>一、误差的来源：偏差与方差</h1><p>对学习算法除了通过实验估计其泛化性能，人们往往还希望能了解为什么具有这样的性能。误差来源于偏差 (bias) 和方差 (Variance)以及不可避免的噪声。</p><h2 id="1-偏差和方差的概念和来源"><a href="#1-偏差和方差的概念和来源" class="headerlink" title="1. 偏差和方差的概念和来源"></a>1. 偏差和方差的概念和来源</h2><p><strong>偏差</strong>度量了预测值与真实值的偏离程度，对应的是学习算法本身的拟合能力；<strong>方差</strong>度量了数据扰动对模型的影响，对应的是模型的稳定性；<strong>噪声</strong>则是对应问题对应的难度。</p><p>上面的结论说明，模型的性能是由模型能力、数据的充分性以及问题本身的难度决定的。由于噪音是问题本身的特性，不好解决，因此要想提升模型的性能，就需要采取措施降低偏差和方差。</p><h2 id="4-偏差和方差分解：重新考虑欠拟合、过拟合问题"><a href="#4-偏差和方差分解：重新考虑欠拟合、过拟合问题" class="headerlink" title="4. 偏差和方差分解：重新考虑欠拟合、过拟合问题"></a>4. 偏差和方差分解：重新考虑欠拟合、过拟合问题</h2><p>为了避免过拟合，我们经常会在模型的拟合能力和复杂度之间进行权衡。拟合能力强的模型一般复杂度会比较高，容易导致过拟合。相反，如果限制模型的复杂度，降低其拟合能力，有可能导致欠拟合。因此，如何在模型的拟合能力与复杂度之间取得平衡，对机器学习算法来说十分重要。</p><p>偏差-方差分解为我们提供了一个很好用的分析工具。数学推导过程比较复杂，结论为：</p><p><strong>最小化期望错误等价于最小化偏差和方差之和。</strong></p><p>下图给出了机器学习模型四种偏差和方差的组合情况。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/偏差和方差.png" alt></p><p>每个图的中心店为最优模型$f(x)$，蓝点为不同训练集上得到的模型$f^*(x)$。左上角的图是一种理想情况，方差和偏差都比较低；右上角为高偏差低方差的情况，表示模型的泛化能力很好，但是拟合能力不足，类似于我们之前的线性模型；左下角是低偏差高方差的情况，表示模型的拟合能力很好，但是泛化能力较差。当训练数据比较少的时候往往会出现这种情况，我们一般把他称之为过拟合；右下角为高偏差高方差的情况，是最差的情况，等于没训练。</p><p>方差一般会随着训练样本的增加而减小。当样本比较多，方差比较小，这是可以选择能力强的模型来减少偏差；当训练集比较有限时，最优的偏差和方差往往无法兼顾，我们称之为“偏差-方差窘境”。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/偏差方差窘境.png" alt></p><p>偏差-方差窘境说的是，模型复杂度增加后，虽然你和能力变强导致偏差减少，但是方差会增大，过拟合现象会导致性能下降。</p><p>偏差-方差分解给机器学习模型提供了一种分析途径，能够指导我们解决过拟合和欠拟合的问题，但是实际操作中很难直接衡量。下面是一些通用的方法。</p><pre><code>欠拟合现象：模型在训练集上的错误率很高，此时说明模型偏差较大。欠拟合解决：1. 增加数据特征2. 提高模型复杂度3. 减小正则化系数</code></pre><pre><code>过拟合现象：模型在训练集上的错误率较低，但是在测试集上较高，此时说明模型方差较大。过拟合解决：1. 降低模型复杂度；2. 加大正则化系数；3. 引入先验知识，比如数据清洗，剔除无用特征等；4. 使用更多数据进行训练，但这个往往成本最高。</code></pre><p>此外，还有一种降低方差的方法：集成模型，即通过多个高方差模型进行平均，来降低方差。这背后的原理在介绍集成学习时会介绍。</p><h1 id="二、模型选择方法"><a href="#二、模型选择方法" class="headerlink" title="二、模型选择方法"></a>二、模型选择方法</h1><p>前面讨论了机器学习的三个步骤，以及如何改进模型的性能。那么我们如何正确评估模型呢？</p><p>之前的例子中，我们把数据集分成两部分，Training Set、Test Set。一般比例控制在4:1左右。我们在训练结束后，使用测试集进行评估，并判断训练效果。</p><p>这样做其实存在问题。如果我们真的使用测试集来指导模型的选择、参数的改进等各个步骤，那算不算模型正在学习测试集的内容呢？就好像学生做完考卷后，老师虽然不会直接告诉学生正确答案，但是会不断地给学生机会，告诉你：你这里错了，那里错了。这算不算是另一种泄题呢？</p><p>上面的数据泄露现象是经常发生的。为了避免数据泄露，一般可以通过把数据分成三部分：训练集 (Training Set)、验证集 (Validation Set) 和测试集 (Test Set)。模型参数的改进、模型选择等过程，只使用验证集；最后评估模型时使用测试集。</p><p>但是这样又会带来另一个问题：本来数据就匮乏，又分割出两大部分不能使用，可供学习的数据就更少了。<strong>k折交叉验证</strong>可以解决这个问题。</p><h2 id="2-1-交叉验证"><a href="#2-1-交叉验证" class="headerlink" title="2.1 交叉验证"></a>2.1 交叉验证</h2><p>交叉验证 (Cross-Validation) 是一种评估泛化性能的统计学方法。它比单次划分训练集和测试集的方法更加稳定。最常用的交叉验证方法是<strong>k折交叉验证</strong>。</p><p>首先把数据集分成大致相等的k个部分，每一部分叫做一折；接下来训练k个模型，第一个模型以第1折数据作为测试集，其他作为训练集；第二个模型以第2折数据作为测试集，其他作为训练集……最后我们得到了k个模型，以及对应的k个精度值。模型的最终精度就是这k个精度值的平均值。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/交叉验证.png" alt></p><p>交叉验证的优点：</p><ol><li>帮助更好评估模型的真实泛化性能，防止数据集分割时的随机性影响评估准确率；</li><li>所有数据都有机会被训练，提高数据利用率；</li></ol><p>交叉验证的缺点：</p><ul><li>显著增加了计算成本，需要训练k个模型，是原来的k倍。</li></ul><h1 id="三、梯度下降算法详解，以及优化方法"><a href="#三、梯度下降算法详解，以及优化方法" class="headerlink" title="三、梯度下降算法详解，以及优化方法"></a>三、梯度下降算法详解，以及优化方法</h1><p>目前，机器学习中参数学习的主要方式是通过梯度下降法来寻找一组最小化损失函数的参数。再具体视线中，梯度下降法可以分为：批量梯度下降、随机梯度下降以及小批量梯度下降三种形式。</p><p>本节还会介绍一些梯度下降算法的变种，他们大多改善了以下两部分的内容：1) 调整学习率，使优化更稳定；2) 梯度估计修正，提升训练速度。</p><h2 id="3-1-梯度下降算法的类别"><a href="#3-1-梯度下降算法的类别" class="headerlink" title="3.1 梯度下降算法的类别"></a>3.1 梯度下降算法的类别</h2><h3 id="3-1-1-批量梯度下降"><a href="#3-1-1-批量梯度下降" class="headerlink" title="3.1.1 批量梯度下降"></a>3.1.1 批量梯度下降</h3><p>最传统的梯度下降算法，上篇笔记已经介绍。</p><p>梯度下降的具体过程是这样的：</p><ol><li>选择一个初始$\theta_0$</li><li>计算该位置下$\theta$对L的微分，这个微分对应函数在此处下降最快的方向；</li><li>将$\theta_0$朝着这个方向移动一小步$ \eta$</li><li>在新的位置开始新一轮迭代计算，直到$w$不再变化为止。</li></ol><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/批量梯度下降法.png" alt></p><p>其中$\eta$叫做学习率，它控制了每次梯度下降迭代的优化幅度。$\eta$越大，学习得越快。然而$\eta$并不是越大越好。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/批量梯度下降法的梯度深谷.png" alt></p><p>如果$\eta$过大，优化过程很可能会跨越最低点，从而永远达不到最优解；如果$\eta$过小，则会造成训练缓慢，甚至陷入局部最优解。因此调整学习率$\eta$是优化梯度下降算法的一大思路。</p><p>梯度下降算法的另一个问题是，当训练数据很大时（这很常见），梯度下降算法会花费很长时间去遍历整个训练集。大多数情况下，我们不必遍历所有训练集。</p><h3 id="3-1-2-随机梯度下降"><a href="#3-1-2-随机梯度下降" class="headerlink" title="3.1.2 随机梯度下降"></a>3.1.2 随机梯度下降</h3><p>在训练机器学习模型时，训练数据的规模比较大的情况下，批量梯度下降的每次梯度迭代都要遍历整个数据集，这会造成计算资源的浪费。为了解决批量梯度下降算法导致的训练缓慢的问题，随机梯度下降算法应运而生。其思想是通过随机选取少量训练输入样本来计算$\nabla L_x$，进而估算$\nabla L$。</p><p>更准确地说，随机梯度下降通过随机选取少量的m个训练数据来训练。也叫做小批量梯度下降法 (Mini-Batch Gradient Descent)。</p><p>假设样本数量m足够大，我们期望$\nabla L<em>{X_j}$的平均值大致相等于整个$\nabla L</em>{x}$的平均值，即：</p><script type="math/tex; mode=display">\frac{\sum^m_{j-1}\nabla L_{X_j}}{m}\approx \frac{\sum_x \nabla L_x}{n}=\nabla L</script><p>这里第二个求和符号实在整个训练数据上进行的。交换两边我们得到：</p><script type="math/tex; mode=display">\nabla L \approx \frac{1}{m}\sum^m_{j=1}\nabla L_{X_j}</script><p>证实了我们可以进通过计算随机选取的小批量数据的梯度来估算整体的梯度。</p><p>影响小批量梯度下降的主要因素有：1）批大小m，2）学习率$\eta$，3）梯度估计方法。</p><h2 id="3-2-批量大小选择"><a href="#3-2-批量大小选择" class="headerlink" title="3.2 批量大小选择"></a>3.2 批量大小选择</h2><p>直观来说，批包含的数据越大，方差也就越小，训练越稳定。该种情况下，可以设置一个较大的学习率。学习率越大，需要的批大小就越大。</p><p>另外，根据经验，批越大越可能收敛到“尖锐最小值”；批越小越能收敛到“平坦最小值”。</p><h2 id="3-3-学习率调整"><a href="#3-3-学习率调整" class="headerlink" title="3.3 学习率调整"></a>3.3 学习率调整</h2><p>学习率过大会导致模型不收敛，如果过小会导致收敛太慢。由此，有一些学者根据学习的不同阶段，制定了学习率的不同变化策略。</p><p>从经验上看，学习率一开始要保持大一些，来保证收敛到最优点附近；然后要减小学习率来避免震荡。那么我们自然可以想到，学习率跟随训练轮次变大逐渐变小。这就是<strong>学习率衰减</strong>。</p><p>在刚开始训练时，由于参数是随机初始化的，梯度也往往比较大，再加上比较大的初始学习率，会使得训练不稳定。因此我们希望刚开始几轮迭代的学习率较小，等梯度下降到一定程度时再恢复学习率。这种方法称之为<strong>学习率预热</strong>。等预热完毕后在进行学习率衰减。</p><h3 id="3-3-3-AdaGrad-算法"><a href="#3-3-3-AdaGrad-算法" class="headerlink" title="3.3.3 AdaGrad 算法"></a>3.3.3 AdaGrad 算法</h3><p>AdaGrad算法的做法是，每次迭代时，每个参数的学习率都把它除上之前微分的均方根。</p><p>普通的梯度下降算法采用的参数更新思路：</p><script type="math/tex; mode=display">w^{t+1}\leftarrow w^t-\eta^tg^t \\\eta^t=\frac{\eta^t}{\sqrt{t + 1}}</script><p>则Adagram是这样更新的：</p><script type="math/tex; mode=display">w^{t+1}\leftarrow w^t-\frac{\eta^t}{\sigma^t}g^t \\g^t =\frac{\partial L(\theta^t)}{\partial w}</script><p>其中$\sigma^t$是之前参数的所有微分的均方根，对于每个参数都是不一样的。</p><p>Adagram的设计思路为，如果某个参数的偏导数累计比较大，其学习率就会相对较小；相反，如果偏导数累计较小，则学习率相对较大。但是整体上迭代次数越多，学习率越小。</p><p>AdaGram的缺点是，如果在经过一定次数的迭代，依然没有找到最优点时，由于学习率已经非常小，很难再继续优化了。</p><h3 id="3-3-4-RMSprop-算法"><a href="#3-3-4-RMSprop-算法" class="headerlink" title="3.3.4 RMSprop 算法"></a>3.3.4 RMSprop 算法</h3><h2 id="3-4-梯度估计"><a href="#3-4-梯度估计" class="headerlink" title="3.4 梯度估计"></a>3.4 梯度估计</h2><h3 id="3-4-1-动量法"><a href="#3-4-1-动量法" class="headerlink" title="3.4.1 动量法"></a>3.4.1 动量法</h3><h3 id="3-4-3-Adam-算法"><a href="#3-4-3-Adam-算法" class="headerlink" title="3.4.3 Adam 算法"></a>3.4.3 Adam 算法</h3><h2 id="3-3-数据预处理"><a href="#3-3-数据预处理" class="headerlink" title="3.3 数据预处理"></a>3.3 数据预处理</h2><p>一般而言，样本特征由于来源以及度量单位不同，他们的尺度 (Scale) 即取值范围也不同。对于某些机器学习模型，会对那些尺度更大的数据更敏感。因此对于尺度敏感的模型，必须先对样本进行预处理，将各个维度的特征转换到相同的取值区间内，并且消除不同特征之间的相关性，才能取得理想的效果。</p><p>归一化 (Normalization) 方法泛指把数据特征转换为相同尺度的方法，比如把数据特征映射到 [0,1] 区间内，或者直接映射为均值为0、方差为1的正态分布。</p><p><strong>最小最大值归一化</strong>：将每个特征缩放到 [0,1] 或者 [-1,1] 之间。假设我们有 N 哥样本 ${x^{(n)}}^{N}_{n=1}$，对每一维特征x，归一化后的特征为：</p><script type="math/tex; mode=display">\hat{x}^{(n)}=\frac{x^{(n)}-\min_n(x^{(n)})}{\max_n(x^{(n)})-\min_n(x^{(n)})}</script><p><strong>标准化</strong>：将每维特征都调整到标准正态分布：</p><script type="math/tex; mode=display">\hat{x}^{(n)}=\frac{x^{(n)}-\mu}{\sigma}</script><h2 id="3-4-梯度下降算法的局限性"><a href="#3-4-梯度下降算法的局限性" class="headerlink" title="3.4 梯度下降算法的局限性"></a>3.4 梯度下降算法的局限性</h2><p>上一篇笔记已经有提到该部分内容：</p><p>首先，<strong>梯度下降算法容易陷入局部最优，找不到全局最优解</strong>。</p><p>如下图所示，当梯度下降算法优化到local minima 时，前面有座高山，它的梯度是正的，优化算法会强迫我们往回走。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/梯度下降算法局部最优.png" alt></p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/梯度下降局部最优2.png" alt></p><p>这个问题在我们的线性回归模型里暂时遇不到，因为线性模型非常简单，是一个凸函数。但当我们试图利用梯度下降算法训练复杂模型，比如神经网络时，就很有可能遭遇该问题。</p><p>有很多方法能够解决梯度下降算法的全局最优解问题，包括调整每次行进的步长$\eta$，使其更有希望跨越“大山”等手段等等。</p><p>其次，梯度下降算法要求目标函数是可微的，这在某些情况下会<strong>产生相当大的计算代价</strong>。假设我们的问题有上百万维，计算二阶偏导数就需要上万亿（百万的平方）次！</p><p>再次，<strong>当训练数据相当多时，梯度下降算法会变得很慢</strong>。在实践中，为了计算梯度$\nabla L$，我们需要为了每个训练样本x单独计算梯度$\nabla L_x$，然后求平均值。这会花费很长时间。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：偏差和方差、模型选择、梯度下降算法。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h1 id=&quot;一、误差的来源：偏差与方差&quot;&gt;&lt;a href=&quot;#一、误差的来源：偏差与方差&quot; class=&quot;headerlink&quot;</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
    <category term="Gradient Descent" scheme="https://superlova.github.io/tags/Gradient-Descent/"/>
    
    <category term="Bias and Variance" scheme="https://superlova.github.io/tags/Bias-and-Variance/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】李宏毅2019ML课程-Note1</title>
    <link href="https://superlova.github.io/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/"/>
    <id>https://superlova.github.io/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/</id>
    <published>2022-10-10T12:57:00.000Z</published>
    <updated>2022-10-12T13:44:37.134Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：机器学习的概念和分类、利用线性模型解决回归问题以及梯度下降。<br><!--more---></p><h3 id="一、机器学习介绍"><a href="#一、机器学习介绍" class="headerlink" title="一、机器学习介绍"></a>一、<a href="https://www.bilibili.com/video/av59538266">机器学习介绍</a></h3><p>说一下我学这门课的初衷吧。其实机器学习的课程在研究生阶段就已经上过了，但是工作之后才发现有一些基础没搞懂或者已经遗忘，因此在学习新知识时存在障碍。恰逢Datawhale给了这次组队学习的机会，我便想与群友们一起把这门基础课程搞定。</p><h4 id="1-概要"><a href="#1-概要" class="headerlink" title="1. 概要"></a>1. 概要</h4><p>本节通俗易懂地介绍了机器学习的概念，介绍了AI的发展历史，以及与传统规则的区别。简单来说，<strong>机器学习</strong>是一种从有限数据中学习规律并对未知数据进行预测的方法。</p><p>刚开始讲了比较多的名词和概念。老师最后的图里很好地总结了本次课的内容：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/机器学习介绍01.png" alt></p><h4 id="首先，机器学习的过程分成三个步骤："><a href="#首先，机器学习的过程分成三个步骤：" class="headerlink" title="首先，机器学习的过程分成三个步骤："></a>首先，机器学习的过程分成三个步骤：</h4><ol><li>根据问题的不同，选择模型（function）；</li><li>根据模型的不同，定义能够度量学习效果的损失函数；</li><li>从有限数据中持续训练模型，使得损失函数最小。</li></ol><h4 id="其次，根据学习所需的数据是否存在标签，机器学习可以分成如下几类（图中蓝色框框的部分）："><a href="#其次，根据学习所需的数据是否存在标签，机器学习可以分成如下几类（图中蓝色框框的部分）：" class="headerlink" title="其次，根据学习所需的数据是否存在标签，机器学习可以分成如下几类（图中蓝色框框的部分）："></a>其次，根据学习所需的数据是否存在标签，机器学习可以分成如下几类（图中蓝色框框的部分）：</h4><ol><li>监督学习，指学习所需的数据是经过人工标注的；</li><li>无监督学习，指学习所需的数据不需人工标注；</li><li>其他学习方法，比如半监督学习、迁移学习、强化学习等。</li></ol><h4 id="在监督学习里，根据想要解决的问题不同，可分为如下几类（图中黄色框框的部分）："><a href="#在监督学习里，根据想要解决的问题不同，可分为如下几类（图中黄色框框的部分）：" class="headerlink" title="在监督学习里，根据想要解决的问题不同，可分为如下几类（图中黄色框框的部分）："></a>在监督学习里，根据想要解决的问题不同，可分为如下几类（图中黄色框框的部分）：</h4><ol><li>分类问题，指我们希望模型给出yes或no这样具体的评价；</li><li>回归问题，指我们希望模型能够给出一个数值，这个数值的大小有现实意义；</li><li>结构化问题，我们希望模型直接输出结构化结果，比如语言翻译模型能够产出一段文本，dall-e能够生成图像，等等。</li></ol><h4 id="分类问题是比较简单的问题，有很多模型可以解决分类问题（图中绿色框框的部分）："><a href="#分类问题是比较简单的问题，有很多模型可以解决分类问题（图中绿色框框的部分）：" class="headerlink" title="分类问题是比较简单的问题，有很多模型可以解决分类问题（图中绿色框框的部分）："></a>分类问题是比较简单的问题，有很多模型可以解决分类问题（图中绿色框框的部分）：</h4><ol><li>线性模型；</li><li>非线性模型，比如深度学习、SVM、决策树等方法。</li></ol><p>后续我们会学到什么是线性模型、什么是非线性模型，以及上面的具体模型的设计细节。</p><h4 id="为什么要学习机器学习这门课程？"><a href="#为什么要学习机器学习这门课程？" class="headerlink" title="为什么要学习机器学习这门课程？"></a>为什么要学习机器学习这门课程？</h4><p><del>当然是为了挣钱了</del></p><p>由于目前还没有一个普适的学习模型，能够解决世界一切可以用机器学习方法解决的问题，因此，我们需要依赖经验和知识，来根据不同的问题，选择不同的学习模型和和损失函数。还记得机器学习的三个步骤吗？选择模型、定义损失函数、训练模型过程的知识，都是能够帮助我们得到更加可靠的机器学习系统的技能。学习这门课程，能够帮助我们成为一名更好的机器学习工程师。</p><h3 id="二、机器学习案例——回归问题"><a href="#二、机器学习案例——回归问题" class="headerlink" title="二、机器学习案例——回归问题"></a>二、<a href="https://www.bilibili.com/video/BV1Ht411g7Ef?p=3">机器学习案例——回归问题</a></h3><p>这部分以一个回归问题为引子，引导我们探索机器学习的三大步骤。回归问题比较普遍，像股票预测、温度预测、房价预测等，都是回归问题。</p><p>机器学习的三大步骤分别为：</p><ol><li>根据问题的不同，选择模型（function）；</li><li>根据模型的不同，定义能够度量学习效果的损失函数；</li><li>从有限数据中持续训练模型，使得损失函数最小。</li></ol><h4 id="第一步，为回归问题选择合适的模型"><a href="#第一步，为回归问题选择合适的模型" class="headerlink" title="第一步，为回归问题选择合适的模型"></a>第一步，为回归问题选择合适的模型</h4><p>这里我们使用线性模型试试水。所谓<strong>线性模型</strong> (Linear Model) 是把输入数据的各种特征，通过线性组合的方式进行预测。</p><script type="math/tex; mode=display">y=b+\Sigma\omega_{i}x_{i}</script><p>其中y是预测值，x是特征本身，w是特征对应的参数。我们的学习目标就是找到正确的w，令预测值y尽可能靠谱。</p><h4 id="第二步，为线性模型选择合适的评估函数"><a href="#第二步，为线性模型选择合适的评估函数" class="headerlink" title="第二步，为线性模型选择合适的评估函数"></a>第二步，为线性模型选择合适的评估函数</h4><p>知错能改，善莫大焉。每当模型预测得到一个值，为了让模型认识到自己的预测值y与真实值$\hat{y}$的差距，我们不妨直接取二者之差，作为计算差距的<strong>损失函数</strong> (Loss Function)。</p><script type="math/tex; mode=display">L(f)=\sum (y-\hat{y})</script><p>这样肯定是有问题的，假设我们有两组数据，一组超过真实值0.5，另一组低于预测值0.5，它们与真实值的差值分别是0.5与-0.5。结果经过我们的计算，损失函数居然为0，也就是没有损失？</p><p>为了避免上述情况的出现，我们选择平方损失函数作为衡量差距的手段：</p><script type="math/tex; mode=display">L(f)=\sum (y-\hat{y})^2</script><p>其实也有其他的损失函数定义方法可以规避第一个损失函数的问题，比如使用绝对值。但这里我们就钦定平方损失函数了，它有很多好处，但是我们先按下不表。</p><h4 id="第三步，进行训练，并利用损失函数指导训练过程"><a href="#第三步，进行训练，并利用损失函数指导训练过程" class="headerlink" title="第三步，进行训练，并利用损失函数指导训练过程"></a>第三步，进行训练，并利用损失函数指导训练过程</h4><p>到目前为止，我们有很多带标签的数据 $(x,\hat{y})$，有线性模型，有损失函数。那我该怎么得到训练好的模型呢？</p><p><strong>最好的模型</strong>到底是什么？对于线性模型来说，其实求解最优的$w$和$b$，从而可以让我们的模型无论输入什么$x$，都能准确得到与真实值相差无几的$y$。</p><p>让我们忘掉w和b的具体含义、晦涩不清的L函数，专心解决这个问题：如何优化$w$和$b$，以便L达到最小？</p><script type="math/tex; mode=display">w^*=arg\min_{w}L(w) \\b^*=arg\min_{b}L(b)</script><p>一种解决问题的方法使用微积分来嗯算，通过计算导数去寻找L的极值点。运气好的话，L的变量不多，该方法看似可行；运气不好的话，我们面对的问题过于复杂、参数过多，问题就不好解决了。</p><p>还有一种笨办法是穷举所有可能的$w$，选择能使L最小的$w^*$即可。这种方法虽然可行，但是没有效率。</p><p>有一种通用的最优化方法，叫做<strong>梯度下降</strong> (Gradient Descent) 方法，专门用于解决这种凸优化问题。使用梯度下降算法的前提是优化目标是可微的。</p><h5 id="1-梯度下降算法简介"><a href="#1-梯度下降算法简介" class="headerlink" title="1. 梯度下降算法简介"></a>1. 梯度下降算法简介</h5><p>恰好，我们的损失函数L是可微的，也就是二阶可导的。如果我们当初选取损失函数时，使用绝对值作为损失函数，处理起来便没有这么便利了。</p><p>梯度下降的具体过程是这样的：</p><ol><li>选择一个初始$w_0$</li><li>计算该位置下w对L的微分，这个微分对应函数在此处下降最快的方向；</li><li>将$w_0$朝着这个方向移动一小步$ \eta$<script type="math/tex; mode=display">w\prime=w_0-\eta\frac{\mathrm{d}L}{\mathrm{d}w}</script></li><li>在新的位置开始新一轮迭代计算，直到$w$不再变化为止。</li></ol><p>不妨将梯度下降算法的求解过程想象为人下山的过程，人会先找到下山最快的方向，然后朝着那个方向走一步，直到抵达最低点。</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/模拟梯度下降.png" alt></p><p>在上图中，每一条线围成的圈就是等高线，代表损失函数的值，颜色约深的区域代表的损失函数越小；红色的箭头代表等高线的法线方向。</p><p>上述过程是求解L在变量w下的最优解的梯度下降过程，但是L还与偏移量b存在对应关系，所以实际上梯度下降在线性模型的具体公式如下：</p><script type="math/tex; mode=display">w_k\rightarrow w'_k=w_k-\eta\frac{\partial C}{\partial w_k} \\b_l\rightarrow b'_l=b_l-\eta\frac{\partial C}{\partial b_l}</script><p>其中，偏微分的具体公式如下：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/线性回归偏微分方程.png" alt></p><h5 id="2-梯度下降算法的局限性"><a href="#2-梯度下降算法的局限性" class="headerlink" title="2. 梯度下降算法的局限性"></a>2. 梯度下降算法的局限性</h5><p>首先，<strong>梯度下降算法容易陷入局部最优，找不到全局最优解</strong>。</p><p>如下图所示，当梯度下降算法优化到local minima 时，前面有座高山，它的梯度是正的，优化算法会强迫我们往回走。</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/梯度下降算法局部最优.png" alt></p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/梯度下降局部最优2.png" alt></p><p>这个问题在我们的线性回归模型里暂时遇不到，因为线性模型非常简单，是一个凸函数。但当我们试图利用梯度下降算法训练复杂模型，比如神经网络时，就很有可能遭遇该问题。</p><p>有很多方法能够解决梯度下降算法的全局最优解问题，包括调整每次行进的步长$\eta$，使其更有希望跨越“大山”等手段等等。</p><p>其次，梯度下降算法要求目标函数是可微的，这在某些情况下会<strong>产生相当大的计算代价</strong>。假设我们的问题有上百万维，计算二阶偏导数就需要上万亿（百万的平方）次！</p><p>再次，<strong>当训练数据相当多时，梯度下降算法会变得很慢</strong>。在实践中，为了计算梯度$\nabla L$，我们需要为了每个训练样本x单独计算梯度$\nabla L_x$，然后求平均值。这会花费很长时间。</p><h4 id="检验：如何检验模型是否训练成功？更加复杂的模型以及对应的风险。"><a href="#检验：如何检验模型是否训练成功？更加复杂的模型以及对应的风险。" class="headerlink" title="检验：如何检验模型是否训练成功？更加复杂的模型以及对应的风险。"></a>检验：如何检验模型是否训练成功？更加复杂的模型以及对应的风险。</h4><p>经过一轮又一轮的优化，我们终于求得了该问题下线性回归模型的最优参数$w$和$b$。接下来该如何检验模型在未知数据上的泛化能力呢？</p><p>我们可以在训练之前，把数据集分成两部分，一部分用作后续的训练，另一部分用作最后的验收。如果在这部分测试数据上的表现与训练过程中的表现一致，那我们就可以验收。</p><p>这种通过划分数据集为训练集和测试集的方法，与其说是一种技术细节，不如说是一种工程实践上的经验。通过训练集和测试集的性能比较，我们可以发现模型存在的潜在问题：</p><p><strong>1. 训练集和测试集上的表现都不太高；</strong></p><p>这种情况我们称之为欠拟合。线性模型由于过于简单，当面对一些较复杂的现实问题时，欠拟合便出现了。你会发现无论如何训练，无论投入多少数据都很难提升模型的性能了。我们需要更加复杂的模型，然后进行上面所说的机器学习三个步骤：选择新模型，选择合适的损失函数，选择优化方法进行训练。</p><p>对于一些现实问题，类似$y=wx+b$这种一次模型确实过于简单了。比如预测房价，可能与房屋面积有关，也可以与房屋面积的平方有关，甚至是三次方、四次方。</p><p>课堂上老师举了一个例子，用一次函数模型预测宝可梦数据集，效果如下：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦1.png" alt></p><p>模型在测试集上的误差平均值为 35.0</p><p>当我们使用更复杂的2次模型时，模型性能有明显好转：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦2.png" alt></p><p>模型的图像从一次函数变为二次函数了，更好拟合了训练集和测试集。测试集平均误差降至 18.4 了。那是不是模型越复杂，性能越好呢？</p><p><strong>2. 训练集上的表现良好，测试集上的表现很差</strong></p><p>当我们持续优化模型复杂度到3次方函数、4次方函数时，测试集平均误差开始停滞，4次方函数的平均误差不降反增：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦3.png" alt></p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦4.png" alt></p><p>训练集平均误差【15.4】【15.3】【14.9】【12.8】<br>测试集平均误差【18.4】【18.1】【28.8】【232.1】</p><p>这种情况称之为过拟合。所谓“过犹不及”，那我们应该怎样选择模型的复杂度，避免过拟合呢？我们可以将模型复杂度与测试集性能之间的关系绘制成图像，通过寻找图像的“拐点”来决定模型的复杂度。</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦性能变化.png" alt></p><h4 id="反思：改进模型性能的其他方法"><a href="#反思：改进模型性能的其他方法" class="headerlink" title="反思：改进模型性能的其他方法"></a>反思：改进模型性能的其他方法</h4><p>上面的宝可梦CP值预测问题，老师之后给出了更多的数据，绘制在坐标图上：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦5.png" alt></p><p>很显然，任何曲线的线性模型都没办法拟合这种数据点。这是哪里出了问题？答案是我们忽略了数据点的其他特征。当我们引入其他特征到模型后，线性模型的表达能力就增强了。下面就是引入了宝可梦种类的特征：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦6.png" alt></p><p>但是当我们继续引入一些无关特征，比如宝可梦的性别、年龄等，过拟合现象再次出现：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦7.png" alt></p><p>这是由于在训练过程中，某些特征的权重过大导致的。一种直观的解释是，模型训练过程也会偷懒！如果数据量不足，模型会自动选取某几个影响力大的特征，赋予较高的权值，然后忽略其他特征，那些被忽略的特征中可能包含更加有用的信息。</p><p>我们希望最终得到的模型不要出现太大的权重，为了防止某些特征的权值过大，限制权重的增长速度，可以使用正则化方法。</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/正则化1.png" alt></p><p>改造后的损失函数，末尾添加上了一个正则化项，这个正则化项的大小仅与模型的参数大小有关。参数越大，正则化项越大。有了正则化项，模型在优化过程就会更倾向于选择参数值更小的模型了。</p><p>在很多应用场景中，并不是 $w$ 越小模型越平滑越好，但是经验值告诉我们 $w$ 越小大部分情况下都是好的。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>机器学习的概念、分类和三个步骤：选择模型、选择损失函数、选择优化方法；</li><li>根据三个步骤，实践解决回归问题：使用线性模型，解决宝可梦CP值预测问题；</li></ol><p>这里的重点是梯度下降算法，后面还会继续学习该算法。</p><ol><li>通过分析例子的优化点，引出模型常用的优化方法以及风险：过拟合、欠拟合，以及他们的解决方案。</li></ol><p>过拟合问题的解决方法有：降低模型复杂度、增加训练样本、增加训练样本的特征、添加正则化项等。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：机器学习的概念和分类、利用线性模型解决回归问题以及梯度下降。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h3 id=&quot;一、机器学习介绍&quot;&gt;&lt;a href=&quot;#一、机器学习介绍&quot; class=&quot;headerlin</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】机器学习编译——Note2</title>
    <link href="https://superlova.github.io/2022/09/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E2%80%94%E2%80%94Note2/"/>
    <id>https://superlova.github.io/2022/09/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E2%80%94%E2%80%94Note2/</id>
    <published>2022-09-13T08:13:40.000Z</published>
    <updated>2022-09-14T06:48:32.068Z</updated>
    
    <content type="html"><![CDATA[<p>陈天奇老师主讲的机器学习编译相关的课程，本节课讨论张量程序抽象以及相关实现。<br><!--more---></p><p>机器学习编译的过程可以看做是对张量函数的变换过程。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;陈天奇老师主讲的机器学习编译相关的课程，本节课讨论张量程序抽象以及相关实现。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;p&gt;机器学习编译的过程可以看做是对张量函数的变换过程。&lt;/p&gt;
</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="Compilation" scheme="https://superlova.github.io/tags/Compilation/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】职业素养</title>
    <link href="https://superlova.github.io/2022/08/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%81%8C%E4%B8%9A%E7%B4%A0%E5%85%BB/"/>
    <id>https://superlova.github.io/2022/08/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%81%8C%E4%B8%9A%E7%B4%A0%E5%85%BB/</id>
    <published>2022-08-01T03:26:14.000Z</published>
    <updated>2022-08-02T13:32:34.818Z</updated>
    
    <content type="html"><![CDATA[<p>有关新人培养手册的一些阅读体会和思考<br><!--more---></p><h2 id="执行任务并漂亮地完成"><a href="#执行任务并漂亮地完成" class="headerlink" title="执行任务并漂亮地完成"></a>执行任务并漂亮地完成</h2><p>分析问题、解决问题是工程师的日常工作，如何对待工作、如何执行工作，反映了一个工程师的基本素质。</p><p>不同级别的工程师适合处理不同难度的任务。根据任务难度，可以分类为：</p><ol><li>任务已经拆解完毕，我只负责某个简单子任务的执行；</li><li>任务已经拆解完毕，我负责各个步骤的执行，或者其中较困难的部分；</li><li>复杂的事情需要自己做拆解然后执行。</li></ol><p>一般我们把简单的执行任务交付给 T5 左右级别工程师去执行，把复杂的任务交给 T7 左右的工程师执行。而 T8 及以上的工程师，我们希望能独立完成任务拆解并执行。T10 以上的工程师，我们希望能够承担一个技术方向上的难题。</p><p>无论任务难度如何，工程师还是要根据结果的完成度来评价。根据结果分类：</p><ul><li>最低评价：不能按时完成任务；</li><li>较低评价：按时完成任务，但是质量较差；</li><li>合格评价：按时完成任务，质量合格。</li></ul><h2 id="独立思考"><a href="#独立思考" class="headerlink" title="独立思考"></a>独立思考</h2><p>什么叫做独立思考？工作是由思考和执行两部分组成的，任务的分析和拆解过程、出现问题后的总结和复盘、对现有技术方案的调研，都算是独立思考。</p><p>比如说，当我负责的模块出现 Bug，或者我的团队负责的模块报警时，进行分析问题、总结和复盘的过程，算是独立思考；<br>比如说，项目的技术方案不够完美或存在缺陷，我对现有技术方案进行调研，算是独立思考；<br>比如说，我对问题进行分析和总结，经常反思是否已经无可挑剔、不会被人挑毛病，算是独立思考。</p><p>独立思考代表着三个工程师宝贵的能力：责任心、事业心和硬实力。一个高绩效的工程师，往往有以下特征：</p><ol><li>工作态度上，对业务负责，从来不会消极怠工，负责的模块出现事故很少；</li><li>做事方法上，规范、及时沟通，事情交给他很放心，不会把自己当成工具人；</li><li>做事效率上，想得清楚做得快，紧急问题得交给他，做事不会反反复复；</li><li>技术实力上，此方向的任何问题都可以咨询他。</li></ol><h2 id="协作沟通"><a href="#协作沟通" class="headerlink" title="协作沟通"></a>协作沟通</h2><p>以上几点是工程师个人的能力评价，但是在公司中免不了要与其他工程师打交道，也就是沟通。</p><p>协作沟通时有一些常见的问题，新手工程师可能不太了解，需要琢磨一下；在此我把一些定律列举出来，方便直接照做：</p><ol><li>收到消息必回复，塑造收到消息及时回复的形象；不仅如此，在自己负责通知时，也要确认相关责任人收到消息。</li><li>沟通时先想好诉求，问对了问题是成功的一半；要有礼貌，先带好称呼，再组织语言，逻辑通顺；要确认下解决问题的时间节点，并且将讨论的共识发到群里进行同步；</li><li>要及时跟进问题，就比如说当你找对方要时间节点时，可能会遇到：“还不知道，需要评估”、“事情太多，不好说”、等等类似的无法给出时间点的情况，这个时候就需要及时跟进。及时跟进的做法不是每过一段时间就问一次，那样很招人烦；正确做法是一起拆分问题，对方一旦完成子任务，就通知自己。</li><li>即便你负责的是一个项目里的一小部分，你也需要知道整个项目的计划节奏。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;有关新人培养手册的一些阅读体会和思考&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h2 id=&quot;执行任务并漂亮地完成&quot;&gt;&lt;a href=&quot;#执行任务并漂亮地完成&quot; class=&quot;headerlink&quot; title=&quot;执行任务并漂亮地完成&quot;&gt;&lt;/a&gt;执行任务并漂亮地完成&lt;/h2</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Career" scheme="https://superlova.github.io/tags/Career/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】如何实现模型的热更新？</title>
    <link href="https://superlova.github.io/2022/07/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%83%AD%E6%9B%B4%E6%96%B0%EF%BC%9F/"/>
    <id>https://superlova.github.io/2022/07/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%83%AD%E6%9B%B4%E6%96%B0%EF%BC%9F/</id>
    <published>2022-07-22T03:11:37.000Z</published>
    <updated>2022-08-02T13:32:34.801Z</updated>
    
    <content type="html"><![CDATA[<p>陈天奇老师主讲的机器学习编译相关的课程，回答了机器学习模型是如何从开发状态到部署状态的。整个过程类似源代码到可执行程序的编译过程，因此得名机器学习编译。据他本人所述，这门课是目前全世界第一门主讲机器学习编译的课程。<br><!--more---></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;陈天奇老师主讲的机器学习编译相关的课程，回答了机器学习模型是如何从开发状态到部署状态的。整个过程类似源代码到可执行程序的编译过程，因此得名机器学习编译。据他本人所述，这门课是目前全世界第一门主讲机器学习编译的课程。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="Compilation" scheme="https://superlova.github.io/tags/Compilation/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】机器学习编译——Note1</title>
    <link href="https://superlova.github.io/2022/07/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E2%80%94%E2%80%94Note1/"/>
    <id>https://superlova.github.io/2022/07/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E2%80%94%E2%80%94Note1/</id>
    <published>2022-07-04T02:36:43.000Z</published>
    <updated>2022-08-02T13:32:34.809Z</updated>
    
    <content type="html"><![CDATA[<p>陈天奇老师主讲的机器学习编译相关的课程，回答了机器学习模型是如何从开发状态到部署状态的。整个过程类似源代码到可执行程序的编译过程，因此得名机器学习编译。据他本人所述，这门课是目前全世界第一门主讲机器学习编译的课程。<br><!--more---></p><p>《机器学习编译》是陈天奇老师的公开课。目前在Bilibili上更新视频。<a href="https://space.bilibili.com/1663273796">这里</a>是陈天奇老师在Bilibili的个人主页，如果对课程感兴趣的小伙伴可以免费观摩。</p><p>另外课件地址在这里：<a href="https://mlc.ai/zh/chapter_introduction/">Machine Learning Compilation</a></p><h1 id="一、什么是机器学习编译？学习这门课的意义是什么？"><a href="#一、什么是机器学习编译？学习这门课的意义是什么？" class="headerlink" title="一、什么是机器学习编译？学习这门课的意义是什么？"></a>一、什么是机器学习编译？学习这门课的意义是什么？</h1><p><strong>机器学习编译</strong> (machine learning compilation, MLC) 是指，将机器学习算法从开发阶段，通过变换和优化算法，使其变成部署状态。</p><p>开发阶段：用 PyTorch、TensorFlow 或 JAX 等通用框架编写的模型描述 + 对应的权重文件。</p><p>部署状态：在考虑支撑代码、内存管理、不同语言的开发接口等因素后，成功执行在设备上的状态。</p><p>该过程类似于把源代码转换为可执行文件的程序编译过程，但是二者有比较大的不同。</p><p>首先是目标不同，机器学习编译的目标有：</p><ul><li>集成打包 + 最小化依赖，即将必要的元素组合在一起以用于部署应用程序；</li><li>利用硬件加速，利用硬件本身的特性进行加速；</li><li>通用优化，以最小化内存使用或提高执行效率为目标转换模型执行方式。</li></ul><p>这些目标没有严格的界限。</p><p>其次，这个过程不一定涉及代码生成；例如将开发状态转化为部署形式，可能只是将抽象的模型定义，转化为对某几个预定义的库函数的调用。</p><p>最后，遇到的挑战和解决方案也大不相同。随着硬件和模型种类的增长，机器学习编译难以表示单一稳定的解决方案。</p><p>那么，我们能够从这门课学习到什么？</p><p>对于在从事机器学习工作工程师，机器学习编译提供了以基础的解决问题的方法和工具。它有助于回答我们可以采用什么方法来特定模型的部署和内存效率，如何将优化模型的单个部分的经验推广到更端到端解决方案等一系列问题。</p><p>对于机器学习科学家，学习机器学习编译可以更深入地了解将模型投入生产所需的步骤。机器学习框架本身隐藏了一些技术复杂性，但是当我们尝试开始部署新模型或将模型部署到框架支持不完善的平台时，仍然会面临巨大的挑战。机器学习编译使机器学习算法科学家有机会了解背后的基本原理，并且知晓为什么我的模型的运行速度不及预期，以及如何来使部署更有效。</p><p>最后，学习 MLC 本身很有趣。借助这套现代机器学习编译工具，我们可以进入机器学习模型从高级、代码优化到裸机的各个阶段。端到端 (end to end) 地了解这里发生的事情并使用它们来解决我们的问题。</p><h1 id="二、机器学习编译的关键要素"><a href="#二、机器学习编译的关键要素" class="headerlink" title="二、机器学习编译的关键要素"></a>二、机器学习编译的关键要素</h1><h2 id="1-张量和张量函数"><a href="#1-张量和张量函数" class="headerlink" title="1. 张量和张量函数"></a>1. 张量和张量函数</h2><p>张量 (Tensor) 是执行中最重要的元素。张量是表示神经网络模型执行的输入、输出和中间结果的多维数组。</p><p>张量函数 (Tensor functions) 指接受张量和输出张量的计算序列。</p><p>下面这张图展示了机器学习编译过程中，两种不同形式的张量函数的变换过程。从左边比较抽象的表示形式，转换为右侧较为具体的表示形式。</p><p><img src="https://mlc.ai/zh/_images/mlc-elem-transform.png" alt="机器学习编译过程中的张量函数变换"></p><p>机器学习编译的过程就是是将上图左侧的内容转换为右侧的过程。在不同的场景中，这个过程可以是手动完成的，也可以使用一些自动转换工具，或两者兼而有之。</p><h2 id="2-抽象和实现"><a href="#2-抽象和实现" class="headerlink" title="2. 抽象和实现"></a>2. 抽象和实现</h2><p>上一部分提到了抽象和实现。对于同样的目标，我们会有不同的 抽象表现，但是不同抽象表示有些细节不同。我们会把更细化的抽象表示称为原有抽象表示的一个具体实现。</p><p>抽象和实现可能是所有计算机系统中最重要的关键字。抽象指定“做什么”，实现提供“如何”做。没有具体的界限。根据我们的看法，for 循环本身可以被视为一种抽象，因为它可以使用 python 解释器实现或编译为本地汇编代码。</p><p>MLC 实际上是在相同或不同抽象下转换和组装张量函数的过程。</p><p>本课程会介绍四种不同形式的抽象表示</p><ul><li>计算图的抽象</li><li>张量程序的抽象</li><li>算子库和运行时的抽象</li><li>硬件层面的抽象</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;陈天奇老师主讲的机器学习编译相关的课程，回答了机器学习模型是如何从开发状态到部署状态的。整个过程类似源代码到可执行程序的编译过程，因此得名机器学习编译。据他本人所述，这门课是目前全世界第一门主讲机器学习编译的课程。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;p&gt;《机器学习编</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="Compilation" scheme="https://superlova.github.io/tags/Compilation/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】科大讯飞-非标准化疾病诉求的简单分诊挑战赛</title>
    <link href="https://superlova.github.io/2022/06/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E-%E9%9D%9E%E6%A0%87%E5%87%86%E5%8C%96%E7%96%BE%E7%97%85%E8%AF%89%E6%B1%82%E7%9A%84%E7%AE%80%E5%8D%95%E5%88%86%E8%AF%8A%E6%8C%91%E6%88%98%E8%B5%9B/"/>
    <id>https://superlova.github.io/2022/06/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E-%E9%9D%9E%E6%A0%87%E5%87%86%E5%8C%96%E7%96%BE%E7%97%85%E8%AF%89%E6%B1%82%E7%9A%84%E7%AE%80%E5%8D%95%E5%88%86%E8%AF%8A%E6%8C%91%E6%88%98%E8%B5%9B/</id>
    <published>2022-06-20T11:55:48.000Z</published>
    <updated>2022-08-02T13:32:34.826Z</updated>
    
    <content type="html"><![CDATA[<p>文本分类问题，收集用户问诊信息，需要分别将一段提问就问诊类型（20种）和疾病类型（60种）进行分类。类别包含较多缺失值，trick较多。笔者用到了Tfidf+Logistic回归作为Baseline，然后使用Pytorch训练RoBERTa用作第一版提交模型，最终根据数据的特征，取得了XX（排名）。<br><!--more---></p><p>比赛地址：<br><a href="https://challenge.xfyun.cn/topic/info?type=disease-claims-2022&amp;option=ssgy">https://challenge.xfyun.cn/topic/info?type=disease-claims-2022&amp;option=ssgy</a></p><p>baseline地址：<br><a href="https://mp.weixin.qq.com/s/KiozLF7FaJ_CVx74J3KNWA">https://mp.weixin.qq.com/s/KiozLF7FaJ_CVx74J3KNWA</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;文本分类问题，收集用户问诊信息，需要分别将一段提问就问诊类型（20种）和疾病类型（60种）进行分类。类别包含较多缺失值，trick较多。笔者用到了Tfidf+Logistic回归作为Baseline，然后使用Pytorch训练RoBERTa用作第一版提交模型，最终根据数据的</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
    <category term="Data Mining" scheme="https://superlova.github.io/tags/Data-Mining/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】MacBook Pro M1Pro Java环境部署和开发准备</title>
    <link href="https://superlova.github.io/2022/05/25/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Java%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E5%92%8C%E5%BC%80%E5%8F%91%E5%87%86%E5%A4%87/"/>
    <id>https://superlova.github.io/2022/05/25/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Java%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E5%92%8C%E5%BC%80%E5%8F%91%E5%87%86%E5%A4%87/</id>
    <published>2022-05-25T07:40:22.000Z</published>
    <updated>2022-08-02T13:32:34.789Z</updated>
    
    <content type="html"><![CDATA[<p>本文记录了在MacBook Pro M1Pro下部署Java开发环境遇到的问题和解决方法<br><!--more---></p><h2 id="下载并安装JDK"><a href="#下载并安装JDK" class="headerlink" title="下载并安装JDK"></a>下载并安装JDK</h2><p>M1芯片带来的坏处是许多jdk版本不支持arm架构。这里可以选择<code>Zulu JDK</code>，可以进入<a href="https://www.azul.com/downloads/?version=java-8-lts&amp;os=macos&amp;architecture=arm-64-bit&amp;package=jdk">此链接下载</a>并安装指定版本的JDK。</p><p><img src="/2022/05/25/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Java%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E5%92%8C%E5%BC%80%E5%8F%91%E5%87%86%E5%A4%87/2022-05-25-15-46-32.png" alt="安装页面"></p><p>安装完毕后，不必设置环境变量，直接在命令行运行<code>java -version</code>检查是否运行成功：</p><pre><code>% java -versionopenjdk version &quot;1.8.0_332&quot;OpenJDK Runtime Environment (Zulu 8.62.0.19-CA-macos-aarch64) (build 1.8.0_332-b09)OpenJDK 64-Bit Server VM (Zulu 8.62.0.19-CA-macos-aarch64) (build 25.332-b09, mixed mode)</code></pre><h2 id="下载并安装Maven"><a href="#下载并安装Maven" class="headerlink" title="下载并安装Maven"></a>下载并安装Maven</h2><p>可在<a href="https://maven.apache.org/download.cgi">此链接中</a>选择如图所示的链接进行下载压缩包：</p><p><img src="/2022/05/25/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Java%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E5%92%8C%E5%BC%80%E5%8F%91%E5%87%86%E5%A4%87/下载Maven.png" alt></p><p>这是个压缩文件，或者说“绿色版”。然后将其解压到你想要放入的文件夹（我一般移动到<code>/opt</code>目录下）。</p><p>接下来需要我们自己配置环境变量。首先需要确认你的BASH类别。</p><p>查看当前使用的SHELL：</p><pre><code>% echo $SHELL/bin/zsh</code></pre><p>查看本机可用的所有SHELL</p><pre><code>% cat /etc/shells# List of acceptable shells for chpass(1).# Ftpd will not allow users to connect who are not using# one of these shells./bin/bash/bin/csh/bin/dash/bin/ksh/bin/sh/bin/tcsh/bin/zsh</code></pre><p>我这里显示是zsh，则我需要修改<code>~/.zshrc</code><br>如果你的shell是<code>/bin/bash</code>，则需要修改<code>~/.bash_profile</code></p><p>将以下文本加入配置文件末尾：</p><pre><code class="lang-sh">export MAVEN_HOME=/opt/apache-maven-3.8.5export PATH=$PATH:$MAVEN_HOME/bin</code></pre><p>然后立即载入环境变量：</p><pre><code>% source ~/.zshrc</code></pre><p>最后测试是否应用修改：</p><pre><code>% mvn -vApache Maven 3.8.5 (3599d3414f046de2324203b78ddcf9b5e4388aa0)Maven home: /opt/apache-maven-3.8.5Java version: 1.8.0_332, vendor: Azul Systems, Inc., runtime: /Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home/jreDefault locale: zh_CN, platform encoding: UTF-8OS name: &quot;mac os x&quot;, version: &quot;12.3.1&quot;, arch: &quot;aarch64&quot;, family: &quot;mac&quot;</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文记录了在MacBook Pro M1Pro下部署Java开发环境遇到的问题和解决方法&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h2 id=&quot;下载并安装JDK&quot;&gt;&lt;a href=&quot;#下载并安装JDK&quot; class=&quot;headerlink&quot; title=&quot;下载并安装JDK</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Java" scheme="https://superlova.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】如何使用树模型预测结构化数据</title>
    <link href="https://superlova.github.io/2022/03/06/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E6%A0%91%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE/"/>
    <id>https://superlova.github.io/2022/03/06/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E6%A0%91%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE/</id>
    <published>2022-03-06T13:16:44.000Z</published>
    <updated>2022-08-02T13:32:34.801Z</updated>
    
    <content type="html"><![CDATA[<p>TODO</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;TODO&lt;/p&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>【学习笔记】Attention和Transformer</title>
    <link href="https://superlova.github.io/2021/08/18/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Attention%E5%92%8CTransformer/"/>
    <id>https://superlova.github.io/2021/08/18/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Attention%E5%92%8CTransformer/</id>
    <published>2021-08-18T01:24:20.000Z</published>
    <updated>2022-08-02T13:32:34.785Z</updated>
    
    <content type="html"><![CDATA[<p>分享下知识，介绍下我对Attention的理解，然后利用Tensorflow实现一个完整的Transformer模型。</p><!--more---><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="常见的NLP任务"><a href="#常见的NLP任务" class="headerlink" title="常见的NLP任务"></a>常见的NLP任务</h2><p>文本分类，实体抽取等</p><h1 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h1><p>Attention是一种编码机制，最初是用于解决机器翻译（seq2seq任务）的长程遗忘问题。因为基于RNN模型不能很好地应对长文本的翻译，即便是LSTM也受限于序列长度等因素的影响。而Attention可以通过训练一个动态的参数矩阵，决定关注长文本的哪些部分。Attention的一个优点是，相较于LSTM的基于时间的反响传播算法，Attention的计算可以并行化，这为其堆叠比较庞大的模型打下了基础。</p><h1 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h1><p>Attention机制其实用在RNN的机器翻译模型上就已经可以提升机器翻译模型的效果了，于是有人想，不如我们直接把RNN从模型中摘掉，不管encoder和decoder都使用Attention计算单元，性能不就更好了？这也是Attention is all you need 这篇论文的由来。</p><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>Transformer是由Attention模块组成的计算单元。</p><p>在encoder阶段，由6个self-attention模块组成的多层编码器负责编码运算，最后使用一层前馈神经网络汇总（其实就是加权汇总）；在decoder方面，输入不但有编码器传进来的一部分信息，而且还有句子本身。</p><p>Transformer里面使用多种Attention，关注序列的不同层次的信息，这种叫做多头注意力机制。在RNN横行天下的时代，这种堆叠模式带来的庞大训练和推理负担使得难以推动此类模型上线。</p><pre><code class="lang-py">class MultiheadAttention(nn.Module):    # n_heads：多头注意力的数量    # hid_dim：每个词输出的向量维度    def __init__(self, hid_dim, n_heads, dropout):        super(MultiheadAttention, self).__init__()        self.hid_dim = hid_dim        self.n_heads = n_heads        # 强制 hid_dim 必须整除 h        assert hid_dim % n_heads == 0        # 定义 W_q 矩阵        self.w_q = nn.Linear(hid_dim, hid_dim)        # 定义 W_k 矩阵        self.w_k = nn.Linear(hid_dim, hid_dim)        # 定义 W_v 矩阵        self.w_v = nn.Linear(hid_dim, hid_dim)        self.fc = nn.Linear(hid_dim, hid_dim)        self.do = nn.Dropout(dropout)        # 缩放        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads]))    def forward(self, query, key, value, mask=None):        # K: [64,10,300], batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维        # V: [64,10,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维        # Q: [64,12,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维        bsz = query.shape[0]        Q = self.w_q(query)        K = self.w_k(key)        V = self.w_v(value)        # 这里把 K Q V 矩阵拆分为多组注意力，变成了一个 4 维的矩阵        # 最后一维就是是用 self.hid_dim // self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300/6=50        # 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 词，50 表示每组注意力的词的向量长度        # K: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]        # V: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]        # Q: [64,12,300] 拆分多组注意力 -&gt; [64,12,6,50] 转置得到 -&gt; [64,6,12,50]        # 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim //                   self.n_heads).permute(0, 2, 1, 3)        K = K.view(bsz, -1, self.n_heads, self.hid_dim //                   self.n_heads).permute(0, 2, 1, 3)        V = V.view(bsz, -1, self.n_heads, self.hid_dim //                   self.n_heads).permute(0, 2, 1, 3)        # 第 1 步：Q 乘以 K的转置，除以scale        # [64,6,12,50] * [64,6,50,10] = [64,6,12,10]        # attention：[64,6,12,10]        attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale        # 把 mask 不为空，那么就把 mask 为 0 的位置的 attention 分数设置为 -1e10        if mask isnotNone:            attention = attention.masked_fill(mask == 0, -1e10)        # 第 2 步：计算上一步结果的 softmax，再经过 dropout，得到 attention。        # 注意，这里是对最后一维做 softmax，也就是在输入序列的维度做 softmax        # attention: [64,6,12,10]        attention = self.do(torch.softmax(attention, dim=-1))        # 第三步，attention结果与V相乘，得到多头注意力的结果        # [64,6,12,10] * [64,6,10,50] = [64,6,12,50]        # x: [64,6,12,50]        x = torch.matmul(attention, V)        # 因为 query 有 12 个词，所以把 12 放到前面，把 5 和 60 放到后面，方便下面拼接多组的结果        # x: [64,6,12,50] 转置-&gt; [64,12,6,50]        x = x.permute(0, 2, 1, 3).contiguous()        # 这里的矩阵转换就是：把多组注意力的结果拼接起来        # 最终结果就是 [64,12,300]        # x: [64,12,6,50] -&gt; [64,12,300]        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))        x = self.fc(x)        return x# batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维query = torch.rand(64, 12, 300)# batch_size 为 64，有 12 个词，每个词的 Key 向量是 300 维key = torch.rand(64, 10, 300)# batch_size 为 64，有 10 个词，每个词的 Value 向量是 300 维value = torch.rand(64, 10, 300)attention = MultiheadAttention(hid_dim=300, n_heads=6, dropout=0.1)output = attention(query, key, value)## output: torch.Size([64, 12, 300])print(output.shape)</code></pre><h1 id="表示序列中单词顺序的方法"><a href="#表示序列中单词顺序的方法" class="headerlink" title="表示序列中单词顺序的方法"></a>表示序列中单词顺序的方法</h1><p>为了解决这个问题，Transformer 模型对每个输入的向量都添加了一个向量。这些向量遵循模型学习到的特定模式，有助于确定每个单词的位置，或者句子中不同单词之间的距离。这种做法背后的直觉是：将这些表示位置的向量添加到词向量中，得到了新的向量，这些新向量映射到 Q/K/V，然后计算点积得到 attention 时，可以提供有意义的信息。</p><h1 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h1><p>编码器结构中有一个需要注意的细节是：编码器的每个子层（Self Attention 层和 FFNN）都有一个残差连接和层标准化（layer-normalization）。在解码器的子层里面也有层标准化（layer-normalization）。</p><p><img src="https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/0-1-transformer-arc.png" alt="11"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;分享下知识，介绍下我对Attention的理解，然后利用Tensorflow实现一个完整的Transformer模型。&lt;/p&gt;
&lt;!--more---&gt;
&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="NLP" scheme="https://superlova.github.io/tags/NLP/"/>
    
    <category term="Attention" scheme="https://superlova.github.io/tags/Attention/"/>
    
    <category term="Transformer&#39;" scheme="https://superlova.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读笔记】实现 MapReduce 论文附录 A 的 Word Count 程序</title>
    <link href="https://superlova.github.io/2021/05/07/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91%E5%AE%9E%E7%8E%B0-MapReduce-%E8%AE%BA%E6%96%87%E9%99%84%E5%BD%95-A-%E7%9A%84-Word-Count-%E7%A8%8B%E5%BA%8F/"/>
    <id>https://superlova.github.io/2021/05/07/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91%E5%AE%9E%E7%8E%B0-MapReduce-%E8%AE%BA%E6%96%87%E9%99%84%E5%BD%95-A-%E7%9A%84-Word-Count-%E7%A8%8B%E5%BA%8F/</id>
    <published>2021-05-07T12:26:13.000Z</published>
    <updated>2022-08-02T13:32:34.880Z</updated>
    
    <content type="html"><![CDATA[<p>给出一个Map和Reduce的具体实现，去除了所有分布式的特性【可能今后会添加】</p><!--more---><p>MapReduce文章最后的附录A有一段C++代码，描述了Map和Reduce函数的编写和使用方法。但是仅靠看代码总是不能深入理解MapReduce的实现细节，阅读其他人的MapReduce学习曲线又太过陡峭。因此我决定自己实现Map函数和Reduce函数，并尽可能使用替代方法将那些没有给出的api实现。</p><p>促使我写这篇文章的另一个原因是，网络上的大部分实现Word Count的文章都是依赖于某某框架的，比如依赖Hadoop。但我觉得过早依赖于某个平台不利于深入理解背后的原理，因此我决定自己实现各种api，体会程序设计者可能遇到的问题。今后在学习分布式系统、分布式框架时，便能够对症下药、有的放矢。</p><p>不过还请各位原谅，这里的Map和Reduce只能运行在单机环境啦。</p><p>下面给出原文中的C++代码：</p><pre><code class="lang-cpp">#include &quot;mapreduce/mapreduce.h&quot;//用户map函数class WordCounter : public Mapper &#123;public:    virtual void Map(const MapInput&amp; input) &#123;        const string&amp; text = input.value();        const int n = text.size();        for (int i = 0; i &lt; n; ) &#123;            //跳过前导空格            while ((i &lt; n) &amp;&amp; isspace(text[i]))                i++;            // 查找单词的结束位置            int start = i;            while ((i &lt; n) &amp;&amp; !isspace(text[i]))                i++;            if (start &lt; i)                Emit(text.substr(start,i-start),&quot;1&quot;);        &#125;    &#125;&#125;;REGISTER_MAPPER(WordCounter);//用户的reduce函数class Adder : public Reducer &#123;    virtual void Reduce(ReduceInput* input) &#123;        //迭代具有相同key的所有条目,并且累加它们的value        int64 value = 0;        while (!input-&gt;done()) &#123;            value += StringToInt(input-&gt;value());            input-&gt;NextValue();        &#125;        //提交这个输入key的综合        Emit(IntToString(value));    &#125;&#125;;REGISTER_REDUCER(Adder);int main(int argc, char** argv) &#123;    ParseCommandLineFlags(argc, argv);    MapReduceSpecification spec;    // 把输入文件列表存入&quot;spec&quot;    for (int i = 1; i &lt; argc; i++) &#123;        MapReduceInput* input = spec.add_input();        input-&gt;set_format(&quot;text&quot;);        input-&gt;set_filepattern(argv[i]);        input-&gt;set_mapper_class(&quot;WordCounter&quot;);    &#125;    //指定输出文件:    // /gfs/test/freq-00000-of-00100    // /gfs/test/freq-00001-of-00100    // ...    MapReduceOutput* out = spec.output();    out-&gt;set_filebase(&quot;/gfs/test/freq&quot;);    out-&gt;set_num_tasks(100);    out-&gt;set_format(&quot;text&quot;);    out-&gt;set_reducer_class(&quot;Adder&quot;);    // 可选操作:在map任务中做部分累加工作,以便节省带宽    out-&gt;set_combiner_class(&quot;Adder&quot;);    // 调整参数: 使用2000台机器,每个任务100MB内存    spec.set_machines(2000);    spec.set_map_megabytes(100);    spec.set_reduce_megabytes(100);    // 运行    MapReduceResult result;    if (!MapReduce(spec, &amp;result)) abort();    // 完成: &#39;result&#39;结构包含计数,花费时间,和使用机器的信息    return 0;&#125;</code></pre><h2 id="抽象基类-Mapper、Reducer"><a href="#抽象基类-Mapper、Reducer" class="headerlink" title="抽象基类 Mapper、Reducer"></a>抽象基类 Mapper、Reducer</h2><p>首先我打算实现<code>WordCounter</code>的父类<code>Mapper</code>和<code>Adder</code>的父类<code>Reducer</code>。</p><pre><code class="lang-cpp">class Mapper &#123;public:    virtual void Map(const MapInput&amp; input) = 0;&#125;;class Reducer &#123;public:    virtual void Reduce(ReduceInput* input) = 0;&#125;;</code></pre><p>简单实现Map和Reduce接口，并把它们设置成纯虚方法。</p><h2 id="WordCounter类"><a href="#WordCounter类" class="headerlink" title="WordCounter类"></a>WordCounter类</h2><pre><code class="lang-cpp">class WordCounter : public Mapper &#123;public:    void Map(const MapInput&amp; input) override &#123;        const string&amp; text = input.value(); // 读取一行文本        const int n = text.size();        for (int i = 0; i &lt; n; ) &#123;            // 跳过行首空白            while ((i &lt; n) &amp;&amp; isspace(text[i])) i++;            // 确定单词的开头和结尾            int start = i;            while ((i &lt; n) &amp;&amp; !isspace(text[i])) i++;            if (start &lt; i)                Emit(text.substr(start, i-start), &quot;1&quot;);        &#125;    &#125;&#125;;</code></pre><p>这一部分相较于原文没什么变化。其主要作用在于分词，然后每个单词组建成一个键值对，以(word, 1)的结构发射出去。发射到哪里呢？我就偷懒直接持久化到本地的消息存储装置了。</p><pre><code class="lang-cpp">static void Emit(const string&amp; key, const string&amp; value) &#123;    mw.put(key, value);&#125;</code></pre><p>mw 是 MiddleWare 的实例，是一个用于保存Emit输出的键值对的全局变量。后续会继续讲解。</p><h2 id="用于保存-Mapper-得到的键值对的-MiddleWare-类"><a href="#用于保存-Mapper-得到的键值对的-MiddleWare-类" class="headerlink" title="用于保存 Mapper 得到的键值对的 MiddleWare 类"></a>用于保存 Mapper 得到的键值对的 MiddleWare 类</h2><pre><code class="lang-cpp">class MiddleWare &#123;private:    vector&lt;pair&lt;string, string&gt;&gt; kv_pairs;    static bool compare_pair(const pair&lt;string, string&gt;&amp; lhs, const pair&lt;string, string&gt;&amp; rhs) &#123;        return lhs.first &lt; rhs.first;    &#125;public:    MiddleWare() = default;    void put(const string&amp; key, const string&amp; value) &#123;        kv_pairs.emplace_back(key, value);    &#125;    vector&lt;pair&lt;string, string&gt;&gt; get() &#123;        std::sort(kv_pairs.begin(), kv_pairs.end(), compare_pair); // 将其按照key相同的一组来排序        return kv_pairs;    &#125;&#125;;MiddleWare mw; // 全局变量：消息队列</code></pre><p>这是我为本地运行顺利而凭空构建出来的类，作用是储存(key, value)对。为了方便使用，内部有get和put方法。其中如果Reducer需要get数据了，那么首先会按照key对这些pair进行排序。这也是与MapReduce的流程相吻合的。</p><p>既然是排序，那么就要定义比较器 compare_pair。我的比较器直接使用string的比较，确保相同key值的pair在相邻位置。</p><h2 id="Adder类"><a href="#Adder类" class="headerlink" title="Adder类"></a>Adder类</h2><pre><code class="lang-cpp">// 用户自定义 Reduce 函数class Adder : public Reducer &#123;public:    void Reduce(ReduceInput* input) override &#123;        // 迭代所有拥有相同key的键值对，把它们的values加起来        int64_t value = 0;        string currentKey = input-&gt;key();        while (!input-&gt;end() &amp;&amp; currentKey == input-&gt;key()) &#123; // 直到下一个键值对的key与当前键值对的key不同为止            value += std::stoi(input-&gt;value());            input-&gt;NextValue(); // 找到下一个拥有相同key的键值对        &#125;        // Emit sum for input-&gt;key()        Emit(to_string(value));    &#125;&#125;;</code></pre><p>与论文中的Adder有逻辑出入，主要变化在把同样key分成不同组的逻辑上，我直接保存了当前组的key。原来论文里是没有这种操作的。</p><h2 id="Map-的输入-MapInput"><a href="#Map-的输入-MapInput" class="headerlink" title="Map 的输入 MapInput"></a>Map 的输入 MapInput</h2><p>观察一下Map的参数里有一个MapInput类型的对象，那么第二步就是新建一个MapInput类。要想跑通Map函数的代码，这个类必须实现value()方法。</p><p>猜测一下，MapInput是Map的输入，而MapReduce框架的输入输出都应该是键值对的形式。因此每个MapInput都应该包含一个key和一个value成员。</p><pre><code class="lang-cpp">class MapInput &#123;private:    string map_value;    string map_key;public:    explicit MapInput(string filename, string text) : map_key(std::move(filename)), map_value(std::move(text)) &#123; &#125;    [[nodiscard]] const string&amp; value() const &#123;        return map_value;    &#125;&#125;;</code></pre><p>MapInput的构造函数接收两个参数，第一个参数是文本文件名，第二个参数是文件的内容。其实第一个参数在我们的程序中没啥作用，但是为了格式的统一，就写上吧。</p><p>explicit修饰构造函数，代表该类的对象禁止发生隐式类型转换，要想转换必须以<strong>明确的(explicit)</strong>方式进行显式类型转换。</p><p>冒号后面的初始化列表中，使用了move特性，避免了函数传参导致的变量复制。</p><p>[[nodiscard]] 含义是该函数的返回值必须被使用，不能丢弃。C++ 17版本新增了几个中括号标识的提示符，当代码不符合要求的时候，编译器也会真的警告。相当于把以前的注释加强了。除[[nodiscard]]之外，还有表示switch语句中不必加break的[[fallthrough]]、变量定义之后没有使用也没关系的标识符[[maybe_unused]]。</p><h2 id="Reduce-的输入-ReduceInput"><a href="#Reduce-的输入-ReduceInput" class="headerlink" title="Reduce 的输入 ReduceInput"></a>Reduce 的输入 ReduceInput</h2><p>ReduceInput的设计就比较麻烦了。首先Reduce函数的输入是ReduceInput的指针，使用到的接口有done()/value()/NextValue()/key()，然后根据Reduce函数的使用方法，感觉ReduceInput像是一个迭代器。</p><pre><code class="lang-cpp">class ReduceInput &#123;private:    vector&lt;pair&lt;string, string&gt;&gt; data;    int currentKey = 0;public:    explicit ReduceInput(vector&lt;pair&lt;string, string&gt;&gt; _data) : data(std::move(_data)) &#123;  &#125;//    bool done() &#123;//        // 直到下一个键值对的key与当前键值对的key不同为止//        // 如果到了末尾，或者下一个key不一样，都是done//        if (currentKey == 0) return false;//        if (end() || data[currentKey].first != data[currentKey-1].first) return true;//        return false;//    &#125;    const string&amp; value() &#123;        return data[currentKey].second;    &#125;    const string&amp; key() &#123;        return data[currentKey].first;    &#125;    void NextValue() &#123;        currentKey++;    &#125;    bool end() &#123;        return currentKey &gt;= data.size();    &#125;&#125;;</code></pre><p>上面是我实现的ReduceInput，偷个懒把所有数据存放到ReduceInput中方便遍历，在真实场景的设计中不会像我这样的。</p><p>此外，Done函数的逻辑是有问题的。关键在于Reduce函数中的这句话：</p><pre><code class="lang-cpp">while (!input-&gt;done()) &#123;    value += StringToInt(input-&gt;value());    input-&gt;NextValue();&#125;</code></pre><p>表面上看起来是希望input作为一个迭代器，当它迭代到key与下一个key不同时，终止迭代（即done返回true表明迭代完成），然而下次迭代的开始还是从这个位置，其结果从程序逻辑上来讲，却又希望返回false。同一个位置，我们希望返回两个不同的值，这显然是说不通的。因此我在Reduce最终实现的主代码部分做了适当的逻辑修改。</p><h2 id="输出结果的单参数Emit"><a href="#输出结果的单参数Emit" class="headerlink" title="输出结果的单参数Emit"></a>输出结果的单参数Emit</h2><p>为了输出方便，最终我定义了单参数的重载Emit，不保存Reducer的计算结果，直接输出：</p><pre><code class="lang-cpp">static void Emit(const string&amp; key) &#123;    cout &lt;&lt; &quot;Sum of values:&quot; &lt;&lt; key &lt;&lt; endl;&#125;</code></pre><h2 id="最后的main"><a href="#最后的main" class="headerlink" title="最后的main"></a>最后的main</h2><pre><code class="lang-cpp">int main(int argc, char* argv[]) &#123;    ifstream in(R&quot;(C:\Users\zyt\CLionProjects\leetcode_2021\lyrics.txt)&quot;);    string content((istreambuf_iterator&lt;char&gt;(in)), istreambuf_iterator&lt;char&gt;());    MapInput minput(&quot;lyrics.txt&quot;, content);    cout &lt;&lt; &quot;minput:\n&quot; &lt;&lt; minput.value() &lt;&lt; endl;    WordCounter wc;    wc.Map(minput);    auto *rinput = new ReduceInput(mw.get());    while (!rinput-&gt;end()) &#123;        cout &lt;&lt; &quot;Key: &quot; &lt;&lt; rinput-&gt;key() &lt;&lt; &quot;\t&quot;;        Adder adder; // 模拟很多 adder        adder.Reduce(rinput);        rinput-&gt;NextValue();    &#125;    return 0;&#125;</code></pre><p>ifstream读入本地文本文档，注意ifstream的参数得是绝对路径（相对路径不知道为什么读取不出来东西）。</p><p>然后WordCount把分词结果的键值对保存在全局变量mw中，使用mw构建ReduceInput，再把ReduceInput输入进Adder里面。</p><p>注意一个Reducer处理一个Group（我把key相同的一组键值对称之为Group），那么我就以While循环来代替啦。</p><p>这就是代码的所有内容了！</p><p><img src="/2021/05/07/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91%E5%AE%9E%E7%8E%B0-MapReduce-%E8%AE%BA%E6%96%87%E9%99%84%E5%BD%95-A-%E7%9A%84-Word-Count-%E7%A8%8B%E5%BA%8F/result.png" alt></p><pre><code class="lang-cpp">#include &quot;stdafx.h&quot;using namespace std;class MiddleWare &#123;private:    vector&lt;pair&lt;string, string&gt;&gt; kv_pairs;    static bool compare_pair(const pair&lt;string, string&gt;&amp; lhs, const pair&lt;string, string&gt;&amp; rhs) &#123;        return lhs.first &lt; rhs.first;    &#125;public:    MiddleWare() = default;    void put(const string&amp; key, const string&amp; value) &#123;        kv_pairs.emplace_back(key, value);    &#125;    vector&lt;pair&lt;string, string&gt;&gt; get() &#123;        std::sort(kv_pairs.begin(), kv_pairs.end(), compare_pair); // 将其按照key相同的一组来排序        return kv_pairs;    &#125;&#125;;MiddleWare mw; // 全局变量：消息队列class MapInput &#123;private:    string map_value;    string map_key;public:    explicit MapInput(string filename, string text) : map_key(std::move(filename)), map_value(std::move(text)) &#123; &#125;    [[nodiscard]] const string&amp; value() const &#123;        return map_value;    &#125;&#125;;class ReduceInput &#123;private:    vector&lt;pair&lt;string, string&gt;&gt; data;    int currentKey = 0;public:    explicit ReduceInput(vector&lt;pair&lt;string, string&gt;&gt; _data) : data(std::move(_data)) &#123;  &#125;//    bool done() &#123;//        // 直到下一个键值对的key与当前键值对的key不同为止//        // 如果到了末尾，或者下一个key不一样，都是done//        if (currentKey == 0) return false;//        if (end() || data[currentKey].first != data[currentKey-1].first) return true;//        return false;//    &#125;    const string&amp; value() &#123;        return data[currentKey].second;    &#125;    const string&amp; key() &#123;        return data[currentKey].first;    &#125;    void NextValue() &#123;        currentKey++;    &#125;    bool end() &#123;        return currentKey &gt;= data.size();    &#125;&#125;;static void Emit(const string&amp; key, const string&amp; value) &#123;    mw.put(key, value);&#125;static void Emit(const string&amp; key) &#123;    cout &lt;&lt; &quot;Sum of values:&quot; &lt;&lt; key &lt;&lt; endl;&#125;class Mapper &#123;public:    virtual void Map(const MapInput&amp; input) = 0;&#125;;class Reducer &#123;public:    virtual void Reduce(ReduceInput* input) = 0;&#125;;class WordCounter : public Mapper &#123;public:    void Map(const MapInput&amp; input) override &#123;        const string&amp; text = input.value(); // 读取一行文本        const int n = text.size();        for (int i = 0; i &lt; n; ) &#123;            // 跳过行首空白            while ((i &lt; n) &amp;&amp; isspace(text[i])) i++;            // 确定单词的开头和结尾            int start = i;            while ((i &lt; n) &amp;&amp; !isspace(text[i])) i++;            if (start &lt; i)                Emit(text.substr(start, i-start), &quot;1&quot;);        &#125;    &#125;&#125;;// 用户自定义 Reduce 函数class Adder : public Reducer &#123;public:    void Reduce(ReduceInput* input) override &#123;        // 迭代所有拥有相同key的键值对，把它们的values加起来        int64_t value = 0;        string currentKey = input-&gt;key();        while (!input-&gt;end() &amp;&amp; currentKey == input-&gt;key()) &#123; // 直到下一个键值对的key与当前键值对的key不同为止            value += std::stoi(input-&gt;value());            input-&gt;NextValue(); // 找到下一个拥有相同key的键值对        &#125;        // Emit sum for input-&gt;key()        Emit(to_string(value));    &#125;&#125;;int main(int argc, char* argv[]) &#123;    ifstream in(R&quot;(C:\Users\zyt\CLionProjects\leetcode_2021\lyrics.txt)&quot;);    string content((istreambuf_iterator&lt;char&gt;(in)), istreambuf_iterator&lt;char&gt;());    MapInput minput(&quot;lyrics.txt&quot;, content);    cout &lt;&lt; &quot;minput:\n&quot; &lt;&lt; minput.value() &lt;&lt; endl;    WordCounter wc;    wc.Map(minput);    auto *rinput = new ReduceInput(mw.get());    while (!rinput-&gt;end()) &#123;        cout &lt;&lt; &quot;Key: &quot; &lt;&lt; rinput-&gt;key() &lt;&lt; &quot;\t&quot;;        Adder adder; // 模拟很多 adder        adder.Reduce(rinput);        rinput-&gt;NextValue();    &#125;    return 0;&#125;</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;给出一个Map和Reduce的具体实现，去除了所有分布式的特性【可能今后会添加】&lt;/p&gt;
&lt;!--more---&gt;
&lt;p&gt;MapReduce文章最后的附录A有一段C++代码，描述了Map和Reduce函数的编写和使用方法。但是仅靠看代码总是不能深入理解MapReduce的实</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Word Count" scheme="https://superlova.github.io/tags/Word-Count/"/>
    
    <category term="MapReduce" scheme="https://superlova.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】用Hadoop实现Word Count</title>
    <link href="https://superlova.github.io/2021/05/06/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8Hadoop%E5%AE%9E%E7%8E%B0Word-Count/"/>
    <id>https://superlova.github.io/2021/05/06/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8Hadoop%E5%AE%9E%E7%8E%B0Word-Count/</id>
    <published>2021-05-06T00:18:12.000Z</published>
    <updated>2022-08-02T13:32:34.816Z</updated>
    
    <content type="html"><![CDATA[<p>好像不管什么Hadoop学习都是从Word Count开始，跨越语言的Word Count</p><!--more---><p>Word Count是MapReduce 的经典入门案例，其目标是统计给定一系列文本文件的单词出现次数。</p><p>编程思路：</p><p><code>map</code>阶段：把输入的数据进行分词，并将其组合成键值对的形式，key是单词，value全部标记为1。</p><p><code>shuffle</code> 阶段：经过默认的排序分区分组，key相同的单词会作为一组数据构成新的键值对。</p><p><code>reduce</code>阶段：处理 <code>shuffle</code> 完的一组数据，该组数据就是该单词所有的键值对。</p><p><img src="/2021/05/06/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8Hadoop%E5%AE%9E%E7%8E%B0Word-Count/Word_Count_MR阶段.png" alt></p><h2 id="Map类的编写"><a href="#Map类的编写" class="headerlink" title="Map类的编写"></a>Map类的编写</h2><pre><code class="lang-java">/** * @description: WordCount Mapper类，对应MapTask * @author: ZYT * KEYIN map阶段的输入 * VALUEIN  todo MapReduce 有默认读取数据的组件：TextInputFormat *          todo 逐行读取。k是偏移量（LongWritable），无意义；v是每行的文本内容（Text）。 * KEYOUT 单词类型，Text * VALUEOUT 次数，LongWritable */public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123;    /**     * 每当TextInputFormat返回一个键值对，map就调用一次。     * 根据TextInputFormat的特性，事实上是每一行文本调用一次Map方法。     * @param key     * @param value     * @param context     * @throws IOException     * @throws InterruptedException     */    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;//        super.map(key, value, context);        String text = value.toString();        String[] words = text.split(&quot;\\s+&quot;);        for (String word : words) &#123;            context.write(new Text(word), new LongWritable(1));        &#125;    &#125;&#125;</code></pre><p>优化</p><pre><code class="lang-java">public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123;    private Text outKey = new Text();    private static final LongWritable outValue = new LongWritable(1);    /**     * 每当TextInputFormat返回一个键值对，map就调用一次。     * 根据TextInputFormat的特性，事实上是每一行文本调用一次Map方法。     * @param key     * @param value     * @param context     * @throws IOException     * @throws InterruptedException     */    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;//        super.map(key, value, context);        String text = value.toString();        String[] words = text.split(&quot;\\s+&quot;);        for (String word : words) &#123;            outKey.set(word);            context.write(outKey, outValue);        &#125;    &#125;&#125;</code></pre><h2 id="Reduce类的编写"><a href="#Reduce类的编写" class="headerlink" title="Reduce类的编写"></a>Reduce类的编写</h2><pre><code class="lang-java">public class WordCountReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt; &#123;    private LongWritable outValue = new LongWritable(0);    /**     * todo q: 当map的输出数据来到reduce之后该如何调用？     * 1. 排序所有pair     * 2. 分组pair，key相同的分成一组     * 3. 每一组调用一次reduce     * @param key     * @param values     * @param context     * @throws IOException     * @throws InterruptedException     * 输出key：该组的单词     * 输出value：该组所有次数的迭代器。     */    @Override    protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123;//        super.reduce(key, values, context);        long count = 0;        for (LongWritable value : values) &#123;            count += value.get();        &#125;        outValue.set(count);        context.write(key, outValue);    &#125;&#125;</code></pre><h2 id="Driver类的编写"><a href="#Driver类的编写" class="headerlink" title="Driver类的编写"></a>Driver类的编写</h2><pre><code class="lang-java">/** * 该类是MapReduce程序客户端驱动类。主要是为了构造Job对象，指定各种组件的属性。 * 包括Mapper、Reducer、输入输出类型、数据路径、提交作业等。 */public class WordCountDriver &#123;    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException &#123;        // 创建配置对象        Configuration conf = new Configuration();        // 构建Job作业实例， 参数为Conf、Job名字        Job job = Job.getInstance(conf, WordCountDriver.class.getSimpleName());        // 设置MR程序运行的主类        job.setJarByClass(WordCountDriver.class);        // 设置本次MR程序的Mapper、Reducer类        job.setMapperClass(WordCountMapper.class);        job.setReducerClass(WordCountReducer.class);        // 指定Mapper阶段输出的kv类型        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(LongWritable.class);        // 指定Reducer阶段kv类型，也是最终输出的kv类型        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(LongWritable.class);        // 配置本次作业的输入输出数据路径        // todo: 默认组件 TextInputFormat、TextOutputFormat        FileInputFormat.setInputPaths(job, new Path(args[0]));        FileOutputFormat.setOutputPath(job, new Path(args[1]));        // 提交作业//        job.submit();        // 采用waitForCompletion方式提交job 参数表示是否开启实时追踪作业执行情况的功能        boolean res_flag = job.waitForCompletion(true);        // 退出程序，和job结果进行绑定        System.exit(res_flag ? 0 : 1);    &#125;&#125;</code></pre><p>继承工具类Tool的Driver</p><pre><code class="lang-java">/** * 使用ToolRunner提交MapReduce作业 */public class WordCountDriver_v2 extends Configured implements Tool &#123;    public static void main(String[] args) throws Exception &#123;        // 创建配置对象        Configuration conf = new Configuration();        // 使用ToolRunner提交程序        int status = ToolRunner.run(conf, new WordCountDriver_v2(), args);        // 退出客户端        System.exit(status);    &#125;    @Override    public int run(String[] args) throws Exception &#123;        // 构建Job作业实例， 参数为Conf、Job名字        Job job = Job.getInstance(getConf(), WordCountDriver_v2.class.getSimpleName());        // 设置MR程序运行的主类        job.setJarByClass(WordCountDriver_v2.class);        // 设置本次MR程序的Mapper、Reducer类        job.setMapperClass(WordCountMapper.class);        job.setReducerClass(WordCountReducer.class);        // 指定Mapper阶段输出的kv类型        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(LongWritable.class);        // 指定Reducer阶段kv类型，也是最终输出的kv类型        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(LongWritable.class);        // 配置本次作业的输入输出数据路径        // todo: 默认组件 TextInputFormat、TextOutputFormat        FileInputFormat.setInputPaths(job, new Path(args[0]));        FileOutputFormat.setOutputPath(job, new Path(args[1]));        return job.waitForCompletion(true) ? 0 : 1;    &#125;&#125;</code></pre><h2 id="如何运行MapReduce程序？"><a href="#如何运行MapReduce程序？" class="headerlink" title="如何运行MapReduce程序？"></a>如何运行MapReduce程序？</h2><p>MapReduce程序是单机运行还是分布式运行？</p><p>MapReduce程序需要的运算资源是Hadoop YARN分配还是本机自己分配？</p><p>运行在何种模式，取决于 mapreduce.framwork.name</p><p>yarn: 集群模式</p><p>local: 本地模式</p><p>如果不指定，默认是local模式。</p><p>在 mapred-default.xml 中定义。如果代码中（conf.set）、运行的环境中（mapred-site.xml）有配置，则会覆盖default的配置。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;好像不管什么Hadoop学习都是从Word Count开始，跨越语言的Word Count&lt;/p&gt;
&lt;!--more---&gt;
&lt;p&gt;Word Count是MapReduce 的经典入门案例，其目标是统计给定一系列文本文件的单词出现次数。&lt;/p&gt;
&lt;p&gt;编程思路：&lt;/p&gt;
&lt;</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Hadoop" scheme="https://superlova.github.io/tags/Hadoop/"/>
    
    <category term="Word Count" scheme="https://superlova.github.io/tags/Word-Count/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】数据密集性应用系统设计CH1笔记</title>
    <link href="https://superlova.github.io/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E6%80%A7%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1CH1%E7%AC%94%E8%AE%B0/"/>
    <id>https://superlova.github.io/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E6%80%A7%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1CH1%E7%AC%94%E8%AE%B0/</id>
    <published>2021-05-04T13:23:13.000Z</published>
    <updated>2022-08-02T13:32:34.805Z</updated>
    
    <content type="html"><![CDATA[<p>第一章：可靠、可扩展和可维护的应用系统<br><!--more---></p><h3 id="数据密集型问题的挑战："><a href="#数据密集型问题的挑战：" class="headerlink" title="数据密集型问题的挑战："></a>数据密集型问题的挑战：</h3><p>数据量，数据复杂度，数据的快速多变性；</p><h3 id="数据密集型应用包含以下模块："><a href="#数据密集型应用包含以下模块：" class="headerlink" title="数据密集型应用包含以下模块："></a>数据密集型应用包含以下模块：</h3><ul><li>数据库；</li><li>高速缓存；</li><li>索引：用户可以按关键字搜索数据井支持各种过滤操作；</li><li>流式处理：持续发送消息至另一个进程，处理采用异步方式；</li><li>批处理：定期处理大量累计数据。</li></ul><h3 id="数据系统"><a href="#数据系统" class="headerlink" title="数据系统"></a>数据系统</h3><p>本章提出<strong>数据系统</strong>的概念，包括<strong>数据库</strong>、<strong>消息队列</strong>和<strong>高速缓存</strong>等不同类型的系统。这样分类的原因有以下几点：</p><ol><li><p>都能将数据保存一段时间。区别在于访问模式不同，以及由于不同访问模式导致的不同性能。</p></li><li><p>新技术、工具拥有多种功能，系统之间的界限变得模糊。比如redis既能<strong>存储</strong>也能作为<strong>消息队列</strong>，kafka作为<strong>消息队列</strong>也能<strong>持久化存储</strong>。</p></li><li><p>系统也需要细分，原有的但各组件无法满足数据处理与存储需求，三分类概念需要进一步分解。例如，即便是一个简单的数据系统，也可能由很多子组件构成：</p></li></ol><p><img src="/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E6%80%A7%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1CH1%E7%AC%94%E8%AE%B0/一种数据系统架构.png" alt></p><p>这个应用包含缓存层（Memcached）和全文索引服务器（Elasticsearch或者Solr）以及数据库。程序员编写应用代码来控制缓存、索引和数据同步操作。</p><h3 id="数据系统的三种关键特性："><a href="#数据系统的三种关键特性：" class="headerlink" title="数据系统的三种关键特性："></a>数据系统的三种关键特性：</h3><ul><li><p>可靠性（Reliability）：当出现意外情况，如软件、硬件故障，人为操作失误等现象时，系统仍可以正常运转的能力。这里的正常运转，是指牺牲部分性能条件下，提供正确的服务。</p></li><li><p>可扩展性（Scalability）：随着问题规模的增长，比如流量、数据量或者任务复杂度，系统以合理方式匹配这种增长的能力。</p></li><li><p>可维护性（Maintainability）：新的开发或者维护人员上手的容易程度，以及适配新场景的能力。</p></li></ul><h3 id="可靠性"><a href="#可靠性" class="headerlink" title="可靠性"></a>可靠性</h3><p>什么是可靠性？即使发生了某些错误，系统仍可以继续正常工作的能力。</p><ul><li>故障或错误（fault）：组件偏离正确规格的现象。</li><li>失效（failure）：系统宕机，无法提供服务。</li></ul><p>为保证可靠性，需要设计容错机制（fault-tolerant）来避免故障引发系统失效，而不是避免故障（当然，避免故障也是提升可靠性的手段）。</p><h3 id="可靠性指标"><a href="#可靠性指标" class="headerlink" title="可靠性指标"></a>可靠性指标</h3><h4 id="失效前平均时间（MTTF）"><a href="#失效前平均时间（MTTF）" class="headerlink" title="失效前平均时间（MTTF）"></a>失效前平均时间（MTTF）</h4><p>是针对不可修复系统而言的，是指系统发生失效前的平均工作（或存储） 时间或工作次数。越高越好。</p><h4 id="平均无故障时间（MTBF）"><a href="#平均无故障时间（MTBF）" class="headerlink" title="平均无故障时间（MTBF）"></a>平均无故障时间（MTBF）</h4><p>是针对可修复系统而言的，指两次相邻失效（故障） 之间的工作时间， 而不是指整个系统的报废时间。越高越好。</p><h4 id="平均修复时间（MTTR）"><a href="#平均修复时间（MTTR）" class="headerlink" title="平均修复时间（MTTR）"></a>平均修复时间（MTTR）</h4><p>是对可修复系统而言的，指从出现故障到修复中间的这段时间。越低越好。</p><h4 id="可用性"><a href="#可用性" class="headerlink" title="可用性"></a>可用性</h4><p>又称为有效性，一般用可用度来定量计算，公式为</p><script type="math/tex; mode=display">A=\frac{\text{MTBF}}{\text{MTBF}+\text{MTTR}}</script><p>可靠性通常低于可用性。对于不可维修系统， 可用度就仅仅决定于且等于可靠度。</p><h3 id="数据系统可能发生哪些故障？如何提升容错能力？"><a href="#数据系统可能发生哪些故障？如何提升容错能力？" class="headerlink" title="数据系统可能发生哪些故障？如何提升容错能力？"></a>数据系统可能发生哪些故障？如何提升容错能力？</h3><h4 id="1-硬件故障"><a href="#1-硬件故障" class="headerlink" title="1. 硬件故障"></a>1. 硬件故障</h4><p>包括硬盘错误、电源错误、网口接触不良等。</p><p>最耐用的硬盘的MTTF为10~50年，因此一个包括10000个磁盘的存储集群中，一天坏一个也是很正常的（假设购买硬盘在时间上是均匀分布）。</p><p>应对方案是添加冗余，比如上述硬盘问题就可以用RAID，电源用双电源，甚至热插拔CPU等。</p><p>以上是从硬件角度解决硬件故障，其实可以通过软件角度解决硬件故障问题。</p><h4 id="2-软件错误"><a href="#2-软件错误" class="headerlink" title="2. 软件错误"></a>2. 软件错误</h4><p>操作系统内核的Bug，系统依赖的服务突然没有响应，某个组件的失控等。</p><p>软件系统的问题有时没有快速解决的办法，只能通过检查依赖假设和系统交互、进行全面测试等方法来预防，或者允许进程重启、评估运行时表现等应对。</p><h4 id="3-人为失误"><a href="#3-人为失误" class="headerlink" title="3. 人为失误"></a>3. 人为失误</h4><p>人是最不可靠的因素。运维人员配置错误往往是系统下线的主要原因。</p><p>这部分的预防以及应对，更加依赖软件工程领域的知识。</p><h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><p>可扩展性是用来描述系统应对负载增加的能力的术语。一个系统添加计算单元越容易，添加计算单元后计算能力的增长越多，就称这个系统的可扩展性越强。</p><h3 id="如何度量系统负载"><a href="#如何度量系统负载" class="headerlink" title="如何度量系统负载"></a>如何度量系统负载</h3><p>每秒请求处理次数、数据库中写入的比例、聊天室的同时活动用户数量、缓存命中率等，</p><p>还要注意分析平均值和峰值对性能的影响</p><h3 id="实例：twitter的两个实现版本"><a href="#实例：twitter的两个实现版本" class="headerlink" title="实例：twitter的两个实现版本"></a>实例：twitter的两个实现版本</h3><p>twitter有两个功能（类比微博，为了方便理解我接下来以微博代替推特），根据其2012年的数据，其功能和负载如下：</p><p>发微博：平均每秒4600条发布申请，峰值12000条推特申请发布。</p><p>收微博：平均每秒300,000条收微博的请求。</p><p>twitter第一个版本使用如下系统设计完成微博的收发：</p><p>发送微博：用户发送的微博插入全局微博集合。</p><p>用户申请查看自己的时间线：  </p><ul><li>遍历所有User的关注对象；</li><li>提取所有关注对象的微博；</li><li>按照发表时间排序并合并。</li></ul><p>随着注册用户变多，负载压力与日俱增，因此采取第二种方法：</p><p>每个用户的时间线维护一个缓存。</p><p>用户发表新的微博：</p><ul><li>查询关注对象；</li><li>插入到每个粉丝的时间线缓存。</li></ul><p>方法二的好处是，用户发布微博时多做一些事情可以加速用户接收微博时的性能。而用户接收微博的请求负载比发送负载高两个数量级。</p><p>Twitter针对那些粉丝量特别多的大V采用方法一，针对粉丝量不太大的绝大多数用户使用方法二。</p><h3 id="如何度量系统性能"><a href="#如何度量系统性能" class="headerlink" title="如何度量系统性能"></a>如何度量系统性能</h3><p><strong>吞吐量</strong></p><p>吞吐量是在一个特定时间段内完成的任务的计数，例如：每秒点击数。</p><p>系统吞吐量几个重要参数：QPS（TPS）、并发数、响应时间</p><p>并发数： 系统同时处理的request/事务数</p><p>TPS：Transactions Per Second（每秒传输的事物处理个数），即服务器每秒处理的事务数。</p><p>QPS：每秒查询率，即对一个特定的查询服务器在规定时间内所处理流量大小。</p><p><strong>响应时间</strong></p><p>从客户端发送请求到接受响应之间的间隔时长。在线系统通常更看重响应时间，批处理系统更关心吞吐量。</p><p>由于每次请求的响应时间服从一个分布，因此我们更关心平均响应时长，或者<strong>百分位数时长</strong>。将响应时长排序，取50百分位数（中位数）比平均数更有意义。其他有意义的百分位数有95、99和99.9百分位数。</p><p>采用较高的响应时间百分位数很重要，因为他们直接影响用户的总体服务体验。95百分位数的响应时间为1.5秒，意味着100个请求中5个请求慢于1.5秒，但是对于电商平台来说，有可能恰恰是这些顾客购买了更多的商品导致访问变慢。</p><p>排队延迟是影响高百分数响应时间的主要延迟来源。因此，如果在性能测试时，负载生成客户端在服务器处理完之前请求后再发送新的请求，就会忽视了排队造成的延迟。</p><h3 id="长尾效应"><a href="#长尾效应" class="headerlink" title="长尾效应"></a>长尾效应</h3><p>一个服务涉及多个不同的后端调用，则最慢的调用会拖累整个服务的响应时间，这种现象称之为长尾效应。</p><p>用户总是需要等待最慢的那个调用完成。因此即便只有很小比例的请求缓慢，也可能由于某一个用户频繁产生这种调用而导致总体变慢。</p><h3 id="如何应对负载增加，提升可扩展性？"><a href="#如何应对负载增加，提升可扩展性？" class="headerlink" title="如何应对负载增加，提升可扩展性？"></a>如何应对负载增加，提升可扩展性？</h3><p>垂直扩展：升级到更强大的机器。</p><p>水平扩展：将负载分布到更多小机器。</p><p>系统设计时，要在这两种扩展中间作取舍。同时要根据不同的吞吐量、请求方式等做针对性优化。</p><h3 id="可维护性"><a href="#可维护性" class="headerlink" title="可维护性"></a>可维护性</h3><p>软件工程师不喜欢读别人留下的代码，不喜欢维护别人开发的老系统，这已经是众所周知的事实。</p><p>但是换个角度，不如说大多数系统在设计之初就没有考虑令后续开发者方便地维护这一个特性。</p><p>为了提升系统的可维护性，减少维护期间的麻烦，可以遵循软件系统的三个设计原则：</p><p>可运维性、简单性、可演化性。</p><h4 id="可运维性"><a href="#可运维性" class="headerlink" title="可运维性"></a>可运维性</h4><p>目标是令运维人员更轻松。之前讨论过，人是系统中最不稳定的因素。因此简化运维人员的操作，使运维人员能够专注于高附加值的任务，能够提升系统的可维护性。</p><p>可以从以下角度提升可运维性：</p><ul><li>提供系统运行时监控工具，方便监控；</li><li>自动化工具；</li><li>避免绑定特定的机器；</li><li>提供良好的文档；</li><li>提供良好的默认配置；</li><li>尝试自我修复等。</li></ul><h4 id="简单性"><a href="#简单性" class="headerlink" title="简单性"></a>简单性</h4><p>简单性是复杂性的反面。而复杂性有各种各样的表现方式，比如模块紧耦合，状态空间膨胀，依赖关系复杂，命名方法混乱，各种性能trick等。</p><p>消除复杂性的最好手段之一是抽象，通过抽象掩盖大量实现细节，提供干净的接口。</p><h4 id="可演化性"><a href="#可演化性" class="headerlink" title="可演化性"></a>可演化性</h4><p>提升可演化性是令系统易于改变的另一种说法。目标是可以轻松地修改数据系统，使其适应不断变化的需求。</p><p>在组织流程方面，敏捷开发模式为适应变化提供了很好的参考。因此敏捷性与可演化性很类似。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;第一章：可靠、可扩展和可维护的应用系统&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h3 id=&quot;数据密集型问题的挑战：&quot;&gt;&lt;a href=&quot;#数据密集型问题的挑战：&quot; class=&quot;headerlink&quot; title=&quot;数据密集型问题的挑战：&quot;&gt;&lt;/a&gt;数据密集型问题的挑战</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Big Data" scheme="https://superlova.github.io/tags/Big-Data/"/>
    
    <category term="Distributed System" scheme="https://superlova.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读笔记】MapReduce: Simplified Data Processing on Large Clusters</title>
    <link href="https://superlova.github.io/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/"/>
    <id>https://superlova.github.io/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/</id>
    <published>2021-05-04T11:27:32.000Z</published>
    <updated>2022-08-02T13:32:34.872Z</updated>
    
    <content type="html"><![CDATA[<p>网络上描写MapReduce的文章不可胜计，唯倜傥非常之文章留存。</p><!--more---><h2 id="一、MapReduce的概念概括"><a href="#一、MapReduce的概念概括" class="headerlink" title="一、MapReduce的概念概括"></a>一、MapReduce的概念概括</h2><p>MapReduce是Google提出的一个软件架构，用于大规模数据集的并行运算。</p><p>MapReduce是一个编程范式，旨在使用<code>map</code>把大规模的问题分解成子问题，然后利用<code>reduce</code>把子问题的解汇总。这种编程范式特别适合应用于分布式系统。</p><p>要理解<code>map</code>和<code>reduce</code>的操作，最重要是要理解下式：</p><p><img src="/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/map和reduce.png" alt></p><p>k1和v1是原始的输入key和value；<br>list(k2, v2)是<code>map</code>把k1和v1分布式计算后的中间结果集合；<br>reduce(k2, list(v2))是<code>reduce</code>函数根据k2的值来合并v2；<br>最终我们想要得到的结果是list(v2)。</p><h2 id="二、MapReduce的应用场景："><a href="#二、MapReduce的应用场景：" class="headerlink" title="二、MapReduce的应用场景："></a>二、MapReduce的应用场景：</h2><ol><li>WordCount——分布式系统的“Hello World”</li></ol><p><code>map</code>函数输出文档中的每个词以及它的频率(word, 1)，<code>reduce</code>函数负责把所有同样词的频率累加起来。得到每个词出现的频率。这一个应用实现请看这里TODO。</p><ol><li>分布式字符串匹配 grep</li></ol><p><code>map</code>函数输出匹配某个模式的一行，<code>reduce</code>则什么计算也不做，只负责将那一行输出。得到匹配的文本行。</p><ol><li>计算 URL 的访问频率</li></ol><p><code>map</code>函数记录requests，输出(url, 1)，<code>reduce</code>函数将相同url的访问加起来，产生(url, sum_of_count)。</p><ol><li>倒转网络链接关系</li></ol><p>搜索引擎排序算法 pagerank 需要使用爬虫爬取所有页面 source 及页面内部的链接 target 。每个 source 内部可能存在多个 target。遇到门户网站，一个 source 有上千个 target 都不奇怪。</p><p>那么搜索引擎是如何决定哪个 target 比较重要的呢？</p><p>pagerank算法解决了这个问题，它假设如果一个网页被很多其他网页所链接，说明它受到普遍的关注和信赖，那么它的排名就高。同时，每个网页的权重不同，那些权重较大的网页拥有的链接更可靠，这些链接的排名往往更靠前。</p><p>这就需要网络爬虫统计每个链接 target 被哪些 source 引用过这种信息了。但是之前我们获得的是 (source, target)数据对，如何把这一部分数据转换成(target, list(source))呢？这就轮到MapReduce出场了。</p><p><code>map</code>负责输出 (source, target) 对，<code>reduce</code> 负责将相同的 target 归并，生成 (target, list(source))。</p><ol><li>确定每个Host的检索词向量（Term-Vector） <span id="1"></span></li></ol><p>检索词向量是一系列（单词，词频）对，这些数据对能够总结一篇文档或者一系列文章的最重要单词（假设词频越高越重要）。</p><p>一个Host可能有非常多文档，如何确定一个Host的检索词向量？</p><p><code>map</code>负责输出每个输入文档的(host, term-vector)，<code>reduce</code>负责汇总给定host的所有term-vector，丢弃所有低频词，输出最终唯一的(host, term-vector)。</p><ol><li>倒排索引</li></ol><p>什么是正排索引？(document, {keys})这种索引形式，从文档出发，检索关键词。</p><p>不过正排索引在搜索引擎中显然没什么作用，因为我们的应用场景是根据关键词检索到对应的所有文档。因此我们更需要(key, {documents})这种索引形式。</p><p>倒排索引就是关键词到文档的映射。每个关键词都对应着一系列的文档。</p><p><code>map</code>分析每个文档，为每个word生成(word, document)的映射，<code>reduce</code>汇总所有相同word的数据对，输出(word, {documents})（可能还会排序）。</p><h2 id="三、MapReduce的实现"><a href="#三、MapReduce的实现" class="headerlink" title="三、MapReduce的实现"></a>三、MapReduce的实现</h2><p>既然MapReduce这么好，那么究竟该怎么实现呢？</p><p>根据不同的集群以及节点性能，MapReduce有多种不同的实现方式。 </p><p>假设存在以下应用场景：普通配置的PC约1000台，机器之间使用交换机连接，网速为百兆，存储介质为廉价的IDE硬盘。用户希望能够通过向调度系统提交job，自动将job对应的一系列tasks分发到集群的各个节点上。</p><h3 id="3-1-MapReduce执行流程概括"><a href="#3-1-MapReduce执行流程概括" class="headerlink" title="3.1 MapReduce执行流程概括"></a>3.1 MapReduce执行流程概括</h3><p>本节我会综合论文原文的 MapReduce 与 Hadoop MapReduce 的具体实现，给出二者的执行步骤。</p><h4 id="3-1-1-论文中的-MapReduce-执行流程"><a href="#3-1-1-论文中的-MapReduce-执行流程" class="headerlink" title="3.1.1 论文中的 MapReduce 执行流程"></a>3.1.1 论文中的 MapReduce 执行流程</h4><ol><li><p>在map阶段，MapReduce会对要处理的数据进行分片（split）操作，为每一个分片分配一个MapTask任务。将输入分成M部分，每部分的大小一般在16M~64M之间（用户来定义）。输出也分为R部分（？）。然后在各个机器上fork程序副本。</p></li><li><p>选定集群中的一个机器为master节点，负责分发任务；其他节点是worker，负责计算和向master提交任务结果。</p></li><li><p>之前指定了M个map任务和R个reduce任务，master节点给每个空闲的worker分配一个map任务或者一个reduce任务。</p></li><li><p>被分配map任务的worker会读取对应的输入片段，输入用户定义的map函数，输出中间结果，将这些中间结果缓存在内存中。这些中间结果会定期地保存在本地此版中。由partition函数将其分成R部分。worker负责将这些缓存数据对在磁盘中的位置上传给master。</p></li><li><p>master负责收集map worker发送回来的数据对位置，然后把这些位置发送给 reduce worker。当一个reduce worker把这些中间结果读取完毕后，它会首先对这些中间结果排序，这样同样key的中间结果就会相邻了。</p></li></ol><p>key很多种类的情况下，排序是有必要的吗？实践表明，排序是有必要的，因为数据分片后，往往同一key的数据在同一片M中。这体现了数据在空间上的局部性。</p><p>但是如果数据量过大，中间结果过多，我们可能需要外部排序。</p><ol><li><p>reduce worker迭代所有中间结果，由于这些中间信息按照key排序过了，因此很容易获得同样key的所有键值对集合(key, {values})。将这一部分整合key后的信息传递给Reduce函数。Reduce函数的输出被追加到所属分区R的输出文件中。</p></li><li><p>当所有Map和Reduce任务都完成后，master唤醒用户程序，用户程序返回调用结果。</p></li></ol><p>下图是MapReduce论文中的流程概括图。</p><p><img src="/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/流程概括.png" alt></p><h4 id="3-1-2-Hadoop-MapReduce-执行流程"><a href="#3-1-2-Hadoop-MapReduce-执行流程" class="headerlink" title="3.1.2 Hadoop MapReduce 执行流程"></a>3.1.2 Hadoop MapReduce 执行流程</h4><p><img src="/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/MR流程.png" alt></p><ol><li><strong>Map阶段执行过程</strong></li></ol><p>对应流程图的左半部分。</p><ul><li>1) 把输入目录下文件按照一定标准逐个进行<strong>逻辑切片</strong>，形成切片规划</li></ul><p>默认切片大小和块大小是相同的，每个切片由一个MapTask来处理。</p><ul><li>2) 对切片中的数据按照一定规则解析并返回(key,value)对</li></ul><p>如果是文本数据，则调用TextInputFormat类。默认按行读取数据，key是每一行的起始偏移量，value是本行的文本内容。</p><p>key对应的偏移量是什么东西？打开notepad++，底栏的Pos就是当前光标所对应字符的偏移量。</p><p><img src="/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/偏移量.png" alt></p><ul><li>3) 调用Mapper类中的map方法处理数据。</li></ul><p>每读取解析出来的一个(key,value)，调用一次map方法。map方法是用户自己定义的业务逻辑。</p><ul><li><p>4) 按照一定的规则对Map输出的键值对进行分区(partition)。分区的数量就是reduce task的数量。</p></li><li><p>5) Map输出数据写入内存缓冲区，达到一定比例后溢出(spill)到磁盘上。溢出的时候根据key排序(sort)。</p></li><li><p>6) 对所有溢出的文件进行合并(merge)，形成一个文件。</p></li></ul><p>至此，Map阶段结束。map worker并不会将自己的结果发送给reduce，而是会静静地等待reduce worker来主动拉取数据。</p><ol><li><strong>Reduce阶段执行过程</strong></li></ol><p>对应流程图的右半部分。</p><ul><li><p>1) Reduce task会主动从MapTask复制拉取其输出的键值对；</p></li><li><p>2) 把复制到Reduce worker的本地数据全部合并(merge)，再对合并的数据排序。</p></li><li><p>3) 对排序后的键值对调用Reduce方法。</p></li></ul><p>键相等的键值对调用一次reduce方法。</p><p>最后把这些输出的键值对写入到HDFS文件中。</p><h3 id="3-2-Master节点的数据结构"><a href="#3-2-Master节点的数据结构" class="headerlink" title="3.2 Master节点的数据结构"></a>3.2 Master节点的数据结构</h3><p>Master节点会保存每个map任务和reduce任务的执行状态（空闲 idle、正在执行 in-progress，执行完毕 completed），还会给被分配任务了的worker保存一个标注。</p><p>Master还会像一个管道一样储存map任务完成后的中间信息存储位置，并把这些位置信息传输给reduce worker。</p><h3 id="3-3-容错机制"><a href="#3-3-容错机制" class="headerlink" title="3.3 容错机制"></a>3.3 容错机制</h3><h4 id="worker-failure"><a href="#worker-failure" class="headerlink" title="worker failure"></a>worker failure</h4><p>master会周期性地ping每个worker，规定时间内没有返回信息，则master将其标记为fail。master将所有由这个失效的worker做完的（completed）、正在做的（in-progress）<code>map</code>任务标记为初始状态（idle），等待其他worker认领任务。</p><p>worker故障时，对于map任务结果和reduce任务结果的处理方法有所不同。map任务的结果由于需要通知master存储位置，中途中断会导致存储位置丢失，因此失败的map任务需要重新执行；reduce任务的结果存储位置在全局文件系统上，因此不需要再次执行。</p><p>当worker B接手了失败的worker A的task，所有reduce worker都会知晓。因此所有还没有从worker A哪里获得数据的reduce worker会自动向worker B获取数据。</p><h4 id="master-failure"><a href="#master-failure" class="headerlink" title="master failure"></a>master failure</h4><p>首先要有检查点（checkpoint）机制，周期性地将master节点存储的数据保存至磁盘。</p><p>但是由于只有一个master进程，因此master失效后MapReduce运算会中止。由用户亲自检查，根据需要重新执行MapReduce。</p><h4 id="Semantics-in-the-Presence-of-Failures"><a href="#Semantics-in-the-Presence-of-Failures" class="headerlink" title="Semantics in the Presence of Failures"></a>Semantics in the Presence of Failures</h4><p>Semantics，语义。这个词不太好理解，总之本节讨论Failure的出现对程序语义的影响。</p><p>个人理解，对于一个确定性的程序而言，程序的寓意就是程序的执行顺序；但是对于非确定性程序而言，也就是执行多次可能得到不同结果的程序而言，语义是否能保证，直接与最后结果是否正确相关。</p><p>MapReduce保证，当Map和Reduce的操作都是确定性函数（只要相同输入就会得到相同输出的函数），那么MapReduce处理得到的结果也都是确定性的，不论集群内部有没有错误、执行顺序。</p><p>这种强保证是由Map和Reduce中的commit操作的原子性来保证的。</p><p>每个 in-progress task 都将其输出写进私有临时文件中。每个reduce产生一个私有临时文件，每个map产生R个私有临时文件（因为对应R个reduce任务）。</p><p>当map任务完成，map worker发送给master的是那R个临时文件的名称，并标注“我做完了”。master在收到消息后，就将这R个文件名记录在自己的数据结构中。<strong>如果这个时候由于某些错误</strong>，master又收到一遍“我做完了”，master将会忽略。</p><p>当reduce任务完成，reduce worker把临时文件重命名为最终的输出文件名。重命名操作是原子的，即要不就全部重命名成功，要不就一个都不会重命名。这里存在一种语义风险，那就是如果同一个reduce task在多台机器上执行，同一个结果文件有可能被重命名多次。为保证最终文件系统只包含一个reduce任务产生的数据，MapReduce依赖底层文件系统提供的重命名操作（？）。</p><p>坦白说，关于弱语义如何保证这一块儿没看懂，等今后再回来补吧。TODO</p><h3 id="3-4-存储位置"><a href="#3-4-存储位置" class="headerlink" title="3.4 存储位置"></a>3.4 存储位置</h3><p>这一部分的设计要尽可能节约带宽，因为带宽是相当匮乏的资源。</p><p>MapReduce的解决方案是通过GFS (Google File System)将每个文件分成 64 MB的块，然后将每块保存几个副本（通常为3份）在不同的机器上。MapReduce 尽量将这些位置信息保存下来然后尽量将含有某个文件主机的任务分配给它，这样就可以减少网络的传递使用。如果失败，那么将会尝试从靠近输入数据的一个副本主机去启动这个任务。当在一个集群上执行大型的 MapReduce 操作的时候，输入数据一般都是本地读取，减少网络带宽的使用。</p><h3 id="3-5-任务粒度"><a href="#3-5-任务粒度" class="headerlink" title="3.5 任务粒度"></a>3.5 任务粒度</h3><p>理想状况下，M和R应当与worker数目大很多，这样才能提高集群的动态负载均衡能力，并且能加快故障恢复的速度，原因是失效的worker上执行的map任务可以分布到所有其他的worker机器上执行。</p><p>但是M和R也是有限制的，这一部分限制主要是由于master需要执行O(M+R)次调度。</p><p>我们通常会按照这样的比例执行：M=200,000，R=5,000，worker有2,000台。</p><h3 id="3-6-备用任务"><a href="#3-6-备用任务" class="headerlink" title="3.6 备用任务"></a>3.6 备用任务</h3><p>长尾分布现象（或者说“水桶效应”）在MapReduce中也有体现，因为MapReduce计算时间往往取决于其运行速度最慢的worker。</p><p>有一个办法来减少“straggler”（落伍的人），master会在任务快完成时，调用backup进程来解决那些 in-progress 任务。这样，无论是原来的进程还是 backup 进程中的哪个先完成，master都立即将其标记为完成。</p><h2 id="四、MapReduce调优技巧"><a href="#四、MapReduce调优技巧" class="headerlink" title="四、MapReduce调优技巧"></a>四、MapReduce调优技巧</h2><h3 id="4-1-分区函数"><a href="#4-1-分区函数" class="headerlink" title="4.1 分区函数"></a>4.1 分区函数</h3><p>分区函数一般是Hash，<code>Hash(key) % R</code>，这样就能把key分成R份了。但是有的时候我们希望自己定义R的分区方法，比如在第二章的应用场景<a href="#1">Host Term-Vector</a>中，我们希望以Host为分R标准，那么分区函数就可以这么写：<code>hash(Hostname(urlkey)) % R</code>。，这样具有相同的 hostname 的URL将会出现在同一个输出文件中。</p><h3 id="4-2-顺序保证"><a href="#4-2-顺序保证" class="headerlink" title="4.2 顺序保证"></a>4.2 顺序保证</h3><p>在一个分区R中，MapReduce保证所有中间k/v对都是按key排序的。</p><h3 id="4-3-Combiner"><a href="#4-3-Combiner" class="headerlink" title="4.3 Combiner"></a>4.3 Combiner</h3><p>某些任务的中间结果在从map传输到reduce的时候可以先处理一下再传。比如word count应用，中间结果是一堆(word, 1)数据对，这个时候我们利用某个combiner函数，将本地的中间结果合并一下，比如合并相同的100个(word, 1)为(word, 100)，就大量降低了数据传输占用的带宽。</p><p>Combiner函数会在每台执行Map任务的机器上执行一次。通常情况下，Combiner函数和Reduce函数的实现代码是一样的。</p><h3 id="4-4-输入和输出"><a href="#4-4-输入和输出" class="headerlink" title="4.4 输入和输出"></a>4.4 输入和输出</h3><p>MapReduce库支持不同的格式的输入数据。比如文本模式，key是行数，value是该行内容。</p><p>程序员可以定义Reader接口来适应不同的输入类型。程序员需要保证必须能把输入数据切分成数据片段，且这些话宿儒片段能够由单独的Map任务来处理就行了。</p><p>Reader的数据源可能是数据库，可能是文本文件，甚至是内存等。输入Writer同样可以自定义。</p><h3 id="4-5-副作用"><a href="#4-5-副作用" class="headerlink" title="4.5 副作用"></a>4.5 副作用</h3><p>程序员在写Map和Reduce操作的时候，可能会处于方便，定义很多额外功能，比如生成辅助文件等。但应当时刻记住，Map和Reduce操作应当保证原子性和幂等性。</p><p>比如，一个task生成了多个输出文件，但是我们没有原子化多段commit的操作。这就需要程序员自己保证生成多个输出的任务是确定性任务。</p><h3 id="4-6-跳过损坏的纪录"><a href="#4-6-跳过损坏的纪录" class="headerlink" title="4.6 跳过损坏的纪录"></a>4.6 跳过损坏的纪录</h3><p>有时相比于修复不可执行的Bug，跳过该部分引起Bug的Record更加可取。因此，我们希望MapReduce检测到可能引起崩溃的Record时，自动跳过。</p><p>MapReduce如何自动检测这种现象？首先每个worker会通过一个handler来捕获异常，并利用一个全局变量来保存异常序号。worker会在之后发送给master的工作汇报中写上该signal序号（以UDP发送）。master看到该UDP包中存在多次故障，那么将来该worker失败了，master就不会重复执行该task，而是跳过该record。</p><h3 id="4-7-本地执行"><a href="#4-7-本地执行" class="headerlink" title="4.7 本地执行"></a>4.7 本地执行</h3><p>就是说一上来就在成千上万台机器上进行调试是非常棘手的，因此MapReduce开发了在本地计算机上模拟MapReduce任务的项目，方便调试。</p><h3 id="4-8-状态信息"><a href="#4-8-状态信息" class="headerlink" title="4.8 状态信息"></a>4.8 状态信息</h3><p>master内部有一个内置的HTTP服务器，可以用来展示一组状态信息页面。状态页面会显示计算进度，例如：已经完成的任务数量、正在执行的任务数量、输入的字节数、中间数据的字节数、输出的字节数、处理率等等。</p><p>这些页面也包含了指向每个任务的标准差以及生成的标准输出文件的链接。用户可以使用这些数据来预测计算需要多久才能完成，是否需要往该计算中增加更多资源。当计算消耗的时间比预期时间更长的时候，这些页面也可以用来找出为什么执行速度很慢的原因。</p><p>此外，顶层的状态页面会显示那些故障的worker，以及它们故障时正在运行的Map和Reduce任务。这些信息对于调试用户代码中的bug很有帮助。</p><p>这一点HDFS也有类似实现，比如HDFS 在启动完成之后，还会由内部的 Web 服务提供一个查看集群状态的网页：</p><p><a href="http://localhost:50070/">http://localhost:50070/</a></p><p><strong>提供可视化监控界面，是提升分布式系统的可维护性的重要手段</strong>。</p><h3 id="4-9-计数器"><a href="#4-9-计数器" class="headerlink" title="4.9 计数器"></a>4.9 计数器</h3><p>MapReduce内部提供计数器机制，用来统计不同操作发生次数。要想使用计数器，程序员需要创建Counter对象，然后在Map和Reduce函数中以正确的方式增加counter。</p><p>当聚合这些counter的值时，master会去掉那些重复执行的相同map或者reduce操作的次数，以此避免重复计数（之前提到的备用任务和故障后重新执行任务，这两种情况会导致相同的任务被多次执行）。</p><p>有些counter值是由MapReduce库自动维护的，例如已经处理过的输入键值对的数量以及生成的输出键值对的数量。</p><h2 id="五、MapReduce的性能评估"><a href="#五、MapReduce的性能评估" class="headerlink" title="五、MapReduce的性能评估"></a>五、MapReduce的性能评估</h2><h2 id="六、MapReduce使用经验"><a href="#六、MapReduce使用经验" class="headerlink" title="六、MapReduce使用经验"></a>六、MapReduce使用经验</h2><p>本文只关注MapReduce的技术细节，故第五、六节略过。</p><h2 id="七、参考"><a href="#七、参考" class="headerlink" title="七、参考"></a>七、参考</h2><p>Lassen S B . MapReduce: Simplified Data Processing on Large Clusters (work by Jeffrey Dean and Sanjay Ghemawat). </p><p><a href="https://chunlife.top/2020/04/18/Google-MapReduce%E4%B8%AD%E6%96%87%E7%89%88/">https://chunlife.top/2020/04/18/Google-MapReduce%E4%B8%AD%E6%96%87%E7%89%88/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;网络上描写MapReduce的文章不可胜计，唯倜傥非常之文章留存。&lt;/p&gt;
&lt;!--more---&gt;
&lt;h2 id=&quot;一、MapReduce的概念概括&quot;&gt;&lt;a href=&quot;#一、MapReduce的概念概括&quot; class=&quot;headerlink&quot; title=&quot;一、MapR</summary>
      
    
    
    
    <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
    <category term="Big Data" scheme="https://superlova.github.io/tags/Big-Data/"/>
    
    <category term="Distributed System" scheme="https://superlova.github.io/tags/Distributed-System/"/>
    
    <category term="MapReduce" scheme="https://superlova.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】Hadoop介绍和安装</title>
    <link href="https://superlova.github.io/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Hadoop%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%89%E8%A3%85/"/>
    <id>https://superlova.github.io/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Hadoop%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%89%E8%A3%85/</id>
    <published>2021-05-04T11:26:57.000Z</published>
    <updated>2022-08-02T13:32:34.789Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop学习笔记第一篇。<br><!--more---></p><h1 id="Hadoop介绍"><a href="#Hadoop介绍" class="headerlink" title="Hadoop介绍"></a>Hadoop介绍</h1><p>Apache Hadoop 软件库是一个框架，允许在<strong>集群服务器</strong>上使用简单的<strong>编程模型</strong>，<strong>对大数据集进行分布式处理</strong>。</p><p>Hadoop 可扩展性强，能从单台服务器扩展到数以千计的服务器；Hadoop 高可用，其代码库自身就能在应用层侦测并处理硬件故障。</p><p>Hadoop 的生态系统不仅包含 Hadoop，而且还包含 HDFS、HBase等基本组件。</p><p><img src="/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Hadoop%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%89%E8%A3%85/Hadoop生态系统.png" alt></p><p><strong>HDFS (Hadoop Distributed File System)</strong></p><p>HDFS是分布式文件系统的一种。HDFS是Hadoop生态系统的基本组成，它将数据保存在计算机集群上。HDFS是HBase等工具的基础。</p><p><strong>MapReduce</strong></p><p>MapReduce是一种分布式计算框架，也是一个分布式、并行处理的编程模型。MapReduce把任务分为<code>map</code>阶段和<code>reduce</code>阶段，<code>map</code>阶段将任务分解成子任务后映射到集群上，<code>reduce</code>将结果化简并整合。</p><p>正是利用了MapReduce的工作特性，Hadoop因此能以并行的方式访问数据，从而实现分布式计算。</p><p>关于MapReduce的论文讲解，请看<a href="https://superlova.github.io/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/">这里</a>。</p><p><strong>HBase</strong></p><p>HBase 是一个建立在 HDFS 之上，面向列的 NoSQL 数据库，用于快速读 / 写大量数据。HBase 使用 Zookeeper 进行管理。</p><p><strong>ZooKeeper</strong></p><p>ZooKeeper 为大型分布式计算提供开源的分布式配置服务、同步服务和命名注册。</p><p>Hadoop 的许多组件依赖于 Zookeeper，它运行在计算机集群中，用于管理 Hadoop 集群。</p><p><strong>Pig</strong></p><p>Pig是一个基于Hadoop的大规模数据分析平台，它为 MapReduce 编程模型提供了一个简单的操作和编程接口。它提供的SQL-LIKE语言叫Pig Latin，该语言的编译器会把类SQL的数据分析请求转换为一系列经过优化处理的MapReduce运算。</p><p><strong>Hive</strong><br>Apache Hive是一个建立在Hadoop架构之上的数据仓库。它能够提供数据的精炼，查询和分析。像 Pig 一样，Hive 作为一个抽象层工具，吸引了很多熟悉 SQL 而不是 Java 编程的数据分析师。</p><p>与Pig的区别在于，Pig是一中编程语言，使用命令式操作加载数据、表达转换数据以及存储最终结果。Pig中没有表的概念。而Hive更像是SQL，使用类似于SQL语法进行数据查询。</p><p><strong>Sqoop</strong></p><p>用于在关系数据库、数据仓库和 Hadoop 之间转移数据。</p><p><strong>Flume</strong></p><p>是一个分布式、可靠、高可用的海量日志采集、聚合和传输的系统，用于有效地收集、聚合和将大量日志数据从许多不同的源移动到一个集中的数据存储（如文本、HDFS、Hbase等）。</p><p><strong>Yarn</strong></p><p>是从Hadoop 2.0版本开始沿用的任务调度和集群资源管理的框架。</p><p><strong>Spark</strong></p><p>一个快速通用的 Hadoop 数据计算引擎，具有简单和富有表达力的编程模型，支持数据 ETL（提取、转换和加载）、机器学习、流处理和图形计算等方面的应用。</p><p>Spark 这一分布式内存计算框架就是脱胎于 Hadoop 体系的，它对 HDFS 、YARN 等组件有了良好的继承，同时也改进了 Hadoop 现存的一些不足。</p><p>下图是Hadoop集群的基本架构。</p><p><img src="/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Hadoop%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%89%E8%A3%85/Hadoop集群基本架构.png" alt></p><h1 id="Hadoop-可以做什么"><a href="#Hadoop-可以做什么" class="headerlink" title="Hadoop 可以做什么"></a>Hadoop 可以做什么</h1><p>据Hadoop Wiki记载，阿里巴巴使用15个节点组成的Hadoop集群，每个节点拥有8核心、16GB内存和1.4TB存储。阿里巴巴使用这些节点来处理商业数据的排序和组合，应用于交易网站的垂直搜索。</p><p>Ebay拥有32个节点组成的集群，使用Java编写的MapReduce应用，来优化搜索引擎。</p><p>FaceBook使用Hadoop来存储内部日志和结构化数据源副本，并且将其作为数据报告、数据分析和机器学习的数据源。</p><h1 id="Hadoop-不同版本"><a href="#Hadoop-不同版本" class="headerlink" title="Hadoop 不同版本"></a>Hadoop 不同版本</h1><p><strong>关于发行方：</strong></p><p>目前Hadoop发行版非常多，有Intel发行版，华为发行版、Cloudera发行版（CDH）、Hortonworks版本等，所有这些发行版均是基于Apache Hadoop衍生出来的，之所以有这么多的版本，是由于Apache Hadoop的开源协议决定的：任何人可以对其进行修改，并作为开源或商业产品发布/销售。</p><p><strong>关于版本：</strong></p><p>现在最新的Hadoop已经达到3.X了，然而大部分公司使用Hadoop 2.X。又由于Hadoop 2.X与1.X相比有较大变化，因此直接使用2.X是比较合理的选择。</p><p>Hadoop2.0新增了HDFS HA机制，HA增加了standbynamenode进行热备份，解决了1.0的单点故障问题。</p><p>Hadoop2.0新增了HDFS federation，解决了HDFS水平可扩展能力。 </p><p>2.0相比于1.0 新增了YARN框架，Mapreduce的运行环境发生了变化</p><h1 id="Hadoop-安装"><a href="#Hadoop-安装" class="headerlink" title="Hadoop 安装"></a>Hadoop 安装</h1><p>Hadoop有三种安装方式</p><ul><li>单机模式：安装简单，几乎不用做任何配置，但仅限于调试用途。</li><li>伪分布模式：在单节点上同时启动 NameNode、DataNode、JobTracker、TaskTracker、Secondary Namenode 等 5 个进程，模拟分布式运行的各个节点。</li><li>完全分布式模式：正常的 Hadoop 集群，由多个各司其职的节点构成。</li></ul><p>本文介绍 Hadoop 伪分布式模式部署方法，Hadoop 版本为 2.6.1。</p><h2 id="1-设置用户和组"><a href="#1-设置用户和组" class="headerlink" title="1. 设置用户和组"></a>1. 设置用户和组</h2><pre><code>sudo adduser hadoopsudo usermod -G sudo hadoop</code></pre><h2 id="2-安装JDK"><a href="#2-安装JDK" class="headerlink" title="2. 安装JDK"></a>2. 安装JDK</h2><p>不同版本的 Hadoop 对 Java 的版本需求有细微的差别，可以在<a href="https://cwiki.apache.org/confluence/display/HADOOP2/HadoopJavaVersions">这个网站</a>查询 Hadoop 版本与 Java 版本的关系。</p><p>测试jdk是否部署成功：</p><pre><code class="lang-sh">java -version</code></pre><h2 id="3-配置SSH免密码登录"><a href="#3-配置SSH免密码登录" class="headerlink" title="3. 配置SSH免密码登录"></a>3. 配置SSH免密码登录</h2><p>安装和配置 SSH 的目的是为了让 Hadoop 能够方便地运行远程管理守护进程的相关脚本。这些脚本需要用到 sshd 服务。</p><pre><code class="lang-sh">su hadoopcd /home/hadoopssh-keygen -t rsa# 将生成的公钥添加到主机认证记录中。cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys# 为 authorized_keys 文件赋予写权限chmod 600 .ssh/authorized_keys# 尝试登录到本机ssh localhost</code></pre><h2 id="4-下载-Hadoop"><a href="#4-下载-Hadoop" class="headerlink" title="4. 下载 Hadoop"></a>4. 下载 Hadoop</h2><pre><code class="lang-sh">wget https://archive.apache.org/dist/hadoop/common/hadoop-2.6.1/hadoop-2.6.1.tar.gztar zxvf hadoop-2.6.1.tar.gzsudo mv hadoop-2.6.1 /opt/hadoop-2.6.1sudo chown -R hadoop:hadoop /opt/hadoop-2.6.1vim /home/hadoop/.bashrc</code></pre><p>在 /home/hadoop/.bashrc 文件的末尾添加以下内容：</p><pre><code class="lang-sh">export HADOOP_HOME=/opt/hadoop-2.6.1export JAVA_HOME=/usr/lib/jvm/java-8-oracleexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</code></pre><p>在终端中输入 source 命令来激活新添加的环境变量。</p><pre><code class="lang-sh">source /home/hadoop/.bashrc</code></pre><h2 id="5-伪分布式模式配置"><a href="#5-伪分布式模式配置" class="headerlink" title="5. 伪分布式模式配置"></a>5. 伪分布式模式配置</h2><p>Hadoop 还可以以伪分布式模式运行在单个节点上，通过多个独立的 Java 进程来模拟多节点的情况。在初始学习阶段，暂时没有必要耗费大量的资源来创建不同的节点。</p><p>5.1 <strong>打开 core-site.xml 文件:</strong></p><pre><code class="lang-sh">vim /opt/hadoop-2.6.1/etc/hadoop/core-site.xml</code></pre><p>将 configuration 标签的值修改为以下内容：</p><pre><code class="lang-xml">&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;        &lt;value&gt;/home/hadoop/tmp&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>fs.defaultFS 配置项用于指示集群默认使用的文件系统的位置。</p><p>5.2 <strong>打开另一个配置文件 hdfs-site.xml</strong></p><pre><code class="lang-sh">vim /opt/hadoop-2.6.1/etc/hadoop/hdfs-site.xml</code></pre><p>将 configuration 标签的值修改为以下内容：</p><pre><code class="lang-xml">&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;dfs.replication&lt;/name&gt;        &lt;value&gt;1&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>该配置项用于指示 HDFS 中文件副本的数量，默认情况下是 3 份，由于我们在单台节点上以伪分布式的方式部署，所以将其修改为 1 。</p><p>5.3 <strong>编辑 hadoop-env.sh 文件：</strong></p><pre><code class="lang-sh">vim /opt/hadoop-2.6.1/etc/hadoop/hadoop-env.sh</code></pre><p>将其中 <code>export JAVA_HOME</code> 的值修改为 JDK 的实际位置，即 <code>/usr/lib/jvm/java-8-oracle</code> 。</p><p>5.4 <strong>编辑 yarn-site.xml 文件：</strong></p><pre><code class="lang-sh">vim /opt/hadoop-2.6.1/etc/hadoop/yarn-site.xml</code></pre><p>在 configuration 标签内添加以下内容：</p><pre><code class="lang-xml">&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>5.5 <strong>编辑 mapred-site.xml 文件。首先需要从模板复制过来：</strong></p><pre><code class="lang-sh">cp /opt/hadoop-2.6.1/etc/hadoop/mapred-site.xml.template /opt/hadoop-2.6.1/etc/hadoop/mapred-site.xmlvim /opt/hadoop-2.6.1/etc/hadoop/mapred-site.xml</code></pre><p>在 configuration 标签内添加以下内容：</p><pre><code class="lang-xml">&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;        &lt;value&gt;yarn&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h2 id="6-Hadoop-启动测试"><a href="#6-Hadoop-启动测试" class="headerlink" title="6. Hadoop 启动测试"></a>6. Hadoop 启动测试</h2><pre><code class="lang-sh">su -l hadoopvim /home/hadoop/.bashrc</code></pre><p>向<code>.bashrc</code>添加 Java 的环境变量：</p><pre><code class="lang-sh">export JAVA_HOME=/usr/lib/jvm/java-8-oracleexport PATH=$PATH:$JAVA_HOME/bin</code></pre><h1 id="HDFS-的基本使用"><a href="#HDFS-的基本使用" class="headerlink" title="HDFS 的基本使用"></a>HDFS 的基本使用</h1><h2 id="1-初始化HDFS"><a href="#1-初始化HDFS" class="headerlink" title="1. 初始化HDFS"></a>1. 初始化HDFS</h2><pre><code class="lang-sh">hdfs namenode -format</code></pre><p>格式化的操作只需要进行一次即可，不需要多次格式化。每一次格式化 namenode 都会清除 HDFS 分布式文件系统中的所有数据文件。同时，多次格式化容易出现 namenode 和 datanode 不同步的问题。</p><h2 id="2-启动HDFS"><a href="#2-启动HDFS" class="headerlink" title="2. 启动HDFS"></a>2. 启动HDFS</h2><p>HDFS 初始化完成之后，就可以启动 NameNode 和 DataNode 的守护进程。启动之后，Hadoop 的应用（如 MapReduce 任务）就可以从 HDFS 中读写文件。</p><p>在终端中输入以下命令来启动守护进程：</p><pre><code class="lang-sh">start-dfs.sh</code></pre><p>为了确认伪分布式模式下的 Hadoop 已经成功运行，可以利用 Java 的进程查看工具 <code>jps</code> 来查看是否有相应的进程。</p><p>如果执行 jps 发现没有 NameNode 服务进程，可以先检查一下是否执行了 namenode 的初始化操作。如果没有初始化 namenode ，先执行 stop-dfs.sh ,然后执行 hdfs namenode -format ,最后执行 start-dfs.sh 命令，通常来说这样就能够保证这三个服务进程成功启动</p><h2 id="3-查看日志和WebUI"><a href="#3-查看日志和WebUI" class="headerlink" title="3. 查看日志和WebUI"></a>3. 查看日志和WebUI</h2><p>作为大数据领域的学习者，掌握分析日志的能力与学习相关计算框架的能力同样重要。</p><p>Hadoop 的守护进程日志默认输出在安装目录的 log 文件夹中，在终端中输入以下命令进入到日志目录：</p><pre><code class="lang-sh">cd /opt/hadoop-2.6.1/logsls</code></pre><p>HDFS 在启动完成之后，还会由内部的 Web 服务提供一个查看集群状态的网页：</p><p><a href="http://localhost:50070/">http://localhost:50070/</a></p><p>打开网页后，可以在其中查看到集群的概览、DataNode 的状态等信息。</p><h2 id="4-HDFS文件上传测试"><a href="#4-HDFS文件上传测试" class="headerlink" title="4. HDFS文件上传测试"></a>4. HDFS文件上传测试</h2><p>HDFS 运行起来之后，可将其视作一个文件系统。此处进行文件上传的测试，首先需要按照目录层级逐个创建目录，并尝试将 Linux 系统中的一些文件上传到 HDFS 中。</p><pre><code class="lang-sh">cd ~hdfs dfs -mkdir /userhdfs dfs -mkdir /user/hadoop</code></pre><p>如果需要查看创建好的文件夹，可以使用如下命令：</p><pre><code class="lang-sh">hdfs dfs -ls /user</code></pre><p>目录创建成功之后，使用 <code>hdfs dfs -put</code> 命令将本地磁盘上的文件（此处是随意选取的 Hadoop 配置文件）上传到 HDFS 之中。</p><pre><code class="lang-sh">hdfs dfs -put /opt/hadoop-2.6.1/etc/hadoop /user/hadoop/input</code></pre><p>如果要查看上传的文件，可以执行如下命令：</p><pre><code class="lang-sh">hdfs dfs -ls /user/hadoop/input</code></pre><h1 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h1><p>WordCount 是 Hadoop 的 “HelloWorld” 程序。</p><p>绝大多数部署在实际生产环境并且解决实际问题的 Hadoop 应用程序都是基于 WordCount 所代表的 MapReduce 编程模型变化而来。</p><p>在终端中首先启动 YARN 计算服务：</p><pre><code class="lang-sh">start-yarn.sh</code></pre><p>然后输入以下命令以启动任务</p><pre><code class="lang-sh">hadoop jar /opt/hadoop-2.6.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.1.jar wordcount /user/hadoop/input/ output</code></pre><p>上述参数中，关于路径的参数有三个，分别是 jar 包的位置、输入文件的位置和输出结果的存放位置。在填写路径时，应当养成填写绝对路径的习惯。这样做将有利于定位问题和传递工作。</p><p>等待计算完成，然后将 HDFS 上的文件导出到本地目录查看：</p><pre><code class="lang-sh">rm -rf /home/hadoop/outputhdfs dfs -get /user/hadoop/output outputcat output/*</code></pre><p>计算完毕后，如无其他软件需要使用 HDFS 上的文件，则应及时关闭 HDFS 守护进程。</p><p>作为分布式集群和相关计算框架的使用者，应当养成良好的习惯，在每次涉及到集群开启和关闭、软硬件安装和更新的时候，都主动检查相关软硬件的状态。</p><pre><code class="lang-sh">stop-yarn.shstop-dfs.sh</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Hadoop学习笔记第一篇。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h1 id=&quot;Hadoop介绍&quot;&gt;&lt;a href=&quot;#Hadoop介绍&quot; class=&quot;headerlink&quot; title=&quot;Hadoop介绍&quot;&gt;&lt;/a&gt;Hadoop介绍&lt;/h1&gt;&lt;p&gt;Apache H</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Hadoop" scheme="https://superlova.github.io/tags/Hadoop/"/>
    
    <category term="Big Data" scheme="https://superlova.github.io/tags/Big-Data/"/>
    
    <category term="Distributed System" scheme="https://superlova.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】计算二进制中1的个数</title>
    <link href="https://superlova.github.io/2021/05/03/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%AE%A1%E7%AE%97%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0/"/>
    <id>https://superlova.github.io/2021/05/03/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%AE%A1%E7%AE%97%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0/</id>
    <published>2021-05-03T06:30:06.000Z</published>
    <updated>2022-08-02T13:32:34.818Z</updated>
    
    <content type="html"><![CDATA[<p>二进制中1的个数的两种计算方法</p><!--more---><p>第一种解法，<code>n &amp;= (n - 1);</code>这一句消灭了二进制末尾的1。循环次数与二进制中1的个数相同。</p><pre><code class="lang-cpp">int hammingWeight(uint32_t n) &#123;    int count = 0;    while (n != 0) &#123;        n &amp;= (n - 1);        ++count;    &#125;    return count;&#125;</code></pre><p>第二种解法，是把性能挖掘到极致的解法：</p><pre><code class="lang-cpp">size_t hammingWeight(uint64_t V) &#123;    V -= ((V &gt;&gt; 1) &amp; 0x5555555555555555); // 010101010101    V = (V &amp; 0x3333333333333333) + ((V &gt;&gt; 2) &amp; 0x3333333333333333);    return ((V + (V &gt;&gt; 4) &amp; 0xF0F0F0F0F0F0F0F) * 0x101010101010101) &gt;&gt; 56;&#125;</code></pre><p>只依靠位运算，不进行条件判断，方便并行计算。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;二进制中1的个数的两种计算方法&lt;/p&gt;
&lt;!--more---&gt;
&lt;p&gt;第一种解法，&lt;code&gt;n &amp;amp;= (n - 1);&lt;/code&gt;这一句消灭了二进制末尾的1。循环次数与二进制中1的个数相同。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;lang-cpp&quot;&gt;in</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="leetcode" scheme="https://superlova.github.io/tags/leetcode/"/>
    
    <category term="binary" scheme="https://superlova.github.io/tags/binary/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】不限精度的大整数表示</title>
    <link href="https://superlova.github.io/2021/05/03/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%8D%E9%99%90%E7%B2%BE%E5%BA%A6%E7%9A%84%E6%95%B4%E6%95%B0%E4%B9%98%E6%B3%95/"/>
    <id>https://superlova.github.io/2021/05/03/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%8D%E9%99%90%E7%B2%BE%E5%BA%A6%E7%9A%84%E6%95%B4%E6%95%B0%E4%B9%98%E6%B3%95/</id>
    <published>2021-05-03T06:27:43.000Z</published>
    <updated>2022-08-02T13:32:34.821Z</updated>
    
    <content type="html"><![CDATA[<p>两个大整数相乘，如何在不损失精度的前提下计算得到正确的结果？<br><!--more---></p><h1 id="不限精度的整数运算"><a href="#不限精度的整数运算" class="headerlink" title="不限精度的整数运算"></a>不限精度的整数运算</h1><p>前一篇文章我们讨论了两个64位整数的乘法，本篇文章我们讨论高精度数值的加减运算问题。</p><h2 id="大整数存储"><a href="#大整数存储" class="headerlink" title="大整数存储"></a>大整数存储</h2><p>很简单，使用数组即可。注意整数的高位存储在数组的末尾，整数的低位存储在数组的开头。</p><pre><code class="lang-py"># 235813d[0] = 3d[1] = 1d[2] = 8d[3] = 5d[4] = 3d[5] = 2</code></pre><p>这样做的原因是，四则运算一般是从低位向高位计算的，数组则是从0开始遍历的。不过如果数字以字符串存储则不然。比如<code>s=&quot;235813&quot;</code>这个字符串的存储顺序和逻辑顺序恰恰相反。</p><pre><code class="lang-cpp">class BigInt &#123;public:    int d[1000]&#123;&#125;;    int len;    BigInt() &#123;        memset(d, 0, sizeof(d));        len = 0;    &#125;    static BigInt change(std::string str);    static int compare(BigInt a, BigInt b);    static BigInt add(BigInt a, BigInt b);    static BigInt sub(BigInt a, BigInt b);&#125;;</code></pre><p>输入大整数时，一般都以<code>string</code>类型输入，因此需要执行<code>reverse</code>操作，或者直接逆序赋值。</p><h2 id="大整数运算"><a href="#大整数运算" class="headerlink" title="大整数运算"></a>大整数运算</h2><h3 id="加法"><a href="#加法" class="headerlink" title="加法"></a>加法</h3><pre><code class="lang-cpp">BigInt BigInt::add(BigInt a, BigInt b) &#123;    BigInt c;    int carry = 0;    for (int i = 0; i &lt; a.len || i &lt; b.len; ++i) &#123;        int temp = a.d[i] + b.d[i] + carry;        c.d[c.len++] = temp % 10;        carry = temp / 10;    &#125;    if (carry != 0) &#123;        c.d[c.len++] = carry;    &#125;    return c;&#125;</code></pre><h3 id="减法"><a href="#减法" class="headerlink" title="减法"></a>减法</h3><pre><code class="lang-cpp">BigInt BigInt::sub(BigInt a, BigInt b) &#123;    BigInt c;    for (int i = 0; i &lt; a.len || i &lt; b.len; ++i) &#123;        if (a.d[i] &lt; b.d[i]) &#123;            a.d[i+1]--;            a.d[i] += 10; // 借位        &#125;        c.d[c.len++] = a.d[i] - b.d[i];    &#125;    while (c.len - 1 &gt;= 1 &amp;&amp; c.d[c.len - 1] == 0) &#123;        c.len--;    &#125;    return c;&#125;</code></pre><h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><pre><code class="lang-cpp">int BigInt::compare(BigInt a, BigInt b) &#123;    if (a.len &gt; b.len) return 1;    else if (a.len &lt; b.len) return -1;    else &#123;        for (int i = a.len-1; i &gt;= 0; --i) &#123;            if (a.d[i] &gt; b.d[i]) return 1;            else if (a.d[i] &lt; b.d[i]) return -1;        &#125;        return 0;    &#125;&#125;</code></pre><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>算法笔记 5.6 节</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;两个大整数相乘，如何在不损失精度的前提下计算得到正确的结果？&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h1 id=&quot;不限精度的整数运算&quot;&gt;&lt;a href=&quot;#不限精度的整数运算&quot; class=&quot;headerlink&quot; title=&quot;不限精度的整数运算&quot;&gt;&lt;/a&gt;不限精度的</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="algorithm" scheme="https://superlova.github.io/tags/algorithm/"/>
    
    <category term="leetcode" scheme="https://superlova.github.io/tags/leetcode/"/>
    
    <category term="searching" scheme="https://superlova.github.io/tags/searching/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】在 Clion 中使用 Google Test</title>
    <link href="https://superlova.github.io/2021/05/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9C%A8-Clion-%E4%B8%AD%E4%BD%BF%E7%94%A8-Google-Test/"/>
    <id>https://superlova.github.io/2021/05/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9C%A8-Clion-%E4%B8%AD%E4%BD%BF%E7%94%A8-Google-Test/</id>
    <published>2021-05-01T00:41:43.000Z</published>
    <updated>2022-08-02T13:32:34.799Z</updated>
    
    <content type="html"><![CDATA[<p>Google Test 是著名的 C++ 单元测试框架。如何在 CLion中使用 Google Test？<br><!--more---></p><h1 id="1-下载和安装"><a href="#1-下载和安装" class="headerlink" title="1. 下载和安装"></a>1. 下载和安装</h1><p>首先去<a href="https://github.com/google/googletest">这个网站</a>下载 google test 最新版本。</p><p><img src="/2021/05/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9C%A8-Clion-%E4%B8%AD%E4%BD%BF%E7%94%A8-Google-Test/readme.png" alt></p><p>笔者写作时， google test 的版本为 1.10。</p><p>然后下载最新版本的压缩文件。</p><p><img src="/2021/05/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9C%A8-Clion-%E4%B8%AD%E4%BD%BF%E7%94%A8-Google-Test/download.png" alt></p><p>解压得到文件夹<code>googletest-release-1.10.0</code>，打开文件夹后，把<code>googletest</code>文件夹复制到你的目标工程目录。</p><p><img src="/2021/05/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9C%A8-Clion-%E4%B8%AD%E4%BD%BF%E7%94%A8-Google-Test/googletestdir.png" alt></p><p><img src="/2021/05/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9C%A8-Clion-%E4%B8%AD%E4%BD%BF%E7%94%A8-Google-Test/putitinproject.png" alt></p><p>还没结束，我们需要修改工程根目录的<code>CMakeLists.txt</code>，以使 google test 生效。注意不要修改<code>googletest/CMakeLists.txt</code>！</p><p>向<code>CMakeLists.txt</code>添加如下代码：</p><pre><code class="lang-sh">add_subdirectory(./googletest)include_directories($&#123;PROJECT_SOURCE_DIR&#125;/src/include ./googletest/include)link_directories($&#123;PROJECT_SOURCE_DIR&#125;/lib $&#123;PROJECT_SOURCE_DIR&#125;/googletest)target_link_libraries($&#123;PROJECT_NAME&#125; gtest)</code></pre><p>如果我们想要使用 google test，就在任意cpp文件添加头<code>#include &quot;gtest/gtest.h&quot;</code></p><h1 id="2-简单的用例"><a href="#2-简单的用例" class="headerlink" title="2. 简单的用例"></a>2. 简单的用例</h1><p>做一个简单的测试，我编写了一个将<code>unsigned long long</code>转为二进制数据的函数<code>ulld_to_b</code>，该函数能将一个无符号64位整数转换为64位长的二进制字符串。</p><pre><code class="lang-cpp">string ulld_to_b(uint64_t i) &#123;    return bitset&lt;64&gt;(i).to_string();&#125;</code></pre><p>现在对其进行测试。测试用例为18，我的预期结果为10010，但是前面应该补零至64位长：</p><pre><code class="lang-cpp">TEST(TestCase, test1) &#123;    EXPECT_STREQ(&quot;10010&quot;, ulld_to_b(18ULL).c_str());&#125;TEST(TestCase, test2) &#123;    EXPECT_STREQ(&quot;00000000000000000000000000010010&quot;, ulld_to_b(18ULL).c_str());&#125;TEST(TestCase, test3) &#123;    EXPECT_STREQ(&quot;0000000000000000000000000000000000000000000000000000000000010010&quot;, ulld_to_b(18ULL).c_str());&#125;</code></pre><p>写完测试用例后，需要改造下<code>main()</code>使其运行所有测试用例：</p><pre><code class="lang-cpp">int main(int argc, char** argv) &#123;    testing::InitGoogleTest(&amp;argc, argv);    return RUN_ALL_TESTS();&#125;</code></pre><p>预期结果为前两个测试用例不通过，后一个测试用例通过。</p><p>编译运行后结果如下，完全符合我们的预期：</p><p><img src="/2021/05/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9C%A8-Clion-%E4%B8%AD%E4%BD%BF%E7%94%A8-Google-Test/results.png" alt></p><p>当然这里的使用是不恰当的，我们应当尽可能令所有单元测试都通过。</p><p>在CLion的代码编辑器中可以方便地查看出错的测试用例：</p><p><img src="/2021/05/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9C%A8-Clion-%E4%B8%AD%E4%BD%BF%E7%94%A8-Google-Test/result_wrong.png" alt></p><h1 id="3-批量开关测试用例"><a href="#3-批量开关测试用例" class="headerlink" title="3. 批量开关测试用例"></a>3. 批量开关测试用例</h1><p>在大型工程中，项目上线前需要关闭所有测试用例，逐个删除未免显得笨拙。这个时候我们可以使用宏定义。</p><p>首先将<code>googletest</code>的头文件放在一个统一的头文件<code>stdafx.h</code>中，然后以<code>#ifdef</code>包裹起来：</p><pre><code class="lang-cpp">#ifndef LEETCODE_2021_STDAFX_H#define LEETCODE_2021_STDAFX_H#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;string&gt;#include &lt;map&gt;#include &lt;set&gt;#include &lt;algorithm&gt;#include &lt;stack&gt;#include &lt;memory&gt;#include &lt;bitset&gt;// 开启测试模式#define DEBUG_MODE#ifdef DEBUG_MODE#include &quot;gtest/gtest.h&quot;#endif#endif //LEETCODE_2021_STDAFX_H</code></pre><p>如果我把<code>#define DEBUG_MODE</code>删除的话，<code>#include &quot;gtest/gtest.h&quot;</code>就不会执行了。</p><p>然后回到每个函数实现处，我们在这里写了很多的测试用例，以及修改了<code>main()</code>函数。同样将它们用<code>#ifdef</code>包裹起来：</p><pre><code class="lang-cpp">#ifdef DEBUG_MODETEST(TestCase, test1) &#123;    EXPECT_STREQ(&quot;10010&quot;, ulld_to_b(18ULL).c_str());&#125;TEST(TestCase, test2) &#123;    EXPECT_STREQ(&quot;00000000000000000000000000010010&quot;, ulld_to_b(18ULL).c_str());&#125;TEST(TestCase, test3) &#123;    EXPECT_STREQ(&quot;0000000000000000000000000000000000000000000000000000000000010010&quot;, ulld_to_b(18ULL).c_str());&#125;#endif</code></pre><p>这些测试用例会随着<code>&quot;stdafx.h&quot;</code>中<code>#define DEBUG_MODE</code>语句的删除而失效。</p><p>最后来看看<code>main()</code>函数如何处理：</p><pre><code class="lang-cpp">#ifndef DEBUG_MODEint main() &#123;    /*...*/    return 0;&#125;#elseint main(int argc, char** argv) &#123;    testing::InitGoogleTest(&amp;argc, argv);    return RUN_ALL_TESTS();&#125;#endif</code></pre><p>如果没定义<code>DEBUG_MODE</code>，则执行正常的<code>main()</code>；否则执行所有测试用例的<code>main(int argc, char** argv)</code>。</p><p>在项目需要上线时，把<code>#define DEBUG_MODE</code>那一行删除即可。</p><h1 id="4-测试用例编写"><a href="#4-测试用例编写" class="headerlink" title="4. 测试用例编写"></a>4. 测试用例编写</h1><h2 id="4-1-EXPECT和ASSERT"><a href="#4-1-EXPECT和ASSERT" class="headerlink" title="4.1 EXPECT和ASSERT"></a>4.1 <code>EXPECT</code>和<code>ASSERT</code></h2><p>google test 使用 TEST 宏声明测试用例。TEST() 有两个参数，<code>TestCaseName</code>，<code>TestName</code>。</p><p>我们之前使用了<code>EXPECT_STREQ</code>这个宏，其含义为验证内部的两个参数为相同的字符串。除此之外，还有很多以EXPECT开头的宏，它们功能各不相同，比如<code>EXPECT_EQ</code>这个宏比较两个数字是否相等。</p><p>EXPECT系列和ASSERT系列的区别是，EXPECT失败后，继续往下执行；ASSERT失败则直接终止程序。</p><p>EXPECT和ASSERT的返回是一个流对象，这意味着我们可以在之后使用<code>&lt;&lt;</code>运算符输出额外信息：</p><pre><code class="lang-cpp">for (int i = 0; i &lt; x.size(); ++i) &#123;    EXPECT_EQ(x[i], y[i]) &lt;&lt; &quot;Vectors x and y differ at index &quot; &lt;&lt; i;&#125;</code></pre><h2 id="4-2-各种不同的宏断言"><a href="#4-2-各种不同的宏断言" class="headerlink" title="4.2 各种不同的宏断言"></a>4.2 各种不同的宏断言</h2><p>本节只列出了<code>ASSERT</code>，实际上每个<code>ASSERT</code>对应一个<code>EXPECT</code>版本。</p><p><strong>编写测试用例时，如果有两个参数，注意把待测函数输出放在后面！前面的参数是Ground Truth，也就是答案。</strong></p><h3 id="布尔值断言，只有一个参数，参数只能为true或者false："><a href="#布尔值断言，只有一个参数，参数只能为true或者false：" class="headerlink" title="布尔值断言，只有一个参数，参数只能为true或者false："></a>布尔值断言，只有一个参数，参数只能为true或者false：</h3><pre><code class="lang-cpp">ASSERT_TRUE(condition)ASSERT_FALSE(condition)</code></pre><h3 id="数值断言："><a href="#数值断言：" class="headerlink" title="数值断言："></a>数值断言：</h3><pre><code class="lang-cpp">ASSERT_EQ(v1, v2) // v1 == v2ASSERT_NE(v1, v2) // v1 != v2ASSERT_LT(v1, v2) // v1 &lt; v2ASSERT_LE(v1, v2) // v1 &lt;= v2ASSERT_GT(v1, v2) // v1 &gt; v2ASSERT_GE(v1, v2) // v1 &gt;= v2</code></pre><h3 id="字符串断言"><a href="#字符串断言" class="headerlink" title="字符串断言"></a>字符串断言</h3><p>这里的参数为C风格字符串，因此当参数为<code>string</code>类型时，你需要调用<code>c_str()</code>方法。</p><pre><code class="lang-cpp">ASSERT_STREQ(s1, s2) // s1和s2内容相同ASSERT_STRNE(s1, s2) // s1和s2内容不同ASSERT_STRCASEEQ(s1, s2) // 忽略大小写，s1和s2内容相同ASSERT_STRCASENE(s1, s2) // 忽略大小写，s1和s2内容不同</code></pre><h3 id="断言返回成功或者失败"><a href="#断言返回成功或者失败" class="headerlink" title="断言返回成功或者失败"></a>断言返回成功或者失败</h3><pre><code class="lang-cpp">TEST(ExplicitTest, Demo)&#123;    ADD_FAILURE() &lt;&lt; &quot;Sorry&quot;; // None Fatal Asserton，继续往下执行。    FAIL(); // Fatal Assertion，不往下执行该案例。    SUCCEED();&#125;</code></pre><h3 id="抛出异常的断言"><a href="#抛出异常的断言" class="headerlink" title="抛出异常的断言"></a>抛出异常的断言</h3><pre><code class="lang-cpp">int Foo(int a, int b)&#123;    if (a == 0 || b == 0)    &#123;        throw &quot;don&#39;t do that&quot;;    &#125;    int c = a % b;    if (c == 0)        return b;    return Foo(b, c);&#125;TEST(FooTest, HandleZeroInput)&#123;    EXPECT_ANY_THROW(Foo(10, 0)); // Foo应当抛出异常    EXPECT_THROW(Foo(0, 5), char*); // Foo应当抛出字符串类型的异常    EXPECT_NO_THROW(Foo(1, 1)); // Foo不应该抛出异常&#125;</code></pre><h3 id="浮点数断言"><a href="#浮点数断言" class="headerlink" title="浮点数断言"></a>浮点数断言</h3><pre><code class="lang-cpp">ASSERT_FLOAT_EQ(exp, act) // 检验两个浮点数是否**几乎**相等ASSERT_DOUBLE_EQ(exp, act) // 同上，只不过精度更高ASSERT_NEAR(exp, act, abs_error) // exp和act之间的差值不会超过abs_error</code></pre><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.cnblogs.com/coderzh/archive/2009/04/06/1426755.html">https://www.cnblogs.com/coderzh/archive/2009/04/06/1426755.html</a></p><p><a href="https://blog.csdn.net/zhizhengguan/article/details/110313265">https://blog.csdn.net/zhizhengguan/article/details/110313265</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Google Test 是著名的 C++ 单元测试框架。如何在 CLion中使用 Google Test？&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h1 id=&quot;1-下载和安装&quot;&gt;&lt;a href=&quot;#1-下载和安装&quot; class=&quot;headerlink&quot; title=&quot;1</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="clion" scheme="https://superlova.github.io/tags/clion/"/>
    
    <category term="google test" scheme="https://superlova.github.io/tags/google-test/"/>
    
    <category term="unit test" scheme="https://superlova.github.io/tags/unit-test/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】大整数相乘 I</title>
    <link href="https://superlova.github.io/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/"/>
    <id>https://superlova.github.io/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/</id>
    <published>2021-04-28T06:49:39.000Z</published>
    <updated>2022-08-02T13:32:34.821Z</updated>
    
    <content type="html"><![CDATA[<p>两个大整数相乘，如何在不损失精度的前提下计算得到正确的结果？<br><!--more---></p><h1 id="unsigned-long-long-整数之间的乘法"><a href="#unsigned-long-long-整数之间的乘法" class="headerlink" title="unsigned long long 整数之间的乘法"></a><code>unsigned long long</code> 整数之间的乘法</h1><h2 id="1-问题引入"><a href="#1-问题引入" class="headerlink" title="1. 问题引入"></a>1. 问题引入</h2><p>如果我们使用 <code>C++</code> 语言，在 64 位系统上，<code>unsigned long long</code> 类型（或者 <code>uint64_t</code> 类型）的变量能够保存的数值范围是 $\left[0,2^{64}\right)$。任何小于该数值的数字都能被正确存储。</p><script type="math/tex; mode=display">2^{64}-1=18,446,744,073,709,551,615</script><p>这个十进制的整数足足有20位，一般情况下我们是不需要考虑溢出问题的。</p><p>但是如果两个<code>uint64_t</code>的变量要做乘法，就必须考虑溢出问题了。因为两个64位整数相乘，其结果可能达到128位，超过了一个变量能存储的数据的极限大小。</p><p>两个<code>uint64_t</code>变量的乘法该如何正确计算？考虑极端情况，两个<code>uint64_t</code>整数所能表达的最大数字<code>UINT_MAX</code>相乘：</p><script type="math/tex; mode=display">(2^{64}-1)\times (2^{64}-1)=2^{128}-2^{65}+1</script><p>这里面最大的数字是$2^{128}$，因此我们最极端情况下有128位的信息需要保存。</p><h2 id="2-问题简化"><a href="#2-问题简化" class="headerlink" title="2. 问题简化"></a>2. 问题简化</h2><p>首先让我们<strong>简化一下问题</strong>。假设我现在只能用一款古董电脑，它只支持保存四位十进制整数的变量，也就是一个变量的保存范围为$[0,9999]$。如何计算两个四位整数相乘？</p><p>还记得乘法是怎么计算的吗？我们应该都学习过小学数学，通过将数字中的每一个字符看作计算单元，然后利用99乘法表就能够口算出结果了。</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/2021-04-29-08-44-58.png" alt></p><p>或者可以每两个字符看成计算的基本单元，这样就需要计算两位数乘法。</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/2021-04-29-08-50-09.png" alt></p><p>同样可以计算得到正确结果。假设每个方框是计算机的一个变量，后一种计算方式可以保证每个变量中保存的元素大小不超过计算机的保存能力（假设计算机保存能力为2位）。</p><p>回到题目本身。该运算的结果达到7位，超过了我们计算机限制的4位，因此可以使用两个变量保存该结果：<code>lo</code>保存结果的低四位，<code>hi</code>保存结果的高四位。</p><p>另外我们使用四个变量将两个乘数拆分成前后两个部分，<code>a</code>保存第一个操作数的高2位，<code>b</code>保存第一个操作数的低2位,<code>c</code>保存第二个操作数的高2位，<code>d</code>保存第二个操作数的低2位。</p><p>为什么要这么做？因为计算机的存储能力为4位，两个四位数字相乘，结果最多能达到八位，所以我们肯定不能容忍两个四位数字相乘；但是两个两位数字相乘，结果最多达到四位，是可以被当前的计算机所存储的。所以我们需要把四位数字拆分成前后两个部分。</p><p>抽象一下计算过程，以下不考虑进位。</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/2021-04-29-09-07-00.png" alt></p><p>第一步，计算b与d的乘积，得到结果<code>bd</code>和进位k_1：</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/计算b与d的乘积.png" alt></p><p>第二步，计算a与d的乘积，并且加上bd的进位k_1，得到结果ad和进位k_2，注意这个k_2最终会与ac求和：</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/计算a与d的乘积.png" alt></p><p>第三步，计算b与c的乘积，得到结果bc和进位k_3：</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/计算b与c的乘积.png" alt></p><p>第四步，计算a与c的乘积，并且加上bc的进位k_3，得到结果ac和进位k_4，注意这个k_4最终会成为高位hi的最大的部分，因此ac得到的k_4没必要和ac分离开：</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/计算a与c的乘积.png" alt></p><p>第五步，低位结果计算，首先将ad和bc加起来，得到ad+bc以及可能存在的进位k_5；然后将ad+bc向左移位两格，再加上bd，就是低位结果lo。</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/低位结果计算.png" alt></p><p>第六步，高位结果计算，将ac和进位k_5加起来，注意此时的ac包含了进位k_4，并没有分离出去。我们还需要加上计算ad得到的k_2，因此hi=ac+k_2+k_5。</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/高位结果计算.png" alt></p><h2 id="3-示例"><a href="#3-示例" class="headerlink" title="3. 示例"></a>3. 示例</h2><p>举一个具体点的例子：如何利用该电脑正确计算<code>1213*1214</code>？</p><script type="math/tex; mode=display">1213\times 1214=1,472,582</script><pre><code>a = 12b = 13c = 12d = 14</code></pre><ol><li>计算b与d的乘积，得到结果<code>bd</code>和进位k_1：<br>```<br>b <em> d = 13 </em> 14 = 182</li></ol><hr><p>k_1 = 1<br>bd = 82</p><pre><code>2. 计算a与d的乘积，并且加上bd的进位k_1，得到结果ad和进位k_2</code></pre><p>a <em> d + k_1 = 12 </em> 14 + 1 = 169<br>k_2 = 1<br>ad = 69</p><pre><code>3. 计算b与c的乘积，得到结果bc和进位k_3</code></pre><p>b <em> c = 13 </em> 12 = 156<br>k_3 = 1<br>bc = 56</p><pre><code>4. 计算a与c的乘积，并且加上bc的进位k_3，得到结果ac。此时不用分离进位，因此没有k_4。</code></pre><p>a <em> c + k_3 = 12 </em> 12 + 1 = 145<br>ac = 145</p><pre><code>5. 低位结果计算，首先将ad和bc加起来，得到ad+bc以及可能存在的进位k_5；然后将ad+bc向左移位两格，再加上bd，就是低位结果lo</code></pre><p>ad + bc = 125<br>k_5 = 1<br>adbc = 25<br>lo = adbc &lt;&lt; 2 + bd = 2500 + 82 = 2582</p><pre><code>6. 高位结果计算，将ac和进位k_5加起来，注意此时的ac包含了进位k_4，并没有分离出去。我们还需要加上计算ad得到的k_2，因此hi=ac+k_2+k_5</code></pre><p>hi = ac + k_2 + k_5 = 145 + 1 + 1 = 147</p><pre><code>最终结果的高位部分为147，低位部分为2582，与答案1472582相同。## 迁移到64位整数相乘以上我们讨论的都是四位整数相乘的问题，其实64位二进制数相乘也是一样的道理。首先我们将两个64位乘数OP1和OP2各自分成两半32位整数，也就是a/b/c/d，然后按照上述算法计算，即可得到结果的高64位和低64位。算法如下：```cppvoid mul_64_64_to_128(uint64_t op1, uint64_t op2, uint64_t *hi, uint64_t *lo) &#123;    // 取abcd    uint64_t a, b, c, d;    b = (op1 &amp; 0xFFFFFFFF);    a = (op1 &gt;&gt; 32);    d = (op2 &amp; 0xFFFFFFFF);    c = (op2 &gt;&gt; 32);    // bd    uint64_t k_1 = b * d;    uint64_t bd = (k_1 &amp; 0xFFFFFFFF);    k_1 &gt;&gt;= 32;    // ad    uint64_t k_2 = a * d + k_1;    uint64_t ad = (k_2 &amp; 0xFFFFFFFF);    k_2 &gt;&gt;= 32;    // bc    uint64_t k_3 = b * c;    uint64_t bc = (k_3 &amp; 0xFFFFFFFF);    k_3 &gt;&gt;= 32;    // ac    uint64_t ac = a * c + k_3;    // lo    uint64_t k_5 = ad + bc;    uint64_t ad_bc = (k_5 &amp; 0xFFFFFFFF);    k_5 &gt;&gt;= 32;    *lo = (ad_bc &lt;&lt; 32) + bd;    // hi    *hi = ac + k_2 + k_5;&#125;</code></pre><p>验证一下结果：</p><pre><code class="lang-cpp">int main() &#123;    uint64_t op1 = 1e12;    uint64_t op2 = 1e12;    uint64_t hi = 0ULL, lo = 0ULL;    mul_64_64_to_128(op1, op2, &amp;hi, &amp;lo);    cout &lt;&lt; hi &lt;&lt; &quot;\n&quot; &lt;&lt; lo &lt;&lt; endl;    return 0;&#125;&gt;&gt;&gt;542102003764205206896640Process finished with exit code 0</code></pre><p>乍一看结果，<code>1e12*1e12</code>结果怎么不是<code>1e24</code>呢？其实这是由于结果是十进制导致不直观，实际上我们的结果是正确的。我们比较下二进制就可以了。</p><p>首先定义一个函数<code>ulld_to_b</code>，将<code>uint64_t</code>变量转换为64位的二进制字符串：</p><pre><code class="lang-cpp">string ulld_to_b(uint64_t i) &#123;    return bitset&lt;64&gt;(i).to_string();&#125;</code></pre><p>然后查询<code>1e24</code>的二进制表示为：</p><pre><code>00000000000000000000000000000000000000000000000011010011110000100001101111001110110011001110110110100001000000000000000000000000</code></pre><p>该数值是我在<a href="https://www.sojson.com/hexconvert.html">这个网站</a>上查询的。</p><p>然后编写测试用例：</p><pre><code class="lang-cpp">TEST(TestCase, test1) &#123;    uint64_t op1 = 1e12;    uint64_t op2 = 1e12;    uint64_t hi = 0ULL, lo = 0ULL;    mul_64_64_to_128(op1, op2, &amp;hi, &amp;lo);    string res = &quot;&quot;;    res = ulld_to_b(hi) + ulld_to_b(lo);    cout &lt;&lt; res &lt;&lt; endl;    string ground_truth = &quot;00000000000000000000000000000000000000000000000011010011110000100001101111001110110011001110110110100001000000000000000000000000&quot;;    EXPECT_STREQ(ground_truth.c_str(), res.c_str());&#125;int main(int argc, char** argv) &#123;    testing::InitGoogleTest(&amp;argc, argv);    return RUN_ALL_TESTS();&#125;</code></pre><p>没问题，通过了。当然，仅通过一个测试用例不算证明程序的正确性，勤劳的你可以多用几个测试用例试试。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.codeproject.com/Tips/618570/UInt-Multiplication-Squaring">https://www.codeproject.com/Tips/618570/UInt-Multiplication-Squaring</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;两个大整数相乘，如何在不损失精度的前提下计算得到正确的结果？&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h1 id=&quot;unsigned-long-long-整数之间的乘法&quot;&gt;&lt;a href=&quot;#unsigned-long-long-整数之间的乘法&quot; class=&quot;heade</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="algorithm" scheme="https://superlova.github.io/tags/algorithm/"/>
    
    <category term="leetcode" scheme="https://superlova.github.io/tags/leetcode/"/>
    
    <category term="searching" scheme="https://superlova.github.io/tags/searching/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】Docker综合实践</title>
    <link href="https://superlova.github.io/2021/04/24/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Docker%E7%BB%BC%E5%90%88%E5%AE%9E%E8%B7%B5/"/>
    <id>https://superlova.github.io/2021/04/24/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Docker%E7%BB%BC%E5%90%88%E5%AE%9E%E8%B7%B5/</id>
    <published>2021-04-24T14:16:40.000Z</published>
    <updated>2022-08-02T13:32:34.787Z</updated>
    
    <content type="html"><![CDATA[<p>Datawhale Docker学习笔记第六篇<br><!--more---></p><p>在没有学习docker之前，部署项目都是直接启动文件，比如java项目就是java –jar xxxx.jar的方式，python项目就是python xxxx.py。如果采用docker的方式去部署这些项目，一般有两种方式，以jar包项目为例</p><h1 id="方式一、挂载部署"><a href="#方式一、挂载部署" class="headerlink" title="方式一、挂载部署"></a>方式一、挂载部署</h1><p>这种方式类似于常规部署，通过数据卷的方式将宿主机的jar包挂载到容器中，然后执行jar包的jdk选择容器中的而非采用本地的。</p><ol><li>将jar包上传到服务器的指定目录，比如/root/docker/jar。</li></ol><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9yobtp2tj30o007njt5.jpg" alt></p><ol><li>通过docker pull openjdk:8命令获取镜像</li></ol><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9yord00xj30o006bju3.jpg" alt></p><ol><li>编写docker-compose.yml文件</li></ol><pre><code class="lang-yaml">version:&#39;3.0&#39;services:  java:    image: docker.io/openjdk    restart:always    container_name: myopenjdk    ports:      - 8080:8001    volumes:      - /root/docker/jar/xxxx.jar:/root:z      - /etc/localtime:/etc/localtime    environment:      - TZ=&quot;Asia/Shanghai&quot;    entrypoint: java -jar /root/xxxx.jar    mynetwork:      ipv4_address: 192.168.1.13networks:  mynetwork:   ipam:     config:      - subnet: 192.168.1.0/24</code></pre><p>参数解释：</p><ul><li><p>build 指定dockerfile所在文件夹的路径 context指定dockerfile文件所在路径 dockerfile指定文件的具体名称</p></li><li><p>container_name 指定容器名称</p></li><li><p>volumes 挂载路径  z是用来设置selinux，或者直接在linux通过命令临时关闭或者永久关闭</p></li><li><p>ports 暴露端口信息</p></li><li><p>networks是用来给容器设置固定的ip</p></li></ul><ol><li>执行命令docker-compose up –d启动jar包, 可以通过docker ps查看容器是否在运行，需要注意的是默认查看所有运行中的容器，如果想查看所有容器，需要添加参数-a</li></ol><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9yp7mo2wj30o0057aay.jpg" alt></p><ol><li><p>注意如果容器启动失败或者状态异常，可以通过docker logs查看日志</p></li><li><p>通过docker inspect myopenjdk查看容器详细信息，可以看到容器ip已经设置成功</p></li></ol><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9ypl8wgdj30o009wabq.jpg" alt></p><ol><li>然后在虚拟机中打开浏览器输入jar包项目的访问地址，就可以看到运行的项目，需要注意访问端口是映射过的端口而非项目实际端口</li></ol><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9yq1vynoj30o00dytaj.jpg" alt></p><h2 id="方式二、构建镜像部署"><a href="#方式二、构建镜像部署" class="headerlink" title="方式二、构建镜像部署"></a>方式二、构建镜像部署</h2><ol><li><p>将jar包上传到服务器的指定目录，比如/root/docker/jar。</p></li><li><p>在该目录下创建Dockerfile文件，通过vim等编辑工具在Dockerfile中编辑以下内容</p></li></ol><pre><code class="lang-dockerfile">FROM java:8MAINTAINER YHFLABEL description=”learn docker”ADD xxx.jarEXPOSE 8001ENTRYPOINT [“java”,”-jar”,”xxxx.jar”]</code></pre><p>参数解释：</p><ul><li><p>FROM java:8 指定所创建镜像的基础镜像</p></li><li><p>MAINTAINER yhf 指定作者为yhf</p></li><li><p>LABEL 为生成的镜像添加元数据标签信息</p></li><li><p>ADD xxxx.jar 添加内容到镜像</p></li><li><p>EXPOSE 8080 声明镜像内服务监听的端口</p></li><li><p>ENTRYPOINT 指定镜像的默认入口命令，支持两种格式ENTRYPOINT[“java”,”-jar”,”xxxx.jar”]；ENTRYPOINT java –jar xxxx.jar。注意每个dokcerfile中只能有一个ENTRYPOINT，如果指定多个只有最后一个生效。</p></li></ul><ol><li><p>Dockerfile构建完成以后可以通过命令docker build构建镜像，然后再运行容器，这里咱们用docker-compose命令直接编排构建镜像和运行容器。</p></li><li><p>编写docker-compose.yml文件</p><p>```yaml<br>version: ‘3’</p></li></ol><p>services:</p><p>  java_2:<br>    restart: always<br>    image: yhfopenjdk:latest<br>    container_name: myopenjdk<br>    ports:</p><pre><code>  - 8080:8001volumes:  - /etc/localtime:/etc/localtimeenvironment:  - TZ=&quot;Asia/Shanghai&quot;entrypoint: java -jar /root/datawhale-admin-1.0.0.jarnetworks:  mynetwork:     ipv4_address: 192.168.1.13</code></pre><p>networks:<br>  mynetwork:<br>   ipam:<br>     config:</p><pre><code>  - subnet: 192.168.1.0/24</code></pre><p> ```</p><p>参数解释同方式一：</p><ol><li>执行docker-compose up –d直接启动基于文件构建的自定义镜像，如果镜像不存在会自动构建，如果已存在那么直接启动。如果想重新构建镜像，则执行docker-compose build。如果想在执行compose文件的时候重构，则执行docker-compose up –d –build。</li></ol><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9yqi1lv0j30o007ign6.jpg" alt></p><p>此使通过dockerfile文件构建的镜像已经创建</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9yqxa7t1j30o005qdi5.jpg" alt></p><p>通过镜像运行的容器已经正常启动，可以通过docker ps查看容器是否在运行，需要注意的是默认查看所有运行中的容器，如果想查看所有容器，需要添加参数-a</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9yra1jg1j30o0013q33.jpg" alt></p><ol><li>在浏览器中输入访问路径可以看到项目已经正常运行</li></ol><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9yrmnnzzj30o00d975v.jpg" alt></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Datawhale Docker学习笔记第六篇&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;p&gt;在没有学习docker之前，部署项目都是直接启动文件，比如java项目就是java –jar xxxx.jar的方式，python项目就是python xxxx.py。如果采用do</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Docker" scheme="https://superlova.github.io/tags/Docker/"/>
    
  </entry>
  
</feed>
