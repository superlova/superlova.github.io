<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Superlova</title>
  
  <subtitle>Welcome...</subtitle>
  <link href="https://superlova.github.io/atom.xml" rel="self"/>
  
  <link href="https://superlova.github.io/"/>
  <updated>2022-10-29T12:03:48.206Z</updated>
  <id>https://superlova.github.io/</id>
  
  <author>
    <name>Superlova</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【学习笔记】李宏毅ML-Note6-注意力机制</title>
    <link href="https://superlova.github.io/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <id>https://superlova.github.io/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</id>
    <published>2022-10-23T13:57:40.000Z</published>
    <updated>2022-10-29T12:03:48.206Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：自注意力机制。<br><!--more---></p><h2 id="一、输入从图像到序列"><a href="#一、输入从图像到序列" class="headerlink" title="一、输入从图像到序列"></a>一、输入从图像到序列</h2><p>图像分类任务的输入是固定的，比如都是28*28像素的黑白图片等等。但是一些输入和输出不是定长序列的任务，比如机器翻译、语音转文字等任务，传统模型在这些任务上的表现不好。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/不同输出对应不同任务.png" alt></p><p>比如词性标注任务，模型的输入是N个向量，输出是N个分类标签；比如文本分类任务，模型的输入是N个向量，输出只是一个分类标签；比如文本翻译任务，模型的输入是N个向量，输出可能是N’个向量，此时N’是未知的，需要机器自己进行判断和学习；这种任务叫做 sequences to sequences (seq2seq) 。</p><p>这些任务的输入特点是，都有多个向量作为一个输入，且向量的相对位置会影响输入的含义。</p><p>以词性标注任务举例，该任务的输入和输出向量个数都为N。我们首先把一句话切分为N个词，然后利用一些编码方式（比如 one-hot 或者 word2vec）将其变成N个定长向量。我们将这N个向量作为一个输入序列，输入到模型中，寄希望于模型能够把每个词的词性标注出来。所以这其实是一个分类任务。</p><p>但是当我们输入句子”I saw a saw.“时，前后两个saw的词性是不同的，如果模型只以词汇向量为输入，不考虑上下文的话，是无法得知这件事的。</p><p>为此，前人们做了以下改进，试图使用传统模型解决这个问题：</p><ol><li><p>通过 N-gram 模型，将两三个词打包成一个新词，”I saw a saw“经过3-gram模型的编码，会产生如下输入序列：</p><pre><code> I saw a saw -&gt; [i saw a], [saw a saw]</code></pre><p> 由此，模型能够看到的上下文就扩展到了三个单词。但是该种方法会快速扩大单词量，增加计算负担，且无法把距离较远的上下文也加入进来。</p></li><li><p>使用TextCNN</p><p> TextCNN 是把图像领域大货成功的CNN模型的经验移植到了NLP领域的成果，它使用一维卷积核。但究其本质还是受限于卷积核的大小。</p></li></ol><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/向量之间的相对位置.png" alt></p><p>有没有一种方法能够考虑输入向量的全部上下文呢？ self-attention 可以做到。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention与FC的区别.png" alt></p><p>self-attention 接受N个输入向量，输出N个输出向量。输出的向量不仅保留了原来向量的语义信息，还额外添加了该向量上下文的信息。</p><p>有了这个全新的向量，我们便不必使用诸如 n-gram 等特征工程手段了，self-attention 就能够产生非常好的稠密向量，该向量考虑的上下文范围是整篇文档。</p><p>self-attention 适合编码整篇文档的信息，产出向量后可以接一个全连接网络来进行具体的分类；也可以像之前的卷积网络一样，多层 self-attention + FC全连接进行堆叠。这实际上就是Transformer的基本思想。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/transformer的简化结构.png" alt></p><p>self-attention 是怎么做到的呢？</p><h2 id="二、self-attention-的实现原理"><a href="#二、self-attention-的实现原理" class="headerlink" title="二、self-attention 的实现原理"></a>二、self-attention 的实现原理</h2><p>self-attention 的输入是N个向量组成的输入序列，输出也是N个向量组成的序列。区别在于，输入的向量本身不包含任何与上下文有关的信息，但是与输入向量对应的输出向量会包含一定的上下文信息。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention的抽象结构.png" alt></p><p>如果要产生b1向量，其实是考虑了a1/a2/a3/a4全部四个向量后的结果。</p><p>具体地，首先我们需要考虑a1向量与其他向量a2/a3/a4之间的相关性，两个向量之间的相关性是一个标量，我们用$\alpha$来表示：</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理1_相关性.png" alt></p><p>相关性是如何计算得到的？有很多做法可以实现相关性的计算。需要注意的是这里的<strong>相关性</strong>并非矩阵向量空间中的相似性，而是一个<strong>需要机器从数据中学习</strong>的参数。下图是两种不同的相关性的计算方式。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理2_相关性的计算方法.png" alt></p><p>上图左边是一种常用的向量相关性计算法，首先把两个向量各自乘以一个参数矩阵，其中输入向量会得到对应的查询向量 query，上下文向量会得到对应的关键向量 key，然后我们把query和key进行点积，得到的就是两个向量的相关性。参数矩阵是可以通过数据进行学习的。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理3_相关性计算方法2.png" alt></p><p>由此我们推广开来，每个输入向量都可以计算属于自己的查询向量query，以及其他输入向量的关键向量key。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理4_相关性计算方法3.png" alt></p><p>输入向量还需要与自己计算相关性，这样凑齐4个相关性数值。由此我们就得到了输入向量a1与其他（包含自己的）四个向量之间的相关程度。</p><p>接下来我们需要做的，就是把四个输入向量都与该相关性进行乘积，然后相加，就得到了包含上下文相关信息的输出向量b1。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理5_相关性计算方法4.png" alt></p><p>再分别对a2/a3/a4向量如法炮制，就可以分别得到四个输出向量。</p><p>需要注意，输入向量最后参与计算时，为了将其转化为稠密向量，也需要乘以一个矩阵后才能参与计算。</p><h2 id="三、self-attention-的并行计算"><a href="#三、self-attention-的并行计算" class="headerlink" title="三、self-attention 的并行计算"></a>三、self-attention 的并行计算</h2><p>向量的转化操作都是矩阵乘法，这意味着对向量进行转化的操作是可以并行计算的。</p><ol><li>通过参数学习，将输入向量（一般是稀疏的）转化为query/key/value（稠密向量），然后将向量拼合形成矩阵，得到矩阵Q、K、V：</li></ol><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention的并行计算1.png" alt></p><ol><li>矩阵Q与K做点乘，得到相关性系数矩阵A</li></ol><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention-parallel-2.png" alt></p><ol><li>相关性矩阵A经过softmax进行概率归一化后，与矩阵V进行点乘，得到输出向量B</li></ol><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention-parallel-3.png" alt></p><p><strong>self-attention将输入向量转化为输出向量的过程到此结束。整个过程总结如下</strong>：</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention-parallel-summary.png" alt></p><h2 id="四、多头注意力-Multihead-Self-attention"><a href="#四、多头注意力-Multihead-Self-attention" class="headerlink" title="四、多头注意力 Multihead Self-attention"></a>四、多头注意力 Multihead Self-attention</h2><p>多头注意力机制，就是同时学习多种相关性的机制。</p><p>这样做的理由是：语言是存在同义词、一词多义等现象的，即便是同一句话，在不同语境下阐述也会产生不同的效果。如果只算一种相关性，模型便无法掌握一词多义等能力，因此我们需要学习多个相关性矩阵A。</p><p>如果想要学习多个相关性矩阵A，</p><ol><li>就必须有多个输入矩阵Q/K/V与之对应，每多一个A，就要多学习一类参数矩阵；</li><li>会产生多组输出矩阵B，需要再学习一个参数矩阵，将一系列的输出矩阵合并起来；</li></ol><p>多头注意力机制在原有注意力机制的基础上，把原有的query向量再多乘n个参数矩阵，将query复制为n个子向量；key和value向量亦是如此：</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/multi-head-self-attention-1.png" alt></p><p>query向量的第一个复制体会与各个key向量的第一个复制体进行计算相关性，生成相关性矩阵A1，第二个复制体会与各个key向量的第二个复制体计算相关性，生成相关性矩阵A2，以此类推。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/multi-head-self-attention-2.png" alt></p><p>由此会产出n份不同的输出向量，他们分别是考虑n种不同相关性下计算出来的输出向量。使用一个参数矩阵将他们合并起来：</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/multi-head-self-attention-3.png" alt></p><p>这就是多头注意力机制的设计思路。如果要计算N种相关性，就叫做N头注意力机制。</p><h2 id="五、位置编码"><a href="#五、位置编码" class="headerlink" title="五、位置编码"></a>五、位置编码</h2><p>上面的机制只介绍了 self-attention 如何把输入向量和它的上下文信息进行编码，但是忽略了上下文也是有位置关系的，理论上离当前位置更近的上下文应该获得更多注意力。因此我们需要额外添加一个能够表示位置的参数。</p><p>做法也很简单，只需要在输入层为每个向量添加一个额外的位置向量即可。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/positional-encoding-1.png" alt></p><p>这个位置向量可以是你自己定义的，也可以作为参数放到网络中进行学习。总之它理论上应该能够表示位置的远近关系。</p><h2 id="六、self-attention-的具体应用"><a href="#六、self-attention-的具体应用" class="headerlink" title="六、self-attention 的具体应用"></a>六、self-attention 的具体应用</h2><p>上面我们是以文本为例介绍的self-attention，实际上有很多其他任务也可以用self-attention来解决（虽然不见得是最佳方案）。</p><p>语音识别任务里，通过将一段音频按照一定时间窗口进行切分，也可以得到一个包含很多向量的输入序列；则处理方法与文本任务大同小异，只不过语音分割时产生的输入序列会非常长，此时需要对计算过程加以优化。例子：Truncate Self-Attention</p><p>图像识别任务里，每个图像按照像素进行分割，并逐行遍历，也可以形成一个序列，这个序列里的每个输入向量是(R,G,B)的三维向量。例子：Self-Attention GAN</p><h2 id="七、总结和提问"><a href="#七、总结和提问" class="headerlink" title="七、总结和提问"></a>七、总结和提问</h2><h3 id="1-Self-Attention-的核心思想？"><a href="#1-Self-Attention-的核心思想？" class="headerlink" title="1. Self-Attention 的核心思想？"></a>1. Self-Attention 的核心思想？</h3><p>通过综合考虑输入词与上下文之间的相关关系，得到包含上下文语义信息的输出向量。</p><h3 id="2-Self-attention-是如何计算的？"><a href="#2-Self-attention-是如何计算的？" class="headerlink" title="2. Self-attention 是如何计算的？"></a>2. Self-attention 是如何计算的？</h3><script type="math/tex; mode=display">\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V</script><p>其中Q、K、V都是从输入向量学习而来的稠密向量组成的矩阵，它们不包含上下文的语义信息；</p><ul><li>Q：查询向量，目标字作为 Query；</li><li>K：键向量，其上下文的各个字作为 Key；</li><li>V：值向量，上下文各个字的 Value；</li></ul><p>$d_k$是Q/K/V的维度。这样做是为了降低分量之间的方差，防止输入向量的维度过高导致点击出来的结果过大，使softmax出来的结果出现一些极端情况（比如只有一个分量是0.99999，其他分量都是0），进而导致训练困难的现象。</p><h3 id="3-Self-Attention-和-CNN-的异同？"><a href="#3-Self-Attention-和-CNN-的异同？" class="headerlink" title="3. Self-Attention 和 CNN 的异同？"></a>3. Self-Attention 和 CNN 的异同？</h3><p>CNN是一类特殊的Attention，即将注意力聚焦于感受野（卷积核）中的一种self-attention网络结构。</p><p>而self-attention的感受野范围是整个序列，可以自行学习哪些是需要重点关注的。</p><p>这也就意味着self-attention相较于CNN而言更复杂、参数更多，需要更多数据进行训练。</p><p>有讨论二者关系的论文：</p><blockquote><p>On the Relationship between Self-Attention and Convolutional Layers</p></blockquote><p>从所需的数据量和准确率比较上，可以辅证这一点：</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention-versus-cnn.png" alt></p><p>那就是在数据量较小的情况下，CNN作为简单模型可以表现得更好；但是数据量充足时，self-attention具有更高的上限。</p><h3 id="4-Self-Attention-与-RNN-的比较？"><a href="#4-Self-Attention-与-RNN-的比较？" class="headerlink" title="4. Self-Attention 与 RNN 的比较？"></a>4. Self-Attention 与 RNN 的比较？</h3><p>RNN虽然也能够学习长距离依赖关系，但是它的结构和运作方式（要想计算b2，必须先计算b1，存在先后依赖关系）决定了它难以并行化，这就导致它的训练过程非常缓慢。</p><p>另外RNN在输入序列过长时，也存在梯度消失或者梯度爆炸的问题，导致无法记忆长距离信息。</p><p>Self-Attention的内部有大量矩阵乘法运算，可以被GPU优化的很快。</p><h3 id="5-MultiHead-Attention-的生效原理？"><a href="#5-MultiHead-Attention-的生效原理？" class="headerlink" title="5. MultiHead Attention 的生效原理？"></a>5. MultiHead Attention 的生效原理？</h3><p>借鉴了CNN中同一卷积层内使用多个卷积核的思想。类似于CNN中通过多通道机制进行特征选择。</p><p>Transformer中使用切头(split)的方法，是为了在不增加复杂度（$O(n^2 d)$）的前提下享受类似CNN中“不同卷积核”的优势。</p><p>在每个头的计算过程中，彼此之间相互独立，参数不共享，仅在最后将结果拼接起来，这样可以允许模型在不同的表示子空间里学习到相关的信息。</p><p>最后整合多个向量，把他们降维到一个向量的长度，这个过程也是在学习”到底哪个头学到的知识是有效的”。</p><h3 id="6-位置编码的计算方法？"><a href="#6-位置编码的计算方法？" class="headerlink" title="6. 位置编码的计算方法？"></a>6. 位置编码的计算方法？</h3><p>Attention is all you need 论文中的位置编码的实现方式如下：</p><script type="math/tex; mode=display">\text{PosEncoding}_{(\text{pos}, 2i)}=\sin{(\frac{\text{pos}}{10000^{2i/d_{model}}})} \\\text{PosEncoding}_{(\text{pos}, 2i+1)}=\cos{(\frac{\text{pos}}{10000^{2i/d_{model}}})}</script><p>其中pos表示词在句子中的位置，i则表示向量的分量。这种计算方法无需学习任何参数。</p><p>在BERT论文中，采用的位置编码就变成了Embedding的方法自动学习得到。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：自注意力机制。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h2 id=&quot;一、输入从图像到序列&quot;&gt;&lt;a href=&quot;#一、输入从图像到序列&quot; class=&quot;headerlink&quot; title=&quot;一、输入从图像到序</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
    <category term="Self-Attention" scheme="https://superlova.github.io/tags/Self-Attention/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】李宏毅2019ML-Note5-卷积神经网络</title>
    <link href="https://superlova.github.io/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://superlova.github.io/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2022-10-22T13:48:58.000Z</published>
    <updated>2022-10-22T18:46:52.357Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：卷积神经网络。<br><!--more---></p><h2 id="为什么我们要用CNN？"><a href="#为什么我们要用CNN？" class="headerlink" title="为什么我们要用CNN？"></a>为什么我们要用CNN？</h2><p>要回答这个问题，首先要说明为什么全连接网络不适合做图像识别任务。</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/为什么要用CNN_1.png" alt></p><p>以图像为输入，会导致输入序列特别长。假设输入是一张<code>100*100</code>像素的图片，每个像素有<code>R/G/B</code>三个分量，要想将它整理为输入序列，每张图片的维度为<code>100*100*3=30000</code>。</p><p>输入层的参数过多，会导致中间层的参数过多，网络不仅训练代价提升，还有过拟合的风险。实际上图像分类任务用不着那么多参数，原因有三：</p><ol><li>每种特征所需的仅仅是图片的一小部分</li></ol><p>从神经网络的可解释性上来说，网络的第一层学习到的是最简单的特征分类，比如一张输入图片”有没有绿色出现，有没有黄色出现，有没有斜的条纹“等等；第二层会学到更复杂一些的特征组合，越往后学到的特征越抽象。</p><p>但是对于一个检测”是否有鸟嘴“的分类器来说，不需要看整张图片就可以知道这件事；<strong>输入序列可以从一整张图片优化为原图的一小部分</strong>。</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/为什么要用CNN_2.png" alt></p><ol><li>识别同一特征的分类器的部分参数可以共享</li></ol><p>对于检测鸟嘴的分类器而言，鸟嘴出现在图片的不同位置根本无所谓。我们不需要训练两组参数，来分别检测到底是左下方的鸟嘴还是右上方的鸟嘴。这意味着<strong>做同一特征分类的参数可以共享</strong>。</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/为什么要用CNN_3_参数共享.png" alt></p><ol><li><strong>输入序列的部分信息可以略过</strong></li></ol><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/为什么要用CNN_4.png" alt></p><p>对图片进行采样，得到的像素更低的图片，并不会影响人对这张图片的理解。</p><h2 id="CNN-架构"><a href="#CNN-架构" class="headerlink" title="CNN 架构"></a>CNN 架构</h2><p>卷积神经网络有两个基本单元：卷积层(Convolution Layers)和池化层(MaxPooling Layers)，可以实现上面三点性能优化。</p><p>一个完整的卷积神经网络大概长这样：</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CNN架构_1.png" alt></p><p>卷积层：是一种利用卷积操作提取图像的局部特征的网络结构。通过选择卷积核，不断执行卷积操作，得到原图像的特征图；后续的网络根据该特征图为输入，继续提取特征。</p><p>池化层：有最大池化和平均池化两种方式，最大池化是取滑动窗口内最大的元素作为输出。</p><h2 id="CNN-卷积层与全连接层的联系"><a href="#CNN-卷积层与全连接层的联系" class="headerlink" title="CNN 卷积层与全连接层的联系"></a>CNN 卷积层与全连接层的联系</h2><p>卷积核中的权值每次滑动计算时只是局部连接，且在卷积列中的神经元共享参数——计算局部信息，而全连接层神经元的权值与所有输入相连——计算全局信息。</p><p>卷积层的作用是从输入数据中采集关键数据内容。全连接层在深度卷积神经网络中的作用是将前面经过多次卷积后高度抽象的特征进行整合。</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CNN卷积层与全连接层的关系.png" alt></p><h2 id="用Keras实现CNN网络"><a href="#用Keras实现CNN网络" class="headerlink" title="用Keras实现CNN网络"></a>用Keras实现CNN网络</h2><p>以<a href="https://www.tensorflow.org/tutorials/images/cnn">tensorflow的图像分类教程</a>为例，实践使用keras搭建卷积模型的方法：</p><p>先导入相关包</p><pre><code class="lang-py">import tensorflow as tffrom tensorflow.keras import datasets, layers, modelsimport matplotlib.pyplot as plt</code></pre><p>然后下载并准备 CIFAR10 数据集</p><pre><code class="lang-py">(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()# 归一化像素值到 0 ~ 1train_images, test_images = train_images / 255.0, test_images / 255.0</code></pre><p>验证数据</p><pre><code class="lang-py">class_names = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;,               &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;]plt.figure(figsize=(10,10))for i in range(25):    plt.subplot(5,5,i+1)    plt.xticks([])    plt.yticks([])    plt.grid(False)    plt.imshow(train_images[i])    # The CIFAR labels happen to be arrays,     # which is why you need the extra index    plt.xlabel(class_names[train_labels[i][0]])plt.show()</code></pre><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CIFAR10样例.png" alt></p><p>接下来开始搭建网络：</p><pre><code class="lang-py">model = models.Sequential()model.add(layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(32, 32, 3)))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;))</code></pre><p>卷积网络被封装为<code>Conv2D</code>，其中第一个参数为卷积核个数，你可以理解为隐藏层神经元个数；我们这里定义为32个，它会提取32种不同类别的特征。</p><p>第二个参数是(3,3)，代表卷积核的大小是<code>3*3</code>的，这与李老师课堂上的示例一致；</p><p>第三个参数代表我们选择激活函数为<code>relu</code>；</p><p>第四个参数是只有在临近输入层的卷积层才会有的，代表输入向量的维度。这里的图像是彩色的，不但有长宽，还有通道数(channels)为3，因此输入向量的大小为(32,32,3)。</p><p>在搭建完第一层Conv2D后，紧接着会搭建最大池化层，池化层filter为<code>2*2</code>。后续的操作也是重复的，都是一层卷积、一层池化。到目前为止，模型结构如下：</p><pre><code class="lang-py">model.summary()</code></pre><pre><code>Model: &quot;sequential&quot;_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= conv2d (Conv2D)             (None, 30, 30, 32)        896        max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0          )                                                                conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496      max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0          2D)                                                              conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     =================================================================Total params: 56,320Trainable params: 56,320Non-trainable params: 0_________________________________________________________________</code></pre><p>我们可以看到，第一层Conv2D的输出向量的大小变成了(30,30,32)，这里32是我们之前定义的卷积核个数，<code>30*30</code>则是特征图大小。每次卷积操作都会导致宽度和高度的收缩。</p><p>参数量为896，这是如何计算来的呢？首先每个卷积核是<code>3*3</code>，同时图像有3个channel，因此每个卷积核的参数个数为<code>3*3*3</code>，再加上一个偏置量，就是28个参数。一共32个卷积核，最后的参数量即为<code>[(height * width * channel) + 1] * filter</code>。</p><p>卷积层只是特征提取器，最后为了完成分类操作，我们需要在卷积层后面拼接全连接层。不用太多，几层就够了：</p><pre><code class="lang-py">model.add(layers.Flatten())model.add(layers.Dense(64, activation=&#39;relu&#39;))model.add(layers.Dense(10))model.summary()</code></pre><pre><code>Model: &quot;sequential&quot;_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= conv2d (Conv2D)             (None, 30, 30, 32)        896        max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0          )                                                                conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496      max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0          2D)                                                              conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928      flatten (Flatten)           (None, 1024)              0          dense (Dense)               (None, 64)                65600      dense_1 (Dense)             (None, 10)                650       =================================================================Total params: 122,570Trainable params: 122,570Non-trainable params: 0_________________________________________________________________</code></pre><p>卷积层的输出在输入两个 Dense 层之前需要被展平（Flatten）为形状为 (1024) 的向量。</p><p>仅添加了两层全连接层，参数量就多了65600+650个，可以看到全连接层真的比CNN多很多冗余参数。</p><p>最后编译并训练一下：</p><pre><code class="lang-py">model.compile(optimizer=&#39;adam&#39;,              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),              metrics=[&#39;accuracy&#39;])history = model.fit(train_images, train_labels, epochs=10,                     validation_data=(test_images, test_labels))plt.plot(history.history[&#39;accuracy&#39;], label=&#39;accuracy&#39;)plt.plot(history.history[&#39;val_accuracy&#39;], label = &#39;val_accuracy&#39;)plt.xlabel(&#39;Epoch&#39;)plt.ylabel(&#39;Accuracy&#39;)plt.ylim([0.5, 1])plt.legend(loc=&#39;lower right&#39;)plt.show()test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)</code></pre><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/训练结果.png" alt></p><p>准确率只有不到70%，可以改进的地方还有很多。不过对于一个简单模型而言，在10分类任务上能达到如此成绩已是不容易了。</p><h2 id="CNN学到了什么？"><a href="#CNN学到了什么？" class="headerlink" title="CNN学到了什么？"></a>CNN学到了什么？</h2><p>分析第一层Conv到底学到了什么还是很容易的，因为每个卷积核实际上是在计算小范围内是否包含指定特征。但是后续Conv会根据该特征图计算更抽象的特征，我们该如何分析？</p><p>可以采用这种分析方法，首先固定参数，然后通过梯度下降法去找能够让某个神经元的输出值最大的输入图片。</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/生成最能激活某个神经元的图片.png" alt></p><p>上面就是12个filter对应的结果，这些图片的特征是某种纹路在图上不断的重复。因此实际上神经元会对这种不断重复的花纹做出最大的反应。</p><p>当我们以同样的分析方法分析卷积网络最后的全连接层（这里汇聚了卷积层提取出来的各路特征），得到的图像为：</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/全连接_生成最能激活某个神经元的图片.png" alt></p><p>发现最能激活某个神经元的图片，不再是那些纹路单一的图像了，而是带有某种花纹，似乎有某些含义。</p><p>但是当我们用同样的方法分析输出层时，得到的图像理论上应该是确切的数字，实际上确实乱码一样无法被辨认的图片：</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/输出层_生成最能激活某个神经元的图片.png" alt></p><p>人类虽然无法辨识，但是机器十分确定地告诉我们，左上角的图片就是0，右下角就是8。这种现象很有意思。</p><p>这种思想被应用于对抗样本攻击之中，可以生成一些令模型误判的诡异图片，并将其改造原本的数据集，就可以令模型犯错。</p><p>关于文本领域对抗样本的讨论在<a href="https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/">这里</a></p><p>Deep Dream则利用该思想，将CNN变成一个图像生成器，夸大化原有的输入图像：</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/DeepDream夸大的图像.png" alt></p><p>右边有一只熊，这个熊原来是一个石头(对机器来说，这个石头有点像熊，它就会强化这件事情，所以它就真的变成了一只熊)。</p><h2 id="CNN的其他应用场景"><a href="#CNN的其他应用场景" class="headerlink" title="CNN的其他应用场景"></a>CNN的其他应用场景</h2><p>CNN可以应用于游戏领域，比如围棋</p><p>CNN可以应用于自然语言处理领域，比如文本分类、语音识别</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：卷积神经网络。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h2 id=&quot;为什么我们要用CNN？&quot;&gt;&lt;a href=&quot;#为什么我们要用CNN？&quot; class=&quot;headerlink&quot; title=&quot;为什么我们要用</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
    <category term="Convolutional Neural Network" scheme="https://superlova.github.io/tags/Convolutional-Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】李宏毅ML-Note4-深度学习优化</title>
    <link href="https://superlova.github.io/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/"/>
    <id>https://superlova.github.io/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/</id>
    <published>2022-10-19T11:03:40.000Z</published>
    <updated>2022-10-19T14:21:27.005Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：深度学习算法的优化。<br><!--more---></p><p>神经网络训练不好怎么办？为什么loss不下降？这里讨论的训练不好，是指在训练过程中的loss始终降不下来。从数学优化的角度，此时可能陷入了局部最小值或者鞍点等微分为零的点。</p><h2 id="1-优化陷入鞍点-Saddle-Point-或局部极小值-Local-Minima"><a href="#1-优化陷入鞍点-Saddle-Point-或局部极小值-Local-Minima" class="headerlink" title="1. 优化陷入鞍点 (Saddle Point) 或局部极小值 (Local Minima)"></a>1. 优化陷入鞍点 (Saddle Point) 或局部极小值 (Local Minima)</h2><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/陷入鞍点或者局部最小点.png" alt></p><p>鞍点是指在一个方向上的gradient=0，但是在其他方向的gradient不为零，说明loss仍有继续优化的空间。</p><p>局部最小点则是所有方向的gradient均为零，此时无论往哪个方向走都无法令loss继续变小。这种情况比较棘手。</p><p>如何确认到底是鞍点还是局部极值？</p><p>这里有个结论：如果该点处的二阶偏导数矩阵（海塞矩阵）是正定的，即该海塞矩阵的所有特征值都大于零，说明该点附近的所有值均大于该点，即该点为局部最小值。</p><p>如果海塞矩阵的特征值全为负，则说明这里是局部最大值。</p><p>其他情况下，如果海塞矩阵的特征值有正有负，说明这里是个鞍点。</p><p>求解该点处的二阶偏导数不但可以帮助判断是否为鞍点，还可以帮助我们选择优化的方向。如果该处是一个鞍点，则我们就选择一个海塞矩阵特征值为负值的特征向量，朝着这个特征向量的方向进行移动，就能够最快速走出鞍点。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/快速走出鞍点.png" alt></p><p>实际上求海塞矩阵是一个计算量比较大的操作，除非有必要，否则这种方法其实很少使用。事实上，很少有优化算法具备把损失函数给优化到局部最小值或者鞍点的程度，在此之前我们会受到其他问题的困扰，比如batch没有选择好、优化方法并不够优秀等。</p><h2 id="2-batch没有选择好"><a href="#2-batch没有选择好" class="headerlink" title="2. batch没有选择好"></a>2. batch没有选择好</h2><p>在使用随机梯度下降算法时，我们会定义一个batch大小，并把训练集分割成batch大小的一个个子集，每次梯度下降只在子集上进行遍历，每次训练完一个子集的内容叫做一个epoch。</p><p>batch的大小自然会影响网络训练的速度以及优化效果。小batch可以加速训练，缺点是小batch随机性更大，没办法代表整体的数据集。极端情况下，batch=1时，甚至会出现loss在原地不断震荡的情况。</p><p>然而batch过大则失去了意义，因为这会导致训练速度变慢。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/batch大小的权衡.png" alt></p><p>如果引入并行计算（即GPU运算），那么batch_size在一定程度内都可以很快。但是不同GPU支持的batch_size是有极限的（取决于显存）。</p><p>batch_size 对最终的优化效果有影响吗？是有的。大的batch_size可能会导致优化阶段产生问题。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/batch_size对最终优化效果的影响.png" alt></p><p>这种问题是怎么产生的呢？有一种解释是这样的，当我们试图以一个比较大的batch_size训练模型时，可能会卡在客观存在的局部极值点；但是如果我们使用比较小的batch，相当于人为引入了更多的随机性。w在batch_A上是局部极值点，但是在batch_B上就不一定了。</p><p>另外，在testing的时候，选择不同的batch也会让accuracy不一样。如果大的Batch对应的Testing结果差 ，代表 Overfitting 。</p><p>最后总结一下，batch_size可能造成的影响：</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/batch_size的选择总结.png" alt></p><h2 id="3-虽然-gradient-没有降低到0，但是-loss-已经不再-变小-了（没有选择合适的优化算法）"><a href="#3-虽然-gradient-没有降低到0，但是-loss-已经不再-变小-了（没有选择合适的优化算法）" class="headerlink" title="3. 虽然 gradient 没有降低到0，但是 loss 已经不再 变小 了（没有选择合适的优化算法）"></a>3. 虽然 gradient 没有降低到0，但是 loss 已经不再 变小 了（没有选择合适的优化算法）</h2><p>多数情况下，不会有真正陷入 critical point 的机会，而是在 critical point 附近徘徊。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/训练卡住了并不一定意味着gradient很小.png" alt></p><p>因此需要我们调整学习率。每个不同的参数应该在训练的不同阶段拥有不同的学习率。这种通过改变学习率来优化算法的训练速度的方法，在Note2中已有介绍。</p><p>常用的自适应改变参数学习率的算法有Adagrad算法、RMSProp算法等。</p><p>一种方法是使用带有momentum的优化算法。动量法在Note2里已有介绍，我们将梯度下降的优化过程比喻成小球滚落山坡的过程。但是梯度下降的优化过程忽略了小球本身具有动量，与现实中有一定区别。如果我们将梯度下降算法添加上动量的模拟，则小球在陷入局部最优解时，足够高的动量能帮助小球“冲”出局部最优解。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/动量法模拟.png" alt></p><p>使用动量法的优化算法有Adam等。实际上，Adam是Adagrad和Momentum动量法的结合。</p><p>此外，从经验上看，学习率一开始要保持大一些，来保证收敛到最优点附近；然后要减小学习率来避免震荡。那么我们自然可以想到，学习率跟随训练轮次变大逐渐变小。这就是<strong>学习率衰减</strong>。</p><p>在刚开始训练时，由于参数是随机初始化的，梯度也往往比较大，再加上比较大的初始学习率，会使得训练不稳定。因此我们希望刚开始几轮迭代的学习率较小，等梯度下降到一定程度时再恢复学习率。这种方法称之为<strong>学习率预热</strong>。等预热完毕后在进行学习率衰减。</p><p>利用这两个思想，改进Adam后，就是优化算法RAdam。</p><h2 id="4-损失函数也会有影响"><a href="#4-损失函数也会有影响" class="headerlink" title="4. 损失函数也会有影响"></a>4. 损失函数也会有影响</h2><p>分类问题的损失函数可以选择MSE和Cross-entropy。现在我们更多选择交叉熵。为什么MSE不行呢？</p><p>MSE在loss大的地方非常平坦，梯度趋近于零，最后stuck卡住！难以优化。</p><p>Cross-entropy ：左上角有斜率，可以透过梯度，一路往右下角走。更易收敛。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/MSE和Cross-entropy.png" alt></p><h2 id="5-也许需要-normalization"><a href="#5-也许需要-normalization" class="headerlink" title="5. 也许需要 normalization"></a>5. 也许需要 normalization</h2><p>一般而言，样本特征由于来源以及度量单位不同，他们的尺度 (Scale) 即取值范围也不同。对于某些机器学习模型，会对那些尺度更大的数据更敏感。因此对于尺度敏感的模型，必须先对样本进行预处理，将各个维度的特征转换到相同的取值区间内，并且消除不同特征之间的相关性，才能取得理想的效果。</p><p>归一化 (Normalization) 方法泛指把数据特征转换为相同尺度的方法，比如把数据特征映射到 [0,1] 区间内，或者直接映射为均值为0、方差为1的正态分布。</p><p><strong>最小最大值归一化</strong>：将每个特征缩放到 [0,1] 或者 [-1,1] 之间。假设我们有 N 哥样本 ${x^{(n)}}^{N}_{n=1}$，对每一维特征x，归一化后的特征为：</p><script type="math/tex; mode=display">\hat{x}^{(n)}=\frac{x^{(n)}-\min_n(x^{(n)})}{\max_n(x^{(n)})-\min_n(x^{(n)})}</script><p><strong>标准化</strong>：将每维特征都调整到标准正态分布：</p><script type="math/tex; mode=display">\hat{x}^{(n)}=\frac{x^{(n)}-\mu}{\sigma}</script><p>除此之外，还有Batch-Normalization和Layer-Normalization的算法。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：深度学习算法的优化。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;p&gt;神经网络训练不好怎么办？为什么loss不下降？这里讨论的训练不好，是指在训练过程中的loss始终降不下来。从数学优化的角度，此时可能陷入了局部</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
    <category term="Deep Learning" scheme="https://superlova.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】李宏毅2019ML-Note3-深度学习介绍和反向传播</title>
    <link href="https://superlova.github.io/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <id>https://superlova.github.io/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</id>
    <published>2022-10-16T10:32:13.000Z</published>
    <updated>2022-10-16T13:56:17.399Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：深度学习算法介绍、反向传播机制。<br><!--more---></p><h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><p>深度学习大家已经很熟悉了。深度学习技术属于一类机器学习技术。使用深度学习解决问题的过程同样需要三步骤：</p><p>Step1：选择深度学习模型：神经网络（Neural network），神经网络有各种不同类型，比如前馈神经网络、卷积神经网络等；<br>Step2：模型评估（Goodness of function）；<br>Step3：选择最优函数（Pick best function），不同的网络结构需要定义不同的参数学习方法，比如反向传播算法；</p><p>接下来逐一介绍三个步骤的具体内容。</p><h2 id="第一步：选择神经网络：从前馈网络开始"><a href="#第一步：选择神经网络：从前馈网络开始" class="headerlink" title="第一步：选择神经网络：从前馈网络开始"></a>第一步：选择神经网络：从前馈网络开始</h2><p>前馈（feedforward）也可以称为前向，从信号流向来理解就是输入信号进入网络后，信号流动是单向的，即信号从前一层流向后一层，一直到输出层，其中任意两层之间的连接并没有反馈（feedback），亦即信号没有从后一层又返回到前一层。</p><p>下图是一个由一层输入层、N层隐藏层和一层输出层构成的、每层包含M个神经元的全连接网络，神经元的激活函数为sigmoid：</p><p><img src="/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/前馈网络.png" alt></p><p>为什么叫全链接呢？因为layer1与layer2之间两两都有连接，所以叫做Fully Connect；</p><p>为什么叫前馈呢？因为现在传递的方向是由后往前传，所以叫做Feedforward。</p><p>当隐藏层N非常大时，我们就说这个网络非常深。现如今的网络结构动辄几百层，随之带来很大的计算开销。一个一个计算神经元的输入输出是不现实的，一种加速方法是将神经网络的信息传递过程具象化为矩阵运算。</p><p>假设下图的前馈网络：</p><p><img src="/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/前馈网络-矩阵计算演示.png" alt></p><p>计算方法就是：sigmoid（权重w【黄色】 * 输入【蓝色】+ 偏移量b【绿色】）= 输出</p><p>如果有很多层，就进行嵌套</p><p><img src="/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/很多层的全连接网络的矩阵计算.png" alt></p><p>所以整个神经网络运算就相当于一连串的矩阵运算。矩阵运算是可以被GPU并行加速的，因此深度学习模型才能够在如今这个算力大爆发的年代大放异彩。</p><p><strong>深度学习的本质是利用隐藏层学习到了输入特征的有效表示方法，代替了之前依靠数据科学家的经验的特征工程。</strong></p><h2 id="第二步：模型评估方法：选择损失函数"><a href="#第二步：模型评估方法：选择损失函数" class="headerlink" title="第二步：模型评估方法：选择损失函数"></a>第二步：模型评估方法：选择损失函数</h2><p>对于模型的评估，我们一般采用损失函数来反应模型的好差，所以对于神经网络来说，我们采用交叉熵（cross entropy）函数来对$y$和$\hat{y}$的损失进行计算，接下来我们就是调整参数，让交叉熵越小越好。</p><h2 id="第三步：选择最优模型：梯度下降，以及用反向传播算法优化计算"><a href="#第三步：选择最优模型：梯度下降，以及用反向传播算法优化计算" class="headerlink" title="第三步：选择最优模型：梯度下降，以及用反向传播算法优化计算"></a>第三步：选择最优模型：梯度下降，以及用反向传播算法优化计算</h2><p>梯度下降算法在之前的笔记已经讲过，神经网络的梯度下降算法也是相同的。</p><p><img src="/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/梯度下降.png" alt></p><p>梯度下降的具体过程：</p><p><img src="/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/梯度下降的过程.png" alt></p><ol><li>给到$\theta$(weight and bias)</li><li>先选择一个初始的 $\theta^0$，计算 $\theta^0$ 的损失函数（Loss Function）设一个参数的偏微分</li><li>计算完这个向量（vector）偏微分，然后就可以去更新的你 $\theta$</li><li>百万级别的参数（millions of parameters）</li><li>反向传播（Backpropagation）是一个比较有效率的算法，让你计算梯度（Gradient） 的向量（Vector）时，可以有效率的计算出来</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：深度学习算法介绍、反向传播机制。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h1 id=&quot;深度学习&quot;&gt;&lt;a href=&quot;#深度学习&quot; class=&quot;headerlink&quot; title=&quot;深度学习&quot;&gt;&lt;/a&gt;深度</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
    <category term="Deep Learning" scheme="https://superlova.github.io/tags/Deep-Learning/"/>
    
    <category term="Backpropagation" scheme="https://superlova.github.io/tags/Backpropagation/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】李宏毅2019ML课程-Note2</title>
    <link href="https://superlova.github.io/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/"/>
    <id>https://superlova.github.io/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/</id>
    <published>2022-10-13T02:52:43.000Z</published>
    <updated>2022-10-15T02:44:09.974Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：偏差和方差、模型选择、梯度下降算法。<br><!--more---></p><h1 id="一、误差的来源：偏差与方差"><a href="#一、误差的来源：偏差与方差" class="headerlink" title="一、误差的来源：偏差与方差"></a>一、误差的来源：偏差与方差</h1><p>对学习算法除了通过实验估计其泛化性能，人们往往还希望能了解为什么具有这样的性能。误差来源于偏差 (bias) 和方差 (Variance)以及不可避免的噪声。</p><h2 id="1-偏差和方差的概念和来源"><a href="#1-偏差和方差的概念和来源" class="headerlink" title="1. 偏差和方差的概念和来源"></a>1. 偏差和方差的概念和来源</h2><p><strong>偏差</strong>度量了预测值与真实值的偏离程度，对应的是学习算法本身的拟合能力；<strong>方差</strong>度量了数据扰动对模型的影响，对应的是模型的稳定性；<strong>噪声</strong>则是对应问题对应的难度。</p><p>上面的结论说明，模型的性能是由模型能力、数据的充分性以及问题本身的难度决定的。由于噪音是问题本身的特性，不好解决，因此要想提升模型的性能，就需要采取措施降低偏差和方差。</p><h2 id="4-偏差和方差分解：重新考虑欠拟合、过拟合问题"><a href="#4-偏差和方差分解：重新考虑欠拟合、过拟合问题" class="headerlink" title="4. 偏差和方差分解：重新考虑欠拟合、过拟合问题"></a>4. 偏差和方差分解：重新考虑欠拟合、过拟合问题</h2><p>为了避免过拟合，我们经常会在模型的拟合能力和复杂度之间进行权衡。拟合能力强的模型一般复杂度会比较高，容易导致过拟合。相反，如果限制模型的复杂度，降低其拟合能力，有可能导致欠拟合。因此，如何在模型的拟合能力与复杂度之间取得平衡，对机器学习算法来说十分重要。</p><p>偏差-方差分解为我们提供了一个很好用的分析工具。数学推导过程比较复杂，结论为：</p><p><strong>最小化期望错误等价于最小化偏差和方差之和。</strong></p><p>下图给出了机器学习模型四种偏差和方差的组合情况。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/偏差和方差.png" alt></p><p>每个图的中心店为最优模型$f(x)$，蓝点为不同训练集上得到的模型$f^*(x)$。左上角的图是一种理想情况，方差和偏差都比较低；右上角为高偏差低方差的情况，表示模型的泛化能力很好，但是拟合能力不足，类似于我们之前的线性模型；左下角是低偏差高方差的情况，表示模型的拟合能力很好，但是泛化能力较差。当训练数据比较少的时候往往会出现这种情况，我们一般把他称之为过拟合；右下角为高偏差高方差的情况，是最差的情况，等于没训练。</p><p>方差一般会随着训练样本的增加而减小。当样本比较多，方差比较小，这是可以选择能力强的模型来减少偏差；当训练集比较有限时，最优的偏差和方差往往无法兼顾，我们称之为“偏差-方差窘境”。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/偏差方差窘境.png" alt></p><p>偏差-方差窘境说的是，模型复杂度增加后，虽然你和能力变强导致偏差减少，但是方差会增大，过拟合现象会导致性能下降。</p><p>偏差-方差分解给机器学习模型提供了一种分析途径，能够指导我们解决过拟合和欠拟合的问题，但是实际操作中很难直接衡量。下面是一些通用的方法。</p><pre><code>欠拟合现象：模型在训练集上的错误率很高，此时说明模型偏差较大。欠拟合解决：1. 增加数据特征2. 提高模型复杂度3. 减小正则化系数</code></pre><pre><code>过拟合现象：模型在训练集上的错误率较低，但是在测试集上较高，此时说明模型方差较大。过拟合解决：1. 降低模型复杂度；2. 加大正则化系数；3. 引入先验知识，比如数据清洗，剔除无用特征等；4. 使用更多数据进行训练，但这个往往成本最高。</code></pre><p>此外，还有一种降低方差的方法：集成模型，即通过多个高方差模型进行平均，来降低方差。这背后的原理在介绍集成学习时会介绍。</p><h1 id="二、模型选择方法"><a href="#二、模型选择方法" class="headerlink" title="二、模型选择方法"></a>二、模型选择方法</h1><p>前面讨论了机器学习的三个步骤，以及如何改进模型的性能。那么我们如何正确评估模型呢？</p><p>之前的例子中，我们把数据集分成两部分，Training Set、Test Set。一般比例控制在4:1左右。我们在训练结束后，使用测试集进行评估，并判断训练效果。</p><p>这样做其实存在问题。如果我们真的使用测试集来指导模型的选择、参数的改进等各个步骤，那算不算模型正在学习测试集的内容呢？就好像学生做完考卷后，老师虽然不会直接告诉学生正确答案，但是会不断地给学生机会，告诉你：你这里错了，那里错了。这算不算是另一种泄题呢？</p><p>上面的数据泄露现象是经常发生的。为了避免数据泄露，一般可以通过把数据分成三部分：训练集 (Training Set)、验证集 (Validation Set) 和测试集 (Test Set)。模型参数的改进、模型选择等过程，只使用验证集；最后评估模型时使用测试集。</p><p>但是这样又会带来另一个问题：本来数据就匮乏，又分割出两大部分不能使用，可供学习的数据就更少了。<strong>k折交叉验证</strong>可以解决这个问题。</p><h2 id="2-1-交叉验证"><a href="#2-1-交叉验证" class="headerlink" title="2.1 交叉验证"></a>2.1 交叉验证</h2><p>交叉验证 (Cross-Validation) 是一种评估泛化性能的统计学方法。它比单次划分训练集和测试集的方法更加稳定。最常用的交叉验证方法是<strong>k折交叉验证</strong>。</p><p>首先把数据集分成大致相等的k个部分，每一部分叫做一折；接下来训练k个模型，第一个模型以第1折数据作为测试集，其他作为训练集；第二个模型以第2折数据作为测试集，其他作为训练集……最后我们得到了k个模型，以及对应的k个精度值。模型的最终精度就是这k个精度值的平均值。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/交叉验证.png" alt></p><p>交叉验证的优点：</p><ol><li>帮助更好评估模型的真实泛化性能，防止数据集分割时的随机性影响评估准确率；</li><li>所有数据都有机会被训练，提高数据利用率；</li></ol><p>交叉验证的缺点：</p><ul><li>显著增加了计算成本，需要训练k个模型，是原来的k倍。</li></ul><h1 id="三、梯度下降算法详解，以及优化方法"><a href="#三、梯度下降算法详解，以及优化方法" class="headerlink" title="三、梯度下降算法详解，以及优化方法"></a>三、梯度下降算法详解，以及优化方法</h1><p>目前，机器学习中参数学习的主要方式是通过梯度下降法来寻找一组最小化损失函数的参数。再具体视线中，梯度下降法可以分为：批量梯度下降、随机梯度下降以及小批量梯度下降三种形式。</p><p>本节还会介绍一些梯度下降算法的变种，他们大多改善了以下两部分的内容：1) 调整学习率，使优化更稳定；2) 梯度估计修正，提升训练速度。</p><h2 id="3-1-梯度下降算法的类别"><a href="#3-1-梯度下降算法的类别" class="headerlink" title="3.1 梯度下降算法的类别"></a>3.1 梯度下降算法的类别</h2><h3 id="3-1-1-批量梯度下降"><a href="#3-1-1-批量梯度下降" class="headerlink" title="3.1.1 批量梯度下降"></a>3.1.1 批量梯度下降</h3><p>最传统的梯度下降算法，上篇笔记已经介绍。</p><p>梯度下降的具体过程是这样的：</p><ol><li>选择一个初始$\theta_0$</li><li>计算该位置下$\theta$对L的微分，这个微分对应函数在此处下降最快的方向；</li><li>将$\theta_0$朝着这个方向移动一小步$ \eta$</li><li>在新的位置开始新一轮迭代计算，直到$w$不再变化为止。</li></ol><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/批量梯度下降法.png" alt></p><p>其中$\eta$叫做学习率，它控制了每次梯度下降迭代的优化幅度。$\eta$越大，学习得越快。然而$\eta$并不是越大越好。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/批量梯度下降法的梯度深谷.png" alt></p><p>如果$\eta$过大，优化过程很可能会跨越最低点，从而永远达不到最优解；如果$\eta$过小，则会造成训练缓慢，甚至陷入局部最优解。因此调整学习率$\eta$是优化梯度下降算法的一大思路。</p><p>梯度下降算法的另一个问题是，当训练数据很大时（这很常见），梯度下降算法会花费很长时间去遍历整个训练集。大多数情况下，我们不必遍历所有训练集。</p><h3 id="3-1-2-随机梯度下降"><a href="#3-1-2-随机梯度下降" class="headerlink" title="3.1.2 随机梯度下降"></a>3.1.2 随机梯度下降</h3><p>在训练机器学习模型时，训练数据的规模比较大的情况下，批量梯度下降的每次梯度迭代都要遍历整个数据集，这会造成计算资源的浪费。为了解决批量梯度下降算法导致的训练缓慢的问题，随机梯度下降算法应运而生。其思想是通过随机选取少量训练输入样本来计算$\nabla L_x$，进而估算$\nabla L$。</p><p>更准确地说，随机梯度下降通过随机选取少量的m个训练数据来训练。也叫做小批量梯度下降法 (Mini-Batch Gradient Descent)。</p><p>假设样本数量m足够大，我们期望$\nabla L<em>{X_j}$的平均值大致相等于整个$\nabla L</em>{x}$的平均值，即：</p><script type="math/tex; mode=display">\frac{\sum^m_{j-1}\nabla L_{X_j}}{m}\approx \frac{\sum_x \nabla L_x}{n}=\nabla L</script><p>这里第二个求和符号实在整个训练数据上进行的。交换两边我们得到：</p><script type="math/tex; mode=display">\nabla L \approx \frac{1}{m}\sum^m_{j=1}\nabla L_{X_j}</script><p>证实了我们可以进通过计算随机选取的小批量数据的梯度来估算整体的梯度。</p><p>影响小批量梯度下降的主要因素有：1）批大小m，2）学习率$\eta$，3）梯度估计方法。</p><h2 id="3-2-批量大小选择"><a href="#3-2-批量大小选择" class="headerlink" title="3.2 批量大小选择"></a>3.2 批量大小选择</h2><p>直观来说，批包含的数据越大，方差也就越小，训练越稳定。该种情况下，可以设置一个较大的学习率。学习率越大，需要的批大小就越大。</p><p>另外，根据经验，批越大越可能收敛到“尖锐最小值”；批越小越能收敛到“平坦最小值”。</p><h2 id="3-3-学习率调整"><a href="#3-3-学习率调整" class="headerlink" title="3.3 学习率调整"></a>3.3 学习率调整</h2><p>学习率过大会导致模型不收敛，如果过小会导致收敛太慢。由此，有一些学者根据学习的不同阶段，制定了学习率的不同变化策略。</p><p>从经验上看，学习率一开始要保持大一些，来保证收敛到最优点附近；然后要减小学习率来避免震荡。那么我们自然可以想到，学习率跟随训练轮次变大逐渐变小。这就是<strong>学习率衰减</strong>。</p><p>在刚开始训练时，由于参数是随机初始化的，梯度也往往比较大，再加上比较大的初始学习率，会使得训练不稳定。因此我们希望刚开始几轮迭代的学习率较小，等梯度下降到一定程度时再恢复学习率。这种方法称之为<strong>学习率预热</strong>。等预热完毕后在进行学习率衰减。</p><h3 id="3-3-3-AdaGrad-算法"><a href="#3-3-3-AdaGrad-算法" class="headerlink" title="3.3.3 AdaGrad 算法"></a>3.3.3 AdaGrad 算法</h3><p>AdaGrad算法的做法是，每次迭代时，每个参数的学习率都把它除上之前微分的均方根。</p><p>普通的梯度下降算法采用的参数更新思路：</p><script type="math/tex; mode=display">w^{t+1}\leftarrow w^t-\eta^tg^t \\\eta^t=\frac{\eta^t}{\sqrt{t + 1}}</script><p>则Adagrad是这样更新的：</p><script type="math/tex; mode=display">w^{t+1}\leftarrow w^t-\frac{\eta^t}{\sigma^t}g^t \\g^t =\frac{\partial L(\theta^t)}{\partial w}</script><p>其中$\sigma^t$是之前参数的所有微分的均方根，对于每个参数都是不一样的。</p><p>将Adagrad的式子化简：</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/化简后的adagrad.png" alt></p><p>Adagrad的设计思路为，如果某个参数的偏导数累计比较大，其学习率就会相对较小；相反，如果偏导数累计较小，则学习率相对较大。但是整体上迭代次数越多，学习率越小。</p><p>AdaGrad的缺点是，如果在经过一定次数的迭代，依然没有找到最优点时，由于学习率已经非常小，很难再继续优化了。</p><h3 id="3-3-4-RMSprop-算法"><a href="#3-3-4-RMSprop-算法" class="headerlink" title="3.3.4 RMSprop 算法"></a>3.3.4 RMSprop 算法</h3><p>RMSProp算法可以帮助解决某些情况下Adagrad算法过早衰减的问题。</p><p>RMSProp算法在AdaGrad的基础上，改进了参数偏导数的累计方式。Adagrad算法是计算每个参数梯度平方的累计值：</p><script type="math/tex; mode=display">G_t=\sum^t_{\tau=1}\textbf{g}_{\tau}\odot\textbf{g}_{\tau}</script><p>其中$\textbf{g}_{\tau}\in\mathbb{R}^{|\theta|}$是第$\tau$ 次迭代时的梯度。</p><p>RMSProp算法首先计算每次迭代梯度 $\textbf{g}_t$平方的指数衰减移动平均：</p><script type="math/tex; mode=display">G_t=\beta G_{t-1}+(1-\beta)\textbf{g}_t\odot\textbf{g}_t \\=(1-\beta)\sum^t_{\tau=1}\beta^{t-\tau}\textbf{g}_{\tau}\odot\textbf{g}_{\tau}</script><p>其中$\beta$为衰减率，一般为0.9。</p><p>RMSProp算法的参数更新差值为：</p><script type="math/tex; mode=display">\Delta\theta_t=-\frac{\alpha}{\sqrt{G_t+\epsilon}}\odot\textbf{g}_t</script><p>其中$\alpha$是初始的学习率，比如0.001。</p><p>在迭代过程中，由于每个参数的学习率并不是衰减趋势，因此既可以变小也可以变大。</p><h2 id="3-4-梯度估计"><a href="#3-4-梯度估计" class="headerlink" title="3.4 梯度估计"></a>3.4 梯度估计</h2><p>除了调整学习率之外，还可以进行梯度估计的修正。这样做的原因是，小批量梯度下降选取的样本具有随机性，如果每次选取的样本数量比较小，则损失可能会呈现震荡的方式下降。一般我们可以采取使用最近一段时间内的平均梯度来代替当前时刻的随机梯度的做法来缓解随机性，提升优化速度。</p><h3 id="3-4-1-动量法"><a href="#3-4-1-动量法" class="headerlink" title="3.4.1 动量法"></a>3.4.1 动量法</h3><p>还记得之前我们说过的，梯度下降算法容易陷入局部最优解吗？我们将梯度下降的优化过程比喻成小球滚落山坡的过程。但是梯度下降的优化过程忽略了小球本身具有动量，与现实中有一定区别。如果我们将梯度下降算法添加上动量的模拟，则小球在陷入局部最优解时，足够高的动量能帮助小球“冲”出局部最优解。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/梯度下降算法容易陷入局部最优.png" alt></p><p>在第t次迭代是，计算负梯度的“加权移动平均”作为参数的更新方向：</p><script type="math/tex; mode=display">\Delta\theta_t=\rho\Delta\theta_{t-1}-\alpha\textbf{g}_t=-\alpha\sum^t_{\tau=1}\rho^{t-\tau}\textbf{g}_{\tau}</script><p>其中$\rho$为动量因子，一般为0.9，$\alpha$为学习率。</p><p>这样一来，每个参数的实际更新差值取决于最近一段时间内梯度的加权平均值。当某个参数在最近一段时间内的梯度方向都一致，其真是更新参数的幅度会变大，相当于加速冲刺；当某个参数在最近一段时间内的梯度方向不一致，则参数更新幅度会变小，相当于刹车。从某种角度来说，当前梯度叠加部分上次的梯度，这种做法可以近似看做二阶梯度。</p><h3 id="3-4-3-Adam-算法"><a href="#3-4-3-Adam-算法" class="headerlink" title="3.4.3 Adam 算法"></a>3.4.3 Adam 算法</h3><p>Adam算法是如今最常用的优化算法了，它是RMSProp算法和动量法的结合，不但使用动量作为参数更新方向，而且可以自适应调整学习率。</p><h2 id="3-3-数据预处理"><a href="#3-3-数据预处理" class="headerlink" title="3.3 数据预处理"></a>3.3 数据预处理</h2><p>一般而言，样本特征由于来源以及度量单位不同，他们的尺度 (Scale) 即取值范围也不同。对于某些机器学习模型，会对那些尺度更大的数据更敏感。因此对于尺度敏感的模型，必须先对样本进行预处理，将各个维度的特征转换到相同的取值区间内，并且消除不同特征之间的相关性，才能取得理想的效果。</p><p>归一化 (Normalization) 方法泛指把数据特征转换为相同尺度的方法，比如把数据特征映射到 [0,1] 区间内，或者直接映射为均值为0、方差为1的正态分布。</p><p><strong>最小最大值归一化</strong>：将每个特征缩放到 [0,1] 或者 [-1,1] 之间。假设我们有 N 哥样本 ${x^{(n)}}^{N}_{n=1}$，对每一维特征x，归一化后的特征为：</p><script type="math/tex; mode=display">\hat{x}^{(n)}=\frac{x^{(n)}-\min_n(x^{(n)})}{\max_n(x^{(n)})-\min_n(x^{(n)})}</script><p><strong>标准化</strong>：将每维特征都调整到标准正态分布：</p><script type="math/tex; mode=display">\hat{x}^{(n)}=\frac{x^{(n)}-\mu}{\sigma}</script><h2 id="3-4-梯度下降算法的局限性"><a href="#3-4-梯度下降算法的局限性" class="headerlink" title="3.4 梯度下降算法的局限性"></a>3.4 梯度下降算法的局限性</h2><p>上一篇笔记已经有提到该部分内容：</p><p>首先，<strong>梯度下降算法容易陷入局部最优，找不到全局最优解</strong>。</p><p>如下图所示，当梯度下降算法优化到local minima 时，前面有座高山，它的梯度是正的，优化算法会强迫我们往回走。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/梯度下降算法局部最优.png" alt></p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/梯度下降局部最优2.png" alt></p><p>这个问题在我们的线性回归模型里暂时遇不到，因为线性模型非常简单，是一个凸函数。但当我们试图利用梯度下降算法训练复杂模型，比如神经网络时，就很有可能遭遇该问题。</p><p>有很多方法能够解决梯度下降算法的全局最优解问题，包括调整每次行进的步长$\eta$，使其更有希望跨越“大山”等手段等等。</p><p>其次，梯度下降算法要求目标函数是可微的，这在某些情况下会<strong>产生相当大的计算代价</strong>。假设我们的问题有上百万维，计算二阶偏导数就需要上万亿（百万的平方）次！</p><p>再次，<strong>当训练数据相当多时，梯度下降算法会变得很慢</strong>。在实践中，为了计算梯度$\nabla L$，我们需要为了每个训练样本x单独计算梯度$\nabla L_x$，然后求平均值。这会花费很长时间。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：偏差和方差、模型选择、梯度下降算法。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h1 id=&quot;一、误差的来源：偏差与方差&quot;&gt;&lt;a href=&quot;#一、误差的来源：偏差与方差&quot; class=&quot;headerlink&quot;</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
    <category term="Gradient Descent" scheme="https://superlova.github.io/tags/Gradient-Descent/"/>
    
    <category term="Bias and Variance" scheme="https://superlova.github.io/tags/Bias-and-Variance/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】李宏毅2019ML课程-Note1</title>
    <link href="https://superlova.github.io/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/"/>
    <id>https://superlova.github.io/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/</id>
    <published>2022-10-10T12:57:00.000Z</published>
    <updated>2022-10-12T13:44:37.134Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：机器学习的概念和分类、利用线性模型解决回归问题以及梯度下降。<br><!--more---></p><h3 id="一、机器学习介绍"><a href="#一、机器学习介绍" class="headerlink" title="一、机器学习介绍"></a>一、<a href="https://www.bilibili.com/video/av59538266">机器学习介绍</a></h3><p>说一下我学这门课的初衷吧。其实机器学习的课程在研究生阶段就已经上过了，但是工作之后才发现有一些基础没搞懂或者已经遗忘，因此在学习新知识时存在障碍。恰逢Datawhale给了这次组队学习的机会，我便想与群友们一起把这门基础课程搞定。</p><h4 id="1-概要"><a href="#1-概要" class="headerlink" title="1. 概要"></a>1. 概要</h4><p>本节通俗易懂地介绍了机器学习的概念，介绍了AI的发展历史，以及与传统规则的区别。简单来说，<strong>机器学习</strong>是一种从有限数据中学习规律并对未知数据进行预测的方法。</p><p>刚开始讲了比较多的名词和概念。老师最后的图里很好地总结了本次课的内容：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/机器学习介绍01.png" alt></p><h4 id="首先，机器学习的过程分成三个步骤："><a href="#首先，机器学习的过程分成三个步骤：" class="headerlink" title="首先，机器学习的过程分成三个步骤："></a>首先，机器学习的过程分成三个步骤：</h4><ol><li>根据问题的不同，选择模型（function）；</li><li>根据模型的不同，定义能够度量学习效果的损失函数；</li><li>从有限数据中持续训练模型，使得损失函数最小。</li></ol><h4 id="其次，根据学习所需的数据是否存在标签，机器学习可以分成如下几类（图中蓝色框框的部分）："><a href="#其次，根据学习所需的数据是否存在标签，机器学习可以分成如下几类（图中蓝色框框的部分）：" class="headerlink" title="其次，根据学习所需的数据是否存在标签，机器学习可以分成如下几类（图中蓝色框框的部分）："></a>其次，根据学习所需的数据是否存在标签，机器学习可以分成如下几类（图中蓝色框框的部分）：</h4><ol><li>监督学习，指学习所需的数据是经过人工标注的；</li><li>无监督学习，指学习所需的数据不需人工标注；</li><li>其他学习方法，比如半监督学习、迁移学习、强化学习等。</li></ol><h4 id="在监督学习里，根据想要解决的问题不同，可分为如下几类（图中黄色框框的部分）："><a href="#在监督学习里，根据想要解决的问题不同，可分为如下几类（图中黄色框框的部分）：" class="headerlink" title="在监督学习里，根据想要解决的问题不同，可分为如下几类（图中黄色框框的部分）："></a>在监督学习里，根据想要解决的问题不同，可分为如下几类（图中黄色框框的部分）：</h4><ol><li>分类问题，指我们希望模型给出yes或no这样具体的评价；</li><li>回归问题，指我们希望模型能够给出一个数值，这个数值的大小有现实意义；</li><li>结构化问题，我们希望模型直接输出结构化结果，比如语言翻译模型能够产出一段文本，dall-e能够生成图像，等等。</li></ol><h4 id="分类问题是比较简单的问题，有很多模型可以解决分类问题（图中绿色框框的部分）："><a href="#分类问题是比较简单的问题，有很多模型可以解决分类问题（图中绿色框框的部分）：" class="headerlink" title="分类问题是比较简单的问题，有很多模型可以解决分类问题（图中绿色框框的部分）："></a>分类问题是比较简单的问题，有很多模型可以解决分类问题（图中绿色框框的部分）：</h4><ol><li>线性模型；</li><li>非线性模型，比如深度学习、SVM、决策树等方法。</li></ol><p>后续我们会学到什么是线性模型、什么是非线性模型，以及上面的具体模型的设计细节。</p><h4 id="为什么要学习机器学习这门课程？"><a href="#为什么要学习机器学习这门课程？" class="headerlink" title="为什么要学习机器学习这门课程？"></a>为什么要学习机器学习这门课程？</h4><p><del>当然是为了挣钱了</del></p><p>由于目前还没有一个普适的学习模型，能够解决世界一切可以用机器学习方法解决的问题，因此，我们需要依赖经验和知识，来根据不同的问题，选择不同的学习模型和和损失函数。还记得机器学习的三个步骤吗？选择模型、定义损失函数、训练模型过程的知识，都是能够帮助我们得到更加可靠的机器学习系统的技能。学习这门课程，能够帮助我们成为一名更好的机器学习工程师。</p><h3 id="二、机器学习案例——回归问题"><a href="#二、机器学习案例——回归问题" class="headerlink" title="二、机器学习案例——回归问题"></a>二、<a href="https://www.bilibili.com/video/BV1Ht411g7Ef?p=3">机器学习案例——回归问题</a></h3><p>这部分以一个回归问题为引子，引导我们探索机器学习的三大步骤。回归问题比较普遍，像股票预测、温度预测、房价预测等，都是回归问题。</p><p>机器学习的三大步骤分别为：</p><ol><li>根据问题的不同，选择模型（function）；</li><li>根据模型的不同，定义能够度量学习效果的损失函数；</li><li>从有限数据中持续训练模型，使得损失函数最小。</li></ol><h4 id="第一步，为回归问题选择合适的模型"><a href="#第一步，为回归问题选择合适的模型" class="headerlink" title="第一步，为回归问题选择合适的模型"></a>第一步，为回归问题选择合适的模型</h4><p>这里我们使用线性模型试试水。所谓<strong>线性模型</strong> (Linear Model) 是把输入数据的各种特征，通过线性组合的方式进行预测。</p><script type="math/tex; mode=display">y=b+\Sigma\omega_{i}x_{i}</script><p>其中y是预测值，x是特征本身，w是特征对应的参数。我们的学习目标就是找到正确的w，令预测值y尽可能靠谱。</p><h4 id="第二步，为线性模型选择合适的评估函数"><a href="#第二步，为线性模型选择合适的评估函数" class="headerlink" title="第二步，为线性模型选择合适的评估函数"></a>第二步，为线性模型选择合适的评估函数</h4><p>知错能改，善莫大焉。每当模型预测得到一个值，为了让模型认识到自己的预测值y与真实值$\hat{y}$的差距，我们不妨直接取二者之差，作为计算差距的<strong>损失函数</strong> (Loss Function)。</p><script type="math/tex; mode=display">L(f)=\sum (y-\hat{y})</script><p>这样肯定是有问题的，假设我们有两组数据，一组超过真实值0.5，另一组低于预测值0.5，它们与真实值的差值分别是0.5与-0.5。结果经过我们的计算，损失函数居然为0，也就是没有损失？</p><p>为了避免上述情况的出现，我们选择平方损失函数作为衡量差距的手段：</p><script type="math/tex; mode=display">L(f)=\sum (y-\hat{y})^2</script><p>其实也有其他的损失函数定义方法可以规避第一个损失函数的问题，比如使用绝对值。但这里我们就钦定平方损失函数了，它有很多好处，但是我们先按下不表。</p><h4 id="第三步，进行训练，并利用损失函数指导训练过程"><a href="#第三步，进行训练，并利用损失函数指导训练过程" class="headerlink" title="第三步，进行训练，并利用损失函数指导训练过程"></a>第三步，进行训练，并利用损失函数指导训练过程</h4><p>到目前为止，我们有很多带标签的数据 $(x,\hat{y})$，有线性模型，有损失函数。那我该怎么得到训练好的模型呢？</p><p><strong>最好的模型</strong>到底是什么？对于线性模型来说，其实求解最优的$w$和$b$，从而可以让我们的模型无论输入什么$x$，都能准确得到与真实值相差无几的$y$。</p><p>让我们忘掉w和b的具体含义、晦涩不清的L函数，专心解决这个问题：如何优化$w$和$b$，以便L达到最小？</p><script type="math/tex; mode=display">w^*=arg\min_{w}L(w) \\b^*=arg\min_{b}L(b)</script><p>一种解决问题的方法使用微积分来嗯算，通过计算导数去寻找L的极值点。运气好的话，L的变量不多，该方法看似可行；运气不好的话，我们面对的问题过于复杂、参数过多，问题就不好解决了。</p><p>还有一种笨办法是穷举所有可能的$w$，选择能使L最小的$w^*$即可。这种方法虽然可行，但是没有效率。</p><p>有一种通用的最优化方法，叫做<strong>梯度下降</strong> (Gradient Descent) 方法，专门用于解决这种凸优化问题。使用梯度下降算法的前提是优化目标是可微的。</p><h5 id="1-梯度下降算法简介"><a href="#1-梯度下降算法简介" class="headerlink" title="1. 梯度下降算法简介"></a>1. 梯度下降算法简介</h5><p>恰好，我们的损失函数L是可微的，也就是二阶可导的。如果我们当初选取损失函数时，使用绝对值作为损失函数，处理起来便没有这么便利了。</p><p>梯度下降的具体过程是这样的：</p><ol><li>选择一个初始$w_0$</li><li>计算该位置下w对L的微分，这个微分对应函数在此处下降最快的方向；</li><li>将$w_0$朝着这个方向移动一小步$ \eta$<script type="math/tex; mode=display">w\prime=w_0-\eta\frac{\mathrm{d}L}{\mathrm{d}w}</script></li><li>在新的位置开始新一轮迭代计算，直到$w$不再变化为止。</li></ol><p>不妨将梯度下降算法的求解过程想象为人下山的过程，人会先找到下山最快的方向，然后朝着那个方向走一步，直到抵达最低点。</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/模拟梯度下降.png" alt></p><p>在上图中，每一条线围成的圈就是等高线，代表损失函数的值，颜色约深的区域代表的损失函数越小；红色的箭头代表等高线的法线方向。</p><p>上述过程是求解L在变量w下的最优解的梯度下降过程，但是L还与偏移量b存在对应关系，所以实际上梯度下降在线性模型的具体公式如下：</p><script type="math/tex; mode=display">w_k\rightarrow w'_k=w_k-\eta\frac{\partial C}{\partial w_k} \\b_l\rightarrow b'_l=b_l-\eta\frac{\partial C}{\partial b_l}</script><p>其中，偏微分的具体公式如下：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/线性回归偏微分方程.png" alt></p><h5 id="2-梯度下降算法的局限性"><a href="#2-梯度下降算法的局限性" class="headerlink" title="2. 梯度下降算法的局限性"></a>2. 梯度下降算法的局限性</h5><p>首先，<strong>梯度下降算法容易陷入局部最优，找不到全局最优解</strong>。</p><p>如下图所示，当梯度下降算法优化到local minima 时，前面有座高山，它的梯度是正的，优化算法会强迫我们往回走。</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/梯度下降算法局部最优.png" alt></p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/梯度下降局部最优2.png" alt></p><p>这个问题在我们的线性回归模型里暂时遇不到，因为线性模型非常简单，是一个凸函数。但当我们试图利用梯度下降算法训练复杂模型，比如神经网络时，就很有可能遭遇该问题。</p><p>有很多方法能够解决梯度下降算法的全局最优解问题，包括调整每次行进的步长$\eta$，使其更有希望跨越“大山”等手段等等。</p><p>其次，梯度下降算法要求目标函数是可微的，这在某些情况下会<strong>产生相当大的计算代价</strong>。假设我们的问题有上百万维，计算二阶偏导数就需要上万亿（百万的平方）次！</p><p>再次，<strong>当训练数据相当多时，梯度下降算法会变得很慢</strong>。在实践中，为了计算梯度$\nabla L$，我们需要为了每个训练样本x单独计算梯度$\nabla L_x$，然后求平均值。这会花费很长时间。</p><h4 id="检验：如何检验模型是否训练成功？更加复杂的模型以及对应的风险。"><a href="#检验：如何检验模型是否训练成功？更加复杂的模型以及对应的风险。" class="headerlink" title="检验：如何检验模型是否训练成功？更加复杂的模型以及对应的风险。"></a>检验：如何检验模型是否训练成功？更加复杂的模型以及对应的风险。</h4><p>经过一轮又一轮的优化，我们终于求得了该问题下线性回归模型的最优参数$w$和$b$。接下来该如何检验模型在未知数据上的泛化能力呢？</p><p>我们可以在训练之前，把数据集分成两部分，一部分用作后续的训练，另一部分用作最后的验收。如果在这部分测试数据上的表现与训练过程中的表现一致，那我们就可以验收。</p><p>这种通过划分数据集为训练集和测试集的方法，与其说是一种技术细节，不如说是一种工程实践上的经验。通过训练集和测试集的性能比较，我们可以发现模型存在的潜在问题：</p><p><strong>1. 训练集和测试集上的表现都不太高；</strong></p><p>这种情况我们称之为欠拟合。线性模型由于过于简单，当面对一些较复杂的现实问题时，欠拟合便出现了。你会发现无论如何训练，无论投入多少数据都很难提升模型的性能了。我们需要更加复杂的模型，然后进行上面所说的机器学习三个步骤：选择新模型，选择合适的损失函数，选择优化方法进行训练。</p><p>对于一些现实问题，类似$y=wx+b$这种一次模型确实过于简单了。比如预测房价，可能与房屋面积有关，也可以与房屋面积的平方有关，甚至是三次方、四次方。</p><p>课堂上老师举了一个例子，用一次函数模型预测宝可梦数据集，效果如下：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦1.png" alt></p><p>模型在测试集上的误差平均值为 35.0</p><p>当我们使用更复杂的2次模型时，模型性能有明显好转：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦2.png" alt></p><p>模型的图像从一次函数变为二次函数了，更好拟合了训练集和测试集。测试集平均误差降至 18.4 了。那是不是模型越复杂，性能越好呢？</p><p><strong>2. 训练集上的表现良好，测试集上的表现很差</strong></p><p>当我们持续优化模型复杂度到3次方函数、4次方函数时，测试集平均误差开始停滞，4次方函数的平均误差不降反增：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦3.png" alt></p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦4.png" alt></p><p>训练集平均误差【15.4】【15.3】【14.9】【12.8】<br>测试集平均误差【18.4】【18.1】【28.8】【232.1】</p><p>这种情况称之为过拟合。所谓“过犹不及”，那我们应该怎样选择模型的复杂度，避免过拟合呢？我们可以将模型复杂度与测试集性能之间的关系绘制成图像，通过寻找图像的“拐点”来决定模型的复杂度。</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦性能变化.png" alt></p><h4 id="反思：改进模型性能的其他方法"><a href="#反思：改进模型性能的其他方法" class="headerlink" title="反思：改进模型性能的其他方法"></a>反思：改进模型性能的其他方法</h4><p>上面的宝可梦CP值预测问题，老师之后给出了更多的数据，绘制在坐标图上：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦5.png" alt></p><p>很显然，任何曲线的线性模型都没办法拟合这种数据点。这是哪里出了问题？答案是我们忽略了数据点的其他特征。当我们引入其他特征到模型后，线性模型的表达能力就增强了。下面就是引入了宝可梦种类的特征：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦6.png" alt></p><p>但是当我们继续引入一些无关特征，比如宝可梦的性别、年龄等，过拟合现象再次出现：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦7.png" alt></p><p>这是由于在训练过程中，某些特征的权重过大导致的。一种直观的解释是，模型训练过程也会偷懒！如果数据量不足，模型会自动选取某几个影响力大的特征，赋予较高的权值，然后忽略其他特征，那些被忽略的特征中可能包含更加有用的信息。</p><p>我们希望最终得到的模型不要出现太大的权重，为了防止某些特征的权值过大，限制权重的增长速度，可以使用正则化方法。</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/正则化1.png" alt></p><p>改造后的损失函数，末尾添加上了一个正则化项，这个正则化项的大小仅与模型的参数大小有关。参数越大，正则化项越大。有了正则化项，模型在优化过程就会更倾向于选择参数值更小的模型了。</p><p>在很多应用场景中，并不是 $w$ 越小模型越平滑越好，但是经验值告诉我们 $w$ 越小大部分情况下都是好的。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>机器学习的概念、分类和三个步骤：选择模型、选择损失函数、选择优化方法；</li><li>根据三个步骤，实践解决回归问题：使用线性模型，解决宝可梦CP值预测问题；</li></ol><p>这里的重点是梯度下降算法，后面还会继续学习该算法。</p><ol><li>通过分析例子的优化点，引出模型常用的优化方法以及风险：过拟合、欠拟合，以及他们的解决方案。</li></ol><p>过拟合问题的解决方法有：降低模型复杂度、增加训练样本、增加训练样本的特征、添加正则化项等。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：机器学习的概念和分类、利用线性模型解决回归问题以及梯度下降。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h3 id=&quot;一、机器学习介绍&quot;&gt;&lt;a href=&quot;#一、机器学习介绍&quot; class=&quot;headerlin</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="MOOC" scheme="https://superlova.github.io/tags/MOOC/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】机器学习编译——Note2</title>
    <link href="https://superlova.github.io/2022/09/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E2%80%94%E2%80%94Note2/"/>
    <id>https://superlova.github.io/2022/09/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E2%80%94%E2%80%94Note2/</id>
    <published>2022-09-13T08:13:40.000Z</published>
    <updated>2022-09-14T06:48:32.068Z</updated>
    
    <content type="html"><![CDATA[<p>陈天奇老师主讲的机器学习编译相关的课程，本节课讨论张量程序抽象以及相关实现。<br><!--more---></p><p>机器学习编译的过程可以看做是对张量函数的变换过程。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;陈天奇老师主讲的机器学习编译相关的课程，本节课讨论张量程序抽象以及相关实现。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;p&gt;机器学习编译的过程可以看做是对张量函数的变换过程。&lt;/p&gt;
</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="Compilation" scheme="https://superlova.github.io/tags/Compilation/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】职业素养</title>
    <link href="https://superlova.github.io/2022/08/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%81%8C%E4%B8%9A%E7%B4%A0%E5%85%BB/"/>
    <id>https://superlova.github.io/2022/08/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%81%8C%E4%B8%9A%E7%B4%A0%E5%85%BB/</id>
    <published>2022-08-01T03:26:14.000Z</published>
    <updated>2022-08-02T13:32:34.818Z</updated>
    
    <content type="html"><![CDATA[<p>有关新人培养手册的一些阅读体会和思考<br><!--more---></p><h2 id="执行任务并漂亮地完成"><a href="#执行任务并漂亮地完成" class="headerlink" title="执行任务并漂亮地完成"></a>执行任务并漂亮地完成</h2><p>分析问题、解决问题是工程师的日常工作，如何对待工作、如何执行工作，反映了一个工程师的基本素质。</p><p>不同级别的工程师适合处理不同难度的任务。根据任务难度，可以分类为：</p><ol><li>任务已经拆解完毕，我只负责某个简单子任务的执行；</li><li>任务已经拆解完毕，我负责各个步骤的执行，或者其中较困难的部分；</li><li>复杂的事情需要自己做拆解然后执行。</li></ol><p>一般我们把简单的执行任务交付给 T5 左右级别工程师去执行，把复杂的任务交给 T7 左右的工程师执行。而 T8 及以上的工程师，我们希望能独立完成任务拆解并执行。T10 以上的工程师，我们希望能够承担一个技术方向上的难题。</p><p>无论任务难度如何，工程师还是要根据结果的完成度来评价。根据结果分类：</p><ul><li>最低评价：不能按时完成任务；</li><li>较低评价：按时完成任务，但是质量较差；</li><li>合格评价：按时完成任务，质量合格。</li></ul><h2 id="独立思考"><a href="#独立思考" class="headerlink" title="独立思考"></a>独立思考</h2><p>什么叫做独立思考？工作是由思考和执行两部分组成的，任务的分析和拆解过程、出现问题后的总结和复盘、对现有技术方案的调研，都算是独立思考。</p><p>比如说，当我负责的模块出现 Bug，或者我的团队负责的模块报警时，进行分析问题、总结和复盘的过程，算是独立思考；<br>比如说，项目的技术方案不够完美或存在缺陷，我对现有技术方案进行调研，算是独立思考；<br>比如说，我对问题进行分析和总结，经常反思是否已经无可挑剔、不会被人挑毛病，算是独立思考。</p><p>独立思考代表着三个工程师宝贵的能力：责任心、事业心和硬实力。一个高绩效的工程师，往往有以下特征：</p><ol><li>工作态度上，对业务负责，从来不会消极怠工，负责的模块出现事故很少；</li><li>做事方法上，规范、及时沟通，事情交给他很放心，不会把自己当成工具人；</li><li>做事效率上，想得清楚做得快，紧急问题得交给他，做事不会反反复复；</li><li>技术实力上，此方向的任何问题都可以咨询他。</li></ol><h2 id="协作沟通"><a href="#协作沟通" class="headerlink" title="协作沟通"></a>协作沟通</h2><p>以上几点是工程师个人的能力评价，但是在公司中免不了要与其他工程师打交道，也就是沟通。</p><p>协作沟通时有一些常见的问题，新手工程师可能不太了解，需要琢磨一下；在此我把一些定律列举出来，方便直接照做：</p><ol><li>收到消息必回复，塑造收到消息及时回复的形象；不仅如此，在自己负责通知时，也要确认相关责任人收到消息。</li><li>沟通时先想好诉求，问对了问题是成功的一半；要有礼貌，先带好称呼，再组织语言，逻辑通顺；要确认下解决问题的时间节点，并且将讨论的共识发到群里进行同步；</li><li>要及时跟进问题，就比如说当你找对方要时间节点时，可能会遇到：“还不知道，需要评估”、“事情太多，不好说”、等等类似的无法给出时间点的情况，这个时候就需要及时跟进。及时跟进的做法不是每过一段时间就问一次，那样很招人烦；正确做法是一起拆分问题，对方一旦完成子任务，就通知自己。</li><li>即便你负责的是一个项目里的一小部分，你也需要知道整个项目的计划节奏。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;有关新人培养手册的一些阅读体会和思考&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h2 id=&quot;执行任务并漂亮地完成&quot;&gt;&lt;a href=&quot;#执行任务并漂亮地完成&quot; class=&quot;headerlink&quot; title=&quot;执行任务并漂亮地完成&quot;&gt;&lt;/a&gt;执行任务并漂亮地完成&lt;/h2</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Career" scheme="https://superlova.github.io/tags/Career/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】如何实现模型的热更新？</title>
    <link href="https://superlova.github.io/2022/07/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%83%AD%E6%9B%B4%E6%96%B0%EF%BC%9F/"/>
    <id>https://superlova.github.io/2022/07/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%83%AD%E6%9B%B4%E6%96%B0%EF%BC%9F/</id>
    <published>2022-07-22T03:11:37.000Z</published>
    <updated>2022-08-02T13:32:34.801Z</updated>
    
    <content type="html"><![CDATA[<p>陈天奇老师主讲的机器学习编译相关的课程，回答了机器学习模型是如何从开发状态到部署状态的。整个过程类似源代码到可执行程序的编译过程，因此得名机器学习编译。据他本人所述，这门课是目前全世界第一门主讲机器学习编译的课程。<br><!--more---></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;陈天奇老师主讲的机器学习编译相关的课程，回答了机器学习模型是如何从开发状态到部署状态的。整个过程类似源代码到可执行程序的编译过程，因此得名机器学习编译。据他本人所述，这门课是目前全世界第一门主讲机器学习编译的课程。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="Compilation" scheme="https://superlova.github.io/tags/Compilation/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】机器学习编译——Note1</title>
    <link href="https://superlova.github.io/2022/07/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E2%80%94%E2%80%94Note1/"/>
    <id>https://superlova.github.io/2022/07/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E2%80%94%E2%80%94Note1/</id>
    <published>2022-07-04T02:36:43.000Z</published>
    <updated>2022-08-02T13:32:34.809Z</updated>
    
    <content type="html"><![CDATA[<p>陈天奇老师主讲的机器学习编译相关的课程，回答了机器学习模型是如何从开发状态到部署状态的。整个过程类似源代码到可执行程序的编译过程，因此得名机器学习编译。据他本人所述，这门课是目前全世界第一门主讲机器学习编译的课程。<br><!--more---></p><p>《机器学习编译》是陈天奇老师的公开课。目前在Bilibili上更新视频。<a href="https://space.bilibili.com/1663273796">这里</a>是陈天奇老师在Bilibili的个人主页，如果对课程感兴趣的小伙伴可以免费观摩。</p><p>另外课件地址在这里：<a href="https://mlc.ai/zh/chapter_introduction/">Machine Learning Compilation</a></p><h1 id="一、什么是机器学习编译？学习这门课的意义是什么？"><a href="#一、什么是机器学习编译？学习这门课的意义是什么？" class="headerlink" title="一、什么是机器学习编译？学习这门课的意义是什么？"></a>一、什么是机器学习编译？学习这门课的意义是什么？</h1><p><strong>机器学习编译</strong> (machine learning compilation, MLC) 是指，将机器学习算法从开发阶段，通过变换和优化算法，使其变成部署状态。</p><p>开发阶段：用 PyTorch、TensorFlow 或 JAX 等通用框架编写的模型描述 + 对应的权重文件。</p><p>部署状态：在考虑支撑代码、内存管理、不同语言的开发接口等因素后，成功执行在设备上的状态。</p><p>该过程类似于把源代码转换为可执行文件的程序编译过程，但是二者有比较大的不同。</p><p>首先是目标不同，机器学习编译的目标有：</p><ul><li>集成打包 + 最小化依赖，即将必要的元素组合在一起以用于部署应用程序；</li><li>利用硬件加速，利用硬件本身的特性进行加速；</li><li>通用优化，以最小化内存使用或提高执行效率为目标转换模型执行方式。</li></ul><p>这些目标没有严格的界限。</p><p>其次，这个过程不一定涉及代码生成；例如将开发状态转化为部署形式，可能只是将抽象的模型定义，转化为对某几个预定义的库函数的调用。</p><p>最后，遇到的挑战和解决方案也大不相同。随着硬件和模型种类的增长，机器学习编译难以表示单一稳定的解决方案。</p><p>那么，我们能够从这门课学习到什么？</p><p>对于在从事机器学习工作工程师，机器学习编译提供了以基础的解决问题的方法和工具。它有助于回答我们可以采用什么方法来特定模型的部署和内存效率，如何将优化模型的单个部分的经验推广到更端到端解决方案等一系列问题。</p><p>对于机器学习科学家，学习机器学习编译可以更深入地了解将模型投入生产所需的步骤。机器学习框架本身隐藏了一些技术复杂性，但是当我们尝试开始部署新模型或将模型部署到框架支持不完善的平台时，仍然会面临巨大的挑战。机器学习编译使机器学习算法科学家有机会了解背后的基本原理，并且知晓为什么我的模型的运行速度不及预期，以及如何来使部署更有效。</p><p>最后，学习 MLC 本身很有趣。借助这套现代机器学习编译工具，我们可以进入机器学习模型从高级、代码优化到裸机的各个阶段。端到端 (end to end) 地了解这里发生的事情并使用它们来解决我们的问题。</p><h1 id="二、机器学习编译的关键要素"><a href="#二、机器学习编译的关键要素" class="headerlink" title="二、机器学习编译的关键要素"></a>二、机器学习编译的关键要素</h1><h2 id="1-张量和张量函数"><a href="#1-张量和张量函数" class="headerlink" title="1. 张量和张量函数"></a>1. 张量和张量函数</h2><p>张量 (Tensor) 是执行中最重要的元素。张量是表示神经网络模型执行的输入、输出和中间结果的多维数组。</p><p>张量函数 (Tensor functions) 指接受张量和输出张量的计算序列。</p><p>下面这张图展示了机器学习编译过程中，两种不同形式的张量函数的变换过程。从左边比较抽象的表示形式，转换为右侧较为具体的表示形式。</p><p><img src="https://mlc.ai/zh/_images/mlc-elem-transform.png" alt="机器学习编译过程中的张量函数变换"></p><p>机器学习编译的过程就是是将上图左侧的内容转换为右侧的过程。在不同的场景中，这个过程可以是手动完成的，也可以使用一些自动转换工具，或两者兼而有之。</p><h2 id="2-抽象和实现"><a href="#2-抽象和实现" class="headerlink" title="2. 抽象和实现"></a>2. 抽象和实现</h2><p>上一部分提到了抽象和实现。对于同样的目标，我们会有不同的 抽象表现，但是不同抽象表示有些细节不同。我们会把更细化的抽象表示称为原有抽象表示的一个具体实现。</p><p>抽象和实现可能是所有计算机系统中最重要的关键字。抽象指定“做什么”，实现提供“如何”做。没有具体的界限。根据我们的看法，for 循环本身可以被视为一种抽象，因为它可以使用 python 解释器实现或编译为本地汇编代码。</p><p>MLC 实际上是在相同或不同抽象下转换和组装张量函数的过程。</p><p>本课程会介绍四种不同形式的抽象表示</p><ul><li>计算图的抽象</li><li>张量程序的抽象</li><li>算子库和运行时的抽象</li><li>硬件层面的抽象</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;陈天奇老师主讲的机器学习编译相关的课程，回答了机器学习模型是如何从开发状态到部署状态的。整个过程类似源代码到可执行程序的编译过程，因此得名机器学习编译。据他本人所述，这门课是目前全世界第一门主讲机器学习编译的课程。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;p&gt;《机器学习编</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Machine Learning" scheme="https://superlova.github.io/tags/Machine-Learning/"/>
    
    <category term="Compilation" scheme="https://superlova.github.io/tags/Compilation/"/>
    
  </entry>
  
  <entry>
    <title>【竞赛打卡】科大讯飞-非标准化疾病诉求的简单分诊挑战赛</title>
    <link href="https://superlova.github.io/2022/06/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E-%E9%9D%9E%E6%A0%87%E5%87%86%E5%8C%96%E7%96%BE%E7%97%85%E8%AF%89%E6%B1%82%E7%9A%84%E7%AE%80%E5%8D%95%E5%88%86%E8%AF%8A%E6%8C%91%E6%88%98%E8%B5%9B/"/>
    <id>https://superlova.github.io/2022/06/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E-%E9%9D%9E%E6%A0%87%E5%87%86%E5%8C%96%E7%96%BE%E7%97%85%E8%AF%89%E6%B1%82%E7%9A%84%E7%AE%80%E5%8D%95%E5%88%86%E8%AF%8A%E6%8C%91%E6%88%98%E8%B5%9B/</id>
    <published>2022-06-20T11:55:48.000Z</published>
    <updated>2022-08-02T13:32:34.826Z</updated>
    
    <content type="html"><![CDATA[<p>文本分类问题，收集用户问诊信息，需要分别将一段提问就问诊类型（20种）和疾病类型（60种）进行分类。类别包含较多缺失值，trick较多。笔者用到了Tfidf+Logistic回归作为Baseline，然后使用Pytorch训练RoBERTa用作第一版提交模型，最终根据数据的特征，取得了XX（排名）。<br><!--more---></p><p>比赛地址：<br><a href="https://challenge.xfyun.cn/topic/info?type=disease-claims-2022&amp;option=ssgy">https://challenge.xfyun.cn/topic/info?type=disease-claims-2022&amp;option=ssgy</a></p><p>baseline地址：<br><a href="https://mp.weixin.qq.com/s/KiozLF7FaJ_CVx74J3KNWA">https://mp.weixin.qq.com/s/KiozLF7FaJ_CVx74J3KNWA</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;文本分类问题，收集用户问诊信息，需要分别将一段提问就问诊类型（20种）和疾病类型（60种）进行分类。类别包含较多缺失值，trick较多。笔者用到了Tfidf+Logistic回归作为Baseline，然后使用Pytorch训练RoBERTa用作第一版提交模型，最终根据数据的</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Datawhale" scheme="https://superlova.github.io/tags/Datawhale/"/>
    
    <category term="Data Mining" scheme="https://superlova.github.io/tags/Data-Mining/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】MacBook Pro M1Pro Java环境部署和开发准备</title>
    <link href="https://superlova.github.io/2022/05/25/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Java%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E5%92%8C%E5%BC%80%E5%8F%91%E5%87%86%E5%A4%87/"/>
    <id>https://superlova.github.io/2022/05/25/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Java%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E5%92%8C%E5%BC%80%E5%8F%91%E5%87%86%E5%A4%87/</id>
    <published>2022-05-25T07:40:22.000Z</published>
    <updated>2022-08-02T13:32:34.789Z</updated>
    
    <content type="html"><![CDATA[<p>本文记录了在MacBook Pro M1Pro下部署Java开发环境遇到的问题和解决方法<br><!--more---></p><h2 id="下载并安装JDK"><a href="#下载并安装JDK" class="headerlink" title="下载并安装JDK"></a>下载并安装JDK</h2><p>M1芯片带来的坏处是许多jdk版本不支持arm架构。这里可以选择<code>Zulu JDK</code>，可以进入<a href="https://www.azul.com/downloads/?version=java-8-lts&amp;os=macos&amp;architecture=arm-64-bit&amp;package=jdk">此链接下载</a>并安装指定版本的JDK。</p><p><img src="/2022/05/25/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Java%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E5%92%8C%E5%BC%80%E5%8F%91%E5%87%86%E5%A4%87/2022-05-25-15-46-32.png" alt="安装页面"></p><p>安装完毕后，不必设置环境变量，直接在命令行运行<code>java -version</code>检查是否运行成功：</p><pre><code>% java -versionopenjdk version &quot;1.8.0_332&quot;OpenJDK Runtime Environment (Zulu 8.62.0.19-CA-macos-aarch64) (build 1.8.0_332-b09)OpenJDK 64-Bit Server VM (Zulu 8.62.0.19-CA-macos-aarch64) (build 25.332-b09, mixed mode)</code></pre><h2 id="下载并安装Maven"><a href="#下载并安装Maven" class="headerlink" title="下载并安装Maven"></a>下载并安装Maven</h2><p>可在<a href="https://maven.apache.org/download.cgi">此链接中</a>选择如图所示的链接进行下载压缩包：</p><p><img src="/2022/05/25/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Java%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E5%92%8C%E5%BC%80%E5%8F%91%E5%87%86%E5%A4%87/下载Maven.png" alt></p><p>这是个压缩文件，或者说“绿色版”。然后将其解压到你想要放入的文件夹（我一般移动到<code>/opt</code>目录下）。</p><p>接下来需要我们自己配置环境变量。首先需要确认你的BASH类别。</p><p>查看当前使用的SHELL：</p><pre><code>% echo $SHELL/bin/zsh</code></pre><p>查看本机可用的所有SHELL</p><pre><code>% cat /etc/shells# List of acceptable shells for chpass(1).# Ftpd will not allow users to connect who are not using# one of these shells./bin/bash/bin/csh/bin/dash/bin/ksh/bin/sh/bin/tcsh/bin/zsh</code></pre><p>我这里显示是zsh，则我需要修改<code>~/.zshrc</code><br>如果你的shell是<code>/bin/bash</code>，则需要修改<code>~/.bash_profile</code></p><p>将以下文本加入配置文件末尾：</p><pre><code class="lang-sh">export MAVEN_HOME=/opt/apache-maven-3.8.5export PATH=$PATH:$MAVEN_HOME/bin</code></pre><p>然后立即载入环境变量：</p><pre><code>% source ~/.zshrc</code></pre><p>最后测试是否应用修改：</p><pre><code>% mvn -vApache Maven 3.8.5 (3599d3414f046de2324203b78ddcf9b5e4388aa0)Maven home: /opt/apache-maven-3.8.5Java version: 1.8.0_332, vendor: Azul Systems, Inc., runtime: /Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home/jreDefault locale: zh_CN, platform encoding: UTF-8OS name: &quot;mac os x&quot;, version: &quot;12.3.1&quot;, arch: &quot;aarch64&quot;, family: &quot;mac&quot;</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文记录了在MacBook Pro M1Pro下部署Java开发环境遇到的问题和解决方法&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h2 id=&quot;下载并安装JDK&quot;&gt;&lt;a href=&quot;#下载并安装JDK&quot; class=&quot;headerlink&quot; title=&quot;下载并安装JDK</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Java" scheme="https://superlova.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】如何使用树模型预测结构化数据</title>
    <link href="https://superlova.github.io/2022/03/06/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E6%A0%91%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE/"/>
    <id>https://superlova.github.io/2022/03/06/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E6%A0%91%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE/</id>
    <published>2022-03-06T13:16:44.000Z</published>
    <updated>2022-08-02T13:32:34.801Z</updated>
    
    <content type="html"><![CDATA[<p>TODO</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;TODO&lt;/p&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>【学习笔记】Attention和Transformer</title>
    <link href="https://superlova.github.io/2021/08/18/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Attention%E5%92%8CTransformer/"/>
    <id>https://superlova.github.io/2021/08/18/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Attention%E5%92%8CTransformer/</id>
    <published>2021-08-18T01:24:20.000Z</published>
    <updated>2022-08-02T13:32:34.785Z</updated>
    
    <content type="html"><![CDATA[<p>分享下知识，介绍下我对Attention的理解，然后利用Tensorflow实现一个完整的Transformer模型。</p><!--more---><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="常见的NLP任务"><a href="#常见的NLP任务" class="headerlink" title="常见的NLP任务"></a>常见的NLP任务</h2><p>文本分类，实体抽取等</p><h1 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h1><p>Attention是一种编码机制，最初是用于解决机器翻译（seq2seq任务）的长程遗忘问题。因为基于RNN模型不能很好地应对长文本的翻译，即便是LSTM也受限于序列长度等因素的影响。而Attention可以通过训练一个动态的参数矩阵，决定关注长文本的哪些部分。Attention的一个优点是，相较于LSTM的基于时间的反响传播算法，Attention的计算可以并行化，这为其堆叠比较庞大的模型打下了基础。</p><h1 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h1><p>Attention机制其实用在RNN的机器翻译模型上就已经可以提升机器翻译模型的效果了，于是有人想，不如我们直接把RNN从模型中摘掉，不管encoder和decoder都使用Attention计算单元，性能不就更好了？这也是Attention is all you need 这篇论文的由来。</p><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>Transformer是由Attention模块组成的计算单元。</p><p>在encoder阶段，由6个self-attention模块组成的多层编码器负责编码运算，最后使用一层前馈神经网络汇总（其实就是加权汇总）；在decoder方面，输入不但有编码器传进来的一部分信息，而且还有句子本身。</p><p>Transformer里面使用多种Attention，关注序列的不同层次的信息，这种叫做多头注意力机制。在RNN横行天下的时代，这种堆叠模式带来的庞大训练和推理负担使得难以推动此类模型上线。</p><pre><code class="lang-py">class MultiheadAttention(nn.Module):    # n_heads：多头注意力的数量    # hid_dim：每个词输出的向量维度    def __init__(self, hid_dim, n_heads, dropout):        super(MultiheadAttention, self).__init__()        self.hid_dim = hid_dim        self.n_heads = n_heads        # 强制 hid_dim 必须整除 h        assert hid_dim % n_heads == 0        # 定义 W_q 矩阵        self.w_q = nn.Linear(hid_dim, hid_dim)        # 定义 W_k 矩阵        self.w_k = nn.Linear(hid_dim, hid_dim)        # 定义 W_v 矩阵        self.w_v = nn.Linear(hid_dim, hid_dim)        self.fc = nn.Linear(hid_dim, hid_dim)        self.do = nn.Dropout(dropout)        # 缩放        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads]))    def forward(self, query, key, value, mask=None):        # K: [64,10,300], batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维        # V: [64,10,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维        # Q: [64,12,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维        bsz = query.shape[0]        Q = self.w_q(query)        K = self.w_k(key)        V = self.w_v(value)        # 这里把 K Q V 矩阵拆分为多组注意力，变成了一个 4 维的矩阵        # 最后一维就是是用 self.hid_dim // self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300/6=50        # 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 词，50 表示每组注意力的词的向量长度        # K: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]        # V: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]        # Q: [64,12,300] 拆分多组注意力 -&gt; [64,12,6,50] 转置得到 -&gt; [64,6,12,50]        # 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim //                   self.n_heads).permute(0, 2, 1, 3)        K = K.view(bsz, -1, self.n_heads, self.hid_dim //                   self.n_heads).permute(0, 2, 1, 3)        V = V.view(bsz, -1, self.n_heads, self.hid_dim //                   self.n_heads).permute(0, 2, 1, 3)        # 第 1 步：Q 乘以 K的转置，除以scale        # [64,6,12,50] * [64,6,50,10] = [64,6,12,10]        # attention：[64,6,12,10]        attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale        # 把 mask 不为空，那么就把 mask 为 0 的位置的 attention 分数设置为 -1e10        if mask isnotNone:            attention = attention.masked_fill(mask == 0, -1e10)        # 第 2 步：计算上一步结果的 softmax，再经过 dropout，得到 attention。        # 注意，这里是对最后一维做 softmax，也就是在输入序列的维度做 softmax        # attention: [64,6,12,10]        attention = self.do(torch.softmax(attention, dim=-1))        # 第三步，attention结果与V相乘，得到多头注意力的结果        # [64,6,12,10] * [64,6,10,50] = [64,6,12,50]        # x: [64,6,12,50]        x = torch.matmul(attention, V)        # 因为 query 有 12 个词，所以把 12 放到前面，把 5 和 60 放到后面，方便下面拼接多组的结果        # x: [64,6,12,50] 转置-&gt; [64,12,6,50]        x = x.permute(0, 2, 1, 3).contiguous()        # 这里的矩阵转换就是：把多组注意力的结果拼接起来        # 最终结果就是 [64,12,300]        # x: [64,12,6,50] -&gt; [64,12,300]        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))        x = self.fc(x)        return x# batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维query = torch.rand(64, 12, 300)# batch_size 为 64，有 12 个词，每个词的 Key 向量是 300 维key = torch.rand(64, 10, 300)# batch_size 为 64，有 10 个词，每个词的 Value 向量是 300 维value = torch.rand(64, 10, 300)attention = MultiheadAttention(hid_dim=300, n_heads=6, dropout=0.1)output = attention(query, key, value)## output: torch.Size([64, 12, 300])print(output.shape)</code></pre><h1 id="表示序列中单词顺序的方法"><a href="#表示序列中单词顺序的方法" class="headerlink" title="表示序列中单词顺序的方法"></a>表示序列中单词顺序的方法</h1><p>为了解决这个问题，Transformer 模型对每个输入的向量都添加了一个向量。这些向量遵循模型学习到的特定模式，有助于确定每个单词的位置，或者句子中不同单词之间的距离。这种做法背后的直觉是：将这些表示位置的向量添加到词向量中，得到了新的向量，这些新向量映射到 Q/K/V，然后计算点积得到 attention 时，可以提供有意义的信息。</p><h1 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h1><p>编码器结构中有一个需要注意的细节是：编码器的每个子层（Self Attention 层和 FFNN）都有一个残差连接和层标准化（layer-normalization）。在解码器的子层里面也有层标准化（layer-normalization）。</p><p><img src="https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/0-1-transformer-arc.png" alt="11"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;分享下知识，介绍下我对Attention的理解，然后利用Tensorflow实现一个完整的Transformer模型。&lt;/p&gt;
&lt;!--more---&gt;
&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="NLP" scheme="https://superlova.github.io/tags/NLP/"/>
    
    <category term="Attention" scheme="https://superlova.github.io/tags/Attention/"/>
    
    <category term="Transformer&#39;" scheme="https://superlova.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读笔记】实现 MapReduce 论文附录 A 的 Word Count 程序</title>
    <link href="https://superlova.github.io/2021/05/07/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91%E5%AE%9E%E7%8E%B0-MapReduce-%E8%AE%BA%E6%96%87%E9%99%84%E5%BD%95-A-%E7%9A%84-Word-Count-%E7%A8%8B%E5%BA%8F/"/>
    <id>https://superlova.github.io/2021/05/07/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91%E5%AE%9E%E7%8E%B0-MapReduce-%E8%AE%BA%E6%96%87%E9%99%84%E5%BD%95-A-%E7%9A%84-Word-Count-%E7%A8%8B%E5%BA%8F/</id>
    <published>2021-05-07T12:26:13.000Z</published>
    <updated>2022-08-02T13:32:34.880Z</updated>
    
    <content type="html"><![CDATA[<p>给出一个Map和Reduce的具体实现，去除了所有分布式的特性【可能今后会添加】</p><!--more---><p>MapReduce文章最后的附录A有一段C++代码，描述了Map和Reduce函数的编写和使用方法。但是仅靠看代码总是不能深入理解MapReduce的实现细节，阅读其他人的MapReduce学习曲线又太过陡峭。因此我决定自己实现Map函数和Reduce函数，并尽可能使用替代方法将那些没有给出的api实现。</p><p>促使我写这篇文章的另一个原因是，网络上的大部分实现Word Count的文章都是依赖于某某框架的，比如依赖Hadoop。但我觉得过早依赖于某个平台不利于深入理解背后的原理，因此我决定自己实现各种api，体会程序设计者可能遇到的问题。今后在学习分布式系统、分布式框架时，便能够对症下药、有的放矢。</p><p>不过还请各位原谅，这里的Map和Reduce只能运行在单机环境啦。</p><p>下面给出原文中的C++代码：</p><pre><code class="lang-cpp">#include &quot;mapreduce/mapreduce.h&quot;//用户map函数class WordCounter : public Mapper &#123;public:    virtual void Map(const MapInput&amp; input) &#123;        const string&amp; text = input.value();        const int n = text.size();        for (int i = 0; i &lt; n; ) &#123;            //跳过前导空格            while ((i &lt; n) &amp;&amp; isspace(text[i]))                i++;            // 查找单词的结束位置            int start = i;            while ((i &lt; n) &amp;&amp; !isspace(text[i]))                i++;            if (start &lt; i)                Emit(text.substr(start,i-start),&quot;1&quot;);        &#125;    &#125;&#125;;REGISTER_MAPPER(WordCounter);//用户的reduce函数class Adder : public Reducer &#123;    virtual void Reduce(ReduceInput* input) &#123;        //迭代具有相同key的所有条目,并且累加它们的value        int64 value = 0;        while (!input-&gt;done()) &#123;            value += StringToInt(input-&gt;value());            input-&gt;NextValue();        &#125;        //提交这个输入key的综合        Emit(IntToString(value));    &#125;&#125;;REGISTER_REDUCER(Adder);int main(int argc, char** argv) &#123;    ParseCommandLineFlags(argc, argv);    MapReduceSpecification spec;    // 把输入文件列表存入&quot;spec&quot;    for (int i = 1; i &lt; argc; i++) &#123;        MapReduceInput* input = spec.add_input();        input-&gt;set_format(&quot;text&quot;);        input-&gt;set_filepattern(argv[i]);        input-&gt;set_mapper_class(&quot;WordCounter&quot;);    &#125;    //指定输出文件:    // /gfs/test/freq-00000-of-00100    // /gfs/test/freq-00001-of-00100    // ...    MapReduceOutput* out = spec.output();    out-&gt;set_filebase(&quot;/gfs/test/freq&quot;);    out-&gt;set_num_tasks(100);    out-&gt;set_format(&quot;text&quot;);    out-&gt;set_reducer_class(&quot;Adder&quot;);    // 可选操作:在map任务中做部分累加工作,以便节省带宽    out-&gt;set_combiner_class(&quot;Adder&quot;);    // 调整参数: 使用2000台机器,每个任务100MB内存    spec.set_machines(2000);    spec.set_map_megabytes(100);    spec.set_reduce_megabytes(100);    // 运行    MapReduceResult result;    if (!MapReduce(spec, &amp;result)) abort();    // 完成: &#39;result&#39;结构包含计数,花费时间,和使用机器的信息    return 0;&#125;</code></pre><h2 id="抽象基类-Mapper、Reducer"><a href="#抽象基类-Mapper、Reducer" class="headerlink" title="抽象基类 Mapper、Reducer"></a>抽象基类 Mapper、Reducer</h2><p>首先我打算实现<code>WordCounter</code>的父类<code>Mapper</code>和<code>Adder</code>的父类<code>Reducer</code>。</p><pre><code class="lang-cpp">class Mapper &#123;public:    virtual void Map(const MapInput&amp; input) = 0;&#125;;class Reducer &#123;public:    virtual void Reduce(ReduceInput* input) = 0;&#125;;</code></pre><p>简单实现Map和Reduce接口，并把它们设置成纯虚方法。</p><h2 id="WordCounter类"><a href="#WordCounter类" class="headerlink" title="WordCounter类"></a>WordCounter类</h2><pre><code class="lang-cpp">class WordCounter : public Mapper &#123;public:    void Map(const MapInput&amp; input) override &#123;        const string&amp; text = input.value(); // 读取一行文本        const int n = text.size();        for (int i = 0; i &lt; n; ) &#123;            // 跳过行首空白            while ((i &lt; n) &amp;&amp; isspace(text[i])) i++;            // 确定单词的开头和结尾            int start = i;            while ((i &lt; n) &amp;&amp; !isspace(text[i])) i++;            if (start &lt; i)                Emit(text.substr(start, i-start), &quot;1&quot;);        &#125;    &#125;&#125;;</code></pre><p>这一部分相较于原文没什么变化。其主要作用在于分词，然后每个单词组建成一个键值对，以(word, 1)的结构发射出去。发射到哪里呢？我就偷懒直接持久化到本地的消息存储装置了。</p><pre><code class="lang-cpp">static void Emit(const string&amp; key, const string&amp; value) &#123;    mw.put(key, value);&#125;</code></pre><p>mw 是 MiddleWare 的实例，是一个用于保存Emit输出的键值对的全局变量。后续会继续讲解。</p><h2 id="用于保存-Mapper-得到的键值对的-MiddleWare-类"><a href="#用于保存-Mapper-得到的键值对的-MiddleWare-类" class="headerlink" title="用于保存 Mapper 得到的键值对的 MiddleWare 类"></a>用于保存 Mapper 得到的键值对的 MiddleWare 类</h2><pre><code class="lang-cpp">class MiddleWare &#123;private:    vector&lt;pair&lt;string, string&gt;&gt; kv_pairs;    static bool compare_pair(const pair&lt;string, string&gt;&amp; lhs, const pair&lt;string, string&gt;&amp; rhs) &#123;        return lhs.first &lt; rhs.first;    &#125;public:    MiddleWare() = default;    void put(const string&amp; key, const string&amp; value) &#123;        kv_pairs.emplace_back(key, value);    &#125;    vector&lt;pair&lt;string, string&gt;&gt; get() &#123;        std::sort(kv_pairs.begin(), kv_pairs.end(), compare_pair); // 将其按照key相同的一组来排序        return kv_pairs;    &#125;&#125;;MiddleWare mw; // 全局变量：消息队列</code></pre><p>这是我为本地运行顺利而凭空构建出来的类，作用是储存(key, value)对。为了方便使用，内部有get和put方法。其中如果Reducer需要get数据了，那么首先会按照key对这些pair进行排序。这也是与MapReduce的流程相吻合的。</p><p>既然是排序，那么就要定义比较器 compare_pair。我的比较器直接使用string的比较，确保相同key值的pair在相邻位置。</p><h2 id="Adder类"><a href="#Adder类" class="headerlink" title="Adder类"></a>Adder类</h2><pre><code class="lang-cpp">// 用户自定义 Reduce 函数class Adder : public Reducer &#123;public:    void Reduce(ReduceInput* input) override &#123;        // 迭代所有拥有相同key的键值对，把它们的values加起来        int64_t value = 0;        string currentKey = input-&gt;key();        while (!input-&gt;end() &amp;&amp; currentKey == input-&gt;key()) &#123; // 直到下一个键值对的key与当前键值对的key不同为止            value += std::stoi(input-&gt;value());            input-&gt;NextValue(); // 找到下一个拥有相同key的键值对        &#125;        // Emit sum for input-&gt;key()        Emit(to_string(value));    &#125;&#125;;</code></pre><p>与论文中的Adder有逻辑出入，主要变化在把同样key分成不同组的逻辑上，我直接保存了当前组的key。原来论文里是没有这种操作的。</p><h2 id="Map-的输入-MapInput"><a href="#Map-的输入-MapInput" class="headerlink" title="Map 的输入 MapInput"></a>Map 的输入 MapInput</h2><p>观察一下Map的参数里有一个MapInput类型的对象，那么第二步就是新建一个MapInput类。要想跑通Map函数的代码，这个类必须实现value()方法。</p><p>猜测一下，MapInput是Map的输入，而MapReduce框架的输入输出都应该是键值对的形式。因此每个MapInput都应该包含一个key和一个value成员。</p><pre><code class="lang-cpp">class MapInput &#123;private:    string map_value;    string map_key;public:    explicit MapInput(string filename, string text) : map_key(std::move(filename)), map_value(std::move(text)) &#123; &#125;    [[nodiscard]] const string&amp; value() const &#123;        return map_value;    &#125;&#125;;</code></pre><p>MapInput的构造函数接收两个参数，第一个参数是文本文件名，第二个参数是文件的内容。其实第一个参数在我们的程序中没啥作用，但是为了格式的统一，就写上吧。</p><p>explicit修饰构造函数，代表该类的对象禁止发生隐式类型转换，要想转换必须以<strong>明确的(explicit)</strong>方式进行显式类型转换。</p><p>冒号后面的初始化列表中，使用了move特性，避免了函数传参导致的变量复制。</p><p>[[nodiscard]] 含义是该函数的返回值必须被使用，不能丢弃。C++ 17版本新增了几个中括号标识的提示符，当代码不符合要求的时候，编译器也会真的警告。相当于把以前的注释加强了。除[[nodiscard]]之外，还有表示switch语句中不必加break的[[fallthrough]]、变量定义之后没有使用也没关系的标识符[[maybe_unused]]。</p><h2 id="Reduce-的输入-ReduceInput"><a href="#Reduce-的输入-ReduceInput" class="headerlink" title="Reduce 的输入 ReduceInput"></a>Reduce 的输入 ReduceInput</h2><p>ReduceInput的设计就比较麻烦了。首先Reduce函数的输入是ReduceInput的指针，使用到的接口有done()/value()/NextValue()/key()，然后根据Reduce函数的使用方法，感觉ReduceInput像是一个迭代器。</p><pre><code class="lang-cpp">class ReduceInput &#123;private:    vector&lt;pair&lt;string, string&gt;&gt; data;    int currentKey = 0;public:    explicit ReduceInput(vector&lt;pair&lt;string, string&gt;&gt; _data) : data(std::move(_data)) &#123;  &#125;//    bool done() &#123;//        // 直到下一个键值对的key与当前键值对的key不同为止//        // 如果到了末尾，或者下一个key不一样，都是done//        if (currentKey == 0) return false;//        if (end() || data[currentKey].first != data[currentKey-1].first) return true;//        return false;//    &#125;    const string&amp; value() &#123;        return data[currentKey].second;    &#125;    const string&amp; key() &#123;        return data[currentKey].first;    &#125;    void NextValue() &#123;        currentKey++;    &#125;    bool end() &#123;        return currentKey &gt;= data.size();    &#125;&#125;;</code></pre><p>上面是我实现的ReduceInput，偷个懒把所有数据存放到ReduceInput中方便遍历，在真实场景的设计中不会像我这样的。</p><p>此外，Done函数的逻辑是有问题的。关键在于Reduce函数中的这句话：</p><pre><code class="lang-cpp">while (!input-&gt;done()) &#123;    value += StringToInt(input-&gt;value());    input-&gt;NextValue();&#125;</code></pre><p>表面上看起来是希望input作为一个迭代器，当它迭代到key与下一个key不同时，终止迭代（即done返回true表明迭代完成），然而下次迭代的开始还是从这个位置，其结果从程序逻辑上来讲，却又希望返回false。同一个位置，我们希望返回两个不同的值，这显然是说不通的。因此我在Reduce最终实现的主代码部分做了适当的逻辑修改。</p><h2 id="输出结果的单参数Emit"><a href="#输出结果的单参数Emit" class="headerlink" title="输出结果的单参数Emit"></a>输出结果的单参数Emit</h2><p>为了输出方便，最终我定义了单参数的重载Emit，不保存Reducer的计算结果，直接输出：</p><pre><code class="lang-cpp">static void Emit(const string&amp; key) &#123;    cout &lt;&lt; &quot;Sum of values:&quot; &lt;&lt; key &lt;&lt; endl;&#125;</code></pre><h2 id="最后的main"><a href="#最后的main" class="headerlink" title="最后的main"></a>最后的main</h2><pre><code class="lang-cpp">int main(int argc, char* argv[]) &#123;    ifstream in(R&quot;(C:\Users\zyt\CLionProjects\leetcode_2021\lyrics.txt)&quot;);    string content((istreambuf_iterator&lt;char&gt;(in)), istreambuf_iterator&lt;char&gt;());    MapInput minput(&quot;lyrics.txt&quot;, content);    cout &lt;&lt; &quot;minput:\n&quot; &lt;&lt; minput.value() &lt;&lt; endl;    WordCounter wc;    wc.Map(minput);    auto *rinput = new ReduceInput(mw.get());    while (!rinput-&gt;end()) &#123;        cout &lt;&lt; &quot;Key: &quot; &lt;&lt; rinput-&gt;key() &lt;&lt; &quot;\t&quot;;        Adder adder; // 模拟很多 adder        adder.Reduce(rinput);        rinput-&gt;NextValue();    &#125;    return 0;&#125;</code></pre><p>ifstream读入本地文本文档，注意ifstream的参数得是绝对路径（相对路径不知道为什么读取不出来东西）。</p><p>然后WordCount把分词结果的键值对保存在全局变量mw中，使用mw构建ReduceInput，再把ReduceInput输入进Adder里面。</p><p>注意一个Reducer处理一个Group（我把key相同的一组键值对称之为Group），那么我就以While循环来代替啦。</p><p>这就是代码的所有内容了！</p><p><img src="/2021/05/07/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91%E5%AE%9E%E7%8E%B0-MapReduce-%E8%AE%BA%E6%96%87%E9%99%84%E5%BD%95-A-%E7%9A%84-Word-Count-%E7%A8%8B%E5%BA%8F/result.png" alt></p><pre><code class="lang-cpp">#include &quot;stdafx.h&quot;using namespace std;class MiddleWare &#123;private:    vector&lt;pair&lt;string, string&gt;&gt; kv_pairs;    static bool compare_pair(const pair&lt;string, string&gt;&amp; lhs, const pair&lt;string, string&gt;&amp; rhs) &#123;        return lhs.first &lt; rhs.first;    &#125;public:    MiddleWare() = default;    void put(const string&amp; key, const string&amp; value) &#123;        kv_pairs.emplace_back(key, value);    &#125;    vector&lt;pair&lt;string, string&gt;&gt; get() &#123;        std::sort(kv_pairs.begin(), kv_pairs.end(), compare_pair); // 将其按照key相同的一组来排序        return kv_pairs;    &#125;&#125;;MiddleWare mw; // 全局变量：消息队列class MapInput &#123;private:    string map_value;    string map_key;public:    explicit MapInput(string filename, string text) : map_key(std::move(filename)), map_value(std::move(text)) &#123; &#125;    [[nodiscard]] const string&amp; value() const &#123;        return map_value;    &#125;&#125;;class ReduceInput &#123;private:    vector&lt;pair&lt;string, string&gt;&gt; data;    int currentKey = 0;public:    explicit ReduceInput(vector&lt;pair&lt;string, string&gt;&gt; _data) : data(std::move(_data)) &#123;  &#125;//    bool done() &#123;//        // 直到下一个键值对的key与当前键值对的key不同为止//        // 如果到了末尾，或者下一个key不一样，都是done//        if (currentKey == 0) return false;//        if (end() || data[currentKey].first != data[currentKey-1].first) return true;//        return false;//    &#125;    const string&amp; value() &#123;        return data[currentKey].second;    &#125;    const string&amp; key() &#123;        return data[currentKey].first;    &#125;    void NextValue() &#123;        currentKey++;    &#125;    bool end() &#123;        return currentKey &gt;= data.size();    &#125;&#125;;static void Emit(const string&amp; key, const string&amp; value) &#123;    mw.put(key, value);&#125;static void Emit(const string&amp; key) &#123;    cout &lt;&lt; &quot;Sum of values:&quot; &lt;&lt; key &lt;&lt; endl;&#125;class Mapper &#123;public:    virtual void Map(const MapInput&amp; input) = 0;&#125;;class Reducer &#123;public:    virtual void Reduce(ReduceInput* input) = 0;&#125;;class WordCounter : public Mapper &#123;public:    void Map(const MapInput&amp; input) override &#123;        const string&amp; text = input.value(); // 读取一行文本        const int n = text.size();        for (int i = 0; i &lt; n; ) &#123;            // 跳过行首空白            while ((i &lt; n) &amp;&amp; isspace(text[i])) i++;            // 确定单词的开头和结尾            int start = i;            while ((i &lt; n) &amp;&amp; !isspace(text[i])) i++;            if (start &lt; i)                Emit(text.substr(start, i-start), &quot;1&quot;);        &#125;    &#125;&#125;;// 用户自定义 Reduce 函数class Adder : public Reducer &#123;public:    void Reduce(ReduceInput* input) override &#123;        // 迭代所有拥有相同key的键值对，把它们的values加起来        int64_t value = 0;        string currentKey = input-&gt;key();        while (!input-&gt;end() &amp;&amp; currentKey == input-&gt;key()) &#123; // 直到下一个键值对的key与当前键值对的key不同为止            value += std::stoi(input-&gt;value());            input-&gt;NextValue(); // 找到下一个拥有相同key的键值对        &#125;        // Emit sum for input-&gt;key()        Emit(to_string(value));    &#125;&#125;;int main(int argc, char* argv[]) &#123;    ifstream in(R&quot;(C:\Users\zyt\CLionProjects\leetcode_2021\lyrics.txt)&quot;);    string content((istreambuf_iterator&lt;char&gt;(in)), istreambuf_iterator&lt;char&gt;());    MapInput minput(&quot;lyrics.txt&quot;, content);    cout &lt;&lt; &quot;minput:\n&quot; &lt;&lt; minput.value() &lt;&lt; endl;    WordCounter wc;    wc.Map(minput);    auto *rinput = new ReduceInput(mw.get());    while (!rinput-&gt;end()) &#123;        cout &lt;&lt; &quot;Key: &quot; &lt;&lt; rinput-&gt;key() &lt;&lt; &quot;\t&quot;;        Adder adder; // 模拟很多 adder        adder.Reduce(rinput);        rinput-&gt;NextValue();    &#125;    return 0;&#125;</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;给出一个Map和Reduce的具体实现，去除了所有分布式的特性【可能今后会添加】&lt;/p&gt;
&lt;!--more---&gt;
&lt;p&gt;MapReduce文章最后的附录A有一段C++代码，描述了Map和Reduce函数的编写和使用方法。但是仅靠看代码总是不能深入理解MapReduce的实</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Word Count" scheme="https://superlova.github.io/tags/Word-Count/"/>
    
    <category term="MapReduce" scheme="https://superlova.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】用Hadoop实现Word Count</title>
    <link href="https://superlova.github.io/2021/05/06/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8Hadoop%E5%AE%9E%E7%8E%B0Word-Count/"/>
    <id>https://superlova.github.io/2021/05/06/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8Hadoop%E5%AE%9E%E7%8E%B0Word-Count/</id>
    <published>2021-05-06T00:18:12.000Z</published>
    <updated>2022-08-02T13:32:34.816Z</updated>
    
    <content type="html"><![CDATA[<p>好像不管什么Hadoop学习都是从Word Count开始，跨越语言的Word Count</p><!--more---><p>Word Count是MapReduce 的经典入门案例，其目标是统计给定一系列文本文件的单词出现次数。</p><p>编程思路：</p><p><code>map</code>阶段：把输入的数据进行分词，并将其组合成键值对的形式，key是单词，value全部标记为1。</p><p><code>shuffle</code> 阶段：经过默认的排序分区分组，key相同的单词会作为一组数据构成新的键值对。</p><p><code>reduce</code>阶段：处理 <code>shuffle</code> 完的一组数据，该组数据就是该单词所有的键值对。</p><p><img src="/2021/05/06/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8Hadoop%E5%AE%9E%E7%8E%B0Word-Count/Word_Count_MR阶段.png" alt></p><h2 id="Map类的编写"><a href="#Map类的编写" class="headerlink" title="Map类的编写"></a>Map类的编写</h2><pre><code class="lang-java">/** * @description: WordCount Mapper类，对应MapTask * @author: ZYT * KEYIN map阶段的输入 * VALUEIN  todo MapReduce 有默认读取数据的组件：TextInputFormat *          todo 逐行读取。k是偏移量（LongWritable），无意义；v是每行的文本内容（Text）。 * KEYOUT 单词类型，Text * VALUEOUT 次数，LongWritable */public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123;    /**     * 每当TextInputFormat返回一个键值对，map就调用一次。     * 根据TextInputFormat的特性，事实上是每一行文本调用一次Map方法。     * @param key     * @param value     * @param context     * @throws IOException     * @throws InterruptedException     */    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;//        super.map(key, value, context);        String text = value.toString();        String[] words = text.split(&quot;\\s+&quot;);        for (String word : words) &#123;            context.write(new Text(word), new LongWritable(1));        &#125;    &#125;&#125;</code></pre><p>优化</p><pre><code class="lang-java">public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123;    private Text outKey = new Text();    private static final LongWritable outValue = new LongWritable(1);    /**     * 每当TextInputFormat返回一个键值对，map就调用一次。     * 根据TextInputFormat的特性，事实上是每一行文本调用一次Map方法。     * @param key     * @param value     * @param context     * @throws IOException     * @throws InterruptedException     */    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;//        super.map(key, value, context);        String text = value.toString();        String[] words = text.split(&quot;\\s+&quot;);        for (String word : words) &#123;            outKey.set(word);            context.write(outKey, outValue);        &#125;    &#125;&#125;</code></pre><h2 id="Reduce类的编写"><a href="#Reduce类的编写" class="headerlink" title="Reduce类的编写"></a>Reduce类的编写</h2><pre><code class="lang-java">public class WordCountReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt; &#123;    private LongWritable outValue = new LongWritable(0);    /**     * todo q: 当map的输出数据来到reduce之后该如何调用？     * 1. 排序所有pair     * 2. 分组pair，key相同的分成一组     * 3. 每一组调用一次reduce     * @param key     * @param values     * @param context     * @throws IOException     * @throws InterruptedException     * 输出key：该组的单词     * 输出value：该组所有次数的迭代器。     */    @Override    protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123;//        super.reduce(key, values, context);        long count = 0;        for (LongWritable value : values) &#123;            count += value.get();        &#125;        outValue.set(count);        context.write(key, outValue);    &#125;&#125;</code></pre><h2 id="Driver类的编写"><a href="#Driver类的编写" class="headerlink" title="Driver类的编写"></a>Driver类的编写</h2><pre><code class="lang-java">/** * 该类是MapReduce程序客户端驱动类。主要是为了构造Job对象，指定各种组件的属性。 * 包括Mapper、Reducer、输入输出类型、数据路径、提交作业等。 */public class WordCountDriver &#123;    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException &#123;        // 创建配置对象        Configuration conf = new Configuration();        // 构建Job作业实例， 参数为Conf、Job名字        Job job = Job.getInstance(conf, WordCountDriver.class.getSimpleName());        // 设置MR程序运行的主类        job.setJarByClass(WordCountDriver.class);        // 设置本次MR程序的Mapper、Reducer类        job.setMapperClass(WordCountMapper.class);        job.setReducerClass(WordCountReducer.class);        // 指定Mapper阶段输出的kv类型        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(LongWritable.class);        // 指定Reducer阶段kv类型，也是最终输出的kv类型        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(LongWritable.class);        // 配置本次作业的输入输出数据路径        // todo: 默认组件 TextInputFormat、TextOutputFormat        FileInputFormat.setInputPaths(job, new Path(args[0]));        FileOutputFormat.setOutputPath(job, new Path(args[1]));        // 提交作业//        job.submit();        // 采用waitForCompletion方式提交job 参数表示是否开启实时追踪作业执行情况的功能        boolean res_flag = job.waitForCompletion(true);        // 退出程序，和job结果进行绑定        System.exit(res_flag ? 0 : 1);    &#125;&#125;</code></pre><p>继承工具类Tool的Driver</p><pre><code class="lang-java">/** * 使用ToolRunner提交MapReduce作业 */public class WordCountDriver_v2 extends Configured implements Tool &#123;    public static void main(String[] args) throws Exception &#123;        // 创建配置对象        Configuration conf = new Configuration();        // 使用ToolRunner提交程序        int status = ToolRunner.run(conf, new WordCountDriver_v2(), args);        // 退出客户端        System.exit(status);    &#125;    @Override    public int run(String[] args) throws Exception &#123;        // 构建Job作业实例， 参数为Conf、Job名字        Job job = Job.getInstance(getConf(), WordCountDriver_v2.class.getSimpleName());        // 设置MR程序运行的主类        job.setJarByClass(WordCountDriver_v2.class);        // 设置本次MR程序的Mapper、Reducer类        job.setMapperClass(WordCountMapper.class);        job.setReducerClass(WordCountReducer.class);        // 指定Mapper阶段输出的kv类型        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(LongWritable.class);        // 指定Reducer阶段kv类型，也是最终输出的kv类型        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(LongWritable.class);        // 配置本次作业的输入输出数据路径        // todo: 默认组件 TextInputFormat、TextOutputFormat        FileInputFormat.setInputPaths(job, new Path(args[0]));        FileOutputFormat.setOutputPath(job, new Path(args[1]));        return job.waitForCompletion(true) ? 0 : 1;    &#125;&#125;</code></pre><h2 id="如何运行MapReduce程序？"><a href="#如何运行MapReduce程序？" class="headerlink" title="如何运行MapReduce程序？"></a>如何运行MapReduce程序？</h2><p>MapReduce程序是单机运行还是分布式运行？</p><p>MapReduce程序需要的运算资源是Hadoop YARN分配还是本机自己分配？</p><p>运行在何种模式，取决于 mapreduce.framwork.name</p><p>yarn: 集群模式</p><p>local: 本地模式</p><p>如果不指定，默认是local模式。</p><p>在 mapred-default.xml 中定义。如果代码中（conf.set）、运行的环境中（mapred-site.xml）有配置，则会覆盖default的配置。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;好像不管什么Hadoop学习都是从Word Count开始，跨越语言的Word Count&lt;/p&gt;
&lt;!--more---&gt;
&lt;p&gt;Word Count是MapReduce 的经典入门案例，其目标是统计给定一系列文本文件的单词出现次数。&lt;/p&gt;
&lt;p&gt;编程思路：&lt;/p&gt;
&lt;</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Hadoop" scheme="https://superlova.github.io/tags/Hadoop/"/>
    
    <category term="Word Count" scheme="https://superlova.github.io/tags/Word-Count/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】数据密集性应用系统设计CH1笔记</title>
    <link href="https://superlova.github.io/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E6%80%A7%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1CH1%E7%AC%94%E8%AE%B0/"/>
    <id>https://superlova.github.io/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E6%80%A7%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1CH1%E7%AC%94%E8%AE%B0/</id>
    <published>2021-05-04T13:23:13.000Z</published>
    <updated>2022-08-02T13:32:34.805Z</updated>
    
    <content type="html"><![CDATA[<p>第一章：可靠、可扩展和可维护的应用系统<br><!--more---></p><h3 id="数据密集型问题的挑战："><a href="#数据密集型问题的挑战：" class="headerlink" title="数据密集型问题的挑战："></a>数据密集型问题的挑战：</h3><p>数据量，数据复杂度，数据的快速多变性；</p><h3 id="数据密集型应用包含以下模块："><a href="#数据密集型应用包含以下模块：" class="headerlink" title="数据密集型应用包含以下模块："></a>数据密集型应用包含以下模块：</h3><ul><li>数据库；</li><li>高速缓存；</li><li>索引：用户可以按关键字搜索数据井支持各种过滤操作；</li><li>流式处理：持续发送消息至另一个进程，处理采用异步方式；</li><li>批处理：定期处理大量累计数据。</li></ul><h3 id="数据系统"><a href="#数据系统" class="headerlink" title="数据系统"></a>数据系统</h3><p>本章提出<strong>数据系统</strong>的概念，包括<strong>数据库</strong>、<strong>消息队列</strong>和<strong>高速缓存</strong>等不同类型的系统。这样分类的原因有以下几点：</p><ol><li><p>都能将数据保存一段时间。区别在于访问模式不同，以及由于不同访问模式导致的不同性能。</p></li><li><p>新技术、工具拥有多种功能，系统之间的界限变得模糊。比如redis既能<strong>存储</strong>也能作为<strong>消息队列</strong>，kafka作为<strong>消息队列</strong>也能<strong>持久化存储</strong>。</p></li><li><p>系统也需要细分，原有的但各组件无法满足数据处理与存储需求，三分类概念需要进一步分解。例如，即便是一个简单的数据系统，也可能由很多子组件构成：</p></li></ol><p><img src="/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E6%80%A7%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1CH1%E7%AC%94%E8%AE%B0/一种数据系统架构.png" alt></p><p>这个应用包含缓存层（Memcached）和全文索引服务器（Elasticsearch或者Solr）以及数据库。程序员编写应用代码来控制缓存、索引和数据同步操作。</p><h3 id="数据系统的三种关键特性："><a href="#数据系统的三种关键特性：" class="headerlink" title="数据系统的三种关键特性："></a>数据系统的三种关键特性：</h3><ul><li><p>可靠性（Reliability）：当出现意外情况，如软件、硬件故障，人为操作失误等现象时，系统仍可以正常运转的能力。这里的正常运转，是指牺牲部分性能条件下，提供正确的服务。</p></li><li><p>可扩展性（Scalability）：随着问题规模的增长，比如流量、数据量或者任务复杂度，系统以合理方式匹配这种增长的能力。</p></li><li><p>可维护性（Maintainability）：新的开发或者维护人员上手的容易程度，以及适配新场景的能力。</p></li></ul><h3 id="可靠性"><a href="#可靠性" class="headerlink" title="可靠性"></a>可靠性</h3><p>什么是可靠性？即使发生了某些错误，系统仍可以继续正常工作的能力。</p><ul><li>故障或错误（fault）：组件偏离正确规格的现象。</li><li>失效（failure）：系统宕机，无法提供服务。</li></ul><p>为保证可靠性，需要设计容错机制（fault-tolerant）来避免故障引发系统失效，而不是避免故障（当然，避免故障也是提升可靠性的手段）。</p><h3 id="可靠性指标"><a href="#可靠性指标" class="headerlink" title="可靠性指标"></a>可靠性指标</h3><h4 id="失效前平均时间（MTTF）"><a href="#失效前平均时间（MTTF）" class="headerlink" title="失效前平均时间（MTTF）"></a>失效前平均时间（MTTF）</h4><p>是针对不可修复系统而言的，是指系统发生失效前的平均工作（或存储） 时间或工作次数。越高越好。</p><h4 id="平均无故障时间（MTBF）"><a href="#平均无故障时间（MTBF）" class="headerlink" title="平均无故障时间（MTBF）"></a>平均无故障时间（MTBF）</h4><p>是针对可修复系统而言的，指两次相邻失效（故障） 之间的工作时间， 而不是指整个系统的报废时间。越高越好。</p><h4 id="平均修复时间（MTTR）"><a href="#平均修复时间（MTTR）" class="headerlink" title="平均修复时间（MTTR）"></a>平均修复时间（MTTR）</h4><p>是对可修复系统而言的，指从出现故障到修复中间的这段时间。越低越好。</p><h4 id="可用性"><a href="#可用性" class="headerlink" title="可用性"></a>可用性</h4><p>又称为有效性，一般用可用度来定量计算，公式为</p><script type="math/tex; mode=display">A=\frac{\text{MTBF}}{\text{MTBF}+\text{MTTR}}</script><p>可靠性通常低于可用性。对于不可维修系统， 可用度就仅仅决定于且等于可靠度。</p><h3 id="数据系统可能发生哪些故障？如何提升容错能力？"><a href="#数据系统可能发生哪些故障？如何提升容错能力？" class="headerlink" title="数据系统可能发生哪些故障？如何提升容错能力？"></a>数据系统可能发生哪些故障？如何提升容错能力？</h3><h4 id="1-硬件故障"><a href="#1-硬件故障" class="headerlink" title="1. 硬件故障"></a>1. 硬件故障</h4><p>包括硬盘错误、电源错误、网口接触不良等。</p><p>最耐用的硬盘的MTTF为10~50年，因此一个包括10000个磁盘的存储集群中，一天坏一个也是很正常的（假设购买硬盘在时间上是均匀分布）。</p><p>应对方案是添加冗余，比如上述硬盘问题就可以用RAID，电源用双电源，甚至热插拔CPU等。</p><p>以上是从硬件角度解决硬件故障，其实可以通过软件角度解决硬件故障问题。</p><h4 id="2-软件错误"><a href="#2-软件错误" class="headerlink" title="2. 软件错误"></a>2. 软件错误</h4><p>操作系统内核的Bug，系统依赖的服务突然没有响应，某个组件的失控等。</p><p>软件系统的问题有时没有快速解决的办法，只能通过检查依赖假设和系统交互、进行全面测试等方法来预防，或者允许进程重启、评估运行时表现等应对。</p><h4 id="3-人为失误"><a href="#3-人为失误" class="headerlink" title="3. 人为失误"></a>3. 人为失误</h4><p>人是最不可靠的因素。运维人员配置错误往往是系统下线的主要原因。</p><p>这部分的预防以及应对，更加依赖软件工程领域的知识。</p><h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><p>可扩展性是用来描述系统应对负载增加的能力的术语。一个系统添加计算单元越容易，添加计算单元后计算能力的增长越多，就称这个系统的可扩展性越强。</p><h3 id="如何度量系统负载"><a href="#如何度量系统负载" class="headerlink" title="如何度量系统负载"></a>如何度量系统负载</h3><p>每秒请求处理次数、数据库中写入的比例、聊天室的同时活动用户数量、缓存命中率等，</p><p>还要注意分析平均值和峰值对性能的影响</p><h3 id="实例：twitter的两个实现版本"><a href="#实例：twitter的两个实现版本" class="headerlink" title="实例：twitter的两个实现版本"></a>实例：twitter的两个实现版本</h3><p>twitter有两个功能（类比微博，为了方便理解我接下来以微博代替推特），根据其2012年的数据，其功能和负载如下：</p><p>发微博：平均每秒4600条发布申请，峰值12000条推特申请发布。</p><p>收微博：平均每秒300,000条收微博的请求。</p><p>twitter第一个版本使用如下系统设计完成微博的收发：</p><p>发送微博：用户发送的微博插入全局微博集合。</p><p>用户申请查看自己的时间线：  </p><ul><li>遍历所有User的关注对象；</li><li>提取所有关注对象的微博；</li><li>按照发表时间排序并合并。</li></ul><p>随着注册用户变多，负载压力与日俱增，因此采取第二种方法：</p><p>每个用户的时间线维护一个缓存。</p><p>用户发表新的微博：</p><ul><li>查询关注对象；</li><li>插入到每个粉丝的时间线缓存。</li></ul><p>方法二的好处是，用户发布微博时多做一些事情可以加速用户接收微博时的性能。而用户接收微博的请求负载比发送负载高两个数量级。</p><p>Twitter针对那些粉丝量特别多的大V采用方法一，针对粉丝量不太大的绝大多数用户使用方法二。</p><h3 id="如何度量系统性能"><a href="#如何度量系统性能" class="headerlink" title="如何度量系统性能"></a>如何度量系统性能</h3><p><strong>吞吐量</strong></p><p>吞吐量是在一个特定时间段内完成的任务的计数，例如：每秒点击数。</p><p>系统吞吐量几个重要参数：QPS（TPS）、并发数、响应时间</p><p>并发数： 系统同时处理的request/事务数</p><p>TPS：Transactions Per Second（每秒传输的事物处理个数），即服务器每秒处理的事务数。</p><p>QPS：每秒查询率，即对一个特定的查询服务器在规定时间内所处理流量大小。</p><p><strong>响应时间</strong></p><p>从客户端发送请求到接受响应之间的间隔时长。在线系统通常更看重响应时间，批处理系统更关心吞吐量。</p><p>由于每次请求的响应时间服从一个分布，因此我们更关心平均响应时长，或者<strong>百分位数时长</strong>。将响应时长排序，取50百分位数（中位数）比平均数更有意义。其他有意义的百分位数有95、99和99.9百分位数。</p><p>采用较高的响应时间百分位数很重要，因为他们直接影响用户的总体服务体验。95百分位数的响应时间为1.5秒，意味着100个请求中5个请求慢于1.5秒，但是对于电商平台来说，有可能恰恰是这些顾客购买了更多的商品导致访问变慢。</p><p>排队延迟是影响高百分数响应时间的主要延迟来源。因此，如果在性能测试时，负载生成客户端在服务器处理完之前请求后再发送新的请求，就会忽视了排队造成的延迟。</p><h3 id="长尾效应"><a href="#长尾效应" class="headerlink" title="长尾效应"></a>长尾效应</h3><p>一个服务涉及多个不同的后端调用，则最慢的调用会拖累整个服务的响应时间，这种现象称之为长尾效应。</p><p>用户总是需要等待最慢的那个调用完成。因此即便只有很小比例的请求缓慢，也可能由于某一个用户频繁产生这种调用而导致总体变慢。</p><h3 id="如何应对负载增加，提升可扩展性？"><a href="#如何应对负载增加，提升可扩展性？" class="headerlink" title="如何应对负载增加，提升可扩展性？"></a>如何应对负载增加，提升可扩展性？</h3><p>垂直扩展：升级到更强大的机器。</p><p>水平扩展：将负载分布到更多小机器。</p><p>系统设计时，要在这两种扩展中间作取舍。同时要根据不同的吞吐量、请求方式等做针对性优化。</p><h3 id="可维护性"><a href="#可维护性" class="headerlink" title="可维护性"></a>可维护性</h3><p>软件工程师不喜欢读别人留下的代码，不喜欢维护别人开发的老系统，这已经是众所周知的事实。</p><p>但是换个角度，不如说大多数系统在设计之初就没有考虑令后续开发者方便地维护这一个特性。</p><p>为了提升系统的可维护性，减少维护期间的麻烦，可以遵循软件系统的三个设计原则：</p><p>可运维性、简单性、可演化性。</p><h4 id="可运维性"><a href="#可运维性" class="headerlink" title="可运维性"></a>可运维性</h4><p>目标是令运维人员更轻松。之前讨论过，人是系统中最不稳定的因素。因此简化运维人员的操作，使运维人员能够专注于高附加值的任务，能够提升系统的可维护性。</p><p>可以从以下角度提升可运维性：</p><ul><li>提供系统运行时监控工具，方便监控；</li><li>自动化工具；</li><li>避免绑定特定的机器；</li><li>提供良好的文档；</li><li>提供良好的默认配置；</li><li>尝试自我修复等。</li></ul><h4 id="简单性"><a href="#简单性" class="headerlink" title="简单性"></a>简单性</h4><p>简单性是复杂性的反面。而复杂性有各种各样的表现方式，比如模块紧耦合，状态空间膨胀，依赖关系复杂，命名方法混乱，各种性能trick等。</p><p>消除复杂性的最好手段之一是抽象，通过抽象掩盖大量实现细节，提供干净的接口。</p><h4 id="可演化性"><a href="#可演化性" class="headerlink" title="可演化性"></a>可演化性</h4><p>提升可演化性是令系统易于改变的另一种说法。目标是可以轻松地修改数据系统，使其适应不断变化的需求。</p><p>在组织流程方面，敏捷开发模式为适应变化提供了很好的参考。因此敏捷性与可演化性很类似。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;第一章：可靠、可扩展和可维护的应用系统&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h3 id=&quot;数据密集型问题的挑战：&quot;&gt;&lt;a href=&quot;#数据密集型问题的挑战：&quot; class=&quot;headerlink&quot; title=&quot;数据密集型问题的挑战：&quot;&gt;&lt;/a&gt;数据密集型问题的挑战</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Big Data" scheme="https://superlova.github.io/tags/Big-Data/"/>
    
    <category term="Distributed System" scheme="https://superlova.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读笔记】MapReduce: Simplified Data Processing on Large Clusters</title>
    <link href="https://superlova.github.io/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/"/>
    <id>https://superlova.github.io/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/</id>
    <published>2021-05-04T11:27:32.000Z</published>
    <updated>2022-08-02T13:32:34.872Z</updated>
    
    <content type="html"><![CDATA[<p>网络上描写MapReduce的文章不可胜计，唯倜傥非常之文章留存。</p><!--more---><h2 id="一、MapReduce的概念概括"><a href="#一、MapReduce的概念概括" class="headerlink" title="一、MapReduce的概念概括"></a>一、MapReduce的概念概括</h2><p>MapReduce是Google提出的一个软件架构，用于大规模数据集的并行运算。</p><p>MapReduce是一个编程范式，旨在使用<code>map</code>把大规模的问题分解成子问题，然后利用<code>reduce</code>把子问题的解汇总。这种编程范式特别适合应用于分布式系统。</p><p>要理解<code>map</code>和<code>reduce</code>的操作，最重要是要理解下式：</p><p><img src="/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/map和reduce.png" alt></p><p>k1和v1是原始的输入key和value；<br>list(k2, v2)是<code>map</code>把k1和v1分布式计算后的中间结果集合；<br>reduce(k2, list(v2))是<code>reduce</code>函数根据k2的值来合并v2；<br>最终我们想要得到的结果是list(v2)。</p><h2 id="二、MapReduce的应用场景："><a href="#二、MapReduce的应用场景：" class="headerlink" title="二、MapReduce的应用场景："></a>二、MapReduce的应用场景：</h2><ol><li>WordCount——分布式系统的“Hello World”</li></ol><p><code>map</code>函数输出文档中的每个词以及它的频率(word, 1)，<code>reduce</code>函数负责把所有同样词的频率累加起来。得到每个词出现的频率。这一个应用实现请看这里TODO。</p><ol><li>分布式字符串匹配 grep</li></ol><p><code>map</code>函数输出匹配某个模式的一行，<code>reduce</code>则什么计算也不做，只负责将那一行输出。得到匹配的文本行。</p><ol><li>计算 URL 的访问频率</li></ol><p><code>map</code>函数记录requests，输出(url, 1)，<code>reduce</code>函数将相同url的访问加起来，产生(url, sum_of_count)。</p><ol><li>倒转网络链接关系</li></ol><p>搜索引擎排序算法 pagerank 需要使用爬虫爬取所有页面 source 及页面内部的链接 target 。每个 source 内部可能存在多个 target。遇到门户网站，一个 source 有上千个 target 都不奇怪。</p><p>那么搜索引擎是如何决定哪个 target 比较重要的呢？</p><p>pagerank算法解决了这个问题，它假设如果一个网页被很多其他网页所链接，说明它受到普遍的关注和信赖，那么它的排名就高。同时，每个网页的权重不同，那些权重较大的网页拥有的链接更可靠，这些链接的排名往往更靠前。</p><p>这就需要网络爬虫统计每个链接 target 被哪些 source 引用过这种信息了。但是之前我们获得的是 (source, target)数据对，如何把这一部分数据转换成(target, list(source))呢？这就轮到MapReduce出场了。</p><p><code>map</code>负责输出 (source, target) 对，<code>reduce</code> 负责将相同的 target 归并，生成 (target, list(source))。</p><ol><li>确定每个Host的检索词向量（Term-Vector） <span id="1"></span></li></ol><p>检索词向量是一系列（单词，词频）对，这些数据对能够总结一篇文档或者一系列文章的最重要单词（假设词频越高越重要）。</p><p>一个Host可能有非常多文档，如何确定一个Host的检索词向量？</p><p><code>map</code>负责输出每个输入文档的(host, term-vector)，<code>reduce</code>负责汇总给定host的所有term-vector，丢弃所有低频词，输出最终唯一的(host, term-vector)。</p><ol><li>倒排索引</li></ol><p>什么是正排索引？(document, {keys})这种索引形式，从文档出发，检索关键词。</p><p>不过正排索引在搜索引擎中显然没什么作用，因为我们的应用场景是根据关键词检索到对应的所有文档。因此我们更需要(key, {documents})这种索引形式。</p><p>倒排索引就是关键词到文档的映射。每个关键词都对应着一系列的文档。</p><p><code>map</code>分析每个文档，为每个word生成(word, document)的映射，<code>reduce</code>汇总所有相同word的数据对，输出(word, {documents})（可能还会排序）。</p><h2 id="三、MapReduce的实现"><a href="#三、MapReduce的实现" class="headerlink" title="三、MapReduce的实现"></a>三、MapReduce的实现</h2><p>既然MapReduce这么好，那么究竟该怎么实现呢？</p><p>根据不同的集群以及节点性能，MapReduce有多种不同的实现方式。 </p><p>假设存在以下应用场景：普通配置的PC约1000台，机器之间使用交换机连接，网速为百兆，存储介质为廉价的IDE硬盘。用户希望能够通过向调度系统提交job，自动将job对应的一系列tasks分发到集群的各个节点上。</p><h3 id="3-1-MapReduce执行流程概括"><a href="#3-1-MapReduce执行流程概括" class="headerlink" title="3.1 MapReduce执行流程概括"></a>3.1 MapReduce执行流程概括</h3><p>本节我会综合论文原文的 MapReduce 与 Hadoop MapReduce 的具体实现，给出二者的执行步骤。</p><h4 id="3-1-1-论文中的-MapReduce-执行流程"><a href="#3-1-1-论文中的-MapReduce-执行流程" class="headerlink" title="3.1.1 论文中的 MapReduce 执行流程"></a>3.1.1 论文中的 MapReduce 执行流程</h4><ol><li><p>在map阶段，MapReduce会对要处理的数据进行分片（split）操作，为每一个分片分配一个MapTask任务。将输入分成M部分，每部分的大小一般在16M~64M之间（用户来定义）。输出也分为R部分（？）。然后在各个机器上fork程序副本。</p></li><li><p>选定集群中的一个机器为master节点，负责分发任务；其他节点是worker，负责计算和向master提交任务结果。</p></li><li><p>之前指定了M个map任务和R个reduce任务，master节点给每个空闲的worker分配一个map任务或者一个reduce任务。</p></li><li><p>被分配map任务的worker会读取对应的输入片段，输入用户定义的map函数，输出中间结果，将这些中间结果缓存在内存中。这些中间结果会定期地保存在本地此版中。由partition函数将其分成R部分。worker负责将这些缓存数据对在磁盘中的位置上传给master。</p></li><li><p>master负责收集map worker发送回来的数据对位置，然后把这些位置发送给 reduce worker。当一个reduce worker把这些中间结果读取完毕后，它会首先对这些中间结果排序，这样同样key的中间结果就会相邻了。</p></li></ol><p>key很多种类的情况下，排序是有必要的吗？实践表明，排序是有必要的，因为数据分片后，往往同一key的数据在同一片M中。这体现了数据在空间上的局部性。</p><p>但是如果数据量过大，中间结果过多，我们可能需要外部排序。</p><ol><li><p>reduce worker迭代所有中间结果，由于这些中间信息按照key排序过了，因此很容易获得同样key的所有键值对集合(key, {values})。将这一部分整合key后的信息传递给Reduce函数。Reduce函数的输出被追加到所属分区R的输出文件中。</p></li><li><p>当所有Map和Reduce任务都完成后，master唤醒用户程序，用户程序返回调用结果。</p></li></ol><p>下图是MapReduce论文中的流程概括图。</p><p><img src="/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/流程概括.png" alt></p><h4 id="3-1-2-Hadoop-MapReduce-执行流程"><a href="#3-1-2-Hadoop-MapReduce-执行流程" class="headerlink" title="3.1.2 Hadoop MapReduce 执行流程"></a>3.1.2 Hadoop MapReduce 执行流程</h4><p><img src="/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/MR流程.png" alt></p><ol><li><strong>Map阶段执行过程</strong></li></ol><p>对应流程图的左半部分。</p><ul><li>1) 把输入目录下文件按照一定标准逐个进行<strong>逻辑切片</strong>，形成切片规划</li></ul><p>默认切片大小和块大小是相同的，每个切片由一个MapTask来处理。</p><ul><li>2) 对切片中的数据按照一定规则解析并返回(key,value)对</li></ul><p>如果是文本数据，则调用TextInputFormat类。默认按行读取数据，key是每一行的起始偏移量，value是本行的文本内容。</p><p>key对应的偏移量是什么东西？打开notepad++，底栏的Pos就是当前光标所对应字符的偏移量。</p><p><img src="/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/偏移量.png" alt></p><ul><li>3) 调用Mapper类中的map方法处理数据。</li></ul><p>每读取解析出来的一个(key,value)，调用一次map方法。map方法是用户自己定义的业务逻辑。</p><ul><li><p>4) 按照一定的规则对Map输出的键值对进行分区(partition)。分区的数量就是reduce task的数量。</p></li><li><p>5) Map输出数据写入内存缓冲区，达到一定比例后溢出(spill)到磁盘上。溢出的时候根据key排序(sort)。</p></li><li><p>6) 对所有溢出的文件进行合并(merge)，形成一个文件。</p></li></ul><p>至此，Map阶段结束。map worker并不会将自己的结果发送给reduce，而是会静静地等待reduce worker来主动拉取数据。</p><ol><li><strong>Reduce阶段执行过程</strong></li></ol><p>对应流程图的右半部分。</p><ul><li><p>1) Reduce task会主动从MapTask复制拉取其输出的键值对；</p></li><li><p>2) 把复制到Reduce worker的本地数据全部合并(merge)，再对合并的数据排序。</p></li><li><p>3) 对排序后的键值对调用Reduce方法。</p></li></ul><p>键相等的键值对调用一次reduce方法。</p><p>最后把这些输出的键值对写入到HDFS文件中。</p><h3 id="3-2-Master节点的数据结构"><a href="#3-2-Master节点的数据结构" class="headerlink" title="3.2 Master节点的数据结构"></a>3.2 Master节点的数据结构</h3><p>Master节点会保存每个map任务和reduce任务的执行状态（空闲 idle、正在执行 in-progress，执行完毕 completed），还会给被分配任务了的worker保存一个标注。</p><p>Master还会像一个管道一样储存map任务完成后的中间信息存储位置，并把这些位置信息传输给reduce worker。</p><h3 id="3-3-容错机制"><a href="#3-3-容错机制" class="headerlink" title="3.3 容错机制"></a>3.3 容错机制</h3><h4 id="worker-failure"><a href="#worker-failure" class="headerlink" title="worker failure"></a>worker failure</h4><p>master会周期性地ping每个worker，规定时间内没有返回信息，则master将其标记为fail。master将所有由这个失效的worker做完的（completed）、正在做的（in-progress）<code>map</code>任务标记为初始状态（idle），等待其他worker认领任务。</p><p>worker故障时，对于map任务结果和reduce任务结果的处理方法有所不同。map任务的结果由于需要通知master存储位置，中途中断会导致存储位置丢失，因此失败的map任务需要重新执行；reduce任务的结果存储位置在全局文件系统上，因此不需要再次执行。</p><p>当worker B接手了失败的worker A的task，所有reduce worker都会知晓。因此所有还没有从worker A哪里获得数据的reduce worker会自动向worker B获取数据。</p><h4 id="master-failure"><a href="#master-failure" class="headerlink" title="master failure"></a>master failure</h4><p>首先要有检查点（checkpoint）机制，周期性地将master节点存储的数据保存至磁盘。</p><p>但是由于只有一个master进程，因此master失效后MapReduce运算会中止。由用户亲自检查，根据需要重新执行MapReduce。</p><h4 id="Semantics-in-the-Presence-of-Failures"><a href="#Semantics-in-the-Presence-of-Failures" class="headerlink" title="Semantics in the Presence of Failures"></a>Semantics in the Presence of Failures</h4><p>Semantics，语义。这个词不太好理解，总之本节讨论Failure的出现对程序语义的影响。</p><p>个人理解，对于一个确定性的程序而言，程序的寓意就是程序的执行顺序；但是对于非确定性程序而言，也就是执行多次可能得到不同结果的程序而言，语义是否能保证，直接与最后结果是否正确相关。</p><p>MapReduce保证，当Map和Reduce的操作都是确定性函数（只要相同输入就会得到相同输出的函数），那么MapReduce处理得到的结果也都是确定性的，不论集群内部有没有错误、执行顺序。</p><p>这种强保证是由Map和Reduce中的commit操作的原子性来保证的。</p><p>每个 in-progress task 都将其输出写进私有临时文件中。每个reduce产生一个私有临时文件，每个map产生R个私有临时文件（因为对应R个reduce任务）。</p><p>当map任务完成，map worker发送给master的是那R个临时文件的名称，并标注“我做完了”。master在收到消息后，就将这R个文件名记录在自己的数据结构中。<strong>如果这个时候由于某些错误</strong>，master又收到一遍“我做完了”，master将会忽略。</p><p>当reduce任务完成，reduce worker把临时文件重命名为最终的输出文件名。重命名操作是原子的，即要不就全部重命名成功，要不就一个都不会重命名。这里存在一种语义风险，那就是如果同一个reduce task在多台机器上执行，同一个结果文件有可能被重命名多次。为保证最终文件系统只包含一个reduce任务产生的数据，MapReduce依赖底层文件系统提供的重命名操作（？）。</p><p>坦白说，关于弱语义如何保证这一块儿没看懂，等今后再回来补吧。TODO</p><h3 id="3-4-存储位置"><a href="#3-4-存储位置" class="headerlink" title="3.4 存储位置"></a>3.4 存储位置</h3><p>这一部分的设计要尽可能节约带宽，因为带宽是相当匮乏的资源。</p><p>MapReduce的解决方案是通过GFS (Google File System)将每个文件分成 64 MB的块，然后将每块保存几个副本（通常为3份）在不同的机器上。MapReduce 尽量将这些位置信息保存下来然后尽量将含有某个文件主机的任务分配给它，这样就可以减少网络的传递使用。如果失败，那么将会尝试从靠近输入数据的一个副本主机去启动这个任务。当在一个集群上执行大型的 MapReduce 操作的时候，输入数据一般都是本地读取，减少网络带宽的使用。</p><h3 id="3-5-任务粒度"><a href="#3-5-任务粒度" class="headerlink" title="3.5 任务粒度"></a>3.5 任务粒度</h3><p>理想状况下，M和R应当与worker数目大很多，这样才能提高集群的动态负载均衡能力，并且能加快故障恢复的速度，原因是失效的worker上执行的map任务可以分布到所有其他的worker机器上执行。</p><p>但是M和R也是有限制的，这一部分限制主要是由于master需要执行O(M+R)次调度。</p><p>我们通常会按照这样的比例执行：M=200,000，R=5,000，worker有2,000台。</p><h3 id="3-6-备用任务"><a href="#3-6-备用任务" class="headerlink" title="3.6 备用任务"></a>3.6 备用任务</h3><p>长尾分布现象（或者说“水桶效应”）在MapReduce中也有体现，因为MapReduce计算时间往往取决于其运行速度最慢的worker。</p><p>有一个办法来减少“straggler”（落伍的人），master会在任务快完成时，调用backup进程来解决那些 in-progress 任务。这样，无论是原来的进程还是 backup 进程中的哪个先完成，master都立即将其标记为完成。</p><h2 id="四、MapReduce调优技巧"><a href="#四、MapReduce调优技巧" class="headerlink" title="四、MapReduce调优技巧"></a>四、MapReduce调优技巧</h2><h3 id="4-1-分区函数"><a href="#4-1-分区函数" class="headerlink" title="4.1 分区函数"></a>4.1 分区函数</h3><p>分区函数一般是Hash，<code>Hash(key) % R</code>，这样就能把key分成R份了。但是有的时候我们希望自己定义R的分区方法，比如在第二章的应用场景<a href="#1">Host Term-Vector</a>中，我们希望以Host为分R标准，那么分区函数就可以这么写：<code>hash(Hostname(urlkey)) % R</code>。，这样具有相同的 hostname 的URL将会出现在同一个输出文件中。</p><h3 id="4-2-顺序保证"><a href="#4-2-顺序保证" class="headerlink" title="4.2 顺序保证"></a>4.2 顺序保证</h3><p>在一个分区R中，MapReduce保证所有中间k/v对都是按key排序的。</p><h3 id="4-3-Combiner"><a href="#4-3-Combiner" class="headerlink" title="4.3 Combiner"></a>4.3 Combiner</h3><p>某些任务的中间结果在从map传输到reduce的时候可以先处理一下再传。比如word count应用，中间结果是一堆(word, 1)数据对，这个时候我们利用某个combiner函数，将本地的中间结果合并一下，比如合并相同的100个(word, 1)为(word, 100)，就大量降低了数据传输占用的带宽。</p><p>Combiner函数会在每台执行Map任务的机器上执行一次。通常情况下，Combiner函数和Reduce函数的实现代码是一样的。</p><h3 id="4-4-输入和输出"><a href="#4-4-输入和输出" class="headerlink" title="4.4 输入和输出"></a>4.4 输入和输出</h3><p>MapReduce库支持不同的格式的输入数据。比如文本模式，key是行数，value是该行内容。</p><p>程序员可以定义Reader接口来适应不同的输入类型。程序员需要保证必须能把输入数据切分成数据片段，且这些话宿儒片段能够由单独的Map任务来处理就行了。</p><p>Reader的数据源可能是数据库，可能是文本文件，甚至是内存等。输入Writer同样可以自定义。</p><h3 id="4-5-副作用"><a href="#4-5-副作用" class="headerlink" title="4.5 副作用"></a>4.5 副作用</h3><p>程序员在写Map和Reduce操作的时候，可能会处于方便，定义很多额外功能，比如生成辅助文件等。但应当时刻记住，Map和Reduce操作应当保证原子性和幂等性。</p><p>比如，一个task生成了多个输出文件，但是我们没有原子化多段commit的操作。这就需要程序员自己保证生成多个输出的任务是确定性任务。</p><h3 id="4-6-跳过损坏的纪录"><a href="#4-6-跳过损坏的纪录" class="headerlink" title="4.6 跳过损坏的纪录"></a>4.6 跳过损坏的纪录</h3><p>有时相比于修复不可执行的Bug，跳过该部分引起Bug的Record更加可取。因此，我们希望MapReduce检测到可能引起崩溃的Record时，自动跳过。</p><p>MapReduce如何自动检测这种现象？首先每个worker会通过一个handler来捕获异常，并利用一个全局变量来保存异常序号。worker会在之后发送给master的工作汇报中写上该signal序号（以UDP发送）。master看到该UDP包中存在多次故障，那么将来该worker失败了，master就不会重复执行该task，而是跳过该record。</p><h3 id="4-7-本地执行"><a href="#4-7-本地执行" class="headerlink" title="4.7 本地执行"></a>4.7 本地执行</h3><p>就是说一上来就在成千上万台机器上进行调试是非常棘手的，因此MapReduce开发了在本地计算机上模拟MapReduce任务的项目，方便调试。</p><h3 id="4-8-状态信息"><a href="#4-8-状态信息" class="headerlink" title="4.8 状态信息"></a>4.8 状态信息</h3><p>master内部有一个内置的HTTP服务器，可以用来展示一组状态信息页面。状态页面会显示计算进度，例如：已经完成的任务数量、正在执行的任务数量、输入的字节数、中间数据的字节数、输出的字节数、处理率等等。</p><p>这些页面也包含了指向每个任务的标准差以及生成的标准输出文件的链接。用户可以使用这些数据来预测计算需要多久才能完成，是否需要往该计算中增加更多资源。当计算消耗的时间比预期时间更长的时候，这些页面也可以用来找出为什么执行速度很慢的原因。</p><p>此外，顶层的状态页面会显示那些故障的worker，以及它们故障时正在运行的Map和Reduce任务。这些信息对于调试用户代码中的bug很有帮助。</p><p>这一点HDFS也有类似实现，比如HDFS 在启动完成之后，还会由内部的 Web 服务提供一个查看集群状态的网页：</p><p><a href="http://localhost:50070/">http://localhost:50070/</a></p><p><strong>提供可视化监控界面，是提升分布式系统的可维护性的重要手段</strong>。</p><h3 id="4-9-计数器"><a href="#4-9-计数器" class="headerlink" title="4.9 计数器"></a>4.9 计数器</h3><p>MapReduce内部提供计数器机制，用来统计不同操作发生次数。要想使用计数器，程序员需要创建Counter对象，然后在Map和Reduce函数中以正确的方式增加counter。</p><p>当聚合这些counter的值时，master会去掉那些重复执行的相同map或者reduce操作的次数，以此避免重复计数（之前提到的备用任务和故障后重新执行任务，这两种情况会导致相同的任务被多次执行）。</p><p>有些counter值是由MapReduce库自动维护的，例如已经处理过的输入键值对的数量以及生成的输出键值对的数量。</p><h2 id="五、MapReduce的性能评估"><a href="#五、MapReduce的性能评估" class="headerlink" title="五、MapReduce的性能评估"></a>五、MapReduce的性能评估</h2><h2 id="六、MapReduce使用经验"><a href="#六、MapReduce使用经验" class="headerlink" title="六、MapReduce使用经验"></a>六、MapReduce使用经验</h2><p>本文只关注MapReduce的技术细节，故第五、六节略过。</p><h2 id="七、参考"><a href="#七、参考" class="headerlink" title="七、参考"></a>七、参考</h2><p>Lassen S B . MapReduce: Simplified Data Processing on Large Clusters (work by Jeffrey Dean and Sanjay Ghemawat). </p><p><a href="https://chunlife.top/2020/04/18/Google-MapReduce%E4%B8%AD%E6%96%87%E7%89%88/">https://chunlife.top/2020/04/18/Google-MapReduce%E4%B8%AD%E6%96%87%E7%89%88/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;网络上描写MapReduce的文章不可胜计，唯倜傥非常之文章留存。&lt;/p&gt;
&lt;!--more---&gt;
&lt;h2 id=&quot;一、MapReduce的概念概括&quot;&gt;&lt;a href=&quot;#一、MapReduce的概念概括&quot; class=&quot;headerlink&quot; title=&quot;一、MapR</summary>
      
    
    
    
    <category term="paper" scheme="https://superlova.github.io/categories/paper/"/>
    
    
    <category term="Big Data" scheme="https://superlova.github.io/tags/Big-Data/"/>
    
    <category term="Distributed System" scheme="https://superlova.github.io/tags/Distributed-System/"/>
    
    <category term="MapReduce" scheme="https://superlova.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】Hadoop介绍和安装</title>
    <link href="https://superlova.github.io/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Hadoop%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%89%E8%A3%85/"/>
    <id>https://superlova.github.io/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Hadoop%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%89%E8%A3%85/</id>
    <published>2021-05-04T11:26:57.000Z</published>
    <updated>2022-08-02T13:32:34.789Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop学习笔记第一篇。<br><!--more---></p><h1 id="Hadoop介绍"><a href="#Hadoop介绍" class="headerlink" title="Hadoop介绍"></a>Hadoop介绍</h1><p>Apache Hadoop 软件库是一个框架，允许在<strong>集群服务器</strong>上使用简单的<strong>编程模型</strong>，<strong>对大数据集进行分布式处理</strong>。</p><p>Hadoop 可扩展性强，能从单台服务器扩展到数以千计的服务器；Hadoop 高可用，其代码库自身就能在应用层侦测并处理硬件故障。</p><p>Hadoop 的生态系统不仅包含 Hadoop，而且还包含 HDFS、HBase等基本组件。</p><p><img src="/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Hadoop%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%89%E8%A3%85/Hadoop生态系统.png" alt></p><p><strong>HDFS (Hadoop Distributed File System)</strong></p><p>HDFS是分布式文件系统的一种。HDFS是Hadoop生态系统的基本组成，它将数据保存在计算机集群上。HDFS是HBase等工具的基础。</p><p><strong>MapReduce</strong></p><p>MapReduce是一种分布式计算框架，也是一个分布式、并行处理的编程模型。MapReduce把任务分为<code>map</code>阶段和<code>reduce</code>阶段，<code>map</code>阶段将任务分解成子任务后映射到集群上，<code>reduce</code>将结果化简并整合。</p><p>正是利用了MapReduce的工作特性，Hadoop因此能以并行的方式访问数据，从而实现分布式计算。</p><p>关于MapReduce的论文讲解，请看<a href="https://superlova.github.io/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/">这里</a>。</p><p><strong>HBase</strong></p><p>HBase 是一个建立在 HDFS 之上，面向列的 NoSQL 数据库，用于快速读 / 写大量数据。HBase 使用 Zookeeper 进行管理。</p><p><strong>ZooKeeper</strong></p><p>ZooKeeper 为大型分布式计算提供开源的分布式配置服务、同步服务和命名注册。</p><p>Hadoop 的许多组件依赖于 Zookeeper，它运行在计算机集群中，用于管理 Hadoop 集群。</p><p><strong>Pig</strong></p><p>Pig是一个基于Hadoop的大规模数据分析平台，它为 MapReduce 编程模型提供了一个简单的操作和编程接口。它提供的SQL-LIKE语言叫Pig Latin，该语言的编译器会把类SQL的数据分析请求转换为一系列经过优化处理的MapReduce运算。</p><p><strong>Hive</strong><br>Apache Hive是一个建立在Hadoop架构之上的数据仓库。它能够提供数据的精炼，查询和分析。像 Pig 一样，Hive 作为一个抽象层工具，吸引了很多熟悉 SQL 而不是 Java 编程的数据分析师。</p><p>与Pig的区别在于，Pig是一中编程语言，使用命令式操作加载数据、表达转换数据以及存储最终结果。Pig中没有表的概念。而Hive更像是SQL，使用类似于SQL语法进行数据查询。</p><p><strong>Sqoop</strong></p><p>用于在关系数据库、数据仓库和 Hadoop 之间转移数据。</p><p><strong>Flume</strong></p><p>是一个分布式、可靠、高可用的海量日志采集、聚合和传输的系统，用于有效地收集、聚合和将大量日志数据从许多不同的源移动到一个集中的数据存储（如文本、HDFS、Hbase等）。</p><p><strong>Yarn</strong></p><p>是从Hadoop 2.0版本开始沿用的任务调度和集群资源管理的框架。</p><p><strong>Spark</strong></p><p>一个快速通用的 Hadoop 数据计算引擎，具有简单和富有表达力的编程模型，支持数据 ETL（提取、转换和加载）、机器学习、流处理和图形计算等方面的应用。</p><p>Spark 这一分布式内存计算框架就是脱胎于 Hadoop 体系的，它对 HDFS 、YARN 等组件有了良好的继承，同时也改进了 Hadoop 现存的一些不足。</p><p>下图是Hadoop集群的基本架构。</p><p><img src="/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Hadoop%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%89%E8%A3%85/Hadoop集群基本架构.png" alt></p><h1 id="Hadoop-可以做什么"><a href="#Hadoop-可以做什么" class="headerlink" title="Hadoop 可以做什么"></a>Hadoop 可以做什么</h1><p>据Hadoop Wiki记载，阿里巴巴使用15个节点组成的Hadoop集群，每个节点拥有8核心、16GB内存和1.4TB存储。阿里巴巴使用这些节点来处理商业数据的排序和组合，应用于交易网站的垂直搜索。</p><p>Ebay拥有32个节点组成的集群，使用Java编写的MapReduce应用，来优化搜索引擎。</p><p>FaceBook使用Hadoop来存储内部日志和结构化数据源副本，并且将其作为数据报告、数据分析和机器学习的数据源。</p><h1 id="Hadoop-不同版本"><a href="#Hadoop-不同版本" class="headerlink" title="Hadoop 不同版本"></a>Hadoop 不同版本</h1><p><strong>关于发行方：</strong></p><p>目前Hadoop发行版非常多，有Intel发行版，华为发行版、Cloudera发行版（CDH）、Hortonworks版本等，所有这些发行版均是基于Apache Hadoop衍生出来的，之所以有这么多的版本，是由于Apache Hadoop的开源协议决定的：任何人可以对其进行修改，并作为开源或商业产品发布/销售。</p><p><strong>关于版本：</strong></p><p>现在最新的Hadoop已经达到3.X了，然而大部分公司使用Hadoop 2.X。又由于Hadoop 2.X与1.X相比有较大变化，因此直接使用2.X是比较合理的选择。</p><p>Hadoop2.0新增了HDFS HA机制，HA增加了standbynamenode进行热备份，解决了1.0的单点故障问题。</p><p>Hadoop2.0新增了HDFS federation，解决了HDFS水平可扩展能力。 </p><p>2.0相比于1.0 新增了YARN框架，Mapreduce的运行环境发生了变化</p><h1 id="Hadoop-安装"><a href="#Hadoop-安装" class="headerlink" title="Hadoop 安装"></a>Hadoop 安装</h1><p>Hadoop有三种安装方式</p><ul><li>单机模式：安装简单，几乎不用做任何配置，但仅限于调试用途。</li><li>伪分布模式：在单节点上同时启动 NameNode、DataNode、JobTracker、TaskTracker、Secondary Namenode 等 5 个进程，模拟分布式运行的各个节点。</li><li>完全分布式模式：正常的 Hadoop 集群，由多个各司其职的节点构成。</li></ul><p>本文介绍 Hadoop 伪分布式模式部署方法，Hadoop 版本为 2.6.1。</p><h2 id="1-设置用户和组"><a href="#1-设置用户和组" class="headerlink" title="1. 设置用户和组"></a>1. 设置用户和组</h2><pre><code>sudo adduser hadoopsudo usermod -G sudo hadoop</code></pre><h2 id="2-安装JDK"><a href="#2-安装JDK" class="headerlink" title="2. 安装JDK"></a>2. 安装JDK</h2><p>不同版本的 Hadoop 对 Java 的版本需求有细微的差别，可以在<a href="https://cwiki.apache.org/confluence/display/HADOOP2/HadoopJavaVersions">这个网站</a>查询 Hadoop 版本与 Java 版本的关系。</p><p>测试jdk是否部署成功：</p><pre><code class="lang-sh">java -version</code></pre><h2 id="3-配置SSH免密码登录"><a href="#3-配置SSH免密码登录" class="headerlink" title="3. 配置SSH免密码登录"></a>3. 配置SSH免密码登录</h2><p>安装和配置 SSH 的目的是为了让 Hadoop 能够方便地运行远程管理守护进程的相关脚本。这些脚本需要用到 sshd 服务。</p><pre><code class="lang-sh">su hadoopcd /home/hadoopssh-keygen -t rsa# 将生成的公钥添加到主机认证记录中。cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys# 为 authorized_keys 文件赋予写权限chmod 600 .ssh/authorized_keys# 尝试登录到本机ssh localhost</code></pre><h2 id="4-下载-Hadoop"><a href="#4-下载-Hadoop" class="headerlink" title="4. 下载 Hadoop"></a>4. 下载 Hadoop</h2><pre><code class="lang-sh">wget https://archive.apache.org/dist/hadoop/common/hadoop-2.6.1/hadoop-2.6.1.tar.gztar zxvf hadoop-2.6.1.tar.gzsudo mv hadoop-2.6.1 /opt/hadoop-2.6.1sudo chown -R hadoop:hadoop /opt/hadoop-2.6.1vim /home/hadoop/.bashrc</code></pre><p>在 /home/hadoop/.bashrc 文件的末尾添加以下内容：</p><pre><code class="lang-sh">export HADOOP_HOME=/opt/hadoop-2.6.1export JAVA_HOME=/usr/lib/jvm/java-8-oracleexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</code></pre><p>在终端中输入 source 命令来激活新添加的环境变量。</p><pre><code class="lang-sh">source /home/hadoop/.bashrc</code></pre><h2 id="5-伪分布式模式配置"><a href="#5-伪分布式模式配置" class="headerlink" title="5. 伪分布式模式配置"></a>5. 伪分布式模式配置</h2><p>Hadoop 还可以以伪分布式模式运行在单个节点上，通过多个独立的 Java 进程来模拟多节点的情况。在初始学习阶段，暂时没有必要耗费大量的资源来创建不同的节点。</p><p>5.1 <strong>打开 core-site.xml 文件:</strong></p><pre><code class="lang-sh">vim /opt/hadoop-2.6.1/etc/hadoop/core-site.xml</code></pre><p>将 configuration 标签的值修改为以下内容：</p><pre><code class="lang-xml">&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;        &lt;value&gt;/home/hadoop/tmp&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>fs.defaultFS 配置项用于指示集群默认使用的文件系统的位置。</p><p>5.2 <strong>打开另一个配置文件 hdfs-site.xml</strong></p><pre><code class="lang-sh">vim /opt/hadoop-2.6.1/etc/hadoop/hdfs-site.xml</code></pre><p>将 configuration 标签的值修改为以下内容：</p><pre><code class="lang-xml">&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;dfs.replication&lt;/name&gt;        &lt;value&gt;1&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>该配置项用于指示 HDFS 中文件副本的数量，默认情况下是 3 份，由于我们在单台节点上以伪分布式的方式部署，所以将其修改为 1 。</p><p>5.3 <strong>编辑 hadoop-env.sh 文件：</strong></p><pre><code class="lang-sh">vim /opt/hadoop-2.6.1/etc/hadoop/hadoop-env.sh</code></pre><p>将其中 <code>export JAVA_HOME</code> 的值修改为 JDK 的实际位置，即 <code>/usr/lib/jvm/java-8-oracle</code> 。</p><p>5.4 <strong>编辑 yarn-site.xml 文件：</strong></p><pre><code class="lang-sh">vim /opt/hadoop-2.6.1/etc/hadoop/yarn-site.xml</code></pre><p>在 configuration 标签内添加以下内容：</p><pre><code class="lang-xml">&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>5.5 <strong>编辑 mapred-site.xml 文件。首先需要从模板复制过来：</strong></p><pre><code class="lang-sh">cp /opt/hadoop-2.6.1/etc/hadoop/mapred-site.xml.template /opt/hadoop-2.6.1/etc/hadoop/mapred-site.xmlvim /opt/hadoop-2.6.1/etc/hadoop/mapred-site.xml</code></pre><p>在 configuration 标签内添加以下内容：</p><pre><code class="lang-xml">&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;        &lt;value&gt;yarn&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h2 id="6-Hadoop-启动测试"><a href="#6-Hadoop-启动测试" class="headerlink" title="6. Hadoop 启动测试"></a>6. Hadoop 启动测试</h2><pre><code class="lang-sh">su -l hadoopvim /home/hadoop/.bashrc</code></pre><p>向<code>.bashrc</code>添加 Java 的环境变量：</p><pre><code class="lang-sh">export JAVA_HOME=/usr/lib/jvm/java-8-oracleexport PATH=$PATH:$JAVA_HOME/bin</code></pre><h1 id="HDFS-的基本使用"><a href="#HDFS-的基本使用" class="headerlink" title="HDFS 的基本使用"></a>HDFS 的基本使用</h1><h2 id="1-初始化HDFS"><a href="#1-初始化HDFS" class="headerlink" title="1. 初始化HDFS"></a>1. 初始化HDFS</h2><pre><code class="lang-sh">hdfs namenode -format</code></pre><p>格式化的操作只需要进行一次即可，不需要多次格式化。每一次格式化 namenode 都会清除 HDFS 分布式文件系统中的所有数据文件。同时，多次格式化容易出现 namenode 和 datanode 不同步的问题。</p><h2 id="2-启动HDFS"><a href="#2-启动HDFS" class="headerlink" title="2. 启动HDFS"></a>2. 启动HDFS</h2><p>HDFS 初始化完成之后，就可以启动 NameNode 和 DataNode 的守护进程。启动之后，Hadoop 的应用（如 MapReduce 任务）就可以从 HDFS 中读写文件。</p><p>在终端中输入以下命令来启动守护进程：</p><pre><code class="lang-sh">start-dfs.sh</code></pre><p>为了确认伪分布式模式下的 Hadoop 已经成功运行，可以利用 Java 的进程查看工具 <code>jps</code> 来查看是否有相应的进程。</p><p>如果执行 jps 发现没有 NameNode 服务进程，可以先检查一下是否执行了 namenode 的初始化操作。如果没有初始化 namenode ，先执行 stop-dfs.sh ,然后执行 hdfs namenode -format ,最后执行 start-dfs.sh 命令，通常来说这样就能够保证这三个服务进程成功启动</p><h2 id="3-查看日志和WebUI"><a href="#3-查看日志和WebUI" class="headerlink" title="3. 查看日志和WebUI"></a>3. 查看日志和WebUI</h2><p>作为大数据领域的学习者，掌握分析日志的能力与学习相关计算框架的能力同样重要。</p><p>Hadoop 的守护进程日志默认输出在安装目录的 log 文件夹中，在终端中输入以下命令进入到日志目录：</p><pre><code class="lang-sh">cd /opt/hadoop-2.6.1/logsls</code></pre><p>HDFS 在启动完成之后，还会由内部的 Web 服务提供一个查看集群状态的网页：</p><p><a href="http://localhost:50070/">http://localhost:50070/</a></p><p>打开网页后，可以在其中查看到集群的概览、DataNode 的状态等信息。</p><h2 id="4-HDFS文件上传测试"><a href="#4-HDFS文件上传测试" class="headerlink" title="4. HDFS文件上传测试"></a>4. HDFS文件上传测试</h2><p>HDFS 运行起来之后，可将其视作一个文件系统。此处进行文件上传的测试，首先需要按照目录层级逐个创建目录，并尝试将 Linux 系统中的一些文件上传到 HDFS 中。</p><pre><code class="lang-sh">cd ~hdfs dfs -mkdir /userhdfs dfs -mkdir /user/hadoop</code></pre><p>如果需要查看创建好的文件夹，可以使用如下命令：</p><pre><code class="lang-sh">hdfs dfs -ls /user</code></pre><p>目录创建成功之后，使用 <code>hdfs dfs -put</code> 命令将本地磁盘上的文件（此处是随意选取的 Hadoop 配置文件）上传到 HDFS 之中。</p><pre><code class="lang-sh">hdfs dfs -put /opt/hadoop-2.6.1/etc/hadoop /user/hadoop/input</code></pre><p>如果要查看上传的文件，可以执行如下命令：</p><pre><code class="lang-sh">hdfs dfs -ls /user/hadoop/input</code></pre><h1 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h1><p>WordCount 是 Hadoop 的 “HelloWorld” 程序。</p><p>绝大多数部署在实际生产环境并且解决实际问题的 Hadoop 应用程序都是基于 WordCount 所代表的 MapReduce 编程模型变化而来。</p><p>在终端中首先启动 YARN 计算服务：</p><pre><code class="lang-sh">start-yarn.sh</code></pre><p>然后输入以下命令以启动任务</p><pre><code class="lang-sh">hadoop jar /opt/hadoop-2.6.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.1.jar wordcount /user/hadoop/input/ output</code></pre><p>上述参数中，关于路径的参数有三个，分别是 jar 包的位置、输入文件的位置和输出结果的存放位置。在填写路径时，应当养成填写绝对路径的习惯。这样做将有利于定位问题和传递工作。</p><p>等待计算完成，然后将 HDFS 上的文件导出到本地目录查看：</p><pre><code class="lang-sh">rm -rf /home/hadoop/outputhdfs dfs -get /user/hadoop/output outputcat output/*</code></pre><p>计算完毕后，如无其他软件需要使用 HDFS 上的文件，则应及时关闭 HDFS 守护进程。</p><p>作为分布式集群和相关计算框架的使用者，应当养成良好的习惯，在每次涉及到集群开启和关闭、软硬件安装和更新的时候，都主动检查相关软硬件的状态。</p><pre><code class="lang-sh">stop-yarn.shstop-dfs.sh</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Hadoop学习笔记第一篇。&lt;br&gt;&lt;!--more---&gt;&lt;/p&gt;
&lt;h1 id=&quot;Hadoop介绍&quot;&gt;&lt;a href=&quot;#Hadoop介绍&quot; class=&quot;headerlink&quot; title=&quot;Hadoop介绍&quot;&gt;&lt;/a&gt;Hadoop介绍&lt;/h1&gt;&lt;p&gt;Apache H</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="Hadoop" scheme="https://superlova.github.io/tags/Hadoop/"/>
    
    <category term="Big Data" scheme="https://superlova.github.io/tags/Big-Data/"/>
    
    <category term="Distributed System" scheme="https://superlova.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>【学习笔记】计算二进制中1的个数</title>
    <link href="https://superlova.github.io/2021/05/03/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%AE%A1%E7%AE%97%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0/"/>
    <id>https://superlova.github.io/2021/05/03/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%AE%A1%E7%AE%97%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0/</id>
    <published>2021-05-03T06:30:06.000Z</published>
    <updated>2022-08-02T13:32:34.818Z</updated>
    
    <content type="html"><![CDATA[<p>二进制中1的个数的两种计算方法</p><!--more---><p>第一种解法，<code>n &amp;= (n - 1);</code>这一句消灭了二进制末尾的1。循环次数与二进制中1的个数相同。</p><pre><code class="lang-cpp">int hammingWeight(uint32_t n) &#123;    int count = 0;    while (n != 0) &#123;        n &amp;= (n - 1);        ++count;    &#125;    return count;&#125;</code></pre><p>第二种解法，是把性能挖掘到极致的解法：</p><pre><code class="lang-cpp">size_t hammingWeight(uint64_t V) &#123;    V -= ((V &gt;&gt; 1) &amp; 0x5555555555555555); // 010101010101    V = (V &amp; 0x3333333333333333) + ((V &gt;&gt; 2) &amp; 0x3333333333333333);    return ((V + (V &gt;&gt; 4) &amp; 0xF0F0F0F0F0F0F0F) * 0x101010101010101) &gt;&gt; 56;&#125;</code></pre><p>只依靠位运算，不进行条件判断，方便并行计算。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;二进制中1的个数的两种计算方法&lt;/p&gt;
&lt;!--more---&gt;
&lt;p&gt;第一种解法，&lt;code&gt;n &amp;amp;= (n - 1);&lt;/code&gt;这一句消灭了二进制末尾的1。循环次数与二进制中1的个数相同。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;lang-cpp&quot;&gt;in</summary>
      
    
    
    
    <category term="notes" scheme="https://superlova.github.io/categories/notes/"/>
    
    
    <category term="leetcode" scheme="https://superlova.github.io/tags/leetcode/"/>
    
    <category term="binary" scheme="https://superlova.github.io/tags/binary/"/>
    
  </entry>
  
</feed>
