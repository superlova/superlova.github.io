<!DOCTYPE html>
<html lang="zh-CN">





<head><!-- hexo injector head_begin start --><!--Google tag(gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-6Q0B58X6TL"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date());gtag("config","G-6Q0B58X6TL");</script><!-- hexo injector head_begin end -->
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="Superlova">
  <meta name="keywords" content="">
  <title>【学习笔记】Ernie 模型 - Superlova</title>
  <meta name="google-site-verification" content="CPvPw8mUZw65A1ALJ8uRzsYECq4mXgE4_eCq40VIhPM" />
  <link  rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/5.12.1/css/all.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/mdbootstrap/4.13.0/css/mdb.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/3.0.1/github-markdown.min.css" />

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link  rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css" />

<link  rel="stylesheet" href="/css/main.css" />


  <link defer rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />


<!-- 自定义样式保持在最底部 -->


<meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="Superlova" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Superlova</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">首页</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">归档</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/img/cover_2.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
                <p class="mt-3 post-meta">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>
                  星期日, 十一月 20日 2022, 4:42 下午
                </p>
              

              <p class="mt-1">
                
                  
                  <span class="post-meta">
                    <i class="far fa-chart-bar"></i>
                    2.7k 字
                  </span>
                

                
                  
                  <span class="post-meta">
                      <i class="far fa-clock"></i>
                      10 分钟
                  </span>
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  <span id="busuanzi_container_page_pv" class="post-meta" style="display: none">
                    <i class="far fa-eye" aria-hidden="true"></i>
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>
                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5 z-depth-3" id="board">
          <div class="post-content mx-auto" id="post">
            
              <p
                class="note note-warning">本文最后更新于：星期一, 十一月 21日 2022, 1:38 凌晨</p>
            
            <div class="markdown-body">
              <!--more--->
<p>文心大模型ERNIE是百度发布的产业级知识增强大模型。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/ERNIE">https://github.com/PaddlePaddle/ERNIE</a></p>
<h2 id="Roadmap"><a href="#Roadmap" class="headerlink" title="Roadmap"></a>Roadmap</h2><div class="table-container">
<table>
<thead>
<tr>
<th>时间</th>
<th>发布模型</th>
</tr>
</thead>
<tbody>
<tr>
<td>2019.3.16</td>
<td>ERNIE 1.0</td>
</tr>
<tr>
<td>2019.7.7</td>
<td>ERNIE 2.0</td>
</tr>
<tr>
<td>2021.5.20</td>
<td>多粒度语言知识模型 ERNIE-Gram、超长文本双向建模预训练模型ERNIE-Doc ，以及其他图文多模态模型</td>
</tr>
<tr>
<td>2021.12.3</td>
<td>多语言 ERNIE-M</td>
</tr>
<tr>
<td>2022.5.20</td>
<td>ERNIE 3.0 系列: 110M参数通用模型ERNIE 3.0 Base、280M参数重量级通用模型ERNIE 3.0 XBase、74M轻量级通用模型ERNIE 3.0 Medium</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Ernie-1-0"><a href="#Ernie-1-0" class="headerlink" title="Ernie 1.0"></a>Ernie 1.0</h2><p>Ernie: Enhanced representation through knowledge integration</p>
<p>相较于 BERT，Ernie 1.0 在模型结构上没有改动，都是采用若干 Transformer 的 Encoder 结构。</p>
<p>Ernie 1.0 的改动在预训练任务。</p>
<h3 id="1-Knowledge-Masking"><a href="#1-Knowledge-Masking" class="headerlink" title="1. Knowledge Masking"></a>1. Knowledge Masking</h3><p>BERT 的预训练采用 masked language-model（MLM）的方法，即在训练的时候随即从输入预料上mask掉一些单词，然后通过的上下文预测这些单词，该任务非常像我们在中学时期经常做的完形填空；以及Next Sentence Prediction（NSP）的方法，即判断连个句子是否是具有前后顺承关系的两句话。</p>
<p>ERNIE提出了<code>Knowledge Masking</code>的策略，其包含三个级别：ERNIE将Knowledge分成了三个类别：<code>token级别(Basic-Level)</code>、<code>短语级别(Phrase-Level)</code> 和 <code>实体级别(Entity-Level)</code>。通过对这三个级别的对象进行Masking，提高模型对字词、短语的知识理解。</p>
<p>Basic-Level Masking 就是最基础的 masking 任务，和 BERT 一样，随机选取某些单词进行 mask，令模型来预测这些 mask 是什么词。</p>
<p>Phrase-Level Masking 是对语句中的短语进行 masking，如 a series of；</p>
<p>Entity-Level Masking 是对语句中的实体词进行 masking，如人名 J. K. Rowling。</p>
<p><img src="/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ernie-%E6%A8%A1%E5%9E%8B/knowledge_masking.png" srcset="/img/loading.gif" alt></p>
<h3 id="2-Dialogue-Language-Model"><a href="#2-Dialogue-Language-Model" class="headerlink" title="2. Dialogue Language Model"></a>2. Dialogue Language Model</h3><p>在 Bert 已有的预训练任务中，加入了 Dialogue Language Model 任务，使 ERNIE 学习到对话中的隐含关系，增加模型的语义表达能力。</p>
<p>在生成预训练数据的时候，有一定几率用另外的句子替代里面的问题和答案，所以模型还要预测是否是真实的问答对。</p>
<p><img src="/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ernie-%E6%A8%A1%E5%9E%8B/dialogue_language_model.png" srcset="/img/loading.gif" alt></p>
<p>DLM 中，不再构建如同 “[CLS] + Sentence_A + [SEP] + Sentence_B + [SEP]” 的句子对，</p>
<p>而是如同 “[CLS] + Query + [SEP] + Response_A + [SEP] + Response_B + [SEP]” 的对话三元组；</p>
<p>是否上下文连续的二分类训练目标转为预测该对话是否真实 (real/fake)。</p>
<p>三元组随机地采用 QRQ、QRR、QQR 其中一种构建形式，上面的例子便是其中的 QRR。</p>
<p>为了给处理后的数据添加噪声，部分的 Query 和 Response 使用不相关的语句进行随机替换，以强制模型学习对话中的语境关联。</p>
<p>在训练一般语料时，ERNIE 采用 Knowledge Masking Strategies 改造后的 Masked LM；而在训练对话型语料时，ERNIE 采用 DLM；两者交替使用。</p>
<h3 id="3-添加数据"><a href="#3-添加数据" class="headerlink" title="3. 添加数据"></a>3. 添加数据</h3><p>Ernie 还采用多个来自不同源头的数据，比如百度贴吧，百度新闻，维基百科等等帮助模型训练。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011150266/article/details/116479149">https://blog.csdn.net/u011150266/article/details/116479149</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/shona/p/11820492.html">https://www.cnblogs.com/shona/p/11820492.html</a></p>
<h2 id="Ernie-2-0"><a href="#Ernie-2-0" class="headerlink" title="Ernie 2.0"></a>Ernie 2.0</h2><p>Ernie 2.0: A continual pre-training framework for language understanding</p>
<h3 id="1-总体改动"><a href="#1-总体改动" class="headerlink" title="1. 总体改动"></a>1. 总体改动</h3><p>ERNIE 2.0 将 1.0 版本中的功能特性全部予以保留，并在此基础上做更为丰富的扩展和延伸。</p>
<p>Ernie 2.0 添加了很多其他的自监督学习方法进行训练。该思想与 ALBERT（SOP）、SpanBERT（SBO）类似。</p>
<p>但是一味堆叠任务可能导致各种任务都学不好，Ernie 2.0 提出了一个持续学习的框架，模型可以持续添加任务但又不降低之前任务的精度。</p>
<p>目标是能够更好更有效地获得词法lexical，句法syntactic，语义semantic上的表达。</p>
<p><img src="/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ernie-%E6%A8%A1%E5%9E%8B/ernie2_0_structure.png" srcset="/img/loading.gif" alt></p>
<h3 id="2-添加预训练任务"><a href="#2-添加预训练任务" class="headerlink" title="2. 添加预训练任务"></a>2. 添加预训练任务</h3><p><strong>1）Word-aware Pretraining Tasks</strong>，捕捉词汇 (lexical) 级别的信息</p>
<p>Knowledge Masking Task：沿袭 ERNIE 1.0 中的 Knowledge Masking Strategies，预测被 Mask 的对象。</p>
<p>Capitalization Prediction Task：预测对象是否大小写 (cased/uncased)；ERNIE 2.0 认为大写开头的词汇具备特定的含义，例如人名、地名、机构名等，这将对命名实体识别一类的下游任务有所帮助。</p>
<p>Token-Document Relation Prediction Task：预测对象是否在文档中其他文段有出现；正案例通常包含文档的关键词以及语言通用词，因此添加这一任务有助于模型将更多的注意力放在这一类词汇上。</p>
<p><strong>2）Structure-aware Pretraining Tasks</strong>，捕捉语料中语法 (syntactic) 级别的信息</p>
<p>Sentence Recording Task：针对文档中的每一个段落，以句子为单位划分为 1~m 段，而后对整个文档所有文段进行打乱排列，对每一个文段预测原始位置，成为 k 分类问题。</p>
<p>Sentence Distance Task：取代 Next Sentence Prediction，预测输入句子对的相对距离；三分类问题，0 代表两个句子在同一篇文档且距离相近，1 代表两个句子在同一片文档但距离较远，2 代表两个句子在不同文档。</p>
<p><strong>3）Semantic-aware Pretraining Tasks</strong>，提取语义 (semantic) 类的信息</p>
<p>Discourse Relation Task：预测两个句子之间的语法及修辞关联。</p>
<p><img src="/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ernie-%E6%A8%A1%E5%9E%8B/discourse_relation_task.png" srcset="/img/loading.gif" alt></p>
<p>IR Relevance Task：专门为百度搜索引擎日志设计的任务，预测 Query-Title 对的相关性；三分类问题，0、1、2 分别代表强相关、弱相关以及不相关；这将有助于标题自动生成以及文本摘要类任务。</p>
<h3 id="3-预训练框架"><a href="#3-预训练框架" class="headerlink" title="3. 预训练框架"></a>3. 预训练框架</h3><p>任务叠加，不是一次性进行的（Multi-task learning），而是持续学习(Continual Pre-training)，所以必须避免模型在学了新的任务之后，忘记旧的任务，即在旧的任务上loss变高。</p>
<p>为此，ERNIE 2.0 首次引入 连续预训练机制 —— 以串行的方式进行多任务学习。</p>
<p>3、Sequential Multi-task Learning：ERNIE 2.0中新提出的方法，每当有新任务出现时，使用先前学习的参数来初始化模型，并同时训练新引入的任务和原始任务。这样解决了前两种方法的问题，可以随时引入新任务，并保留先前学到的知识。</p>
<p><img src="/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ernie-%E6%A8%A1%E5%9E%8B/ernie2_0_framework.png" srcset="/img/loading.gif" alt></p>
<h3 id="参考-1"><a href="#参考-1" class="headerlink" title="参考"></a>参考</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_35386727/article/details/105947650">https://blog.csdn.net/qq_35386727/article/details/105947650</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/460151166">https://zhuanlan.zhihu.com/p/460151166</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43269174/article/details/98437096">https://blog.csdn.net/weixin_43269174/article/details/98437096</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2020-04-28-3">https://www.jiqizhixin.com/articles/2020-04-28-3</a></p>
<p><a target="_blank" rel="noopener" href="https://aijishu.com/a/1060000000199742">https://aijishu.com/a/1060000000199742</a></p>
<h2 id="Ernie-gram-和-Ernie-doc"><a href="#Ernie-gram-和-Ernie-doc" class="headerlink" title="Ernie gram 和 Ernie doc"></a>Ernie gram 和 Ernie doc</h2><p>ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding</p>
<p>ERNIE-Doc: A Retrospective Long-Document Modeling Transformer¶</p>
<h3 id="1-Ernie-Gram-的改进"><a href="#1-Ernie-Gram-的改进" class="headerlink" title="1. Ernie-Gram 的改进"></a>1. Ernie-Gram 的改进</h3><p>很简单，即在 MLM 任务中，把最小 mask 单位从字粒度变成 n-gram 粒度。这样做的理由是原有的 masking 粒度太细了，切分的时候容易破坏语义完整性。</p>
<p>因此 Ernie-Gram 直接去预测一个n-gram词，而不是预测一系列连续的token，从而保证n-gram词的语义完整性。</p>
<p>ERNIE-Gram主要提出了两种融合方式：Explictly N-gram MLM 和 Comprehensive N-gram Prediction。</p>
<p>此外，Ernie-Gram 还使用了RTD预训练任务，来识别每个token是否是生成的。</p>
<h3 id="2-Ernie-Doc-的改进"><a href="#2-Ernie-Doc-的改进" class="headerlink" title="2. Ernie-Doc 的改进"></a>2. Ernie-Doc 的改进</h3><p>Ernie-Doc 可以对更长的输入序列进行建模。</p>
<h3 id="参考-2"><a href="#参考-2" class="headerlink" title="参考"></a>参考</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/376000666">https://zhuanlan.zhihu.com/p/376000666</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/jokerxsy/article/details/116482035">https://blog.csdn.net/jokerxsy/article/details/116482035</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sohu.com/a/468508378_129720">https://www.sohu.com/a/468508378_129720</a></p>
<p><a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/pretrain_model/ERNIE-Gram.html">https://paddlepedia.readthedocs.io/en/latest/tutorials/pretrain_model/ERNIE-Gram.html</a></p>
<h2 id="Ernie-3-0"><a href="#Ernie-3-0" class="headerlink" title="Ernie 3.0"></a>Ernie 3.0</h2><p>ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation</p>
<p>ERNIE 3.0同时结合了将自回归和自编码网络，从而模型在文本生成和语言理解任务表现均很好。另外，ERNiE 3.0在预训练阶段中引入了知识图谱数据。</p>
<h3 id="1-网络结构改进"><a href="#1-网络结构改进" class="headerlink" title="1. 网络结构改进"></a>1. 网络结构改进</h3><p>ERNIE 3.0设计了上下两层网络结构：Universal Representation Module 和 Task-specific Representation Module。</p>
<p><img src="/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ernie-%E6%A8%A1%E5%9E%8B/ernie3_0_structure.png" srcset="/img/loading.gif" alt></p>
<p>在模型底层结构上共享参数，用于获取词汇、句法等通用的抽象特征；在顶层对不同任务使用独立参数，用于获取特定任务的特征。</p>
<p>ERNIE 3.0认为不同的任务模式依赖的自然语言的底层特征是相同的，比如词法和句法信息，然而不同的任务模式需要的上层具体的特征是不同的。自然语言理解的任务往往倾向于学习语义连贯性，然而自然语义生成任务却期望能够看见更长的上下文信息。</p>
<p>由于自回归模型在文本生成任务上表现更好，自编码模型在语言理解任务上表现更好。因此，ERNIE 3.0在上层使用了两个网络，一个用于聚焦自然语言理解，一个用于聚焦自然语言生成任务。不同任务适配更合适的网络，能够提高模型在相应任务上的表现。</p>
<p>在fine-tuning阶段，可以固定Universal Representation Module，只微调Task-specific Representation Module参数，提高训练效率。</p>
<p><strong>1）Universal Representation Module</strong></p>
<p>使用Transformer-XL作为骨干网络，Transformer-XL允许模型使用记忆循环机制建模更长的文本序列依赖。</p>
<p>Transformer-XL 在普通 transformer 基础上增加了一个 recurrence memory module 用于建模长文本，避免了普通 transformer 中由于输入长度限制，不得不对长文本进行切分从而导致损失长距离依赖的问题。</p>
<p><strong>2）Task-specific Representation Module</strong></p>
<p>同样使用了Transformer-XL作为骨干网络，但使用了base size模型。</p>
<h3 id="2-引入新的预训练任务"><a href="#2-引入新的预训练任务" class="headerlink" title="2. 引入新的预训练任务"></a>2. 引入新的预训练任务</h3><p>除了 Ernie 2.0 引入的很多个预训练任务之外，Ernie 3.0 还增加了 universal knowledge-text prediction（UKTP）任务</p>
<p>给定一个三元组\<head, relation, tail\>和一个句子，ERNIE 3.0会mask掉三元组中的实体关系relation，或者句子中的单词word，然后让模型去预测这些内容。</head,></p>
<p>模型在预测三元组中的关系时，需要找到对应句子中的head和tail词并确定他们之间的语义关系；而模型在预测句子中的词时，则需要同时考虑句子中的依存信息 (dependency information) 和三元组中的逻辑关系 (logical relationship)。</p>
<p>这个任务之所以有效，是因为我们假设：如果一个句子中同时出现head和tail两个实体，则这个句子能够表达这两个实体的关系。</p>
<p><img src="/2022/11/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ernie-%E6%A8%A1%E5%9E%8B/universal_knowledge_text_pred.png" srcset="/img/loading.gif" alt></p>
<h3 id="3-预训练-Trick"><a href="#3-预训练-Trick" class="headerlink" title="3. 预训练 Trick"></a>3. 预训练 Trick</h3><p>为了加快预训练过程中的模型收敛速度，本文提出了逐渐增加训练正则因子 (training regularization factor) 的方法。</p>
<p>具体来说就是在训练过程中逐步且同时增加输入序列长度、batch size、学习率和dropout rate。</p>
<p>在一般的transformer训练中都会使用warm-up策略逐步增加学习率，本文提出了将batch size等其他因子同时增加的策略。</p>
<h3 id="参考-3"><a href="#参考-3" class="headerlink" title="参考"></a>参考</h3><p><a target="_blank" rel="noopener" href="https://alexyxwang.com/2021/12/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ERNIE-3-0-Large-scale-Knowledge-Enhanced-Pre-training-for-Language-Understanding-and-Generation/">https://alexyxwang.com/2021/12/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-ERNIE-3-0-Large-scale-Knowledge-Enhanced-Pre-training-for-Language-Understanding-and-Generation/</a></p>

            </div>
            <hr>
            <div>
              <p>
                
                  <span>
                <i class="iconfont icon-inbox"></i>
                    
                      <a class="hover-with-bg" href="/categories/notes/">notes</a>
                      &nbsp;
                    
                  </span>&nbsp;&nbsp;
                
                
                  <span>
                <i class="iconfont icon-tag"></i>
                    
                      <a class="hover-with-bg" href="/tags/Ernie/">Ernie</a>
                    
                      <a class="hover-with-bg" href="/tags/BERT/">BERT</a>
                    
                  </span>
                
              </p>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
              
            </div>

            
              <!-- Comments -->
              <div class="comments" id="comments">
                
                
  <script defer src="https://utteranc.es/client.js"
          repo="superlova / superlova.github.io"
          issue-term="pathname"
  
          label="utterances"
    
          theme="github-light"
          crossorigin="anonymous"
  >
  </script>


              </div>
            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc-start"></div>
<div id="toc">
  <p class="h5"><i class="far fa-list-alt"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
<div>
  <span id="timeDate">载入天数...</span>
  <span id="times">载入时分秒...</span>
  <script>
  var now = new Date();
  function createtime(){
      var grt= new Date("02/14/2017 00:00:00");//此处修改你的建站时间或者网站上线时间
      now.setTime(now.getTime()+250);
      days = (now - grt ) / 1000 / 60 / 60 / 24;
      dnum = Math.floor(days);
      hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum);
      hnum = Math.floor(hours);
      if(String(hnum).length ==1 ){
          hnum = "0" + hnum;
      }
      minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
      mnum = Math.floor(minutes);
      if(String(mnum).length ==1 ){
                mnum = "0" + mnum;
      }
      seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
      snum = Math.round(seconds);
      if(String(snum).length ==1 ){
                snum = "0" + snum;
      }
      document.getElementById("timeDate").innerHTML = "本站安全运行&nbsp"+dnum+"&nbsp天";
      document.getElementById("times").innerHTML = hnum + "&nbsp小时&nbsp" + mnum + "&nbsp分&nbsp" + snum + "&nbsp秒";
  }
  setInterval("createtime()",250);
  </script>
</div>

<p id="hitokoto">:D 获取中...</p>
<script>
  fetch('https://v1.hitokoto.cn')
    .then(response => response.json())
    .then(data => {
      const hitokoto = document.getElementById('hitokoto')
      hitokoto.innerText = data.hitokoto
      })
      .catch(console.error)
</script>

  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    </div>
    
  <div>
    
      <!-- 不蒜子统计PV -->
      
      <span id="busuanzi_container_site_pv" style="display: none">
      总访问量 <span id="busuanzi_value_site_pv"></span> 次
    </span>
    
    
      <!-- 不蒜子统计UV -->
      
      <span id="busuanzi_container_site_uv" style="display: none">
      总访客数 <span id="busuanzi_value_site_uv"></span> 人
    </span>
    
  </div>


    

    
  </div>
</footer>



<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/popper.js/1.16.1/umd/popper.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="https://cdn.staticfile.org/mdbootstrap/4.13.0/js/mdb.min.js" ></script>
<script  src="/js/main.js" ></script>


  <script  src="/js/lazyload.js" ></script>



  
  <script  src="https://cdn.staticfile.org/tocbot/4.10.0/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var navHeight = $('#navbar').height();
      var post = $('#post');
      var toc = $('#toc');
      var tocLimMax = post.offset().top + post.height() - navHeight;

      $(window).scroll(function () {
        var tocLimMin = $('#toc-start').offset().top - navHeight;
        var scroH = document.body.scrollTop + document.documentElement.scrollTop;

        if (tocLimMin <= scroH && scroH <= tocLimMax) {
          toc.css({
            'display': 'block',
            'position': 'fixed',
            'top': navHeight,
          });
        } else if (scroH <= tocLimMin) {
          toc.css({
            'position': '',
            'top': '',
          });
        } else if (scroH > tocLimMax) {
          toc.css('display', 'none');
        }
      });
      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '.post-content',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        scrollSmooth: true,
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc > p').css('visibility', 'visible');
      }
      var offset = $('#board-ctn').css('margin-right')
      $('#toc-ctn').css({
        'right': offset
      })
    });
  </script>







  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




<!-- Plugins -->


  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?1040bcc5d25d5f4a9cb5e9855eb2c6db";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  



  <script  src="https://cdn.staticfile.org/prettify/188.0.0/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "【学习笔记】Ernie 模型&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 90,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script defer src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>



  

  
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
              processEscapes: true,
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
          }
      });

      MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });

    </script>

    <script  src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" ></script>

  










<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
