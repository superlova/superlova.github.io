<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="Superlova">
  <meta name="keywords" content="">
  <title>【学习笔记】李宏毅ML-Note6-注意力机制 - Superlova</title>
  <meta name="google-site-verification" content="CPvPw8mUZw65A1ALJ8uRzsYECq4mXgE4_eCq40VIhPM" />
  <link  rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/5.12.1/css/all.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/mdbootstrap/4.13.0/css/mdb.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/3.0.1/github-markdown.min.css" />

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link  rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css" />

<link  rel="stylesheet" href="/css/main.css" />


  <link defer rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />


<!-- 自定义样式保持在最底部 -->


<meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="Superlova" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Superlova</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">首页</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">归档</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/img/cover_2.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
                <p class="mt-3 post-meta">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>
                  星期日, 十月 23日 2022, 9:57 晚上
                </p>
              

              <p class="mt-1">
                
                  
                  <span class="post-meta">
                    <i class="far fa-chart-bar"></i>
                    3.3k 字
                  </span>
                

                
                  
                  <span class="post-meta">
                      <i class="far fa-clock"></i>
                      11 分钟
                  </span>
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  <span id="busuanzi_container_page_pv" class="post-meta" style="display: none">
                    <i class="far fa-eye" aria-hidden="true"></i>
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>
                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5 z-depth-3" id="board">
          <div class="post-content mx-auto" id="post">
            
              <p
                class="note note-warning">本文最后更新于：星期日, 十月 30日 2022, 3:40 凌晨</p>
            
            <div class="markdown-body">
              <p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：自注意力机制。<br><!--more---></p>
<h2 id="一、输入从图像到序列"><a href="#一、输入从图像到序列" class="headerlink" title="一、输入从图像到序列"></a>一、输入从图像到序列</h2><p>图像分类任务的输入是固定的，比如都是28*28像素的黑白图片等等。但是一些输入和输出不是定长序列的任务，比如机器翻译、语音转文字等任务，传统模型在这些任务上的表现不好。</p>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/不同输出对应不同任务.png" srcset="/img/loading.gif" alt></p>
<p>比如词性标注任务，模型的输入是N个向量，输出是N个分类标签；比如文本分类任务，模型的输入是N个向量，输出只是一个分类标签；比如文本翻译任务，模型的输入是N个向量，输出可能是N’个向量，此时N’是未知的，需要机器自己进行判断和学习；这种任务叫做 sequences to sequences (seq2seq) 。</p>
<p>这些任务的输入特点是，都有多个向量作为一个输入，且向量的相对位置会影响输入的含义。</p>
<p>以词性标注任务举例，该任务的输入和输出向量个数都为N。我们首先把一句话切分为N个词，然后利用一些编码方式（比如 one-hot 或者 word2vec）将其变成N个定长向量。我们将这N个向量作为一个输入序列，输入到模型中，寄希望于模型能够把每个词的词性标注出来。所以这其实是一个分类任务。</p>
<p>但是当我们输入句子”I saw a saw.“时，前后两个saw的词性是不同的，如果模型只以词汇向量为输入，不考虑上下文的话，是无法得知这件事的。</p>
<p>为此，前人们做了以下改进，试图使用传统模型解决这个问题：</p>
<ol>
<li><p>通过 N-gram 模型，将两三个词打包成一个新词，”I saw a saw“经过3-gram模型的编码，会产生如下输入序列：</p>
<pre><code> I saw a saw
 -&gt;
 [i saw a], [saw a saw]
</code></pre><p> 由此，模型能够看到的上下文就扩展到了三个单词。但是该种方法会快速扩大单词量，增加计算负担，且无法把距离较远的上下文也加入进来。</p>
</li>
<li><p>使用TextCNN</p>
<p> TextCNN 是把图像领域大货成功的CNN模型的经验移植到了NLP领域的成果，它使用一维卷积核。但究其本质还是受限于卷积核的大小。</p>
</li>
</ol>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/向量之间的相对位置.png" srcset="/img/loading.gif" alt></p>
<p>有没有一种方法能够考虑输入向量的全部上下文呢？ self-attention 可以做到。</p>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention与FC的区别.png" srcset="/img/loading.gif" alt></p>
<p>self-attention 接受N个输入向量，输出N个输出向量。输出的向量不仅保留了原来向量的语义信息，还额外添加了该向量上下文的信息。</p>
<p>有了这个全新的向量，我们便不必使用诸如 n-gram 等特征工程手段了，self-attention 就能够产生非常好的稠密向量，该向量考虑的上下文范围是整篇文档。</p>
<p>self-attention 适合编码整篇文档的信息，产出向量后可以接一个全连接网络来进行具体的分类；也可以像之前的卷积网络一样，多层 self-attention + FC全连接进行堆叠。这实际上就是Transformer的基本思想。</p>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/transformer的简化结构.png" srcset="/img/loading.gif" alt></p>
<p>self-attention 是怎么做到的呢？</p>
<h2 id="二、self-attention-的实现原理"><a href="#二、self-attention-的实现原理" class="headerlink" title="二、self-attention 的实现原理"></a>二、self-attention 的实现原理</h2><p>self-attention 的输入是N个向量组成的输入序列，输出也是N个向量组成的序列。区别在于，输入的向量本身不包含任何与上下文有关的信息，但是与输入向量对应的输出向量会包含一定的上下文信息。</p>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention的抽象结构.png" srcset="/img/loading.gif" alt></p>
<p>如果要产生b1向量，其实是考虑了a1/a2/a3/a4全部四个向量后的结果。</p>
<p>具体地，首先我们需要考虑a1向量与其他向量a2/a3/a4之间的相关性，两个向量之间的相关性是一个标量，我们用$\alpha$来表示：</p>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理1_相关性.png" srcset="/img/loading.gif" alt></p>
<p>相关性是如何计算得到的？有很多做法可以实现相关性的计算。需要注意的是这里的<strong>相关性</strong>并非矩阵向量空间中的相似性，而是一个<strong>需要机器从数据中学习</strong>的参数。下图是两种不同的相关性的计算方式。</p>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理2_相关性的计算方法.png" srcset="/img/loading.gif" alt></p>
<p>上图左边是一种常用的向量相关性计算法，首先把两个向量各自乘以一个参数矩阵，其中输入向量会得到对应的查询向量 query，上下文向量会得到对应的关键向量 key，然后我们把query和key进行点积，得到的就是两个向量的相关性。参数矩阵是可以通过数据进行学习的。</p>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理3_相关性计算方法2.png" srcset="/img/loading.gif" alt></p>
<p>由此我们推广开来，每个输入向量都可以计算属于自己的查询向量query，以及其他输入向量的关键向量key。</p>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理4_相关性计算方法3.png" srcset="/img/loading.gif" alt></p>
<p>输入向量还需要与自己计算相关性，这样凑齐4个相关性数值。由此我们就得到了输入向量a1与其他（包含自己的）四个向量之间的相关程度。</p>
<p>接下来我们需要做的，就是把四个输入向量都与该相关性进行乘积，然后相加，就得到了包含上下文相关信息的输出向量b1。</p>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理5_相关性计算方法4.png" srcset="/img/loading.gif" alt></p>
<p>再分别对a2/a3/a4向量如法炮制，就可以分别得到四个输出向量。</p>
<p>需要注意，输入向量最后参与计算时，为了将其转化为稠密向量，也需要乘以一个矩阵后才能参与计算。</p>
<h2 id="三、self-attention-的并行计算"><a href="#三、self-attention-的并行计算" class="headerlink" title="三、self-attention 的并行计算"></a>三、self-attention 的并行计算</h2><p>向量的转化操作都是矩阵乘法，这意味着对向量进行转化的操作是可以并行计算的。</p>
<ol>
<li>通过参数学习，将输入向量（一般是稀疏的）转化为query/key/value（稠密向量），然后将向量拼合形成矩阵，得到矩阵Q、K、V：</li>
</ol>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention的并行计算1.png" srcset="/img/loading.gif" alt></p>
<ol>
<li>矩阵Q与K做点乘，得到相关性系数矩阵A</li>
</ol>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention-parallel-2.png" srcset="/img/loading.gif" alt></p>
<ol>
<li>相关性矩阵A经过softmax进行概率归一化后，与矩阵V进行点乘，得到输出向量B</li>
</ol>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention-parallel-3.png" srcset="/img/loading.gif" alt></p>
<p><strong>self-attention将输入向量转化为输出向量的过程到此结束。整个过程总结如下</strong>：</p>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention-parallel-summary.png" srcset="/img/loading.gif" alt></p>
<h2 id="四、多头注意力-Multihead-Self-attention"><a href="#四、多头注意力-Multihead-Self-attention" class="headerlink" title="四、多头注意力 Multihead Self-attention"></a>四、多头注意力 Multihead Self-attention</h2><p>多头注意力机制，就是同时学习多种相关性的机制。</p>
<p>这样做的理由是：语言是存在同义词、一词多义等现象的，即便是同一句话，在不同语境下阐述也会产生不同的效果。如果只算一种相关性，模型便无法掌握一词多义等能力，因此我们需要学习多个相关性矩阵A。</p>
<p>如果想要学习多个相关性矩阵A，</p>
<ol>
<li>就必须有多个输入矩阵Q/K/V与之对应，每多一个A，就要多学习一类参数矩阵；</li>
<li>会产生多组输出矩阵B，需要再学习一个参数矩阵，将一系列的输出矩阵合并起来；</li>
</ol>
<p>多头注意力机制在原有注意力机制的基础上，把原有的query向量再多乘n个参数矩阵，将query复制为n个子向量；key和value向量亦是如此：</p>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/multi-head-self-attention-1.png" srcset="/img/loading.gif" alt></p>
<p>query向量的第一个复制体会与各个key向量的第一个复制体进行计算相关性，生成相关性矩阵A1，第二个复制体会与各个key向量的第二个复制体计算相关性，生成相关性矩阵A2，以此类推。</p>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/multi-head-self-attention-2.png" srcset="/img/loading.gif" alt></p>
<p>由此会产出n份不同的输出向量，他们分别是考虑n种不同相关性下计算出来的输出向量。使用一个参数矩阵将他们合并起来：</p>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/multi-head-self-attention-3.png" srcset="/img/loading.gif" alt></p>
<p>这就是多头注意力机制的设计思路。如果要计算N种相关性，就叫做N头注意力机制。</p>
<h2 id="五、位置编码"><a href="#五、位置编码" class="headerlink" title="五、位置编码"></a>五、位置编码</h2><p>上面的机制只介绍了 self-attention 如何把输入向量和它的上下文信息进行编码，但是忽略了上下文也是有位置关系的，理论上离当前位置更近的上下文应该获得更多注意力。因此我们需要额外添加一个能够表示位置的参数。</p>
<p>做法也很简单，只需要在输入层为每个向量添加一个额外的位置向量即可。</p>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/positional-encoding-1.png" srcset="/img/loading.gif" alt></p>
<p>这个位置向量可以是你自己定义的，也可以作为参数放到网络中进行学习。总之它理论上应该能够表示位置的远近关系。</p>
<h2 id="六、self-attention-的具体应用"><a href="#六、self-attention-的具体应用" class="headerlink" title="六、self-attention 的具体应用"></a>六、self-attention 的具体应用</h2><p>上面我们是以文本为例介绍的self-attention，实际上有很多其他任务也可以用self-attention来解决（虽然不见得是最佳方案）。</p>
<p>语音识别任务里，通过将一段音频按照一定时间窗口进行切分，也可以得到一个包含很多向量的输入序列；则处理方法与文本任务大同小异，只不过语音分割时产生的输入序列会非常长，此时需要对计算过程加以优化。例子：Truncate Self-Attention</p>
<p>图像识别任务里，每个图像按照像素进行分割，并逐行遍历，也可以形成一个序列，这个序列里的每个输入向量是(R,G,B)的三维向量。例子：Self-Attention GAN</p>
<h2 id="七、其他变种-self-attention"><a href="#七、其他变种-self-attention" class="headerlink" title="七、其他变种 self-attention"></a>七、其他变种 self-attention</h2><h2 id="八、总结和提问"><a href="#八、总结和提问" class="headerlink" title="八、总结和提问"></a>八、总结和提问</h2><h3 id="1-Self-Attention-的核心思想？"><a href="#1-Self-Attention-的核心思想？" class="headerlink" title="1. Self-Attention 的核心思想？"></a>1. Self-Attention 的核心思想？</h3><p>通过综合考虑输入词与上下文之间的相关关系，得到包含上下文语义信息的输出向量。</p>
<h3 id="2-Self-attention-是如何计算的？"><a href="#2-Self-attention-是如何计算的？" class="headerlink" title="2. Self-attention 是如何计算的？"></a>2. Self-attention 是如何计算的？</h3><script type="math/tex; mode=display">
\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V</script><p>其中Q、K、V都是从输入向量学习而来的稠密向量组成的矩阵，它们不包含上下文的语义信息；</p>
<ul>
<li>Q：查询向量，目标字作为 Query；</li>
<li>K：键向量，其上下文的各个字作为 Key；</li>
<li>V：值向量，上下文各个字的 Value；</li>
</ul>
<p>$d_k$是Q/K/V的维度。这样做是为了降低分量之间的方差，防止输入向量的维度过高导致点击出来的结果过大，使softmax出来的结果出现一些极端情况（比如只有一个分量是0.99999，其他分量都是0），进而导致训练困难的现象。</p>
<h3 id="3-Self-Attention-和-CNN-的异同？"><a href="#3-Self-Attention-和-CNN-的异同？" class="headerlink" title="3. Self-Attention 和 CNN 的异同？"></a>3. Self-Attention 和 CNN 的异同？</h3><p>CNN是一类特殊的Attention，即将注意力聚焦于感受野（卷积核）中的一种self-attention网络结构。</p>
<p>而self-attention的感受野范围是整个序列，可以自行学习哪些是需要重点关注的。</p>
<p>这也就意味着self-attention相较于CNN而言更复杂、参数更多，需要更多数据进行训练。</p>
<p>有讨论二者关系的论文：</p>
<blockquote>
<p>On the Relationship between Self-Attention and Convolutional Layers</p>
</blockquote>
<p>从所需的数据量和准确率比较上，可以辅证这一点：</p>
<p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention-versus-cnn.png" srcset="/img/loading.gif" alt></p>
<p>那就是在数据量较小的情况下，CNN作为简单模型可以表现得更好；但是数据量充足时，self-attention具有更高的上限。</p>
<h3 id="4-Self-Attention-与-RNN-的比较？"><a href="#4-Self-Attention-与-RNN-的比较？" class="headerlink" title="4. Self-Attention 与 RNN 的比较？"></a>4. Self-Attention 与 RNN 的比较？</h3><p>RNN虽然也能够学习长距离依赖关系，但是它的结构和运作方式（要想计算b2，必须先计算b1，存在先后依赖关系）决定了它难以并行化，这就导致它的训练过程非常缓慢。</p>
<p>另外RNN在输入序列过长时，也存在梯度消失或者梯度爆炸的问题，导致无法记忆长距离信息。</p>
<p>Self-Attention的内部有大量矩阵乘法运算，可以被GPU优化的很快。</p>
<h3 id="5-MultiHead-Attention-的生效原理？"><a href="#5-MultiHead-Attention-的生效原理？" class="headerlink" title="5. MultiHead Attention 的生效原理？"></a>5. MultiHead Attention 的生效原理？</h3><p>借鉴了CNN中同一卷积层内使用多个卷积核的思想。类似于CNN中通过多通道机制进行特征选择。</p>
<p>Transformer中使用切头(split)的方法，是为了在不增加复杂度（$O(n^2 d)$）的前提下享受类似CNN中“不同卷积核”的优势。</p>
<p>在每个头的计算过程中，彼此之间相互独立，参数不共享，仅在最后将结果拼接起来，这样可以允许模型在不同的表示子空间里学习到相关的信息。</p>
<p>最后整合多个向量，把他们降维到一个向量的长度，这个过程也是在学习”到底哪个头学到的知识是有效的”。</p>
<h3 id="6-位置编码的计算方法？"><a href="#6-位置编码的计算方法？" class="headerlink" title="6. 位置编码的计算方法？"></a>6. 位置编码的计算方法？</h3><p>Attention is all you need 论文中的位置编码的实现方式如下：</p>
<script type="math/tex; mode=display">
\text{PosEncoding}_{(\text{pos}, 2i)}=\sin{(\frac{\text{pos}}{10000^{2i/d_{model}}})} \\
\text{PosEncoding}_{(\text{pos}, 2i+1)}=\cos{(\frac{\text{pos}}{10000^{2i/d_{model}}})}</script><p>其中pos表示词在句子中的位置，i则表示向量的分量。这种计算方法无需学习任何参数。</p>
<p>在BERT论文中，采用的位置编码就变成了Embedding的方法自动学习得到。</p>
<h2 id="附、self-attention-的代码实现"><a href="#附、self-attention-的代码实现" class="headerlink" title="附、self-attention 的代码实现"></a>附、self-attention 的代码实现</h2>
            </div>
            <hr>
            <div>
              <p>
                
                  <span>
                <i class="iconfont icon-inbox"></i>
                    
                      <a class="hover-with-bg" href="/categories/notes/">notes</a>
                      &nbsp;
                    
                  </span>&nbsp;&nbsp;
                
                
                  <span>
                <i class="iconfont icon-tag"></i>
                    
                      <a class="hover-with-bg" href="/tags/Machine-Learning/">Machine Learning</a>
                    
                      <a class="hover-with-bg" href="/tags/MOOC/">MOOC</a>
                    
                      <a class="hover-with-bg" href="/tags/Self-Attention/">Self-Attention</a>
                    
                  </span>
                
              </p>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
              
            </div>

            
              <!-- Comments -->
              <div class="comments" id="comments">
                
                
  <script defer src="https://utteranc.es/client.js"
          repo="superlova / superlova.github.io"
          issue-term="pathname"
  
          label="utterances"
    
          theme="github-light"
          crossorigin="anonymous"
  >
  </script>


              </div>
            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc-start"></div>
<div id="toc">
  <p class="h5"><i class="far fa-list-alt"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
<div>
  <span id="timeDate">载入天数...</span>
  <span id="times">载入时分秒...</span>
  <script>
  var now = new Date();
  function createtime(){
      var grt= new Date("02/14/2017 00:00:00");//此处修改你的建站时间或者网站上线时间
      now.setTime(now.getTime()+250);
      days = (now - grt ) / 1000 / 60 / 60 / 24;
      dnum = Math.floor(days);
      hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum);
      hnum = Math.floor(hours);
      if(String(hnum).length ==1 ){
          hnum = "0" + hnum;
      }
      minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
      mnum = Math.floor(minutes);
      if(String(mnum).length ==1 ){
                mnum = "0" + mnum;
      }
      seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
      snum = Math.round(seconds);
      if(String(snum).length ==1 ){
                snum = "0" + snum;
      }
      document.getElementById("timeDate").innerHTML = "本站安全运行&nbsp"+dnum+"&nbsp天";
      document.getElementById("times").innerHTML = hnum + "&nbsp小时&nbsp" + mnum + "&nbsp分&nbsp" + snum + "&nbsp秒";
  }
  setInterval("createtime()",250);
  </script>
</div>

<p id="hitokoto">:D 获取中...</p>
<script>
  fetch('https://v1.hitokoto.cn')
    .then(response => response.json())
    .then(data => {
      const hitokoto = document.getElementById('hitokoto')
      hitokoto.innerText = data.hitokoto
      })
      .catch(console.error)
</script>

  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    </div>
    
  <div>
    
      <!-- 不蒜子统计PV -->
      
      <span id="busuanzi_container_site_pv" style="display: none">
      总访问量 <span id="busuanzi_value_site_pv"></span> 次
    </span>
    
    
      <!-- 不蒜子统计UV -->
      
      <span id="busuanzi_container_site_uv" style="display: none">
      总访客数 <span id="busuanzi_value_site_uv"></span> 人
    </span>
    
  </div>


    

    
  </div>
</footer>



<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/popper.js/1.16.1/umd/popper.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="https://cdn.staticfile.org/mdbootstrap/4.13.0/js/mdb.min.js" ></script>
<script  src="/js/main.js" ></script>


  <script  src="/js/lazyload.js" ></script>



  
  <script  src="https://cdn.staticfile.org/tocbot/4.10.0/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var navHeight = $('#navbar').height();
      var post = $('#post');
      var toc = $('#toc');
      var tocLimMax = post.offset().top + post.height() - navHeight;

      $(window).scroll(function () {
        var tocLimMin = $('#toc-start').offset().top - navHeight;
        var scroH = document.body.scrollTop + document.documentElement.scrollTop;

        if (tocLimMin <= scroH && scroH <= tocLimMax) {
          toc.css({
            'display': 'block',
            'position': 'fixed',
            'top': navHeight,
          });
        } else if (scroH <= tocLimMin) {
          toc.css({
            'position': '',
            'top': '',
          });
        } else if (scroH > tocLimMax) {
          toc.css('display', 'none');
        }
      });
      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '.post-content',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        scrollSmooth: true,
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc > p').css('visibility', 'visible');
      }
      var offset = $('#board-ctn').css('margin-right')
      $('#toc-ctn').css({
        'right': offset
      })
    });
  </script>







  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




<!-- Plugins -->


  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?1040bcc5d25d5f4a9cb5e9855eb2c6db";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  



  <script  src="https://cdn.staticfile.org/prettify/188.0.0/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "【学习笔记】李宏毅ML-Note6-注意力机制&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 90,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script defer src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>



  

  
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
              processEscapes: true,
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
          }
      });

      MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });

    </script>

    <script  src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" ></script>

  










<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
