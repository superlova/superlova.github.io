<!DOCTYPE html>
<html lang="zh-CN">





<head><!-- hexo injector head_begin start --><!--Google tag(gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-6Q0B58X6TL"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date());gtag("config","G-6Q0B58X6TL");</script><!-- hexo injector head_begin end -->
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="Superlova">
  <meta name="keywords" content="">
  <title>【学习笔记】tf.data基本使用 - Superlova</title>
  <meta name="google-site-verification" content="CPvPw8mUZw65A1ALJ8uRzsYECq4mXgE4_eCq40VIhPM" />
  <link  rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/5.12.1/css/all.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/mdbootstrap/4.13.0/css/mdb.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/3.0.1/github-markdown.min.css" />

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link  rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css" />

<link  rel="stylesheet" href="/css/main.css" />


  <link defer rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />


<!-- 自定义样式保持在最底部 -->


<meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="Superlova" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Superlova</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">首页</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">归档</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/img/cover_2.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
                <p class="mt-3 post-meta">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>
                  星期六, 八月 15日 2020, 4:01 下午
                </p>
              

              <p class="mt-1">
                
                  
                  <span class="post-meta">
                    <i class="far fa-chart-bar"></i>
                    5.6k 字
                  </span>
                

                
                  
                  <span class="post-meta">
                      <i class="far fa-clock"></i>
                      27 分钟
                  </span>
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  <span id="busuanzi_container_page_pv" class="post-meta" style="display: none">
                    <i class="far fa-eye" aria-hidden="true"></i>
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>
                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5 z-depth-3" id="board">
          <div class="post-content mx-auto" id="post">
            
              <p
                class="note note-warning">本文最后更新于：星期二, 八月 2日 2022, 9:32 晚上</p>
            
            <div class="markdown-body">
              <p>使用 <code>tf.data</code> API 可以轻松处理大量数据，支持多样化的数据格式，还可以方便执行复杂的转换。本文介绍了不同类别源数据转化为 <code>tf.data.Dataset</code> 的方法，以及 <code>Dataset</code> 常见的预处理方法。<br><!--more---></p>
<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>最近几年我们在机器学习的加速计算领域取得了一些突破。虽然我们进行指标运算和矩阵运算所需的时间大大减少了，但是提供数据加速的CPU却没能跟上相应的步伐，这就成为了预处理中的瓶颈。我们本以为可以通过构建更复杂的模型来减少对硬件的需求，但是CPU的效率还是取决于他们拥有多少RAM。</p>
<p><img src="/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91tf-data%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/2020-08-15-22-34-14.png" srcset="/img/loading.gif" alt></p>
<p>对于一些数据集很大的问题，解决问题的方法除了提升CPU的性能之外，还有对数据集进行分批次处理，但是分批预处理需要不断进行训练和合并操作，这对预处理增加了难度。<code>tf.data</code> 可以帮助解决数据集过大造成的预处理瓶颈问题。</p>
<p><code>tf.data</code> 是tensorflow的数据预处理框架。它速度快、灵活且易于使用。</p>
<p>假设你的数据以TFRecord格式存在磁盘，需要将这些数据读取、处理并训练模型，可以先通过<code>TFRecordDataset</code> 开始处理这些数据：</p>
<pre><code class="lang-py">dataset = tf.data.TFRecordDataset(&#39;.../*.tfrecord&#39;)
</code></pre>
<p>然后执行一些数据预处理操作，这个过程可能很消耗资源：</p>
<pre><code class="lang-py">dataset = dataset.map(expensive_preprocess)
</code></pre>
<p>随后你需要打乱数据，以降低模型训练过程中过拟合的可能性：</p>
<pre><code class="lang-py">dataset = dataset.shuffle(buffer_size=1024)
</code></pre>
<p>然后我们需要分批次，以令模型加速计算</p>
<pre><code class="lang-py">dataset = dataset.batch(batch_size=32)
</code></pre>
<p>最终要完成pipeline搭建，这样可以保证模型在运行一个batch的数据时，另一批数据进行预处理以提升效率。</p>
<pre><code class="lang-py">dataset = dataset.prefetch()
</code></pre>
<p>将数据输入到模型，我们可以开始训练了。</p>
<pre><code class="lang-py">model = tf.keras.Model(...)
model.fit(dataset)
</code></pre>
<p>上面就是数据从读取到处理到训练的全部流程，称之为管道（pipeline）。</p>
<p>处理大量原始数据，要经过多次函数变换，这些函数变换都是可重用的。使用<code>tf.data</code>将这些变换整理成管道，一方面可以简化复杂输入的预处理过程，另一方面，由于<code>Dataset</code> 对象可迭代，可以执行分批处理。使用<code>tf.data.Dataset</code>可以方便地整合操作、构造数据集。</p>
<p>有两种方法构造可供训练使用的<code>Dataset</code>数据集：</p>
<ol>
<li>从文件、内存中直接构建<code>Dataset</code></li>
<li>从其他<code>Dataset</code>中转化</li>
</ol>
<p>如果打算从内存中读取数据构建 <code>Dataset</code> ，有 <code>tf.data.Dataset.from_tensors()</code> 和 <code>tf.data.Dataset.from_tensor_slices()</code> 可供选择；如果打算从 <code>TFRecord</code> 格式的文件中读取数据，可以调用 <code>tf.data.TFRecordDataset()</code> 。</p>
<p>当 <code>Dataset</code> 对象构建好了之后，通过使用 <code>Dataset.map()</code> 为其中每个元素施加变换、使用 <code>Dataset.batch()</code> 为整批元素添加变换等等对数据进行预处理。</p>
<h2 id="从内存或文件中构造Dataset"><a href="#从内存或文件中构造Dataset" class="headerlink" title="从内存或文件中构造Dataset"></a>从内存或文件中构造<code>Dataset</code></h2><h3 id="从内存中的array构造Dataset"><a href="#从内存中的array构造Dataset" class="headerlink" title="从内存中的array构造Dataset"></a>从内存中的array构造<code>Dataset</code></h3><p>如果你的所有数据都在内存中，那么最简单构造 <code>Dataset</code> 的方式就是，先将其利用 <code>tf.Tensor</code> 转成tensor，后使用<code>Dataset.from_tensor_slices()</code>。</p>
<pre><code class="lang-python">train, test = tf.keras.datasets.fashion_mnist.load_data()
images, labels = train
images = images/255

dataset = tf.data.Dataset.from_tensor_slices((images, labels))
dataset
</code></pre>
<h3 id="从生成器构造Dataset"><a href="#从生成器构造Dataset" class="headerlink" title="从生成器构造Dataset"></a>从生成器构造<code>Dataset</code></h3><p>你也可以利用 <code>Dataset.from_generator</code> 从Python的生成器来构造 <code>Dataset</code> ，比如从 <code>preprocessing.image.ImageDataGenerator</code> 构造 <code>Dataset</code>。但这种方法受制于Python的GIL，因此效率不会太高。</p>
<p>首先下载花朵图片数据集，一共3670张花朵图片，分成五个类别。</p>
<pre><code class="lang-python">flowers = tf.keras.utils.get_file(
    &#39;flower_photos&#39;,
    &#39;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&#39;,
    untar=True)
</code></pre>
<p>利用 <code>preprocessing.image.ImageDataGenerator</code> 定义数据增强操作，然后将其套用到花朵数据集上。</p>
<pre><code class="lang-python">img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)
images, labels = next(img_gen.flow_from_directory(flowers))
</code></pre>
<p>最后构造 <code>Dataset</code> 。</p>
<pre><code class="lang-python">ds = tf.data.Dataset.from_generator(
    img_gen.flow_from_directory, args=[flowers], 
    output_types=(tf.float32, tf.float32), 
    output_shapes=([32,256,256,3], [32,5])
)
ds
&lt;FlatMapDataset shapes: ((32, 256, 256, 3), (32, 5)), types: (tf.float32, tf.float32)&gt;
</code></pre>
<h3 id="从TFRecord格式文件构造Dataset"><a href="#从TFRecord格式文件构造Dataset" class="headerlink" title="从TFRecord格式文件构造Dataset"></a>从<code>TFRecord</code>格式文件构造<code>Dataset</code></h3><p>有些时候数据不在内存中，而是以特定格式存在磁盘上，比如 <code>TFRecord</code> 格式。这种情况我们可以使用 <code>tf.data.TFRecordDataset</code> 作为数据管道的一部分。</p>
<pre><code class="lang-python"># Creates a dataset that reads all of the examples from two files.
fsns_test_file = tf.keras.utils.get_file(&quot;fsns.tfrec&quot;, &quot;https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001&quot;)

dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])
</code></pre>
<p><code>tf.data.TFRecordDataset</code> API中，<code>filenames</code> 的输入很灵活，既可以是字符串，表明一个文件；也可以是字符串列表，表明多个文件。</p>
<h3 id="从-txt格式文件构造Dataset"><a href="#从-txt格式文件构造Dataset" class="headerlink" title="从.txt格式文件构造Dataset"></a>从<code>.txt</code>格式文件构造<code>Dataset</code></h3><p>如果是 <code>.txt</code> 格式，那么采用 <code>tf.data.TextLineDataset</code> 也可转成 <code>Dataset</code>。</p>
<pre><code class="lang-python">directory_url = &#39;https://storage.googleapis.com/download.tensorflow.org/data/illiad/&#39;
file_names = [&#39;cowper.txt&#39;, &#39;derby.txt&#39;, &#39;butler.txt&#39;]

file_paths = [
    &#39;cowper.txt&#39;,&#39;derby.txt&#39;,&#39;butler.txt&#39;
]
dataset = tf.data.TextLineDataset(file_paths)
</code></pre>
<p>看一下第一个文件的前5行：</p>
<pre><code class="lang-python">for line in dataset.take(5):
  print(line.numpy())

b&quot;\xef\xbb\xbfAchilles sing, O Goddess! Peleus&#39; son;&quot;
b&#39;His wrath pernicious, who ten thousand woes&#39;
b&quot;Caused to Achaia&#39;s host, sent many a soul&quot;
b&#39;Illustrious into Ades premature,&#39;
b&#39;And Heroes gave (so stood the will of Jove)&#39;
</code></pre>
<p>前五行都是 <code>cowper.txt</code> 中的。如果我们希望生成的 <code>Dataset</code> 能够轮流选取三个文件中的元素，可以在构造之初，使用 <code>Dataset.interleave</code> ，并设置 <code>cycle_length</code>：</p>
<pre><code class="lang-python">files_ds = tf.data.Dataset.from_tensor_slices(file_paths)
lines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length=3)

for i, line in enumerate(lines_ds.take(9)):
  if i % 3 == 0:
    print()
  print(i, line.numpy())

0 b&quot;\xef\xbb\xbfAchilles sing, O Goddess! Peleus&#39; son;&quot;
1 b&quot;\xef\xbb\xbfOf Peleus&#39; son, Achilles, sing, O Muse,&quot;
2 b&#39;\xef\xbb\xbfSing, O goddess, the anger of Achilles son of Peleus, that brought&#39;

3 b&#39;His wrath pernicious, who ten thousand woes&#39;
4 b&#39;The vengeance, deep and deadly; whence to Greece&#39;
5 b&#39;countless ills upon the Achaeans. Many a brave soul did it send&#39;

6 b&quot;Caused to Achaia&#39;s host, sent many a soul&quot;
7 b&#39;Unnumbered ills arose; which many a soul&#39;
8 b&#39;hurrying down to Hades, and many a hero did it yield a prey to dogs and&#39;
</code></pre>
<p>有的时候我们不希望录入文件的第一行，或者只要文件中满足要求的特定行，可以分别使用 <code>Dataset.skip()</code> 和 <code>Dataset.filter()</code> 。比如下面的泰坦尼克数据集，去掉第一行后，筛选生存下来的人。</p>
<pre><code class="lang-python">titanic_file = tf.keras.utils.get_file(&quot;train.csv&quot;, &quot;https://storage.googleapis.com/tf-datasets/titanic/train.csv&quot;)
titanic_lines = tf.data.TextLineDataset(titanic_file)

for line in titanic_lines.take(10):
  print(line.numpy())

b&#39;survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone&#39;
b&#39;0,male,22.0,1,0,7.25,Third,unknown,Southampton,n&#39;
b&#39;1,female,38.0,1,0,71.2833,First,C,Cherbourg,n&#39;
b&#39;1,female,26.0,0,0,7.925,Third,unknown,Southampton,y&#39;
b&#39;1,female,35.0,1,0,53.1,First,C,Southampton,n&#39;
b&#39;0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y&#39;
b&#39;0,male,2.0,3,1,21.075,Third,unknown,Southampton,n&#39;
b&#39;1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n&#39;
b&#39;1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n&#39;
b&#39;1,female,4.0,1,1,16.7,Third,G,Southampton,n&#39;

def survived(line):
  return tf.not_equal(tf.strings.substr(line, 0, 1), &quot;0&quot;)

survivors = titanic_lines.skip(1).filter(survived)

for line in survivors.take(10):
  print(line.numpy())

b&#39;1,female,38.0,1,0,71.2833,First,C,Cherbourg,n&#39;
b&#39;1,female,26.0,0,0,7.925,Third,unknown,Southampton,y&#39;
b&#39;1,female,35.0,1,0,53.1,First,C,Southampton,n&#39;
b&#39;1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n&#39;
b&#39;1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n&#39;
b&#39;1,female,4.0,1,1,16.7,Third,G,Southampton,n&#39;
b&#39;1,male,28.0,0,0,13.0,Second,unknown,Southampton,y&#39;
b&#39;1,female,28.0,0,0,7.225,Third,unknown,Cherbourg,y&#39;
b&#39;1,male,28.0,0,0,35.5,First,A,Southampton,y&#39;
b&#39;1,female,38.0,1,5,31.3875,Third,unknown,Southampton,n&#39;
</code></pre>
<h3 id="从csv格式构造Dataset"><a href="#从csv格式构造Dataset" class="headerlink" title="从csv格式构造Dataset"></a>从<code>csv</code>格式构造<code>Dataset</code></h3><p>除了<code>TFRecord</code>和<code>txt</code>格式，还有<code>csv</code>格式也很流行。<code>csv</code>格式能够以纯文本保存表格数据。<code>pandas</code>的<code>to_csv</code>是将<code>csv</code>搬运到内存的良好工具。</p>
<pre><code class="lang-python">titanic_file = tf.keras.utils.get_file(&quot;train.csv&quot;, &quot;https://storage.googleapis.com/tf-datasets/titanic/train.csv&quot;)
df = pd.read_csv(titanic_file, index_col=None)
df.head()

titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))

for feature_batch in titanic_slices.take(1):
  for key, value in feature_batch.items():
    print(&quot;  &#123;!r:20s&#125;: &#123;&#125;&quot;.format(key, value))
</code></pre>
<p>当然<code>tf.data</code>强大之处在于可以处理<code>pandas</code>处理不了的文件大小。<code>experimental.make_csv_dataset</code>函数是用于读取csv文件集的高层接口，它可以自动推导每个<code>column</code>的文件类型。</p>
<pre><code class="lang-python">titanic_batches = tf.data.experimental.make_csv_dataset(
    titanic_file, batch_size=4,
    label_name=&quot;survived&quot;)
</code></pre>
<p>查看第一个<code>batch</code>的内容。</p>
<pre><code class="lang-python">for feature_batch, label_batch in titanic_batches.take(1):
  print(&quot;&#39;survived&#39;: &#123;&#125;&quot;.format(label_batch))
  print(&quot;features:&quot;)
  for key, value in feature_batch.items():
    print(&quot;  &#123;!r:20s&#125;: &#123;&#125;&quot;.format(key, value))
</code></pre>
<p>如果只需要<code>csv</code>的某一列，那么可以使用<code>select_columns</code>参数。</p>
<pre><code class="lang-python">titanic_batches = tf.data.experimental.make_csv_dataset(
    titanic_file, batch_size=4,
    label_name=&quot;survived&quot;, select_columns=[&#39;class&#39;, &#39;fare&#39;, &#39;survived&#39;])

for feature_batch, label_batch in titanic_batches.take(1):
  print(&quot;&#39;survived&#39;: &#123;&#125;&quot;.format(label_batch))
  for key, value in feature_batch.items():
    print(&quot;  &#123;!r:20s&#125;: &#123;&#125;&quot;.format(key, value))

&#39;survived&#39;: [1 1 1 0]
features:
  &#39;sex&#39;               : [b&#39;female&#39; b&#39;female&#39; b&#39;male&#39; b&#39;female&#39;]
  &#39;age&#39;               : [35. 31. 45. 28.]
  &#39;n_siblings_spouses&#39;: [0 1 0 8]
  &#39;parch&#39;             : [0 1 0 2]
  &#39;fare&#39;              : [512.3292  20.525    8.05    69.55  ]
  &#39;class&#39;             : [b&#39;First&#39; b&#39;Third&#39; b&#39;Third&#39; b&#39;Third&#39;]
  &#39;deck&#39;              : [b&#39;unknown&#39; b&#39;unknown&#39; b&#39;unknown&#39; b&#39;unknown&#39;]
  &#39;embark_town&#39;       : [b&#39;Cherbourg&#39; b&#39;Southampton&#39; b&#39;Southampton&#39; b&#39;Southampton&#39;]
  &#39;alone&#39;             : [b&#39;y&#39; b&#39;n&#39; b&#39;y&#39; b&#39;n&#39;]
</code></pre>
<p>还有一个底层的<code>experimental.CsvDataset</code>类，它可以更精细的控制读取<code>csv</code>的过程。不支持列类型推断。</p>
<pre><code class="lang-python">titanic_types  = [tf.int32, tf.string, tf.float32, tf.int32, tf.int32, tf.float32, tf.string, tf.string, tf.string, tf.string] 
dataset = tf.data.experimental.CsvDataset(titanic_file, titanic_types , header=True)

for line in dataset.take(10):
  print([item.numpy() for item in line])

[0, b&#39;male&#39;, 22.0, 1, 0, 7.25, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;n&#39;]
[1, b&#39;female&#39;, 38.0, 1, 0, 71.2833, b&#39;First&#39;, b&#39;C&#39;, b&#39;Cherbourg&#39;, b&#39;n&#39;]
[1, b&#39;female&#39;, 26.0, 0, 0, 7.925, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;y&#39;]
[1, b&#39;female&#39;, 35.0, 1, 0, 53.1, b&#39;First&#39;, b&#39;C&#39;, b&#39;Southampton&#39;, b&#39;n&#39;]
[0, b&#39;male&#39;, 28.0, 0, 0, 8.4583, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Queenstown&#39;, b&#39;y&#39;]
[0, b&#39;male&#39;, 2.0, 3, 1, 21.075, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;n&#39;]
[1, b&#39;female&#39;, 27.0, 0, 2, 11.1333, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;n&#39;]
[1, b&#39;female&#39;, 14.0, 1, 0, 30.0708, b&#39;Second&#39;, b&#39;unknown&#39;, b&#39;Cherbourg&#39;, b&#39;n&#39;]
[1, b&#39;female&#39;, 4.0, 1, 1, 16.7, b&#39;Third&#39;, b&#39;G&#39;, b&#39;Southampton&#39;, b&#39;n&#39;]
[0, b&#39;male&#39;, 20.0, 0, 0, 8.05, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;y&#39;]
</code></pre>
<p><code>CsvDataset</code>还可以指定每列的默认值，供元素为空时填充。</p>
<p>在Colab中，直接书写<code>csv</code>文件：</p>
<pre><code class="lang-bash">%%writefile missing.csv
1,2,3,4
,2,3,4
1,,3,4
1,2,,4
1,2,3,
,,,
</code></pre>
<p>设置每列默认值：</p>
<pre><code class="lang-python"># Creates a dataset that reads all of the records from two CSV files, each with
# four float columns which may have missing values.

record_defaults = [999,999,999,999]
dataset = tf.data.experimental.CsvDataset(&quot;missing.csv&quot;, record_defaults)
dataset = dataset.map(lambda *items: tf.stack(items))

for line in dataset:
  print(line.numpy())

[1 2 3 4]
[999   2   3   4]
[  1 999   3   4]
[  1   2 999   4]
[  1   2   3 999]
[999 999 999 999]
</code></pre>
<p>你也可以选择删除<code>header</code>，或者指定某列输出</p>
<pre><code class="lang-python"># Creates a dataset that reads all of the records from two CSV files with
# headers, extracting float data from columns 2 and 4.
record_defaults = [999, 999] # Only provide defaults for the selected columns
dataset = tf.data.experimental.CsvDataset(&quot;missing.csv&quot;, record_defaults, select_cols=[1, 3])
dataset = dataset.map(lambda *items: tf.stack(items))

for line in dataset:
  print(line.numpy())
[2 4]
[2 4]
[999   4]
[2 4]
[  2 999]
[999 999]
</code></pre>
<h3 id="从文件夹中的每个文件构造Dataset"><a href="#从文件夹中的每个文件构造Dataset" class="headerlink" title="从文件夹中的每个文件构造Dataset"></a>从文件夹中的每个文件构造<code>Dataset</code></h3><p>如果每个单独的文件都是一个数据项（比如图片数据集），这样的数据集如何整理？</p>
<pre><code class="lang-python">flowers_root = tf.keras.utils.get_file(
    &#39;flower_photos&#39;,
    &#39;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&#39;,
    untar=True)
flowers_root = pathlib.Path(flowers_root)

list_ds = tf.data.Dataset.list_files(str(flowers_root/&#39;*/*&#39;))

for f in list_ds.take(5):
  print(f.numpy())

b&#39;/root/.keras/datasets/flower_photos/dandelion/8720503800_cab5c62a34.jpg&#39;
b&#39;/root/.keras/datasets/flower_photos/dandelion/16510864164_3afa8ac37f.jpg&#39;
b&#39;/root/.keras/datasets/flower_photos/tulips/7136973281_b2a935ce20.jpg&#39;
b&#39;/root/.keras/datasets/flower_photos/sunflowers/14623719696_1bb7970208_n.jpg&#39;
b&#39;/root/.keras/datasets/flower_photos/dandelion/4560663938_3557a1f831.jpg&#39;
</code></pre>
<p>通过使用<code>tf.io.read_file</code>读取数据，并从路径中提取<code>label</code>，返回<code>(image, label)</code>数据对。</p>
<pre><code class="lang-python">def process_path(file_path):
  label = tf.strings.split(file_path, os.sep)[-2]
  return tf.io.read_file(file_path), label

labeled_ds = list_ds.map(process_path)

for image_raw, label_text in labeled_ds.take(1):
  print(repr(image_raw.numpy()[:100]))
  print()
  print(label_text.numpy())

b&#39;\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x01\x03|\x03|\x00\x00\xff\xe2\x0cXICC_PROFILE\x00\x01\x01\x00\x00\x0cHLino\x02\x10\x00\x00mntrRGB XYZ \x07\xce\x00\x02\x00\t\x00\x06\x001\x00\x00acspMSFT\x00\x00\x00\x00IEC sRGB\x00\x00\x00\x00\x00\x00&#39;

b&#39;sunflowers&#39;
</code></pre>
<h2 id="批处理数据集元素"><a href="#批处理数据集元素" class="headerlink" title="批处理数据集元素"></a>批处理数据集元素</h2><p>批处理的最简单形式是将数据集的<code>n</code>个连续元素堆叠为单个元素。</p>
<p>batched_dataset = dataset.batch(4)</p>
<BatchDataset shapes: ((none,), (none,)), types: (tf.int64, tf.int64)>

<p>最后一个批次可能未满，使用<code>drop_remainder</code>参数忽略最后一批，使得<code>shape</code>完整：</p>
<p>batched_dataset = dataset.batch(7, drop_remainder=True)<br>batched_dataset</p>
<BatchDataset shapes: ((7,), (7,)), types: (tf.int64, tf.int64)>

<p>许多模型（例如序列模型）都可以使用大小可变（例如长度不同的序列）的输入数据。通过<code>Dataset.padded_batch</code>可以将不同长度的<code>tensor</code>转换成一个<code>batch</code>。</p>
<p>在多个<code>epochs</code>的情况下，需要重复迭代数据集，最简单的方法是将数据集重复<code>epochs</code>遍。可以使用 <code>Dataset.repeat()</code>完成。</p>
<p>原有的<code>titanic_lines</code>数据集中的数据数量为 628</p>
<pre><code class="lang-python">count = 0
for data in titanic_lines:
    count += 1
    # print(data.numpy())
print(count)
</code></pre>
<p>titanic_lines数据集经过<code>repeat</code>之后数目变为原来的两倍，1256</p>
<pre><code class="lang-python">count = 0
for data in titanic_lines.repeat(2):
    count += 1
    # print(data.numpy())
print(count)
</code></pre>
<p>将数据集打散的方法 <code>Dataset.shuffle()</code> 通过维护一个固定大小的缓冲区来实现。</p>
<pre><code class="lang-py">dataset = tf.data.Dataset.zip((counter, lines))
dataset = dataset.shuffle(buffer_size=100)
dataset = dataset.batch(20)
</code></pre>
<p>由于<code>buffer_size</code>为100，而批大小为20，因此第一批不包含索引大于120的元素。</p>
<p>在实际使用中，<code>repeat</code>操作、<code>batch</code>操作和<code>shuffle</code>操作经常一起混用，但是一定要注意操作的先后顺序。</p>
<h2 id="预处理数据"><a href="#预处理数据" class="headerlink" title="预处理数据"></a>预处理数据</h2><p><code>Dataset.map(f)</code> 通过函数 <code>f</code> 对数据集执行变换，<code>f</code> 必须以单个Tensor为输入，单个Tensor为输出（这里指的单个tensor，其意思是由原始数据和标签组成的数据对）。</p>
<p>假设我们定义了单个图像变换函数 <code>parse_image</code> ，只需 <code>images_ds = list_ds.map(parse_image)</code> 即可对 <code>image_ds</code> 数据集中的所有图片执行变换了。</p>
<p>假设我们需要将照片随机旋转，可以定义函数，然后使用map将其应用于数据集的所有图片上。</p>
<pre><code class="lang-py">import scipy.ndimage as ndimage

def random_rotate_image(image):
  image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False)
  return image

image, label = next(iter(images_ds))
image = random_rotate_image(image)
show(image, label)
</code></pre>
<p><img src="https://www.tensorflow.org/guide/data_files/output__wEyL7bS9S6t_1.png" srcset="/img/loading.gif" alt></p>
<p>这里使用的是 <code>scipy</code> 中的旋转函数，需套用 <code>tf.py_function()</code> 才能在Tensorflow的eager_mode里面使用。</p>
<p>接下来使用 <code>Dataset.map</code> ：</p>
<pre><code class="lang-py">def tf_random_rotate_image(image, label):
  im_shape = image.shape
  [image,] = tf.py_function(random_rotate_image, [image], [tf.float32])
  image.set_shape(im_shape)
  return image, label
</code></pre>
<p>在函数内部不但要注意输入和返回值是<code>(image, label)</code>，而且需要描述数据的<code>shape</code>和<code>type</code>，方便调试。</p>
<h2 id="时间序列数据窗口化"><a href="#时间序列数据窗口化" class="headerlink" title="时间序列数据窗口化"></a>时间序列数据窗口化</h2><p>时间序列数据的标签有所不同，一般以下一时刻的输入数据为标签，对未来进行一步一步的密集预测。比如：</p>
<pre><code class="lang-py">range_ds = tf.data.Dataset.range(100000)

batches = range_ds.batch(10, drop_remainder=True)

def dense_1_step(batch):
  # Shift features and labels one step relative to each other.
  return batch[:-1], batch[1:]

predict_dense_1_step = batches.map(dense_1_step)

for features, label in predict_dense_1_step.take(3):
  print(features.numpy(), &quot; =&gt; &quot;, label.numpy())

[0 1 2 3 4 5 6 7 8]  =&gt;  [1 2 3 4 5 6 7 8 9]
[10 11 12 13 14 15 16 17 18]  =&gt;  [11 12 13 14 15 16 17 18 19]
[20 21 22 23 24 25 26 27 28]  =&gt;  [21 22 23 24 25 26 27 28 29]
</code></pre>
<p>如果要预测整个时间窗口而不是固定的偏移量，比如</p>
<pre><code class="lang-py">batches = range_ds.batch(15, drop_remainder=True)

def label_next_5_steps(batch):
  return (batch[:-5],   # Take the first 5 steps
          batch[-5:])   # take the remainder

predict_5_steps = batches.map(label_next_5_steps)

for features, label in predict_5_steps.take(3):
  print(features.numpy(), &quot; =&gt; &quot;, label.numpy())

[0 1 2 3 4 5 6 7 8 9]  =&gt;  [10 11 12 13 14]
[15 16 17 18 19 20 21 22 23 24]  =&gt;  [25 26 27 28 29]
[30 31 32 33 34 35 36 37 38 39]  =&gt;  [40 41 42 43 44]
</code></pre>
<p>或者一个批次的标签和下个批次的输入有重叠：</p>
<pre><code class="lang-py">feature_length = 10
label_length = 5

features = range_ds.batch(feature_length, drop_remainder=True)
labels = range_ds.batch(feature_length).skip(1).map(lambda labels: labels[:-5])

predict_5_steps = tf.data.Dataset.zip((features, labels))

for features, label in predict_5_steps.take(3):
  print(features.numpy(), &quot; =&gt; &quot;, label.numpy())

[0 1 2 3 4 5 6 7 8 9]  =&gt;  [10 11 12 13 14]
[10 11 12 13 14 15 16 17 18 19]  =&gt;  [20 21 22 23 24]
[20 21 22 23 24 25 26 27 28 29]  =&gt;  [30 31 32 33 34]
</code></pre>
<p>有更方便的方法，那就是使用<code>Dataset.window</code>方法</p>
<pre><code class="lang-py">def make_window_dataset(ds, window_size, shift, stride):
  windows = ds.window(window_size, shift=shift, stride=stride)

  def sub_to_batch(sub):
    return sub.batch(window_size, drop_remainder=True)

  windows = windows.flat_map(sub_to_batch)
  return windows
</code></pre>
<p><code>Dataset.window(window_size, shift=shift, stride=stride)</code> 中的window_size代表窗口大小，即每个batch的元素个数；shift代表每次窗口移动的距离；stride代表选择元素的间隔</p>
<pre><code class="lang-py">ds = make_window_dataset(range_ds, window_size=10, shift = 5, stride=3)

for example in ds.take(10):
  print(example.numpy())

[ 0  3  6  9 12 15 18 21 24 27]
[ 5  8 11 14 17 20 23 26 29 32]
[10 13 16 19 22 25 28 31 34 37]
[15 18 21 24 27 30 33 36 39 42]
[20 23 26 29 32 35 38 41 44 47]
[25 28 31 34 37 40 43 46 49 52]
[30 33 36 39 42 45 48 51 54 57]
[35 38 41 44 47 50 53 56 59 62]
[40 43 46 49 52 55 58 61 64 67]
[45 48 51 54 57 60 63 66 69 72]
</code></pre>
<p>提取这些数据的标签方法：</p>
<pre><code class="lang-py">dense_labels_ds = ds.map(dense_1_step)

for inputs,labels in dense_labels_ds.take(3):
  print(inputs.numpy(), &quot;=&gt;&quot;, labels.numpy())

[ 0  3  6  9 12 15 18 21 24] =&gt; [ 3  6  9 12 15 18 21 24 27]
[ 5  8 11 14 17 20 23 26 29] =&gt; [ 8 11 14 17 20 23 26 29 32]
[10 13 16 19 22 25 28 31 34] =&gt; [13 16 19 22 25 28 31 34 37]
</code></pre>
<h2 id="重采样"><a href="#重采样" class="headerlink" title="重采样"></a>重采样</h2><p>有一些数据集，不同类别的数据分布不均匀。这个时候需要对那些不足的类别进行重采样。</p>
<p>给定信用卡欺诈（二分类）数据集，下面首先检查数据集中不同类别的占比</p>
<pre><code class="lang-py">def count(counts, batch):
  features, labels = batch
  class_1 = labels == 1
  class_1 = tf.cast(class_1, tf.int32)

  class_0 = labels == 0
  class_0 = tf.cast(class_0, tf.int32)

  counts[&#39;class_0&#39;] += tf.reduce_sum(class_0)
  counts[&#39;class_1&#39;] += tf.reduce_sum(class_1)

  return counts

counts = creditcard_ds.take(10).reduce(
    initial_state=&#123;&#39;class_0&#39;: 0, &#39;class_1&#39;: 0&#125;,
    reduce_func = count)

counts = np.array([counts[&#39;class_0&#39;].numpy(),
                   counts[&#39;class_1&#39;].numpy()]).astype(np.float32)

fractions = counts/counts.sum()
print(fractions)

[0.9953 0.0047]
</code></pre>
<p>偏差很大，这样训练的二分类器只需全预测为正类，即可达到99.53%的正确率。</p>
<p>重采样数据集的一种方法是使用<code>sample_from_datasets</code> 。当每个类都有单独的<code>data.Dataset</code>时，此方法更适用。</p>
<p>正类和反类分别构建 <code>Dataset</code></p>
<pre><code class="lang-py">negative_ds = (
  creditcard_ds
    .unbatch()
    .filter(lambda features, label: label==0)
    .repeat())
positive_ds = (
  creditcard_ds
    .unbatch()
    .filter(lambda features, label: label==1)
    .repeat())
</code></pre>
<p>要使用<code>tf.data.experimental.sample_from_datasets</code>传递数据集以及每个数据集的权重</p>
<pre><code class="lang-py">balanced_ds = tf.data.experimental.sample_from_datasets(
    [negative_ds, positive_ds], [0.5, 0.5]).batch(10)

for features, labels in balanced_ds.take(10):
  print(labels.numpy())

[0 1 1 0 0 1 0 1 0 0]
[0 1 0 1 1 0 0 1 1 0]
[1 1 1 0 0 1 1 0 1 1]
[0 0 0 0 1 1 0 0 1 0]
[0 1 1 0 0 0 1 0 0 0]
[1 1 0 0 0 0 0 0 0 0]
[0 1 0 1 1 0 0 1 1 1]
[0 0 1 0 1 0 1 0 1 1]
[1 0 1 1 0 1 0 0 1 0]
[0 0 0 1 1 1 1 0 1 1]
</code></pre>
<p>现在数据集就平衡了。</p>
<p>上述<code>experimental.sample_from_datasets</code>方法的一个问题是，每个类需要一个单独的<code>tf.data.Dataset</code>。</p>
<p>可以将<code>data.experimental.rejection_resample</code>函数应用于数据集，它仅加载一次，通过将多余元素将从数据集中删除以实现平衡。</p>
<p><code>data.experimental.rejection_resample</code>采用<code>class_func</code>参数，用于标记每个数据集元素所属的类别。</p>
<p>由于<code>Dataset</code>已经是<code>(features, label)</code>标记好的状态，因此只需</p>
<pre><code class="lang-py">def class_func(features, label):
  return label
</code></pre>
<p>重采样器输入的数据不能为batch后的Dataset，必须经过unbatch。重采样器还需要目标分布，以及可选的初始分布估计。最后经过map中的函数，直接删除掉extra_label即可。</p>
<pre><code class="lang-py">resampler = tf.data.experimental.rejection_resample(
    class_func, target_dist=[0.5, 0.5], initial_dist=fractions)

resample_ds = creditcard_ds.unbatch().apply(resampler).batch(10)

balanced_ds = resample_ds.map(lambda extra_label, features_and_label: features_and_label)
</code></pre>
<h2 id="数据集迭代器的checkpoint"><a href="#数据集迭代器的checkpoint" class="headerlink" title="数据集迭代器的checkpoint"></a>数据集迭代器的checkpoint</h2><p>没想到吧？不只是模型能使用checkpoint，Dataset的处理过程也可以使用checkpoint。如果您有一个很大的数据集，并且不想在每次重新启动时都从头开始，则这可能很有用。但是请注意，迭代器检查点可能很大，因为诸如shuffle和prefetch需要迭代器中的缓冲元素。</p>
<p>下面是示例：</p>
<pre><code class="lang-py">range_ds = tf.data.Dataset.range(20)

iterator = iter(range_ds)
ckpt = tf.train.Checkpoint(step=tf.Variable(0), iterator=iterator)
manager = tf.train.CheckpointManager(ckpt, &#39;/tmp/my_ckpt&#39;, max_to_keep=3)

print([next(iterator).numpy() for _ in range(5)])
[0, 1, 2, 3, 4]

save_path = manager.save()

print([next(iterator).numpy() for _ in range(5)])
[5, 6, 7, 8, 9]

ckpt.restore(manager.latest_checkpoint)

print([next(iterator).numpy() for _ in range(5)])
[5, 6, 7, 8, 9]
</code></pre>
<h2 id="在Keras中使用tf-data"><a href="#在Keras中使用tf-data" class="headerlink" title="在Keras中使用tf.data"></a>在Keras中使用<code>tf.data</code></h2><p>数据集的处理：</p>
<pre><code class="lang-py">train, test = tf.keras.datasets.fashion_mnist.load_data()

images, labels = train
images = images/255.0
labels = labels.astype(np.int32)

fmnist_train_ds = tf.data.Dataset.from_tensor_slices((images, labels))
fmnist_train_ds = fmnist_train_ds.shuffle(5000).batch(32)
</code></pre>
<p>模型构建：</p>
<pre><code class="lang-py">model = tf.keras.Sequential([
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(10)
])

model.compile(optimizer=&#39;adam&#39;,
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), 
              metrics=[&#39;accuracy&#39;])
</code></pre>
<p>模型训练：</p>
<pre><code class="lang-py">model.fit(fmnist_train_ds, epochs=2)
</code></pre>
<p>如果在fit过程中你要对Dataset进行repeat，只需指定每个epochs使用的数据个数，然后不给repeat指定参数，数据集就会变成无限个，一定会满足epochs的要求。</p>
<pre><code class="lang-py">model.fit(fmnist_train_ds.repeat(), epochs=2, steps_per_epoch=20)
</code></pre>
<p>同理，evaluate时也是一样的</p>
<pre><code class="lang-py">loss, accuracy = model.evaluate(fmnist_train_ds.repeat(), steps=10)
print(&quot;Loss :&quot;, loss)
print(&quot;Accuracy :&quot;, accuracy)
Loss : 0.3501795828342438
Accuracy : 0.8968750238418579
</code></pre>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>如果在使用机器学习算法解决问题过程中，数据预处理是主要问题的话，采用合适的数据组织手段可以帮助解决问题。 <code>tf.data</code> 能够快速处理大量数据，并将各个来源的数据归一化成合适的 <code>Dataset</code> 格式。</p>
<p>你可以对构建好的 <code>tf.data.Dataset</code> 做预处理操作，比如随机打乱、分批次、规划时间窗口、重采样等等。</p>
<p>经过处理后的 <code>Dataset</code> 对象可以直接输入到keras进行训练。</p>
</BatchDataset></BatchDataset>
            </div>
            <hr>
            <div>
              <p>
                
                  <span>
                <i class="iconfont icon-inbox"></i>
                    
                      <a class="hover-with-bg" href="/categories/notes/">notes</a>
                      &nbsp;
                    
                  </span>&nbsp;&nbsp;
                
                
                  <span>
                <i class="iconfont icon-tag"></i>
                    
                      <a class="hover-with-bg" href="/tags/Tensorflow/">Tensorflow</a>
                    
                      <a class="hover-with-bg" href="/tags/preprocessing/">preprocessing</a>
                    
                  </span>
                
              </p>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
              
            </div>

            
              <!-- Comments -->
              <div class="comments" id="comments">
                
                
  <script defer src="https://utteranc.es/client.js"
          repo="superlova / superlova.github.io"
          issue-term="pathname"
  
          label="utterances"
    
          theme="github-light"
          crossorigin="anonymous"
  >
  </script>


              </div>
            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc-start"></div>
<div id="toc">
  <p class="h5"><i class="far fa-list-alt"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
<div>
  <span id="timeDate">载入天数...</span>
  <span id="times">载入时分秒...</span>
  <script>
  var now = new Date();
  function createtime(){
      var grt= new Date("02/14/2017 00:00:00");//此处修改你的建站时间或者网站上线时间
      now.setTime(now.getTime()+250);
      days = (now - grt ) / 1000 / 60 / 60 / 24;
      dnum = Math.floor(days);
      hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum);
      hnum = Math.floor(hours);
      if(String(hnum).length ==1 ){
          hnum = "0" + hnum;
      }
      minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
      mnum = Math.floor(minutes);
      if(String(mnum).length ==1 ){
                mnum = "0" + mnum;
      }
      seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
      snum = Math.round(seconds);
      if(String(snum).length ==1 ){
                snum = "0" + snum;
      }
      document.getElementById("timeDate").innerHTML = "本站安全运行&nbsp"+dnum+"&nbsp天";
      document.getElementById("times").innerHTML = hnum + "&nbsp小时&nbsp" + mnum + "&nbsp分&nbsp" + snum + "&nbsp秒";
  }
  setInterval("createtime()",250);
  </script>
</div>

<p id="hitokoto">:D 获取中...</p>
<script>
  fetch('https://v1.hitokoto.cn')
    .then(response => response.json())
    .then(data => {
      const hitokoto = document.getElementById('hitokoto')
      hitokoto.innerText = data.hitokoto
      })
      .catch(console.error)
</script>

  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    </div>
    
  <div>
    
      <!-- 不蒜子统计PV -->
      
      <span id="busuanzi_container_site_pv" style="display: none">
      总访问量 <span id="busuanzi_value_site_pv"></span> 次
    </span>
    
    
      <!-- 不蒜子统计UV -->
      
      <span id="busuanzi_container_site_uv" style="display: none">
      总访客数 <span id="busuanzi_value_site_uv"></span> 人
    </span>
    
  </div>


    

    
  </div>
</footer>



<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/popper.js/1.16.1/umd/popper.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="https://cdn.staticfile.org/mdbootstrap/4.13.0/js/mdb.min.js" ></script>
<script  src="/js/main.js" ></script>


  <script  src="/js/lazyload.js" ></script>



  
  <script  src="https://cdn.staticfile.org/tocbot/4.10.0/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var navHeight = $('#navbar').height();
      var post = $('#post');
      var toc = $('#toc');
      var tocLimMax = post.offset().top + post.height() - navHeight;

      $(window).scroll(function () {
        var tocLimMin = $('#toc-start').offset().top - navHeight;
        var scroH = document.body.scrollTop + document.documentElement.scrollTop;

        if (tocLimMin <= scroH && scroH <= tocLimMax) {
          toc.css({
            'display': 'block',
            'position': 'fixed',
            'top': navHeight,
          });
        } else if (scroH <= tocLimMin) {
          toc.css({
            'position': '',
            'top': '',
          });
        } else if (scroH > tocLimMax) {
          toc.css('display', 'none');
        }
      });
      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '.post-content',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        scrollSmooth: true,
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc > p').css('visibility', 'visible');
      }
      var offset = $('#board-ctn').css('margin-right')
      $('#toc-ctn').css({
        'right': offset
      })
    });
  </script>







  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




<!-- Plugins -->


  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?1040bcc5d25d5f4a9cb5e9855eb2c6db";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  



  <script  src="https://cdn.staticfile.org/prettify/188.0.0/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "【学习笔记】tf.data基本使用&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 90,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script defer src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>












<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
