<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>【学习笔记】通过文字检索视频片段：VCED——跨模态模型介绍</title>
    <link href="/2022/11/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED%E2%80%94%E2%80%94%E8%B7%A8%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/"/>
    <url>/2022/11/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED%E2%80%94%E2%80%94%E8%B7%A8%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<p>第三篇：跨模态模型介绍</p><!--more---><h2 id="一、CLIP-模型"><a href="#一、CLIP-模型" class="headerlink" title="一、CLIP 模型"></a>一、CLIP 模型</h2><p>CLIP模型将原有的图像标签替换为图像的文本描述信息，来监督视觉任务的训练，在下游任务中得到了较好的zero-shot的结果。</p><p>该模型将图像分类问题转换为图文匹配问题，在下游任务（以图像分类为例）的实现中，我们需要先基于图像标签集合，构造 text prompt 并将其通过 clip 的 text encoder 获得文本编码向量，之后，将图片通过 image encoder 获得图像编码向量。</p><p>对于每张图片，我们需要计算它与所有 text prompt 之间的距离（通过计算图像编码向量与文本编码向量之间的余弦相似度或点积得到），选择距离最近的 text prompt 所对应的标签作为该张图片的分类标签。这样的转换使得模型在 zero-shot 场景下可以获得与监督学习比肩的效果，除此之外，clip 模型可广泛应用到图像检索，视频理解，图像生成等其他领域。接下来我们对 CLIP 模型进行简单的介绍。</p><h3 id="1-CLIP-原理"><a href="#1-CLIP-原理" class="headerlink" title="1. CLIP 原理"></a>1. CLIP 原理</h3><p>CLIP将图像的分类任务转化为了图文匹配的任务，将图像信息和语义信息映射到同一多模态语义空间下，再使用对比学习的方法进行训练。</p><p>在预训练阶段，模型的数据来源为从互联网上搜集的4亿个（图像，文本）对。假设batch中有N个（图像，文本）对，那么我们便可以得到N个文本向量，记为：t1,t2,…,tn,以及N个图像向量，记为img1,img2,…,imgn。我们需要做的就是让t(i)与img(i)之间的语义距离尽可能接近，而与其他的img之间的距离尽可能拉远。如果我们将其看为一个矩阵，其中横轴方向为文本，纵轴方向为图像，且文本、图像的标号均按照次序排列，那么我们就可以得到一个N * N的方阵，方阵的每个元素(i,j)的位置为t(i)与img(j)之间的语义相似度，相似度可通过余弦或点积的方式获得。我们的优化目标就是让对角线上的元素的值尽可能的大，而其余部分的值尽可能地小。</p><p>在zero shot阶段，我们可以根据上下文语义建立prompt模板，并将标签集合映射到该模板中，得到prompt text模板。比方说，现在我们需要做对(大象，冰箱，蚂蚁)的三分类任务，prompt模板为 “这是一张有关{类别}的照片”，将分类标签映射到prompt模板后可以得到集合：{“这是一张有关大象的照片”“这是一张有关冰箱的照片”“这是一张有关蚂蚁的照片”}。对集合中的文本，通过clip的text encoder之后，便可以得到三个类别对应的文本特征向量，之后，对于每一张需要分类的图片，我们只需要比较该图片的特征向量与三个类别对应的文本特征向量之间的语义距离，并选择最近的那一条文本所对应的标签作为图像的分类结果。 具体细节可参考论文《Learning transferable visual models from natural language supervision》。</p><h3 id="2-其他多模态模型"><a href="#2-其他多模态模型" class="headerlink" title="2. 其他多模态模型"></a>2. 其他多模态模型</h3><p>在图文生成领域，广为人知的模型有VAE，GAN等。最近大火的diffusion model则使用了一种非常有趣的思想来做生成任务。</p><h2 id="二、多模态与跨模态的应用：新的交互方式"><a href="#二、多模态与跨模态的应用：新的交互方式" class="headerlink" title="二、多模态与跨模态的应用：新的交互方式"></a>二、多模态与跨模态的应用：新的交互方式</h2><p>多模态应用允许我们通过利用每种方式的优势来结合不同的模态。例如，我们可以在对话中同时使用口语和书面语言，以确保我们相互理解。我们还可以使用图片或视频等作为视觉辅助工具，来帮助解释仅用文字难以描述的事物。</p><p>跨模态应用针对的是来自不同模态（如视觉和听觉）的输入和输出。它通过使用一种感官的信息来增强另一种感官，使用户体验比传统应用更上一层楼。比如说，我们可以通过触摸的方式来帮助我们理解在触觉地图或盲文文本中看到的内容。我们还可以使用声音来帮助我们定位环境中的事物，一般通过声纳或雷达来完成。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://medium.com/jina-ai/multimodal-and-crossmodal-applications-the-new-way-to-interact-d73d3e932990">https://medium.com/jina-ai/multimodal-and-crossmodal-applications-the-new-way-to-interact-d73d3e932990</a></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VCED</tag>
      
      <tag>multimodal</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】李宏毅ML-Note8-自监督学习、BERT</title>
    <link href="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/"/>
    <url>/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/</url>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：自监督学习、BERT、GPT。</p><!--more---><h2 id="二、自监督学习"><a href="#二、自监督学习" class="headerlink" title="二、自监督学习"></a>二、自监督学习</h2><p>自监督学习由 Yann LeCun 在 2019 年首次提出，模型会从无标签的数据中采取某些方法构建标签，然后进行学习。由于自监督学习不需要人为对数据进行标签，因此自监督学习可以看做无监督学习的一种。</p><p>自监督学习常用于训练大规模的语言模型，比如 BERT、GPT 等。由于这些模型的参数量非常大，因此训练模型所需的数据量也非常大，甚至大到人工标注无法完成的程度。</p><p>BERT 的训练数据包含整个维基百科的无标签号文本的大语料库中（足足有25亿字！） 和图书语料库（有8亿字）；使用自监督学习能更高效、更直接地从数据中学到知识，避免人工标注带来的成本。</p><p>下面介绍几个 BERT 使用到的自监督学习方法。</p><h3 id="1-Masking-Input"><a href="#1-Masking-Input" class="headerlink" title="1. Masking Input"></a>1. Masking Input</h3><p>BERT 的输入是一个句子，句子由若干 token 组成。Masking Input 训练任务就是随机 Mask 句子中的词，来构建 {填空题, 答案} 这样的数据。</p><p>Mask 方法有很多，可以直接替换为 [MSK] 这种全新的特殊符号，或者替换为任意一个其他字。</p><p>到底用哪一种也是随机决定的。在原文中，80%的数据是由 [MSK] 特殊符号替换方法来生成的，10%的数据是以其他词进行替换生成的，还有 10%的数据是保持原样生成的。</p><p>这个过程 BERT 会预测 [MSK] 对应的字是哪一个，把盖住的部分对应的输出向量做线性变换，做 softmax 产出一个分布。由此，变成了一个监督学习的分类任务。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/masking_input.png" alt></p><p>训练方法：与 ground truth 进行最小化交叉熵。</p><h3 id="2-Next-Sentence-Prediction"><a href="#2-Next-Sentence-Prediction" class="headerlink" title="2. Next Sentence Prediction"></a>2. Next Sentence Prediction</h3><p>选择句子A和B作为预训练样本：B有50%的可能是A的下一句，也有50%的可能是来自语料库的随机句子。</p><pre><code>[cls] w1 w2 [sep] w3 w4 w5</code></pre><p>只取 cls 对应的输出，然后预测 yes or no，判断这两个句子是否相接。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/next_sentence_pred.png" alt></p><p>这一招不太有用，RoBERTa 舍弃了该方法。可能是这个任务太简单了，学不到太有用的东西。</p><h3 id="3-Sentence-Order-Prediction"><a href="#3-Sentence-Order-Prediction" class="headerlink" title="3. Sentence Order Prediction"></a>3. Sentence Order Prediction</h3><p>ALBERT 的训练方法，判断两句的顺序或者逆序。</p><h3 id="4-如何评估学习的效果？"><a href="#4-如何评估学习的效果？" class="headerlink" title="4. 如何评估学习的效果？"></a>4. 如何评估学习的效果？</h3><p>评估方法：GLUE（General Language Understanding Evaluation）</p><p>有九大任务，评估预训练模型的好坏。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/glue_score_9_missions.png" alt></p><h2 id="三、如何使用-BERT"><a href="#三、如何使用-BERT" class="headerlink" title="三、如何使用 BERT"></a>三、如何使用 BERT</h2><p>预训练模型完成训练后，需要将模型学习到的知识迁移到其他领域。这个迁移的过程叫做微调（fine-tuning）。微调需要一定的标注数据，进行监督学习。根据下游任务不同，所需要的标注数据不同。</p><h3 id="1-文本分类："><a href="#1-文本分类：" class="headerlink" title="1. 文本分类："></a>1. 文本分类：</h3><p>文本分类以一串文本为输入，输出为一个概率分布，表明该串文本被分成各个类别的概率。</p><p>INPUT:</p><pre><code>[cls] w1 w2 w3</code></pre><p>OUTPUT:<br>看 [cls] 的输出分布，然后与真实标注进行对比，最小交叉熵。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/bert_classification.png" alt></p><p>训练时，linear 线性变换矩阵的参数是随机初始化的，预训练的参数是直接下载的。</p><p>fine tuning 时，所有部分的参数都会梯度下降进行优化。</p><h3 id="2-序列标注"><a href="#2-序列标注" class="headerlink" title="2. 序列标注"></a>2. 序列标注</h3><p>输入一个序列，输出另一个序列。</p><pre><code>[cls] w1 w2 w3 w4</code></pre><p>BERT 以一个 [cls] 开头的序列为输入，对于每个 token，都会产出向量，并转化为预测该 token 的词性。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/pos_tagging.png" alt></p><h3 id="3-语义推理"><a href="#3-语义推理" class="headerlink" title="3. 语义推理"></a>3. 语义推理</h3><p>Natural Language Inference ，输入两个句子，输出两个句子的逻辑关系，多分类问题。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/natural_language_inference.png" alt></p><p>可以扩展到评论立场判断，输入原文+评论，输出读者对此的态度，支持反对还是中立。</p><h3 id="4-问答系统"><a href="#4-问答系统" class="headerlink" title="4. 问答系统"></a>4. 问答系统</h3><p>给定一篇文章+一个问题，机器产出答案在原文中的位置，分别是起始节点和终止节点。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/extraction_based_question_answering.png" alt></p><p>我们需要将输入整理成 query + document，输出两个位置，start_pos 和 end_pos ，在原文中 [start_pos, end_pos] 内的位置就是模型认为的答案。如果 start_pos 和 end_pos 都为 0，则模型认为没有答案。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/extraction_based_qa_input_output.png" alt></p><p>在这里唯一需要从头开始训练的模块是如上图所示的<font color="Orange">橙色</font>和<font color="Blue">蓝色</font>向量。</p><p>首先把橙色向量跟每个 document 的 N 个词向量对应的输出向量做点积，算出 N 个数值，然后过一下 softmax 得到概率分布。取最高的值为 start_pos。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note8-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81BERT/extraction_based_qa_input_outout_2.png" alt></p><p>然后把蓝色向量跟每个 document 的 N 个词向量错对应的输出向量做点积，算出 N 个数值，同样取最大后决定 end_pos。</p><p>这个过程类比 self-attention，就好像我们要训练 query 向量对应的 Q 矩阵。</p><p>实际上 BERT 的输入序列一般为 512，而一整篇文章往往巨长无比，我们一般每次只拿一小段一小段 document 进行训练，只有一个有正例。</p><h3 id="———"><a href="#———" class="headerlink" title="———"></a>———</h3><p>Bert 损失函数组成</p><p>第一部分是来自 Mask-LM 的单词级别分类任务；<br>另一部分是句子级别的分类任务；</p><p>所用的损失函数叫做负对数似然函数</p><p>BERT 则是通过「transformer 编码器」模块构建的。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>MOOC</tag>
      
      <tag>Self-Supervised Learning</tag>
      
      <tag>BERT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】通过文字检索视频片段：VCED之Jina</title>
    <link href="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED%E4%B9%8BJina/"/>
    <url>/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED%E4%B9%8BJina/</url>
    
    <content type="html"><![CDATA[<h1 id="第二篇：核心组件-Jina-介绍"><a href="#第二篇：核心组件-Jina-介绍" class="headerlink" title="第二篇：核心组件 Jina 介绍"></a>第二篇：核心组件 Jina 介绍</h1><!--more---><p>Jina 是一个能够将非结构化数据例如图像，文档视频等，转换为向量数据的工具。利用该工具，可以快速实现多模态的检索任务。另外，Jina 也是一家新的公司，目前正在优化中。他们的 <a href="https://github.com/jina-ai/jina">GitHub Repo</a>。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED%E4%B9%8BJina/jina%20introduction.png" alt></p><p>其实 Jina 公司提供了包括向量化、服务化到实际部署的全部工具，可以支持包括 PDF 检索、视频检索在内的很多检索操作。</p><p><img src="/2022/11/17/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED%E4%B9%8BJina/jina_全家桶.png" alt></p><h2 id="1-Jina-安装"><a href="#1-Jina-安装" class="headerlink" title="1. Jina 安装"></a>1. Jina 安装</h2><p>在 windows 系统上的安装教程：</p><p><a href="https://blog.csdn.net/Jina_AI/article/details/122820646">https://blog.csdn.net/Jina_AI/article/details/122820646</a></p><p>本文将记录在 mac 上安装 jina 的过程。</p><p>首先确保自己的 Python 版本在 3.7 及以上，然后通过下列方式安装 jina：</p><pre><code class="lang-sh"># via pypipip install jina# via condaconda install jina -c conda-forge# via dockerdocker pull jinaai/jina:latest</code></pre><p>使用 pip 安装 jina 时，提示：</p><pre><code>lz4/_version.c:32:10: fatal error: &#39;Python.h&#39; file not foundERROR: Failed building wheel for lz4Successfully built jina docarray grpcio jcloud python-multipartFailed to build lz4ERROR: Could not build wheels for lz4, which is required to install pyproject.toml-based projects</code></pre><p>原因：系统中没有 Python.h，是因为没有安装 python 的开发版，即 Python-devel 这个包。</p><h2 id="2-Jina-的基本组件"><a href="#2-Jina-的基本组件" class="headerlink" title="2. Jina 的基本组件"></a>2. Jina 的基本组件</h2><p>详细的文档可以参考<a href="https://docs.jina.ai/fundamentals/architecture-overview/">这里</a>。</p><p>Document、Executor 和 Flow 是 Jina 的三个基本概念，分别代表数据类型，算法单元，和工作流。</p><p>简单来说，Document 是一种数据类型的定义方式，Flow 则是负责 Document 在整个架构间的传输，最后 Executor 则实现具体的算法功能。</p><p>比如下面这个简单的 Demo，客户端发起一个请求给服务器端，服务器端处存在定义好的 Flow，Flow 则会把不同的 Executor 串联起来。这里有两个已经定义好的 Executor，分别执行将字符串末尾添加特定字符的操作。</p><p>服务器端代码：</p><pre><code class="lang-py">from jina import DocumentArray, Executor, Flow, requestsclass FooExec(Executor):    @requests    async def add_text(self, docs: DocumentArray, **kwargs):        for d in docs:            d.text += &#39;hello, world!&#39;class BarExec(Executor):    @requests    async def add_text(self, docs: DocumentArray, **kwargs):        for d in docs:            d.text += &#39;goodbye!&#39;f = Flow(port=12345).add(uses=FooExec, replicas=3).add(uses=BarExec, replicas=2)with f:    f.block()</code></pre><p>客户端代码：</p><pre><code class="lang-py">from jina import Client, DocumentArrayc = Client(port=12345)r = c.post(&#39;/&#39;, DocumentArray.empty(2))print(r.texts)</code></pre><p>运行逻辑动图为：</p><p><img src="https://docs.jina.ai/_images/arch-overview.svg" alt="jina_overview"></p><p>返回结果为：</p><pre><code>[&#39;hello, world!goodbye!&#39;, &#39;hello, world!goodbye!&#39;]</code></pre><h2 id="3-启动-jina-示例"><a href="#3-启动-jina-示例" class="headerlink" title="3. 启动 jina 示例"></a>3. 启动 jina 示例</h2><p>安装完毕后，新建 toy.yml 作为 gRPC 服务的配置文件：</p><pre><code class="lang-yaml"># toy.ymljtype: Flowwith:  port: 51000  protocol: grpcexecutors:- uses: FooExecutor  name: foo  py_modules:    - test.py- uses: BarExecutor  name: bar  py_modules:    - test.py</code></pre><p>然后定义 test.py ，定义若干 Executor 的处理逻辑：</p><p>test.py</p><pre><code class="lang-py"># 创建 test.py 文件与 YAML 文件在同一目录下# 导入 document、executor 和 flow 以及 requests 装饰器from jina import DocumentArray, Executor, requests, Document# 编写 FooExecutor 与 BarExecutor 类，类中定义了函数 foo 和 bar# 该函数从网络请求接收 DocumentArray (先暂时不需要理解它是什么)，并在其内容后面附加 &quot;foo was here&quot; 与 &quot;bar was here&quot;class FooExecutor(Executor):    @requests # 用于指定路由，类似网页访问 /index 和 /login 会被路由到不同的方法上是用样的概念，关于 request 下面会再进行详细介绍    def foo(self, docs: DocumentArray, **kwargs):        docs.append(Document(text=&#39;foo was here&#39;))class BarExecutor(Executor):    @requests    def bar(self, docs: DocumentArray, **kwargs):        docs.append(Document(text=&#39;bar was here&#39;))</code></pre><p>然后使用该指令启动服务：</p><pre><code class="lang-sh">jina flow --uses toy.yml</code></pre><p>启动服务之后放着别动，我们新建一个 shell 窗口；然后新建 client.py，用于储存客户端请求消息的逻辑：</p><p>client.py</p><pre><code class="lang-py"># 从 Jina 中导入连接的客户端与 Documentfrom jina import Client, Documentc = Client(host=&#39;grpc://0.0.0.0:51000&#39;)  # 如果运行提示失败，可尝试使用localhostresult = c.post(&#39;/&#39;, Document()) # 将一个空的 Document 传到服务端执行print(result.texts)</code></pre><p>随后启动客户端：</p><pre><code class="lang-sh">python client.py</code></pre><p>最终会打印出一个 “[‘’, ‘foo was here’, ‘bar was here’]” 字符串。</p><h2 id="4-DocArray-简介"><a href="#4-DocArray-简介" class="headerlink" title="4. DocArray 简介"></a>4. DocArray 简介</h2><p>DocArray 也是一个工具包，它被整合在 Jina 中，作为 Jina 的重要组成部分，方便实现跨模态应用。关于 DocArray 的其他资料可以参考<a href="https://docarray.jina.ai/">这里</a>。</p><p>DocArray 类比 Pandas，其基本数据类型为 Document，并整合了多种操作 Document 的方法。DocArray 对数据采用分层结构存储。</p><blockquote><p>可以利用 DocArray 实现在第一层存入该画面的视频，第二层存入该视频的不同镜头，第三层可以是视频的某一帧，也可以存储台台词等等，这使得你可以通过台词去搜索到视频，也可以通过视频定位某几帧画面，这样搜索的颗粒度，结构的多样性和结果的丰富度，都比传统文本检索好很多。</p></blockquote><h2 id="5-通过-DocArray-导入任意模态的数据"><a href="#5-通过-DocArray-导入任意模态的数据" class="headerlink" title="5. 通过 DocArray 导入任意模态的数据"></a>5. 通过 DocArray 导入任意模态的数据</h2><p>项目代码参考<a href="https://github.com/datawhalechina/vced/tree/main/code/jina_demo">这里</a>。有将图片、文本、视频分别导入 Jina 的实例。</p><h3 id="5-1-文本数据导入"><a href="#5-1-文本数据导入" class="headerlink" title="5.1 文本数据导入"></a>5.1 文本数据导入</h3><p><strong>创建文本</strong></p><pre><code class="lang-py">from jina import Document  # 导包# 创建简单的文本数据d = Document(text=&#39;hello, world.&#39;)print(d.text)  # 通过text获取文本数据# 如果文本数据很大，或者自URI，可以先定义URI，然后将文本加载到文档中d = Document(uri=&#39;https://www.w3.org/History/19921103-hypertext/hypertext/README.html&#39;)d.load_uri_to_text()print(d.text)# 支持多语言d = Document(text=&#39;👋    नमस्ते दुनिया!    你好世界！こんにちは世界！    Привет мир!&#39;)print(d.text)</code></pre><p><strong>切割文本</strong></p><pre><code class="lang-py">from jina import Document  # 导包d = Document(text=&#39;👋    नमस्ते दुनिया!    你好世界！こんにちは世界！    Привет мир!&#39;)d.chunks.extend([Document(text=c) for c in d.text.split(&#39;!&#39;)])  # 按&#39;!&#39;分割d.summary()</code></pre><p><strong>文本匹配</strong></p><pre><code class="lang-py">from jina import Document, DocumentArrayd = Document(    uri=&#39;https://www.gutenberg.org/files/1342/1342-0.txt&#39;).load_uri_to_text()  # 链接是傲慢与偏见的电子书，此处将电子书内容加载到 Document 中da = DocumentArray(Document(text=s.strip()) for s in d.text.split(&#39;\n&#39;) if s.strip())  # 按照换行进行分割字符串da.apply(lambda d: d.embed_feature_hashing())q = (    Document(text=&#39;she entered the room&#39;)  # 要匹配的文本    .embed_feature_hashing()  # 通过 hash 方法进行特征编码    .match(da, limit=5, exclude_self=True, metric=&#39;jaccard&#39;, use_scipy=True)  # 找到五个与输入的文本最相似的句子)print(q.matches[:, (&#39;text&#39;, &#39;scores__jaccard&#39;)])  # 输出对应的文本与 jaccard 相似性分数# 输出结果：# [[&#39;staircase, than she entered the breakfast-room, and congratulated&#39;, &#39;of the room.&#39;,#   &#39;She entered the room with an air more than usually ungracious,&#39;,#   &#39;entered the breakfast-room, where Mrs. Bennet was alone, than she&#39;, &#39;those in the room.&#39;],#  [&#123;&#39;value&#39;: 0.6&#125;, &#123;&#39;value&#39;: 0.6666666666666666&#125;, &#123;&#39;value&#39;: 0.6666666666666666&#125;, &#123;&#39;value&#39;: 0.6666666666666666&#125;,#   &#123;&#39;value&#39;: 0.7142857142857143&#125;]]</code></pre><h3 id="5-2-从影片导入"><a href="#5-2-从影片导入" class="headerlink" title="5.2 从影片导入"></a>5.2 从影片导入</h3><pre><code class="lang-py"># 视频需要依赖 av 包# pip install avfrom jina import Documentd = Document(uri=&#39;cat.mp4&#39;)d.load_uri_to_video_tensor()# 相较于图像，视频是一个 4 维数组，第一维表示视频帧 id 或是视频的时间，剩下的三维则和图像一致。print(d.tensor.shape)  # (31, 1080, 1920, 3)# 使用 append 方法将 Document 放入 chunk 中for b in d.tensor:    d.chunks.append(Document(tensor=b))d.chunks.plot_image_sprites(&#39;mov.png&#39;)</code></pre><p>还有许多其他的操作方法，有待后续进一步发掘和使用。总体感觉还是很不错的。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://u84gxokzmi.feishu.cn/docx/doxcn30HXXLbqFrsyR6bL5A6o1g">https://u84gxokzmi.feishu.cn/docx/doxcn30HXXLbqFrsyR6bL5A6o1g</a></p><p><a href="https://github.com/datawhalechina/vced">https://github.com/datawhalechina/vced</a></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VCED</tag>
      
      <tag>multimodal</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】通过文字检索视频片段：VCED</title>
    <link href="/2022/11/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED/"/>
    <url>/2022/11/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED/</url>
    
    <content type="html"><![CDATA[<p>第一篇：环境部署</p><!--more---><p>VCED: Video Clip Extraction by description, 可以通过你的文字描述来自动识别视频中相符合的片段进行视频剪辑。基于跨模态搜索与向量检索技术搭建。</p><p>本项目参考自 <a href="https://github.com/datawhalechina/vced">Datawhale 的 VCED 学习教程</a>。</p><p>环境为 Mac Monterey, Apple M1 Pro 芯片，内存 16GB。</p><p>首先需要安装 docker，在 mac 上安装 docker 只需去官网下载客户端：</p><p><a href="https://docs.docker.com/desktop/install/mac-install/">https://docs.docker.com/desktop/install/mac-install/</a></p><p><img src="/2022/11/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED/mac安装docker.png" alt></p><p>注意，在 mac 上安装 docker 需要提前安装 rosetta。</p><p>安装 docker 完成后，为了方便下载镜像，我们先修改下源。</p><p><img src="/2022/11/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED/mac修改源.png" alt></p><p>在如图所示的文本框内按照 json 格式添加如下文本：</p><pre><code class="lang-json">&quot;registry-mirrors&quot;: [    &quot;https://hub-mirror.c.163.com&quot;,    &quot;https://mirror.baidubce.com&quot;  ]</code></pre><p>之后使用现有的镜像文件即可部署：</p><pre><code class="lang-sh">docker pull nil01/vceddocker run -itd -p 8501:8501 -p 45679:45679 --name vced_arm nil01/vced</code></pre><p>最大的文件有 2GB，需要等待一会儿下载。部署完成后，访问 <code>localhost:8501</code> 即可。</p><p><img src="/2022/11/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED/浏览器交互页面.png" alt></p><p>启动 docker 进程之后，输入指令 <code>docker ps</code> 查看运行中的 container，输入 <code>docker ps &lt;CONTAINER ID&gt;</code> 即可结束该 container。</p><p><img src="/2022/11/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87%E6%96%87%E5%AD%97%E6%A3%80%E7%B4%A2%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%EF%BC%9AVCED/docker操作.png" alt></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://github.com/datawhalechina/vced/blob/main/README.md">https://github.com/datawhalechina/vced/blob/main/README.md</a></p><p><a href="https://docs.jina.ai/get-started/install/windows/">https://docs.jina.ai/get-started/install/windows/</a></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VCED</tag>
      
      <tag>multimodal</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】李宏毅ML-Note7-Transformer</title>
    <link href="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/"/>
    <url>/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/</url>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：Transformer。<br><!--more---></p><h2 id="一、Seq2Seq"><a href="#一、Seq2Seq" class="headerlink" title="一、Seq2Seq"></a>一、Seq2Seq</h2><p>Transformer是一类Seq2seq结构的模型，输入长度为N的序列，输出长度为M的序列。其中M的长度是由模型决定的。诸如文本生成、语音合成、机器翻译等任务都需要应用seq2seq模型。</p><p>很多任务可以被转化为seq2seq任务，但不一定是最优解。</p><p>seq2seq模型由一个编码器（Encodeer）和一个解码器（Decoder）组成，基础架构如下：</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-1.png" alt></p><p>seq2seq模型是一种架构，只规定了输入输出，并没有规定必须使用哪种具体的模型。</p><h2 id="二、Encoder"><a href="#二、Encoder" class="headerlink" title="二、Encoder"></a>二、Encoder</h2><p>如图所示是Transformer的Encoder：</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-2.png" alt></p><p>Transformer 的 Encoder 是由若干的 Block 构成的，每个 Block 包含一个 multihead self-attention 结构和一个全连接网络等，这些 Block 串联起来组成 Encoder。</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-3.png" alt></p><p>需要关注两个细节：</p><ol><li>self-attention 层的向量输出后，需要经过一步残差运算，和原来的输入相加；</li><li>残差运算后的向量需要 LayerNormalization 操作；</li><li>normalized 之后的向量需要输入内部的全连接网络，输出的向量同样要与这一步骤的输入进行残差；</li><li>在输出前同样需要经过 normalization。</li></ol><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-4.png" alt></p><p>实际上，在具体实现时，上面的操作顺序是可以改变的，有若干文章已经在讨论 LayerNormalization 的执行时机对最终性能的影响。</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-layer-norm.png" alt></p><h2 id="三、Decoder"><a href="#三、Decoder" class="headerlink" title="三、Decoder"></a>三、Decoder</h2><p>Decoder负责以Encoder的输出为知识，生成一系列输出向量。</p><h3 id="3-1-Decoder-的执行步骤"><a href="#3-1-Decoder-的执行步骤" class="headerlink" title="3.1 Decoder 的执行步骤"></a>3.1 Decoder 的执行步骤</h3><p>Transformer 的 Decoder 是一个自回归（AutoRegressive）模型，即使用自身以前的产出来预测将来的产出。</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-decoder-example-1.png" alt></p><p>在这里我们以语音转文字任务为例，以一串音频为输入（在图示的例子中，是“机器学习”四个字），Encoder 首先学习适当的语义向量，并将其输入 Decoder 中，Decoder 执行如下操作：</p><ol><li>Decoder 先以 [start] 符号作为输入，通过 Encoder 提供的知识，来预测该符号对应的输出是什么；</li><li>Decoder 产出的预测向量是一个长度为单词表大小的向量，向量的每个分量代表输出为该词时的概率值；</li><li>紧接着 Decoder 会以该输出为输入，来预测输出；</li><li>由此循环往复，直至 Decoder 认为模型的输出为特殊符号 [end]为止。</li></ol><h3 id="3-2-Decoder-的具体架构"><a href="#3-2-Decoder-的具体架构" class="headerlink" title="3.2 Decoder 的具体架构"></a>3.2 Decoder 的具体架构</h3><p>Transformer 的 Decoder 与 Encoder 架构十分类似：</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-decoder-structure.png" alt></p><ol><li>接纳模型输入的 attention 层是 masked self-attention 层，它的特点是，在考虑某个输入向量$x_n$的输出时，self-attention 会综合所有的上下文信息，而 masked self-attention 只会考虑已经出现过的 $x_1~x_n$ 范围内的所有向量，而不会考虑它的下文；<br> 这样做是自然的，因为 Decoder 是以过去的输出预测将来的输出的自回归模型，它无法参考尚未生成的下文内容；</li><li>经过 masked self-attention 的编码后的输出向量，会被当做下一层 self-attention 的 query 查询向量，然后与 encoder 得到的输出向量计算相关性；也就是说，query 来自 Decoder 的输入，而 key 和 value 都来自 Encoder 层。这一步骤叫做 Cross Attention。<br> 在 Decoder 中加入 Cross Attention ，是希望模型能够正确地利用 Encoder 提供的知识，并根据现有的输入，自动生成合适的输出表示。要完成这一步，Encoder 提供的先验知识与 Decoder 提供的查询词都是必不可少的。</li></ol><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-decoder-cross-attention.png" alt></p><p>Transformer 的 Encoder 与 Decoder 的详细架构如图：</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-encoder-decoder.png" alt></p><p>需要注意，原文中 Transformer 的多个 Decoder 结构使用的 Encoder 信息都来自于最后一个 Encoder，这样做是自然地，毕竟我们认为最后一层学习到的知识最抽象，泛化能力最强；</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-encoder-decoder-2.png" alt></p><p>事实上并不总是这样，也有一些文章在研究令不同时期的 Encoder 产出的向量送给不同时期的 Decoder，并取得了一定的效果：</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-cross-attention-various.png" alt></p><h3 id="3-3-非自回归-Decoder"><a href="#3-3-非自回归-Decoder" class="headerlink" title="3.3 非自回归 Decoder"></a>3.3 非自回归 Decoder</h3><p>存在一些 None AutoRegressive Decoder，这些 Decoder 不是以自身之前的输出预测之后的输出，而是一次性给出整个输出序列。</p><p>自回归问题是通过在输出集合中添加特殊符号 [end] 来解决序列不停止的问题的，但是 非自回归 Decoder 一次性输出整个序列，它只能预先设定一个序列长度值，并依此长度进行输出。这种类型的 Decoder 当然也会输出 [end] ，但 Decoder 并不会根据此来终止后续字符的生成。最终的结果会截断 [end] 之后的字符。</p><p>部分非自回归 Decoder 可以通过另外训练一个分类器，通过学习输入序列长度与输出序列长度之间的关系来动态产生序列的长度。</p><p>非自回归 Decoder 的优点在于可并行化，快速；而且输出序列的长度是人为可控的；</p><p>缺点在于准确率等相对不如自回归 Decoder。</p><h2 id="四、Transformer-的训练过程"><a href="#四、Transformer-的训练过程" class="headerlink" title="四、Transformer 的训练过程"></a>四、Transformer 的训练过程</h2><p>其实就是训练一个分类问题。假设问题为一段语音，问题的答案为“我爱中国”四个字，则 Encoder 会学习该语音的向量表示并输出给 Decoder，Decoder 则会分别输入 <code>BOS、我、爱、中、国</code>，作为五个训练样本；他们的正确答案为<code>我、爱、中、国、EOS</code>，每个字对应一个 one-hot 向量。最后通过优化交叉熵损失函数，按照正常的梯度下降算法进行优化即可。</p><p><img src="/2022/10/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note7-Transformer/transformer-training.png" alt></p><p>这种以正确答案为输入的训练方法称作 Teacher Forcing。</p><h2 id="五、Transformer-训练技巧"><a href="#五、Transformer-训练技巧" class="headerlink" title="五、Transformer 训练技巧"></a>五、Transformer 训练技巧</h2><h3 id="5-1-Copy-Mechanism"><a href="#5-1-Copy-Mechanism" class="headerlink" title="5.1 Copy Mechanism"></a>5.1 Copy Mechanism</h3><p>对于一些问答等问题，当输入中包含一些实体时，模型可以直接把该实体复制到输出序列中进行原样输出，称之为“复制机制”，这需要保证输入信息到输出信息之间的畅通。</p><p>实现 Copy Mechanism 的网络结构有指针网络等。</p><h3 id="5-2-Guided-Attention"><a href="#5-2-Guided-Attention" class="headerlink" title="5.2 Guided Attention"></a>5.2 Guided Attention</h3><p>通过人为限制注意力的学习方向、学习重点等，提升模型的学习能力。比如规定语音合成任务的注意力只能从左往右学习等。</p><h3 id="5-3-Beam-Search"><a href="#5-3-Beam-Search" class="headerlink" title="5.3 Beam Search"></a>5.3 Beam Search</h3><p>Beam Search 是一种帮助找到最优解的方法，其思想类似于动态规划，是为了解决 Decoder 在贪心地选择最优输出时，错过了次优输出，导致后续输出全错的问题。</p><p>Beam Search 并不总一定能找到最优解，而且一味追寻最优解，在一些需要发挥创意的任务中，反而不好，比如新闻生成任务等。</p><h3 id="5-4-优化评估指标"><a href="#5-4-优化评估指标" class="headerlink" title="5.4 优化评估指标"></a>5.4 优化评估指标</h3><p>一般而言 Decoder 的损失函数都是交叉熵，这是因为交叉熵方便求导好训练；而评估模型时使用的往往是 BLEU。但如果我们使用 BLEU 这种相对科学一点的评估函数来做损失函数的话，可能需要借助强化学习（Reinforcement Learning, RL）的思想，强行把 BLEU 移植到模型中。</p><h3 id="5-5-scheduled-sampling"><a href="#5-5-scheduled-sampling" class="headerlink" title="5.5 scheduled sampling"></a>5.5 scheduled sampling</h3><p>Decoder 在训练过程中，训练样本永远都是正确答案，缺乏一些负样本，告诉 Decoder 你不应该将预测值预测为该值。为了做到这一点，我们需要在训练时随机在输入序列中添加噪音，但这一举措可能会损失训练的并行化能力。</p><h2 id="六、总结和问答"><a href="#六、总结和问答" class="headerlink" title="六、总结和问答"></a>六、总结和问答</h2><h3 id="为什么要加入残差模块？"><a href="#为什么要加入残差模块？" class="headerlink" title="为什么要加入残差模块？"></a>为什么要加入残差模块？</h3><h3 id="为什么要加入-LayerNormalization-模块？"><a href="#为什么要加入-LayerNormalization-模块？" class="headerlink" title="为什么要加入 LayerNormalization 模块？"></a>为什么要加入 LayerNormalization 模块？</h3><h3 id="BatchNormalization-和-LayerNormalization-的区别？"><a href="#BatchNormalization-和-LayerNormalization-的区别？" class="headerlink" title="BatchNormalization 和 LayerNormalization 的区别？"></a>BatchNormalization 和 LayerNormalization 的区别？</h3><h3 id="Transformer-使用到的几种-Mask"><a href="#Transformer-使用到的几种-Mask" class="headerlink" title="Transformer 使用到的几种 Mask"></a>Transformer 使用到的几种 Mask</h3><h3 id="前馈神经网络在-Transformer-中的作用"><a href="#前馈神经网络在-Transformer-中的作用" class="headerlink" title="前馈神经网络在 Transformer 中的作用"></a>前馈神经网络在 Transformer 中的作用</h3><h3 id="Gelu-的作用"><a href="#Gelu-的作用" class="headerlink" title="Gelu 的作用"></a>Gelu 的作用</h3><h2 id="七、transformer-的代码实现"><a href="#七、transformer-的代码实现" class="headerlink" title="七、transformer 的代码实现"></a>七、transformer 的代码实现</h2>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>MOOC</tag>
      
      <tag>Self-Attention</tag>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】李宏毅ML-Note6-注意力机制</title>
    <link href="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <url>/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：自注意力机制。<br><!--more---></p><h2 id="一、输入从图像到序列"><a href="#一、输入从图像到序列" class="headerlink" title="一、输入从图像到序列"></a>一、输入从图像到序列</h2><p>图像分类任务的输入是固定的，比如都是28*28像素的黑白图片等等。但是一些输入和输出不是定长序列的任务，比如机器翻译、语音转文字等任务，传统模型在这些任务上的表现不好。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/不同输出对应不同任务.png" alt></p><p>比如词性标注任务，模型的输入是N个向量，输出是N个分类标签；比如文本分类任务，模型的输入是N个向量，输出只是一个分类标签；比如文本翻译任务，模型的输入是N个向量，输出可能是N’个向量，此时N’是未知的，需要机器自己进行判断和学习；这种任务叫做 sequences to sequences (seq2seq) 。</p><p>这些任务的输入特点是，都有多个向量作为一个输入，且向量的相对位置会影响输入的含义。</p><p>以词性标注任务举例，该任务的输入和输出向量个数都为N。我们首先把一句话切分为N个词，然后利用一些编码方式（比如 one-hot 或者 word2vec）将其变成N个定长向量。我们将这N个向量作为一个输入序列，输入到模型中，寄希望于模型能够把每个词的词性标注出来。所以这其实是一个分类任务。</p><p>但是当我们输入句子”I saw a saw.“时，前后两个saw的词性是不同的，如果模型只以词汇向量为输入，不考虑上下文的话，是无法得知这件事的。</p><p>为此，前人们做了以下改进，试图使用传统模型解决这个问题：</p><ol><li><p>通过 N-gram 模型，将两三个词打包成一个新词，”I saw a saw“经过3-gram模型的编码，会产生如下输入序列：</p><pre><code> I saw a saw -&gt; [i saw a], [saw a saw]</code></pre><p> 由此，模型能够看到的上下文就扩展到了三个单词。但是该种方法会快速扩大单词量，增加计算负担，且无法把距离较远的上下文也加入进来。</p></li><li><p>使用TextCNN</p><p> TextCNN 是把图像领域大货成功的CNN模型的经验移植到了NLP领域的成果，它使用一维卷积核。但究其本质还是受限于卷积核的大小。</p></li></ol><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/向量之间的相对位置.png" alt></p><p>有没有一种方法能够考虑输入向量的全部上下文呢？ self-attention 可以做到。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention与FC的区别.png" alt></p><p>self-attention 接受N个输入向量，输出N个输出向量。输出的向量不仅保留了原来向量的语义信息，还额外添加了该向量上下文的信息。</p><p>有了这个全新的向量，我们便不必使用诸如 n-gram 等特征工程手段了，self-attention 就能够产生非常好的稠密向量，该向量考虑的上下文范围是整篇文档。</p><p>self-attention 适合编码整篇文档的信息，产出向量后可以接一个全连接网络来进行具体的分类；也可以像之前的卷积网络一样，多层 self-attention + FC全连接进行堆叠。这实际上就是Transformer的基本思想。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/transformer的简化结构.png" alt></p><p>self-attention 是怎么做到的呢？</p><h2 id="二、self-attention-的实现原理"><a href="#二、self-attention-的实现原理" class="headerlink" title="二、self-attention 的实现原理"></a>二、self-attention 的实现原理</h2><p>self-attention 的输入是N个向量组成的输入序列，输出也是N个向量组成的序列。区别在于，输入的向量本身不包含任何与上下文有关的信息，但是与输入向量对应的输出向量会包含一定的上下文信息。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention的抽象结构.png" alt></p><p>如果要产生b1向量，其实是考虑了a1/a2/a3/a4全部四个向量后的结果。</p><p>具体地，首先我们需要考虑a1向量与其他向量a2/a3/a4之间的相关性，两个向量之间的相关性是一个标量，我们用$\alpha$来表示：</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理1_相关性.png" alt></p><p>相关性是如何计算得到的？有很多做法可以实现相关性的计算。需要注意的是这里的<strong>相关性</strong>并非矩阵向量空间中的相似性，而是一个<strong>需要机器从数据中学习</strong>的参数。下图是两种不同的相关性的计算方式。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理2_相关性的计算方法.png" alt></p><p>上图左边是一种常用的向量相关性计算法，首先把两个向量各自乘以一个参数矩阵，其中输入向量会得到对应的查询向量 query，上下文向量会得到对应的关键向量 key，然后我们把query和key进行点积，得到的就是两个向量的相关性。参数矩阵是可以通过数据进行学习的。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理3_相关性计算方法2.png" alt></p><p>由此我们推广开来，每个输入向量都可以计算属于自己的查询向量query，以及其他输入向量的关键向量key。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理4_相关性计算方法3.png" alt></p><p>输入向量还需要与自己计算相关性，这样凑齐4个相关性数值。由此我们就得到了输入向量a1与其他（包含自己的）四个向量之间的相关程度。</p><p>接下来我们需要做的，就是把四个输入向量都与该相关性进行乘积，然后相加，就得到了包含上下文相关信息的输出向量b1。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention原理5_相关性计算方法4.png" alt></p><p>再分别对a2/a3/a4向量如法炮制，就可以分别得到四个输出向量。</p><p>需要注意，输入向量最后参与计算时，为了将其转化为稠密向量，也需要乘以一个矩阵后才能参与计算。</p><h2 id="三、self-attention-的并行计算"><a href="#三、self-attention-的并行计算" class="headerlink" title="三、self-attention 的并行计算"></a>三、self-attention 的并行计算</h2><p>向量的转化操作都是矩阵乘法，这意味着对向量进行转化的操作是可以并行计算的。</p><ol><li>通过参数学习，将输入向量（一般是稀疏的）转化为query/key/value（稠密向量），然后将向量拼合形成矩阵，得到矩阵Q、K、V：</li></ol><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention的并行计算1.png" alt></p><ol><li>矩阵Q与K做点乘，得到相关性系数矩阵A</li></ol><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention-parallel-2.png" alt></p><ol><li>相关性矩阵A经过softmax进行概率归一化后，与矩阵V进行点乘，得到输出向量B</li></ol><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention-parallel-3.png" alt></p><p><strong>self-attention将输入向量转化为输出向量的过程到此结束。整个过程总结如下</strong>：</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention-parallel-summary.png" alt></p><h2 id="四、多头注意力-Multihead-Self-attention"><a href="#四、多头注意力-Multihead-Self-attention" class="headerlink" title="四、多头注意力 Multihead Self-attention"></a>四、多头注意力 Multihead Self-attention</h2><p>多头注意力机制，就是同时学习多种相关性的机制。</p><p>这样做的理由是：语言是存在同义词、一词多义等现象的，即便是同一句话，在不同语境下阐述也会产生不同的效果。如果只算一种相关性，模型便无法掌握一词多义等能力，因此我们需要学习多个相关性矩阵A。</p><p>如果想要学习多个相关性矩阵A，</p><ol><li>就必须有多个输入矩阵Q/K/V与之对应，每多一个A，就要多学习一类参数矩阵；</li><li>会产生多组输出矩阵B，需要再学习一个参数矩阵，将一系列的输出矩阵合并起来；</li></ol><p>多头注意力机制在原有注意力机制的基础上，把原有的query向量再多乘n个参数矩阵，将query复制为n个子向量；key和value向量亦是如此：</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/multi-head-self-attention-1.png" alt></p><p>query向量的第一个复制体会与各个key向量的第一个复制体进行计算相关性，生成相关性矩阵A1，第二个复制体会与各个key向量的第二个复制体计算相关性，生成相关性矩阵A2，以此类推。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/multi-head-self-attention-2.png" alt></p><p>由此会产出n份不同的输出向量，他们分别是考虑n种不同相关性下计算出来的输出向量。使用一个参数矩阵将他们合并起来：</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/multi-head-self-attention-3.png" alt></p><p>这就是多头注意力机制的设计思路。如果要计算N种相关性，就叫做N头注意力机制。</p><h2 id="五、位置编码"><a href="#五、位置编码" class="headerlink" title="五、位置编码"></a>五、位置编码</h2><p>上面的机制只介绍了 self-attention 如何把输入向量和它的上下文信息进行编码，但是忽略了上下文也是有位置关系的，理论上离当前位置更近的上下文应该获得更多注意力。因此我们需要额外添加一个能够表示位置的参数。</p><p>做法也很简单，只需要在输入层为每个向量添加一个额外的位置向量即可。</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/positional-encoding-1.png" alt></p><p>这个位置向量可以是你自己定义的，也可以作为参数放到网络中进行学习。总之它理论上应该能够表示位置的远近关系。</p><h2 id="六、self-attention-的具体应用"><a href="#六、self-attention-的具体应用" class="headerlink" title="六、self-attention 的具体应用"></a>六、self-attention 的具体应用</h2><p>上面我们是以文本为例介绍的self-attention，实际上有很多其他任务也可以用self-attention来解决（虽然不见得是最佳方案）。</p><p>语音识别任务里，通过将一段音频按照一定时间窗口进行切分，也可以得到一个包含很多向量的输入序列；则处理方法与文本任务大同小异，只不过语音分割时产生的输入序列会非常长，此时需要对计算过程加以优化。例子：Truncate Self-Attention</p><p>图像识别任务里，每个图像按照像素进行分割，并逐行遍历，也可以形成一个序列，这个序列里的每个输入向量是(R,G,B)的三维向量。例子：Self-Attention GAN</p><h2 id="七、其他变种-self-attention"><a href="#七、其他变种-self-attention" class="headerlink" title="七、其他变种 self-attention"></a>七、其他变种 self-attention</h2><h2 id="八、总结和提问"><a href="#八、总结和提问" class="headerlink" title="八、总结和提问"></a>八、总结和提问</h2><h3 id="1-Self-Attention-的核心思想？"><a href="#1-Self-Attention-的核心思想？" class="headerlink" title="1. Self-Attention 的核心思想？"></a>1. Self-Attention 的核心思想？</h3><p>通过综合考虑输入词与上下文之间的相关关系，得到包含上下文语义信息的输出向量。</p><h3 id="2-Self-attention-是如何计算的？"><a href="#2-Self-attention-是如何计算的？" class="headerlink" title="2. Self-attention 是如何计算的？"></a>2. Self-attention 是如何计算的？</h3><script type="math/tex; mode=display">\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V</script><p>其中Q、K、V都是从输入向量学习而来的稠密向量组成的矩阵，它们不包含上下文的语义信息；</p><ul><li>Q：查询向量，目标字作为 Query；</li><li>K：键向量，其上下文的各个字作为 Key；</li><li>V：值向量，上下文各个字的 Value；</li></ul><p>$d_k$是Q/K/V的维度。这样做是为了降低分量之间的方差，防止输入向量的维度过高导致点击出来的结果过大，使softmax出来的结果出现一些极端情况（比如只有一个分量是0.99999，其他分量都是0），进而导致训练困难的现象。</p><h3 id="3-Self-Attention-和-CNN-的异同？"><a href="#3-Self-Attention-和-CNN-的异同？" class="headerlink" title="3. Self-Attention 和 CNN 的异同？"></a>3. Self-Attention 和 CNN 的异同？</h3><p>CNN是一类特殊的Attention，即将注意力聚焦于感受野（卷积核）中的一种self-attention网络结构。</p><p>而self-attention的感受野范围是整个序列，可以自行学习哪些是需要重点关注的。</p><p>这也就意味着self-attention相较于CNN而言更复杂、参数更多，需要更多数据进行训练。</p><p>有讨论二者关系的论文：</p><blockquote><p>On the Relationship between Self-Attention and Convolutional Layers</p></blockquote><p>从所需的数据量和准确率比较上，可以辅证这一点：</p><p><img src="/2022/10/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/self-attention-versus-cnn.png" alt></p><p>那就是在数据量较小的情况下，CNN作为简单模型可以表现得更好；但是数据量充足时，self-attention具有更高的上限。</p><h3 id="4-Self-Attention-与-RNN-的比较？"><a href="#4-Self-Attention-与-RNN-的比较？" class="headerlink" title="4. Self-Attention 与 RNN 的比较？"></a>4. Self-Attention 与 RNN 的比较？</h3><p>RNN虽然也能够学习长距离依赖关系，但是它的结构和运作方式（要想计算b2，必须先计算b1，存在先后依赖关系）决定了它难以并行化，这就导致它的训练过程非常缓慢。</p><p>另外RNN在输入序列过长时，也存在梯度消失或者梯度爆炸的问题，导致无法记忆长距离信息。</p><p>Self-Attention的内部有大量矩阵乘法运算，可以被GPU优化的很快。</p><h3 id="5-MultiHead-Attention-的生效原理？"><a href="#5-MultiHead-Attention-的生效原理？" class="headerlink" title="5. MultiHead Attention 的生效原理？"></a>5. MultiHead Attention 的生效原理？</h3><p>借鉴了CNN中同一卷积层内使用多个卷积核的思想。类似于CNN中通过多通道机制进行特征选择。</p><p>Transformer中使用切头(split)的方法，是为了在不增加复杂度（$O(n^2 d)$）的前提下享受类似CNN中“不同卷积核”的优势。</p><p>在每个头的计算过程中，彼此之间相互独立，参数不共享，仅在最后将结果拼接起来，这样可以允许模型在不同的表示子空间里学习到相关的信息。</p><p>最后整合多个向量，把他们降维到一个向量的长度，这个过程也是在学习”到底哪个头学到的知识是有效的”。</p><h3 id="6-位置编码的计算方法？"><a href="#6-位置编码的计算方法？" class="headerlink" title="6. 位置编码的计算方法？"></a>6. 位置编码的计算方法？</h3><p>Attention is all you need 论文中的位置编码的实现方式如下：</p><script type="math/tex; mode=display">\text{PosEncoding}_{(\text{pos}, 2i)}=\sin{(\frac{\text{pos}}{10000^{2i/d_{model}}})} \\\text{PosEncoding}_{(\text{pos}, 2i+1)}=\cos{(\frac{\text{pos}}{10000^{2i/d_{model}}})}</script><p>其中pos表示词在句子中的位置，i则表示向量的分量。这种计算方法无需学习任何参数。</p><p>在BERT论文中，采用的位置编码就变成了Embedding的方法自动学习得到。</p><h2 id="附、self-attention-的代码实现"><a href="#附、self-attention-的代码实现" class="headerlink" title="附、self-attention 的代码实现"></a>附、self-attention 的代码实现</h2>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>MOOC</tag>
      
      <tag>Self-Attention</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】李宏毅2019ML-Note5-卷积神经网络</title>
    <link href="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：卷积神经网络。<br><!--more---></p><h2 id="为什么我们要用CNN？"><a href="#为什么我们要用CNN？" class="headerlink" title="为什么我们要用CNN？"></a>为什么我们要用CNN？</h2><p>要回答这个问题，首先要说明为什么全连接网络不适合做图像识别任务。</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/为什么要用CNN_1.png" alt></p><p>以图像为输入，会导致输入序列特别长。假设输入是一张<code>100*100</code>像素的图片，每个像素有<code>R/G/B</code>三个分量，要想将它整理为输入序列，每张图片的维度为<code>100*100*3=30000</code>。</p><p>输入层的参数过多，会导致中间层的参数过多，网络不仅训练代价提升，还有过拟合的风险。实际上图像分类任务用不着那么多参数，原因有三：</p><ol><li>每种特征所需的仅仅是图片的一小部分</li></ol><p>从神经网络的可解释性上来说，网络的第一层学习到的是最简单的特征分类，比如一张输入图片”有没有绿色出现，有没有黄色出现，有没有斜的条纹“等等；第二层会学到更复杂一些的特征组合，越往后学到的特征越抽象。</p><p>但是对于一个检测”是否有鸟嘴“的分类器来说，不需要看整张图片就可以知道这件事；<strong>输入序列可以从一整张图片优化为原图的一小部分</strong>。</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/为什么要用CNN_2.png" alt></p><ol><li>识别同一特征的分类器的部分参数可以共享</li></ol><p>对于检测鸟嘴的分类器而言，鸟嘴出现在图片的不同位置根本无所谓。我们不需要训练两组参数，来分别检测到底是左下方的鸟嘴还是右上方的鸟嘴。这意味着<strong>做同一特征分类的参数可以共享</strong>。</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/为什么要用CNN_3_参数共享.png" alt></p><ol><li><strong>输入序列的部分信息可以略过</strong></li></ol><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/为什么要用CNN_4.png" alt></p><p>对图片进行采样，得到的像素更低的图片，并不会影响人对这张图片的理解。</p><h2 id="CNN-架构"><a href="#CNN-架构" class="headerlink" title="CNN 架构"></a>CNN 架构</h2><p>卷积神经网络有两个基本单元：卷积层(Convolution Layers)和池化层(MaxPooling Layers)，可以实现上面三点性能优化。</p><p>一个完整的卷积神经网络大概长这样：</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CNN架构_1.png" alt></p><p>卷积层：是一种利用卷积操作提取图像的局部特征的网络结构。通过选择卷积核，不断执行卷积操作，得到原图像的特征图；后续的网络根据该特征图为输入，继续提取特征。</p><p>池化层：有最大池化和平均池化两种方式，最大池化是取滑动窗口内最大的元素作为输出。</p><h2 id="CNN-卷积层与全连接层的联系"><a href="#CNN-卷积层与全连接层的联系" class="headerlink" title="CNN 卷积层与全连接层的联系"></a>CNN 卷积层与全连接层的联系</h2><p>卷积核中的权值每次滑动计算时只是局部连接，且在卷积列中的神经元共享参数——计算局部信息，而全连接层神经元的权值与所有输入相连——计算全局信息。</p><p>卷积层的作用是从输入数据中采集关键数据内容。全连接层在深度卷积神经网络中的作用是将前面经过多次卷积后高度抽象的特征进行整合。</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CNN卷积层与全连接层的关系.png" alt></p><h2 id="用Keras实现CNN网络"><a href="#用Keras实现CNN网络" class="headerlink" title="用Keras实现CNN网络"></a>用Keras实现CNN网络</h2><p>以<a href="https://www.tensorflow.org/tutorials/images/cnn">tensorflow的图像分类教程</a>为例，实践使用keras搭建卷积模型的方法：</p><p>先导入相关包</p><pre><code class="lang-py">import tensorflow as tffrom tensorflow.keras import datasets, layers, modelsimport matplotlib.pyplot as plt</code></pre><p>然后下载并准备 CIFAR10 数据集</p><pre><code class="lang-py">(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()# 归一化像素值到 0 ~ 1train_images, test_images = train_images / 255.0, test_images / 255.0</code></pre><p>验证数据</p><pre><code class="lang-py">class_names = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;,               &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;]plt.figure(figsize=(10,10))for i in range(25):    plt.subplot(5,5,i+1)    plt.xticks([])    plt.yticks([])    plt.grid(False)    plt.imshow(train_images[i])    # The CIFAR labels happen to be arrays,     # which is why you need the extra index    plt.xlabel(class_names[train_labels[i][0]])plt.show()</code></pre><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CIFAR10样例.png" alt></p><p>接下来开始搭建网络：</p><pre><code class="lang-py">model = models.Sequential()model.add(layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(32, 32, 3)))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;))</code></pre><p>卷积网络被封装为<code>Conv2D</code>，其中第一个参数为卷积核个数，你可以理解为隐藏层神经元个数；我们这里定义为32个，它会提取32种不同类别的特征。</p><p>第二个参数是(3,3)，代表卷积核的大小是<code>3*3</code>的，这与李老师课堂上的示例一致；</p><p>第三个参数代表我们选择激活函数为<code>relu</code>；</p><p>第四个参数是只有在临近输入层的卷积层才会有的，代表输入向量的维度。这里的图像是彩色的，不但有长宽，还有通道数(channels)为3，因此输入向量的大小为(32,32,3)。</p><p>在搭建完第一层Conv2D后，紧接着会搭建最大池化层，池化层filter为<code>2*2</code>。后续的操作也是重复的，都是一层卷积、一层池化。到目前为止，模型结构如下：</p><pre><code class="lang-py">model.summary()</code></pre><pre><code>Model: &quot;sequential&quot;_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= conv2d (Conv2D)             (None, 30, 30, 32)        896        max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0          )                                                                conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496      max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0          2D)                                                              conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     =================================================================Total params: 56,320Trainable params: 56,320Non-trainable params: 0_________________________________________________________________</code></pre><p>我们可以看到，第一层Conv2D的输出向量的大小变成了(30,30,32)，这里32是我们之前定义的卷积核个数，<code>30*30</code>则是特征图大小。每次卷积操作都会导致宽度和高度的收缩。</p><p>参数量为896，这是如何计算来的呢？首先每个卷积核是<code>3*3</code>，同时图像有3个channel，因此每个卷积核的参数个数为<code>3*3*3</code>，再加上一个偏置量，就是28个参数。一共32个卷积核，最后的参数量即为<code>[(height * width * channel) + 1] * filter</code>。</p><p>卷积层只是特征提取器，最后为了完成分类操作，我们需要在卷积层后面拼接全连接层。不用太多，几层就够了：</p><pre><code class="lang-py">model.add(layers.Flatten())model.add(layers.Dense(64, activation=&#39;relu&#39;))model.add(layers.Dense(10))model.summary()</code></pre><pre><code>Model: &quot;sequential&quot;_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= conv2d (Conv2D)             (None, 30, 30, 32)        896        max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0          )                                                                conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496      max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0          2D)                                                              conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928      flatten (Flatten)           (None, 1024)              0          dense (Dense)               (None, 64)                65600      dense_1 (Dense)             (None, 10)                650       =================================================================Total params: 122,570Trainable params: 122,570Non-trainable params: 0_________________________________________________________________</code></pre><p>卷积层的输出在输入两个 Dense 层之前需要被展平（Flatten）为形状为 (1024) 的向量。</p><p>仅添加了两层全连接层，参数量就多了65600+650个，可以看到全连接层真的比CNN多很多冗余参数。</p><p>最后编译并训练一下：</p><pre><code class="lang-py">model.compile(optimizer=&#39;adam&#39;,              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),              metrics=[&#39;accuracy&#39;])history = model.fit(train_images, train_labels, epochs=10,                     validation_data=(test_images, test_labels))plt.plot(history.history[&#39;accuracy&#39;], label=&#39;accuracy&#39;)plt.plot(history.history[&#39;val_accuracy&#39;], label = &#39;val_accuracy&#39;)plt.xlabel(&#39;Epoch&#39;)plt.ylabel(&#39;Accuracy&#39;)plt.ylim([0.5, 1])plt.legend(loc=&#39;lower right&#39;)plt.show()test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)</code></pre><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/训练结果.png" alt></p><p>准确率只有不到70%，可以改进的地方还有很多。不过对于一个简单模型而言，在10分类任务上能达到如此成绩已是不容易了。</p><h2 id="CNN学到了什么？"><a href="#CNN学到了什么？" class="headerlink" title="CNN学到了什么？"></a>CNN学到了什么？</h2><p>分析第一层Conv到底学到了什么还是很容易的，因为每个卷积核实际上是在计算小范围内是否包含指定特征。但是后续Conv会根据该特征图计算更抽象的特征，我们该如何分析？</p><p>可以采用这种分析方法，首先固定参数，然后通过梯度下降法去找能够让某个神经元的输出值最大的输入图片。</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/生成最能激活某个神经元的图片.png" alt></p><p>上面就是12个filter对应的结果，这些图片的特征是某种纹路在图上不断的重复。因此实际上神经元会对这种不断重复的花纹做出最大的反应。</p><p>当我们以同样的分析方法分析卷积网络最后的全连接层（这里汇聚了卷积层提取出来的各路特征），得到的图像为：</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/全连接_生成最能激活某个神经元的图片.png" alt></p><p>发现最能激活某个神经元的图片，不再是那些纹路单一的图像了，而是带有某种花纹，似乎有某些含义。</p><p>但是当我们用同样的方法分析输出层时，得到的图像理论上应该是确切的数字，实际上确实乱码一样无法被辨认的图片：</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/输出层_生成最能激活某个神经元的图片.png" alt></p><p>人类虽然无法辨识，但是机器十分确定地告诉我们，左上角的图片就是0，右下角就是8。这种现象很有意思。</p><p>这种思想被应用于对抗样本攻击之中，可以生成一些令模型误判的诡异图片，并将其改造原本的数据集，就可以令模型犯错。</p><p>关于文本领域对抗样本的讨论在<a href="https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/">这里</a></p><p>Deep Dream则利用该思想，将CNN变成一个图像生成器，夸大化原有的输入图像：</p><p><img src="/2022/10/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/DeepDream夸大的图像.png" alt></p><p>右边有一只熊，这个熊原来是一个石头(对机器来说，这个石头有点像熊，它就会强化这件事情，所以它就真的变成了一只熊)。</p><h2 id="CNN的其他应用场景"><a href="#CNN的其他应用场景" class="headerlink" title="CNN的其他应用场景"></a>CNN的其他应用场景</h2><p>CNN可以应用于游戏领域，比如围棋</p><p>CNN可以应用于自然语言处理领域，比如文本分类、语音识别</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>MOOC</tag>
      
      <tag>Convolutional Neural Network</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】李宏毅ML-Note4-深度学习优化</title>
    <link href="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/"/>
    <url>/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：深度学习算法的优化。<br><!--more---></p><p>神经网络训练不好怎么办？为什么loss不下降？这里讨论的训练不好，是指在训练过程中的loss始终降不下来。从数学优化的角度，此时可能陷入了局部最小值或者鞍点等微分为零的点。</p><h2 id="1-优化陷入鞍点-Saddle-Point-或局部极小值-Local-Minima"><a href="#1-优化陷入鞍点-Saddle-Point-或局部极小值-Local-Minima" class="headerlink" title="1. 优化陷入鞍点 (Saddle Point) 或局部极小值 (Local Minima)"></a>1. 优化陷入鞍点 (Saddle Point) 或局部极小值 (Local Minima)</h2><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/陷入鞍点或者局部最小点.png" alt></p><p>鞍点是指在一个方向上的gradient=0，但是在其他方向的gradient不为零，说明loss仍有继续优化的空间。</p><p>局部最小点则是所有方向的gradient均为零，此时无论往哪个方向走都无法令loss继续变小。这种情况比较棘手。</p><p>如何确认到底是鞍点还是局部极值？</p><p>这里有个结论：如果该点处的二阶偏导数矩阵（海塞矩阵）是正定的，即该海塞矩阵的所有特征值都大于零，说明该点附近的所有值均大于该点，即该点为局部最小值。</p><p>如果海塞矩阵的特征值全为负，则说明这里是局部最大值。</p><p>其他情况下，如果海塞矩阵的特征值有正有负，说明这里是个鞍点。</p><p>求解该点处的二阶偏导数不但可以帮助判断是否为鞍点，还可以帮助我们选择优化的方向。如果该处是一个鞍点，则我们就选择一个海塞矩阵特征值为负值的特征向量，朝着这个特征向量的方向进行移动，就能够最快速走出鞍点。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/快速走出鞍点.png" alt></p><p>实际上求海塞矩阵是一个计算量比较大的操作，除非有必要，否则这种方法其实很少使用。事实上，很少有优化算法具备把损失函数给优化到局部最小值或者鞍点的程度，在此之前我们会受到其他问题的困扰，比如batch没有选择好、优化方法并不够优秀等。</p><h2 id="2-batch没有选择好"><a href="#2-batch没有选择好" class="headerlink" title="2. batch没有选择好"></a>2. batch没有选择好</h2><p>在使用随机梯度下降算法时，我们会定义一个batch大小，并把训练集分割成batch大小的一个个子集，每次梯度下降只在子集上进行遍历，每次训练完一个子集的内容叫做一个epoch。</p><p>batch的大小自然会影响网络训练的速度以及优化效果。小batch可以加速训练，缺点是小batch随机性更大，没办法代表整体的数据集。极端情况下，batch=1时，甚至会出现loss在原地不断震荡的情况。</p><p>然而batch过大则失去了意义，因为这会导致训练速度变慢。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/batch大小的权衡.png" alt></p><p>如果引入并行计算（即GPU运算），那么batch_size在一定程度内都可以很快。但是不同GPU支持的batch_size是有极限的（取决于显存）。</p><p>batch_size 对最终的优化效果有影响吗？是有的。大的batch_size可能会导致优化阶段产生问题。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/batch_size对最终优化效果的影响.png" alt></p><p>这种问题是怎么产生的呢？有一种解释是这样的，当我们试图以一个比较大的batch_size训练模型时，可能会卡在客观存在的局部极值点；但是如果我们使用比较小的batch，相当于人为引入了更多的随机性。w在batch_A上是局部极值点，但是在batch_B上就不一定了。</p><p>另外，在testing的时候，选择不同的batch也会让accuracy不一样。如果大的Batch对应的Testing结果差 ，代表 Overfitting 。</p><p>最后总结一下，batch_size可能造成的影响：</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/batch_size的选择总结.png" alt></p><h2 id="3-虽然-gradient-没有降低到0，但是-loss-已经不再-变小-了（没有选择合适的优化算法）"><a href="#3-虽然-gradient-没有降低到0，但是-loss-已经不再-变小-了（没有选择合适的优化算法）" class="headerlink" title="3. 虽然 gradient 没有降低到0，但是 loss 已经不再 变小 了（没有选择合适的优化算法）"></a>3. 虽然 gradient 没有降低到0，但是 loss 已经不再 变小 了（没有选择合适的优化算法）</h2><p>多数情况下，不会有真正陷入 critical point 的机会，而是在 critical point 附近徘徊。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/训练卡住了并不一定意味着gradient很小.png" alt></p><p>因此需要我们调整学习率。每个不同的参数应该在训练的不同阶段拥有不同的学习率。这种通过改变学习率来优化算法的训练速度的方法，在Note2中已有介绍。</p><p>常用的自适应改变参数学习率的算法有Adagrad算法、RMSProp算法等。</p><p>一种方法是使用带有momentum的优化算法。动量法在Note2里已有介绍，我们将梯度下降的优化过程比喻成小球滚落山坡的过程。但是梯度下降的优化过程忽略了小球本身具有动量，与现实中有一定区别。如果我们将梯度下降算法添加上动量的模拟，则小球在陷入局部最优解时，足够高的动量能帮助小球“冲”出局部最优解。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/动量法模拟.png" alt></p><p>使用动量法的优化算法有Adam等。实际上，Adam是Adagrad和Momentum动量法的结合。</p><p>此外，从经验上看，学习率一开始要保持大一些，来保证收敛到最优点附近；然后要减小学习率来避免震荡。那么我们自然可以想到，学习率跟随训练轮次变大逐渐变小。这就是<strong>学习率衰减</strong>。</p><p>在刚开始训练时，由于参数是随机初始化的，梯度也往往比较大，再加上比较大的初始学习率，会使得训练不稳定。因此我们希望刚开始几轮迭代的学习率较小，等梯度下降到一定程度时再恢复学习率。这种方法称之为<strong>学习率预热</strong>。等预热完毕后在进行学习率衰减。</p><p>利用这两个思想，改进Adam后，就是优化算法RAdam。</p><h2 id="4-损失函数也会有影响"><a href="#4-损失函数也会有影响" class="headerlink" title="4. 损失函数也会有影响"></a>4. 损失函数也会有影响</h2><p>分类问题的损失函数可以选择MSE和Cross-entropy。现在我们更多选择交叉熵。为什么MSE不行呢？</p><p>MSE在loss大的地方非常平坦，梯度趋近于零，最后stuck卡住！难以优化。</p><p>Cross-entropy ：左上角有斜率，可以透过梯度，一路往右下角走。更易收敛。</p><p><img src="/2022/10/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%85ML-Note4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/MSE和Cross-entropy.png" alt></p><h2 id="5-也许需要-normalization"><a href="#5-也许需要-normalization" class="headerlink" title="5. 也许需要 normalization"></a>5. 也许需要 normalization</h2><p>一般而言，样本特征由于来源以及度量单位不同，他们的尺度 (Scale) 即取值范围也不同。对于某些机器学习模型，会对那些尺度更大的数据更敏感。因此对于尺度敏感的模型，必须先对样本进行预处理，将各个维度的特征转换到相同的取值区间内，并且消除不同特征之间的相关性，才能取得理想的效果。</p><p>归一化 (Normalization) 方法泛指把数据特征转换为相同尺度的方法，比如把数据特征映射到 [0,1] 区间内，或者直接映射为均值为0、方差为1的正态分布。</p><p><strong>最小最大值归一化</strong>：将每个特征缩放到 [0,1] 或者 [-1,1] 之间。假设我们有 N 哥样本 ${x^{(n)}}^{N}_{n=1}$，对每一维特征x，归一化后的特征为：</p><script type="math/tex; mode=display">\hat{x}^{(n)}=\frac{x^{(n)}-\min_n(x^{(n)})}{\max_n(x^{(n)})-\min_n(x^{(n)})}</script><p><strong>标准化</strong>：将每维特征都调整到标准正态分布：</p><script type="math/tex; mode=display">\hat{x}^{(n)}=\frac{x^{(n)}-\mu}{\sigma}</script><p>除此之外，还有Batch-Normalization和Layer-Normalization的算法。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>MOOC</tag>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】李宏毅2019ML-Note3-深度学习介绍和反向传播</title>
    <link href="/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <url>/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：深度学习算法介绍、反向传播机制。<br><!--more---></p><h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><p>深度学习大家已经很熟悉了。深度学习技术属于一类机器学习技术。使用深度学习解决问题的过程同样需要三步骤：</p><p>Step1：选择深度学习模型：神经网络（Neural network），神经网络有各种不同类型，比如前馈神经网络、卷积神经网络等；<br>Step2：模型评估（Goodness of function）；<br>Step3：选择最优函数（Pick best function），不同的网络结构需要定义不同的参数学习方法，比如反向传播算法；</p><p>接下来逐一介绍三个步骤的具体内容。</p><h2 id="第一步：选择神经网络：从前馈网络开始"><a href="#第一步：选择神经网络：从前馈网络开始" class="headerlink" title="第一步：选择神经网络：从前馈网络开始"></a>第一步：选择神经网络：从前馈网络开始</h2><p>前馈（feedforward）也可以称为前向，从信号流向来理解就是输入信号进入网络后，信号流动是单向的，即信号从前一层流向后一层，一直到输出层，其中任意两层之间的连接并没有反馈（feedback），亦即信号没有从后一层又返回到前一层。</p><p>下图是一个由一层输入层、N层隐藏层和一层输出层构成的、每层包含M个神经元的全连接网络，神经元的激活函数为sigmoid：</p><p><img src="/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/前馈网络.png" alt></p><p>为什么叫全链接呢？因为layer1与layer2之间两两都有连接，所以叫做Fully Connect；</p><p>为什么叫前馈呢？因为现在传递的方向是由后往前传，所以叫做Feedforward。</p><p>当隐藏层N非常大时，我们就说这个网络非常深。现如今的网络结构动辄几百层，随之带来很大的计算开销。一个一个计算神经元的输入输出是不现实的，一种加速方法是将神经网络的信息传递过程具象化为矩阵运算。</p><p>假设下图的前馈网络：</p><p><img src="/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/前馈网络-矩阵计算演示.png" alt></p><p>计算方法就是：sigmoid（权重w【黄色】 * 输入【蓝色】+ 偏移量b【绿色】）= 输出</p><p>如果有很多层，就进行嵌套</p><p><img src="/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/很多层的全连接网络的矩阵计算.png" alt></p><p>所以整个神经网络运算就相当于一连串的矩阵运算。矩阵运算是可以被GPU并行加速的，因此深度学习模型才能够在如今这个算力大爆发的年代大放异彩。</p><p><strong>深度学习的本质是利用隐藏层学习到了输入特征的有效表示方法，代替了之前依靠数据科学家的经验的特征工程。</strong></p><h2 id="第二步：模型评估方法：选择损失函数"><a href="#第二步：模型评估方法：选择损失函数" class="headerlink" title="第二步：模型评估方法：选择损失函数"></a>第二步：模型评估方法：选择损失函数</h2><p>对于模型的评估，我们一般采用损失函数来反应模型的好差，所以对于神经网络来说，我们采用交叉熵（cross entropy）函数来对$y$和$\hat{y}$的损失进行计算，接下来我们就是调整参数，让交叉熵越小越好。</p><h2 id="第三步：选择最优模型：梯度下降，以及用反向传播算法优化计算"><a href="#第三步：选择最优模型：梯度下降，以及用反向传播算法优化计算" class="headerlink" title="第三步：选择最优模型：梯度下降，以及用反向传播算法优化计算"></a>第三步：选择最优模型：梯度下降，以及用反向传播算法优化计算</h2><p>梯度下降算法在之前的笔记已经讲过，神经网络的梯度下降算法也是相同的。</p><p><img src="/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/梯度下降.png" alt></p><p>梯度下降的具体过程：</p><p><img src="/2022/10/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML-Note3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/梯度下降的过程.png" alt></p><ol><li>给到$\theta$(weight and bias)</li><li>先选择一个初始的 $\theta^0$，计算 $\theta^0$ 的损失函数（Loss Function）设一个参数的偏微分</li><li>计算完这个向量（vector）偏微分，然后就可以去更新的你 $\theta$</li><li>百万级别的参数（millions of parameters）</li><li>反向传播（Backpropagation）是一个比较有效率的算法，让你计算梯度（Gradient） 的向量（Vector）时，可以有效率的计算出来</li></ol>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>MOOC</tag>
      
      <tag>Backpropagation</tag>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】李宏毅2019ML课程-Note2</title>
    <link href="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/"/>
    <url>/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/</url>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：偏差和方差、模型选择、梯度下降算法。<br><!--more---></p><h1 id="一、误差的来源：偏差与方差"><a href="#一、误差的来源：偏差与方差" class="headerlink" title="一、误差的来源：偏差与方差"></a>一、误差的来源：偏差与方差</h1><p>对学习算法除了通过实验估计其泛化性能，人们往往还希望能了解为什么具有这样的性能。误差来源于偏差 (bias) 和方差 (Variance)以及不可避免的噪声。</p><h2 id="1-偏差和方差的概念和来源"><a href="#1-偏差和方差的概念和来源" class="headerlink" title="1. 偏差和方差的概念和来源"></a>1. 偏差和方差的概念和来源</h2><p><strong>偏差</strong>度量了预测值与真实值的偏离程度，对应的是学习算法本身的拟合能力；<strong>方差</strong>度量了数据扰动对模型的影响，对应的是模型的稳定性；<strong>噪声</strong>则是对应问题对应的难度。</p><p>上面的结论说明，模型的性能是由模型能力、数据的充分性以及问题本身的难度决定的。由于噪音是问题本身的特性，不好解决，因此要想提升模型的性能，就需要采取措施降低偏差和方差。</p><h2 id="4-偏差和方差分解：重新考虑欠拟合、过拟合问题"><a href="#4-偏差和方差分解：重新考虑欠拟合、过拟合问题" class="headerlink" title="4. 偏差和方差分解：重新考虑欠拟合、过拟合问题"></a>4. 偏差和方差分解：重新考虑欠拟合、过拟合问题</h2><p>为了避免过拟合，我们经常会在模型的拟合能力和复杂度之间进行权衡。拟合能力强的模型一般复杂度会比较高，容易导致过拟合。相反，如果限制模型的复杂度，降低其拟合能力，有可能导致欠拟合。因此，如何在模型的拟合能力与复杂度之间取得平衡，对机器学习算法来说十分重要。</p><p>偏差-方差分解为我们提供了一个很好用的分析工具。数学推导过程比较复杂，结论为：</p><p><strong>最小化期望错误等价于最小化偏差和方差之和。</strong></p><p>下图给出了机器学习模型四种偏差和方差的组合情况。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/偏差和方差.png" alt></p><p>每个图的中心店为最优模型$f(x)$，蓝点为不同训练集上得到的模型$f^*(x)$。左上角的图是一种理想情况，方差和偏差都比较低；右上角为高偏差低方差的情况，表示模型的泛化能力很好，但是拟合能力不足，类似于我们之前的线性模型；左下角是低偏差高方差的情况，表示模型的拟合能力很好，但是泛化能力较差。当训练数据比较少的时候往往会出现这种情况，我们一般把他称之为过拟合；右下角为高偏差高方差的情况，是最差的情况，等于没训练。</p><p>方差一般会随着训练样本的增加而减小。当样本比较多，方差比较小，这是可以选择能力强的模型来减少偏差；当训练集比较有限时，最优的偏差和方差往往无法兼顾，我们称之为“偏差-方差窘境”。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/偏差方差窘境.png" alt></p><p>偏差-方差窘境说的是，模型复杂度增加后，虽然你和能力变强导致偏差减少，但是方差会增大，过拟合现象会导致性能下降。</p><p>偏差-方差分解给机器学习模型提供了一种分析途径，能够指导我们解决过拟合和欠拟合的问题，但是实际操作中很难直接衡量。下面是一些通用的方法。</p><pre><code>欠拟合现象：模型在训练集上的错误率很高，此时说明模型偏差较大。欠拟合解决：1. 增加数据特征2. 提高模型复杂度3. 减小正则化系数</code></pre><pre><code>过拟合现象：模型在训练集上的错误率较低，但是在测试集上较高，此时说明模型方差较大。过拟合解决：1. 降低模型复杂度；2. 加大正则化系数；3. 引入先验知识，比如数据清洗，剔除无用特征等；4. 使用更多数据进行训练，但这个往往成本最高。</code></pre><p>此外，还有一种降低方差的方法：集成模型，即通过多个高方差模型进行平均，来降低方差。这背后的原理在介绍集成学习时会介绍。</p><h1 id="二、模型选择方法"><a href="#二、模型选择方法" class="headerlink" title="二、模型选择方法"></a>二、模型选择方法</h1><p>前面讨论了机器学习的三个步骤，以及如何改进模型的性能。那么我们如何正确评估模型呢？</p><p>之前的例子中，我们把数据集分成两部分，Training Set、Test Set。一般比例控制在4:1左右。我们在训练结束后，使用测试集进行评估，并判断训练效果。</p><p>这样做其实存在问题。如果我们真的使用测试集来指导模型的选择、参数的改进等各个步骤，那算不算模型正在学习测试集的内容呢？就好像学生做完考卷后，老师虽然不会直接告诉学生正确答案，但是会不断地给学生机会，告诉你：你这里错了，那里错了。这算不算是另一种泄题呢？</p><p>上面的数据泄露现象是经常发生的。为了避免数据泄露，一般可以通过把数据分成三部分：训练集 (Training Set)、验证集 (Validation Set) 和测试集 (Test Set)。模型参数的改进、模型选择等过程，只使用验证集；最后评估模型时使用测试集。</p><p>但是这样又会带来另一个问题：本来数据就匮乏，又分割出两大部分不能使用，可供学习的数据就更少了。<strong>k折交叉验证</strong>可以解决这个问题。</p><h2 id="2-1-交叉验证"><a href="#2-1-交叉验证" class="headerlink" title="2.1 交叉验证"></a>2.1 交叉验证</h2><p>交叉验证 (Cross-Validation) 是一种评估泛化性能的统计学方法。它比单次划分训练集和测试集的方法更加稳定。最常用的交叉验证方法是<strong>k折交叉验证</strong>。</p><p>首先把数据集分成大致相等的k个部分，每一部分叫做一折；接下来训练k个模型，第一个模型以第1折数据作为测试集，其他作为训练集；第二个模型以第2折数据作为测试集，其他作为训练集……最后我们得到了k个模型，以及对应的k个精度值。模型的最终精度就是这k个精度值的平均值。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/交叉验证.png" alt></p><p>交叉验证的优点：</p><ol><li>帮助更好评估模型的真实泛化性能，防止数据集分割时的随机性影响评估准确率；</li><li>所有数据都有机会被训练，提高数据利用率；</li></ol><p>交叉验证的缺点：</p><ul><li>显著增加了计算成本，需要训练k个模型，是原来的k倍。</li></ul><h1 id="三、梯度下降算法详解，以及优化方法"><a href="#三、梯度下降算法详解，以及优化方法" class="headerlink" title="三、梯度下降算法详解，以及优化方法"></a>三、梯度下降算法详解，以及优化方法</h1><p>目前，机器学习中参数学习的主要方式是通过梯度下降法来寻找一组最小化损失函数的参数。再具体视线中，梯度下降法可以分为：批量梯度下降、随机梯度下降以及小批量梯度下降三种形式。</p><p>本节还会介绍一些梯度下降算法的变种，他们大多改善了以下两部分的内容：1) 调整学习率，使优化更稳定；2) 梯度估计修正，提升训练速度。</p><h2 id="3-1-梯度下降算法的类别"><a href="#3-1-梯度下降算法的类别" class="headerlink" title="3.1 梯度下降算法的类别"></a>3.1 梯度下降算法的类别</h2><h3 id="3-1-1-批量梯度下降"><a href="#3-1-1-批量梯度下降" class="headerlink" title="3.1.1 批量梯度下降"></a>3.1.1 批量梯度下降</h3><p>最传统的梯度下降算法，上篇笔记已经介绍。</p><p>梯度下降的具体过程是这样的：</p><ol><li>选择一个初始$\theta_0$</li><li>计算该位置下$\theta$对L的微分，这个微分对应函数在此处下降最快的方向；</li><li>将$\theta_0$朝着这个方向移动一小步$ \eta$</li><li>在新的位置开始新一轮迭代计算，直到$w$不再变化为止。</li></ol><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/批量梯度下降法.png" alt></p><p>其中$\eta$叫做学习率，它控制了每次梯度下降迭代的优化幅度。$\eta$越大，学习得越快。然而$\eta$并不是越大越好。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/批量梯度下降法的梯度深谷.png" alt></p><p>如果$\eta$过大，优化过程很可能会跨越最低点，从而永远达不到最优解；如果$\eta$过小，则会造成训练缓慢，甚至陷入局部最优解。因此调整学习率$\eta$是优化梯度下降算法的一大思路。</p><p>梯度下降算法的另一个问题是，当训练数据很大时（这很常见），梯度下降算法会花费很长时间去遍历整个训练集。大多数情况下，我们不必遍历所有训练集。</p><h3 id="3-1-2-随机梯度下降"><a href="#3-1-2-随机梯度下降" class="headerlink" title="3.1.2 随机梯度下降"></a>3.1.2 随机梯度下降</h3><p>在训练机器学习模型时，训练数据的规模比较大的情况下，批量梯度下降的每次梯度迭代都要遍历整个数据集，这会造成计算资源的浪费。为了解决批量梯度下降算法导致的训练缓慢的问题，随机梯度下降算法应运而生。其思想是通过随机选取少量训练输入样本来计算$\nabla L_x$，进而估算$\nabla L$。</p><p>更准确地说，随机梯度下降通过随机选取少量的m个训练数据来训练。也叫做小批量梯度下降法 (Mini-Batch Gradient Descent)。</p><p>假设样本数量m足够大，我们期望$\nabla L<em>{X_j}$的平均值大致相等于整个$\nabla L</em>{x}$的平均值，即：</p><script type="math/tex; mode=display">\frac{\sum^m_{j-1}\nabla L_{X_j}}{m}\approx \frac{\sum_x \nabla L_x}{n}=\nabla L</script><p>这里第二个求和符号实在整个训练数据上进行的。交换两边我们得到：</p><script type="math/tex; mode=display">\nabla L \approx \frac{1}{m}\sum^m_{j=1}\nabla L_{X_j}</script><p>证实了我们可以进通过计算随机选取的小批量数据的梯度来估算整体的梯度。</p><p>影响小批量梯度下降的主要因素有：1）批大小m，2）学习率$\eta$，3）梯度估计方法。</p><h2 id="3-2-批量大小选择"><a href="#3-2-批量大小选择" class="headerlink" title="3.2 批量大小选择"></a>3.2 批量大小选择</h2><p>直观来说，批包含的数据越大，方差也就越小，训练越稳定。该种情况下，可以设置一个较大的学习率。学习率越大，需要的批大小就越大。</p><p>另外，根据经验，批越大越可能收敛到“尖锐最小值”；批越小越能收敛到“平坦最小值”。</p><h2 id="3-3-学习率调整"><a href="#3-3-学习率调整" class="headerlink" title="3.3 学习率调整"></a>3.3 学习率调整</h2><p>学习率过大会导致模型不收敛，如果过小会导致收敛太慢。由此，有一些学者根据学习的不同阶段，制定了学习率的不同变化策略。</p><p>从经验上看，学习率一开始要保持大一些，来保证收敛到最优点附近；然后要减小学习率来避免震荡。那么我们自然可以想到，学习率跟随训练轮次变大逐渐变小。这就是<strong>学习率衰减</strong>。</p><p>在刚开始训练时，由于参数是随机初始化的，梯度也往往比较大，再加上比较大的初始学习率，会使得训练不稳定。因此我们希望刚开始几轮迭代的学习率较小，等梯度下降到一定程度时再恢复学习率。这种方法称之为<strong>学习率预热</strong>。等预热完毕后在进行学习率衰减。</p><h3 id="3-3-3-AdaGrad-算法"><a href="#3-3-3-AdaGrad-算法" class="headerlink" title="3.3.3 AdaGrad 算法"></a>3.3.3 AdaGrad 算法</h3><p>AdaGrad算法的做法是，每次迭代时，每个参数的学习率都把它除上之前微分的均方根。</p><p>普通的梯度下降算法采用的参数更新思路：</p><script type="math/tex; mode=display">w^{t+1}\leftarrow w^t-\eta^tg^t \\\eta^t=\frac{\eta^t}{\sqrt{t + 1}}</script><p>则Adagrad是这样更新的：</p><script type="math/tex; mode=display">w^{t+1}\leftarrow w^t-\frac{\eta^t}{\sigma^t}g^t \\g^t =\frac{\partial L(\theta^t)}{\partial w}</script><p>其中$\sigma^t$是之前参数的所有微分的均方根，对于每个参数都是不一样的。</p><p>将Adagrad的式子化简：</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/化简后的adagrad.png" alt></p><p>Adagrad的设计思路为，如果某个参数的偏导数累计比较大，其学习率就会相对较小；相反，如果偏导数累计较小，则学习率相对较大。但是整体上迭代次数越多，学习率越小。</p><p>AdaGrad的缺点是，如果在经过一定次数的迭代，依然没有找到最优点时，由于学习率已经非常小，很难再继续优化了。</p><h3 id="3-3-4-RMSprop-算法"><a href="#3-3-4-RMSprop-算法" class="headerlink" title="3.3.4 RMSprop 算法"></a>3.3.4 RMSprop 算法</h3><p>RMSProp算法可以帮助解决某些情况下Adagrad算法过早衰减的问题。</p><p>RMSProp算法在AdaGrad的基础上，改进了参数偏导数的累计方式。Adagrad算法是计算每个参数梯度平方的累计值：</p><script type="math/tex; mode=display">G_t=\sum^t_{\tau=1}\textbf{g}_{\tau}\odot\textbf{g}_{\tau}</script><p>其中$\textbf{g}_{\tau}\in\mathbb{R}^{|\theta|}$是第$\tau$ 次迭代时的梯度。</p><p>RMSProp算法首先计算每次迭代梯度 $\textbf{g}_t$平方的指数衰减移动平均：</p><script type="math/tex; mode=display">G_t=\beta G_{t-1}+(1-\beta)\textbf{g}_t\odot\textbf{g}_t \\=(1-\beta)\sum^t_{\tau=1}\beta^{t-\tau}\textbf{g}_{\tau}\odot\textbf{g}_{\tau}</script><p>其中$\beta$为衰减率，一般为0.9。</p><p>RMSProp算法的参数更新差值为：</p><script type="math/tex; mode=display">\Delta\theta_t=-\frac{\alpha}{\sqrt{G_t+\epsilon}}\odot\textbf{g}_t</script><p>其中$\alpha$是初始的学习率，比如0.001。</p><p>在迭代过程中，由于每个参数的学习率并不是衰减趋势，因此既可以变小也可以变大。</p><h2 id="3-4-梯度估计"><a href="#3-4-梯度估计" class="headerlink" title="3.4 梯度估计"></a>3.4 梯度估计</h2><p>除了调整学习率之外，还可以进行梯度估计的修正。这样做的原因是，小批量梯度下降选取的样本具有随机性，如果每次选取的样本数量比较小，则损失可能会呈现震荡的方式下降。一般我们可以采取使用最近一段时间内的平均梯度来代替当前时刻的随机梯度的做法来缓解随机性，提升优化速度。</p><h3 id="3-4-1-动量法"><a href="#3-4-1-动量法" class="headerlink" title="3.4.1 动量法"></a>3.4.1 动量法</h3><p>还记得之前我们说过的，梯度下降算法容易陷入局部最优解吗？我们将梯度下降的优化过程比喻成小球滚落山坡的过程。但是梯度下降的优化过程忽略了小球本身具有动量，与现实中有一定区别。如果我们将梯度下降算法添加上动量的模拟，则小球在陷入局部最优解时，足够高的动量能帮助小球“冲”出局部最优解。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/梯度下降算法容易陷入局部最优.png" alt></p><p>在第t次迭代是，计算负梯度的“加权移动平均”作为参数的更新方向：</p><script type="math/tex; mode=display">\Delta\theta_t=\rho\Delta\theta_{t-1}-\alpha\textbf{g}_t=-\alpha\sum^t_{\tau=1}\rho^{t-\tau}\textbf{g}_{\tau}</script><p>其中$\rho$为动量因子，一般为0.9，$\alpha$为学习率。</p><p>这样一来，每个参数的实际更新差值取决于最近一段时间内梯度的加权平均值。当某个参数在最近一段时间内的梯度方向都一致，其真是更新参数的幅度会变大，相当于加速冲刺；当某个参数在最近一段时间内的梯度方向不一致，则参数更新幅度会变小，相当于刹车。从某种角度来说，当前梯度叠加部分上次的梯度，这种做法可以近似看做二阶梯度。</p><h3 id="3-4-3-Adam-算法"><a href="#3-4-3-Adam-算法" class="headerlink" title="3.4.3 Adam 算法"></a>3.4.3 Adam 算法</h3><p>Adam算法是如今最常用的优化算法了，它是RMSProp算法和动量法的结合，不但使用动量作为参数更新方向，而且可以自适应调整学习率。</p><h2 id="3-3-数据预处理"><a href="#3-3-数据预处理" class="headerlink" title="3.3 数据预处理"></a>3.3 数据预处理</h2><p>一般而言，样本特征由于来源以及度量单位不同，他们的尺度 (Scale) 即取值范围也不同。对于某些机器学习模型，会对那些尺度更大的数据更敏感。因此对于尺度敏感的模型，必须先对样本进行预处理，将各个维度的特征转换到相同的取值区间内，并且消除不同特征之间的相关性，才能取得理想的效果。</p><p>归一化 (Normalization) 方法泛指把数据特征转换为相同尺度的方法，比如把数据特征映射到 [0,1] 区间内，或者直接映射为均值为0、方差为1的正态分布。</p><p><strong>最小最大值归一化</strong>：将每个特征缩放到 [0,1] 或者 [-1,1] 之间。假设我们有 N 哥样本 ${x^{(n)}}^{N}_{n=1}$，对每一维特征x，归一化后的特征为：</p><script type="math/tex; mode=display">\hat{x}^{(n)}=\frac{x^{(n)}-\min_n(x^{(n)})}{\max_n(x^{(n)})-\min_n(x^{(n)})}</script><p><strong>标准化</strong>：将每维特征都调整到标准正态分布：</p><script type="math/tex; mode=display">\hat{x}^{(n)}=\frac{x^{(n)}-\mu}{\sigma}</script><h2 id="3-4-梯度下降算法的局限性"><a href="#3-4-梯度下降算法的局限性" class="headerlink" title="3.4 梯度下降算法的局限性"></a>3.4 梯度下降算法的局限性</h2><p>上一篇笔记已经有提到该部分内容：</p><p>首先，<strong>梯度下降算法容易陷入局部最优，找不到全局最优解</strong>。</p><p>如下图所示，当梯度下降算法优化到local minima 时，前面有座高山，它的梯度是正的，优化算法会强迫我们往回走。</p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/梯度下降算法局部最优.png" alt></p><p><img src="/2022/10/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-Note2/梯度下降局部最优2.png" alt></p><p>这个问题在我们的线性回归模型里暂时遇不到，因为线性模型非常简单，是一个凸函数。但当我们试图利用梯度下降算法训练复杂模型，比如神经网络时，就很有可能遭遇该问题。</p><p>有很多方法能够解决梯度下降算法的全局最优解问题，包括调整每次行进的步长$\eta$，使其更有希望跨越“大山”等手段等等。</p><p>其次，梯度下降算法要求目标函数是可微的，这在某些情况下会<strong>产生相当大的计算代价</strong>。假设我们的问题有上百万维，计算二阶偏导数就需要上万亿（百万的平方）次！</p><p>再次，<strong>当训练数据相当多时，梯度下降算法会变得很慢</strong>。在实践中，为了计算梯度$\nabla L$，我们需要为了每个训练样本x单独计算梯度$\nabla L_x$，然后求平均值。这会花费很长时间。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>MOOC</tag>
      
      <tag>Gradient Descent</tag>
      
      <tag>Bias and Variance</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】李宏毅2019ML课程-Note1</title>
    <link href="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/"/>
    <url>/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/</url>
    
    <content type="html"><![CDATA[<p>李宏毅老师主讲的机器学习MOOC课程的笔记，本篇记录的知识点有：机器学习的概念和分类、利用线性模型解决回归问题以及梯度下降。<br><!--more---></p><h3 id="一、机器学习介绍"><a href="#一、机器学习介绍" class="headerlink" title="一、机器学习介绍"></a>一、<a href="https://www.bilibili.com/video/av59538266">机器学习介绍</a></h3><p>说一下我学这门课的初衷吧。其实机器学习的课程在研究生阶段就已经上过了，但是工作之后才发现有一些基础没搞懂或者已经遗忘，因此在学习新知识时存在障碍。恰逢Datawhale给了这次组队学习的机会，我便想与群友们一起把这门基础课程搞定。</p><h4 id="1-概要"><a href="#1-概要" class="headerlink" title="1. 概要"></a>1. 概要</h4><p>本节通俗易懂地介绍了机器学习的概念，介绍了AI的发展历史，以及与传统规则的区别。简单来说，<strong>机器学习</strong>是一种从有限数据中学习规律并对未知数据进行预测的方法。</p><p>刚开始讲了比较多的名词和概念。老师最后的图里很好地总结了本次课的内容：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/机器学习介绍01.png" alt></p><h4 id="首先，机器学习的过程分成三个步骤："><a href="#首先，机器学习的过程分成三个步骤：" class="headerlink" title="首先，机器学习的过程分成三个步骤："></a>首先，机器学习的过程分成三个步骤：</h4><ol><li>根据问题的不同，选择模型（function）；</li><li>根据模型的不同，定义能够度量学习效果的损失函数；</li><li>从有限数据中持续训练模型，使得损失函数最小。</li></ol><h4 id="其次，根据学习所需的数据是否存在标签，机器学习可以分成如下几类（图中蓝色框框的部分）："><a href="#其次，根据学习所需的数据是否存在标签，机器学习可以分成如下几类（图中蓝色框框的部分）：" class="headerlink" title="其次，根据学习所需的数据是否存在标签，机器学习可以分成如下几类（图中蓝色框框的部分）："></a>其次，根据学习所需的数据是否存在标签，机器学习可以分成如下几类（图中蓝色框框的部分）：</h4><ol><li>监督学习，指学习所需的数据是经过人工标注的；</li><li>无监督学习，指学习所需的数据不需人工标注；</li><li>其他学习方法，比如半监督学习、迁移学习、强化学习等。</li></ol><h4 id="在监督学习里，根据想要解决的问题不同，可分为如下几类（图中黄色框框的部分）："><a href="#在监督学习里，根据想要解决的问题不同，可分为如下几类（图中黄色框框的部分）：" class="headerlink" title="在监督学习里，根据想要解决的问题不同，可分为如下几类（图中黄色框框的部分）："></a>在监督学习里，根据想要解决的问题不同，可分为如下几类（图中黄色框框的部分）：</h4><ol><li>分类问题，指我们希望模型给出yes或no这样具体的评价；</li><li>回归问题，指我们希望模型能够给出一个数值，这个数值的大小有现实意义；</li><li>结构化问题，我们希望模型直接输出结构化结果，比如语言翻译模型能够产出一段文本，dall-e能够生成图像，等等。</li></ol><h4 id="分类问题是比较简单的问题，有很多模型可以解决分类问题（图中绿色框框的部分）："><a href="#分类问题是比较简单的问题，有很多模型可以解决分类问题（图中绿色框框的部分）：" class="headerlink" title="分类问题是比较简单的问题，有很多模型可以解决分类问题（图中绿色框框的部分）："></a>分类问题是比较简单的问题，有很多模型可以解决分类问题（图中绿色框框的部分）：</h4><ol><li>线性模型；</li><li>非线性模型，比如深度学习、SVM、决策树等方法。</li></ol><p>后续我们会学到什么是线性模型、什么是非线性模型，以及上面的具体模型的设计细节。</p><h4 id="为什么要学习机器学习这门课程？"><a href="#为什么要学习机器学习这门课程？" class="headerlink" title="为什么要学习机器学习这门课程？"></a>为什么要学习机器学习这门课程？</h4><p><del>当然是为了挣钱了</del></p><p>由于目前还没有一个普适的学习模型，能够解决世界一切可以用机器学习方法解决的问题，因此，我们需要依赖经验和知识，来根据不同的问题，选择不同的学习模型和和损失函数。还记得机器学习的三个步骤吗？选择模型、定义损失函数、训练模型过程的知识，都是能够帮助我们得到更加可靠的机器学习系统的技能。学习这门课程，能够帮助我们成为一名更好的机器学习工程师。</p><h3 id="二、机器学习案例——回归问题"><a href="#二、机器学习案例——回归问题" class="headerlink" title="二、机器学习案例——回归问题"></a>二、<a href="https://www.bilibili.com/video/BV1Ht411g7Ef?p=3">机器学习案例——回归问题</a></h3><p>这部分以一个回归问题为引子，引导我们探索机器学习的三大步骤。回归问题比较普遍，像股票预测、温度预测、房价预测等，都是回归问题。</p><p>机器学习的三大步骤分别为：</p><ol><li>根据问题的不同，选择模型（function）；</li><li>根据模型的不同，定义能够度量学习效果的损失函数；</li><li>从有限数据中持续训练模型，使得损失函数最小。</li></ol><h4 id="第一步，为回归问题选择合适的模型"><a href="#第一步，为回归问题选择合适的模型" class="headerlink" title="第一步，为回归问题选择合适的模型"></a>第一步，为回归问题选择合适的模型</h4><p>这里我们使用线性模型试试水。所谓<strong>线性模型</strong> (Linear Model) 是把输入数据的各种特征，通过线性组合的方式进行预测。</p><script type="math/tex; mode=display">y=b+\Sigma\omega_{i}x_{i}</script><p>其中y是预测值，x是特征本身，w是特征对应的参数。我们的学习目标就是找到正确的w，令预测值y尽可能靠谱。</p><h4 id="第二步，为线性模型选择合适的评估函数"><a href="#第二步，为线性模型选择合适的评估函数" class="headerlink" title="第二步，为线性模型选择合适的评估函数"></a>第二步，为线性模型选择合适的评估函数</h4><p>知错能改，善莫大焉。每当模型预测得到一个值，为了让模型认识到自己的预测值y与真实值$\hat{y}$的差距，我们不妨直接取二者之差，作为计算差距的<strong>损失函数</strong> (Loss Function)。</p><script type="math/tex; mode=display">L(f)=\sum (y-\hat{y})</script><p>这样肯定是有问题的，假设我们有两组数据，一组超过真实值0.5，另一组低于预测值0.5，它们与真实值的差值分别是0.5与-0.5。结果经过我们的计算，损失函数居然为0，也就是没有损失？</p><p>为了避免上述情况的出现，我们选择平方损失函数作为衡量差距的手段：</p><script type="math/tex; mode=display">L(f)=\sum (y-\hat{y})^2</script><p>其实也有其他的损失函数定义方法可以规避第一个损失函数的问题，比如使用绝对值。但这里我们就钦定平方损失函数了，它有很多好处，但是我们先按下不表。</p><h4 id="第三步，进行训练，并利用损失函数指导训练过程"><a href="#第三步，进行训练，并利用损失函数指导训练过程" class="headerlink" title="第三步，进行训练，并利用损失函数指导训练过程"></a>第三步，进行训练，并利用损失函数指导训练过程</h4><p>到目前为止，我们有很多带标签的数据 $(x,\hat{y})$，有线性模型，有损失函数。那我该怎么得到训练好的模型呢？</p><p><strong>最好的模型</strong>到底是什么？对于线性模型来说，其实求解最优的$w$和$b$，从而可以让我们的模型无论输入什么$x$，都能准确得到与真实值相差无几的$y$。</p><p>让我们忘掉w和b的具体含义、晦涩不清的L函数，专心解决这个问题：如何优化$w$和$b$，以便L达到最小？</p><script type="math/tex; mode=display">w^*=arg\min_{w}L(w) \\b^*=arg\min_{b}L(b)</script><p>一种解决问题的方法使用微积分来嗯算，通过计算导数去寻找L的极值点。运气好的话，L的变量不多，该方法看似可行；运气不好的话，我们面对的问题过于复杂、参数过多，问题就不好解决了。</p><p>还有一种笨办法是穷举所有可能的$w$，选择能使L最小的$w^*$即可。这种方法虽然可行，但是没有效率。</p><p>有一种通用的最优化方法，叫做<strong>梯度下降</strong> (Gradient Descent) 方法，专门用于解决这种凸优化问题。使用梯度下降算法的前提是优化目标是可微的。</p><h5 id="1-梯度下降算法简介"><a href="#1-梯度下降算法简介" class="headerlink" title="1. 梯度下降算法简介"></a>1. 梯度下降算法简介</h5><p>恰好，我们的损失函数L是可微的，也就是二阶可导的。如果我们当初选取损失函数时，使用绝对值作为损失函数，处理起来便没有这么便利了。</p><p>梯度下降的具体过程是这样的：</p><ol><li>选择一个初始$w_0$</li><li>计算该位置下w对L的微分，这个微分对应函数在此处下降最快的方向；</li><li>将$w_0$朝着这个方向移动一小步$ \eta$<script type="math/tex; mode=display">w\prime=w_0-\eta\frac{\mathrm{d}L}{\mathrm{d}w}</script></li><li>在新的位置开始新一轮迭代计算，直到$w$不再变化为止。</li></ol><p>不妨将梯度下降算法的求解过程想象为人下山的过程，人会先找到下山最快的方向，然后朝着那个方向走一步，直到抵达最低点。</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/模拟梯度下降.png" alt></p><p>在上图中，每一条线围成的圈就是等高线，代表损失函数的值，颜色约深的区域代表的损失函数越小；红色的箭头代表等高线的法线方向。</p><p>上述过程是求解L在变量w下的最优解的梯度下降过程，但是L还与偏移量b存在对应关系，所以实际上梯度下降在线性模型的具体公式如下：</p><script type="math/tex; mode=display">w_k\rightarrow w'_k=w_k-\eta\frac{\partial C}{\partial w_k} \\b_l\rightarrow b'_l=b_l-\eta\frac{\partial C}{\partial b_l}</script><p>其中，偏微分的具体公式如下：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/线性回归偏微分方程.png" alt></p><h5 id="2-梯度下降算法的局限性"><a href="#2-梯度下降算法的局限性" class="headerlink" title="2. 梯度下降算法的局限性"></a>2. 梯度下降算法的局限性</h5><p>首先，<strong>梯度下降算法容易陷入局部最优，找不到全局最优解</strong>。</p><p>如下图所示，当梯度下降算法优化到local minima 时，前面有座高山，它的梯度是正的，优化算法会强迫我们往回走。</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/梯度下降算法局部最优.png" alt></p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/梯度下降局部最优2.png" alt></p><p>这个问题在我们的线性回归模型里暂时遇不到，因为线性模型非常简单，是一个凸函数。但当我们试图利用梯度下降算法训练复杂模型，比如神经网络时，就很有可能遭遇该问题。</p><p>有很多方法能够解决梯度下降算法的全局最优解问题，包括调整每次行进的步长$\eta$，使其更有希望跨越“大山”等手段等等。</p><p>其次，梯度下降算法要求目标函数是可微的，这在某些情况下会<strong>产生相当大的计算代价</strong>。假设我们的问题有上百万维，计算二阶偏导数就需要上万亿（百万的平方）次！</p><p>再次，<strong>当训练数据相当多时，梯度下降算法会变得很慢</strong>。在实践中，为了计算梯度$\nabla L$，我们需要为了每个训练样本x单独计算梯度$\nabla L_x$，然后求平均值。这会花费很长时间。</p><h4 id="检验：如何检验模型是否训练成功？更加复杂的模型以及对应的风险。"><a href="#检验：如何检验模型是否训练成功？更加复杂的模型以及对应的风险。" class="headerlink" title="检验：如何检验模型是否训练成功？更加复杂的模型以及对应的风险。"></a>检验：如何检验模型是否训练成功？更加复杂的模型以及对应的风险。</h4><p>经过一轮又一轮的优化，我们终于求得了该问题下线性回归模型的最优参数$w$和$b$。接下来该如何检验模型在未知数据上的泛化能力呢？</p><p>我们可以在训练之前，把数据集分成两部分，一部分用作后续的训练，另一部分用作最后的验收。如果在这部分测试数据上的表现与训练过程中的表现一致，那我们就可以验收。</p><p>这种通过划分数据集为训练集和测试集的方法，与其说是一种技术细节，不如说是一种工程实践上的经验。通过训练集和测试集的性能比较，我们可以发现模型存在的潜在问题：</p><p><strong>1. 训练集和测试集上的表现都不太高；</strong></p><p>这种情况我们称之为欠拟合。线性模型由于过于简单，当面对一些较复杂的现实问题时，欠拟合便出现了。你会发现无论如何训练，无论投入多少数据都很难提升模型的性能了。我们需要更加复杂的模型，然后进行上面所说的机器学习三个步骤：选择新模型，选择合适的损失函数，选择优化方法进行训练。</p><p>对于一些现实问题，类似$y=wx+b$这种一次模型确实过于简单了。比如预测房价，可能与房屋面积有关，也可以与房屋面积的平方有关，甚至是三次方、四次方。</p><p>课堂上老师举了一个例子，用一次函数模型预测宝可梦数据集，效果如下：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦1.png" alt></p><p>模型在测试集上的误差平均值为 35.0</p><p>当我们使用更复杂的2次模型时，模型性能有明显好转：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦2.png" alt></p><p>模型的图像从一次函数变为二次函数了，更好拟合了训练集和测试集。测试集平均误差降至 18.4 了。那是不是模型越复杂，性能越好呢？</p><p><strong>2. 训练集上的表现良好，测试集上的表现很差</strong></p><p>当我们持续优化模型复杂度到3次方函数、4次方函数时，测试集平均误差开始停滞，4次方函数的平均误差不降反增：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦3.png" alt></p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦4.png" alt></p><p>训练集平均误差【15.4】【15.3】【14.9】【12.8】<br>测试集平均误差【18.4】【18.1】【28.8】【232.1】</p><p>这种情况称之为过拟合。所谓“过犹不及”，那我们应该怎样选择模型的复杂度，避免过拟合呢？我们可以将模型复杂度与测试集性能之间的关系绘制成图像，通过寻找图像的“拐点”来决定模型的复杂度。</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦性能变化.png" alt></p><h4 id="反思：改进模型性能的其他方法"><a href="#反思：改进模型性能的其他方法" class="headerlink" title="反思：改进模型性能的其他方法"></a>反思：改进模型性能的其他方法</h4><p>上面的宝可梦CP值预测问题，老师之后给出了更多的数据，绘制在坐标图上：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦5.png" alt></p><p>很显然，任何曲线的线性模型都没办法拟合这种数据点。这是哪里出了问题？答案是我们忽略了数据点的其他特征。当我们引入其他特征到模型后，线性模型的表达能力就增强了。下面就是引入了宝可梦种类的特征：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦6.png" alt></p><p>但是当我们继续引入一些无关特征，比如宝可梦的性别、年龄等，过拟合现象再次出现：</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/宝可梦7.png" alt></p><p>这是由于在训练过程中，某些特征的权重过大导致的。一种直观的解释是，模型训练过程也会偷懒！如果数据量不足，模型会自动选取某几个影响力大的特征，赋予较高的权值，然后忽略其他特征，那些被忽略的特征中可能包含更加有用的信息。</p><p>我们希望最终得到的模型不要出现太大的权重，为了防止某些特征的权值过大，限制权重的增长速度，可以使用正则化方法。</p><p><img src="/2022/10/10/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9D%8E%E5%AE%8F%E6%AF%852019ML%E8%AF%BE%E7%A8%8B-P1/正则化1.png" alt></p><p>改造后的损失函数，末尾添加上了一个正则化项，这个正则化项的大小仅与模型的参数大小有关。参数越大，正则化项越大。有了正则化项，模型在优化过程就会更倾向于选择参数值更小的模型了。</p><p>在很多应用场景中，并不是 $w$ 越小模型越平滑越好，但是经验值告诉我们 $w$ 越小大部分情况下都是好的。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>机器学习的概念、分类和三个步骤：选择模型、选择损失函数、选择优化方法；</li><li>根据三个步骤，实践解决回归问题：使用线性模型，解决宝可梦CP值预测问题；</li></ol><p>这里的重点是梯度下降算法，后面还会继续学习该算法。</p><ol><li>通过分析例子的优化点，引出模型常用的优化方法以及风险：过拟合、欠拟合，以及他们的解决方案。</li></ol><p>过拟合问题的解决方法有：降低模型复杂度、增加训练样本、增加训练样本的特征、添加正则化项等。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>MOOC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】机器学习编译——Note2</title>
    <link href="/2022/09/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E2%80%94%E2%80%94Note2/"/>
    <url>/2022/09/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E2%80%94%E2%80%94Note2/</url>
    
    <content type="html"><![CDATA[<p>陈天奇老师主讲的机器学习编译相关的课程，本节课讨论张量程序抽象以及相关实现。<br><!--more---></p><p>机器学习编译的过程可以看做是对张量函数的变换过程。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>Compilation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】职业素养</title>
    <link href="/2022/08/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%81%8C%E4%B8%9A%E7%B4%A0%E5%85%BB/"/>
    <url>/2022/08/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%81%8C%E4%B8%9A%E7%B4%A0%E5%85%BB/</url>
    
    <content type="html"><![CDATA[<p>有关新人培养手册的一些阅读体会和思考<br><!--more---></p><h2 id="执行任务并漂亮地完成"><a href="#执行任务并漂亮地完成" class="headerlink" title="执行任务并漂亮地完成"></a>执行任务并漂亮地完成</h2><p>分析问题、解决问题是工程师的日常工作，如何对待工作、如何执行工作，反映了一个工程师的基本素质。</p><p>不同级别的工程师适合处理不同难度的任务。根据任务难度，可以分类为：</p><ol><li>任务已经拆解完毕，我只负责某个简单子任务的执行；</li><li>任务已经拆解完毕，我负责各个步骤的执行，或者其中较困难的部分；</li><li>复杂的事情需要自己做拆解然后执行。</li></ol><p>一般我们把简单的执行任务交付给 T5 左右级别工程师去执行，把复杂的任务交给 T7 左右的工程师执行。而 T8 及以上的工程师，我们希望能独立完成任务拆解并执行。T10 以上的工程师，我们希望能够承担一个技术方向上的难题。</p><p>无论任务难度如何，工程师还是要根据结果的完成度来评价。根据结果分类：</p><ul><li>最低评价：不能按时完成任务；</li><li>较低评价：按时完成任务，但是质量较差；</li><li>合格评价：按时完成任务，质量合格。</li></ul><h2 id="独立思考"><a href="#独立思考" class="headerlink" title="独立思考"></a>独立思考</h2><p>什么叫做独立思考？工作是由思考和执行两部分组成的，任务的分析和拆解过程、出现问题后的总结和复盘、对现有技术方案的调研，都算是独立思考。</p><p>比如说，当我负责的模块出现 Bug，或者我的团队负责的模块报警时，进行分析问题、总结和复盘的过程，算是独立思考；<br>比如说，项目的技术方案不够完美或存在缺陷，我对现有技术方案进行调研，算是独立思考；<br>比如说，我对问题进行分析和总结，经常反思是否已经无可挑剔、不会被人挑毛病，算是独立思考。</p><p>独立思考代表着三个工程师宝贵的能力：责任心、事业心和硬实力。一个高绩效的工程师，往往有以下特征：</p><ol><li>工作态度上，对业务负责，从来不会消极怠工，负责的模块出现事故很少；</li><li>做事方法上，规范、及时沟通，事情交给他很放心，不会把自己当成工具人；</li><li>做事效率上，想得清楚做得快，紧急问题得交给他，做事不会反反复复；</li><li>技术实力上，此方向的任何问题都可以咨询他。</li></ol><h2 id="协作沟通"><a href="#协作沟通" class="headerlink" title="协作沟通"></a>协作沟通</h2><p>以上几点是工程师个人的能力评价，但是在公司中免不了要与其他工程师打交道，也就是沟通。</p><p>协作沟通时有一些常见的问题，新手工程师可能不太了解，需要琢磨一下；在此我把一些定律列举出来，方便直接照做：</p><ol><li>收到消息必回复，塑造收到消息及时回复的形象；不仅如此，在自己负责通知时，也要确认相关责任人收到消息。</li><li>沟通时先想好诉求，问对了问题是成功的一半；要有礼貌，先带好称呼，再组织语言，逻辑通顺；要确认下解决问题的时间节点，并且将讨论的共识发到群里进行同步；</li><li>要及时跟进问题，就比如说当你找对方要时间节点时，可能会遇到：“还不知道，需要评估”、“事情太多，不好说”、等等类似的无法给出时间点的情况，这个时候就需要及时跟进。及时跟进的做法不是每过一段时间就问一次，那样很招人烦；正确做法是一起拆分问题，对方一旦完成子任务，就通知自己。</li><li>即便你负责的是一个项目里的一小部分，你也需要知道整个项目的计划节奏。</li></ol>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Career</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】如何实现模型的热更新？</title>
    <link href="/2022/07/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%83%AD%E6%9B%B4%E6%96%B0%EF%BC%9F/"/>
    <url>/2022/07/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%83%AD%E6%9B%B4%E6%96%B0%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<p>陈天奇老师主讲的机器学习编译相关的课程，回答了机器学习模型是如何从开发状态到部署状态的。整个过程类似源代码到可执行程序的编译过程，因此得名机器学习编译。据他本人所述，这门课是目前全世界第一门主讲机器学习编译的课程。<br><!--more---></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>Compilation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】机器学习编译——Note1</title>
    <link href="/2022/07/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E2%80%94%E2%80%94Note1/"/>
    <url>/2022/07/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E2%80%94%E2%80%94Note1/</url>
    
    <content type="html"><![CDATA[<p>陈天奇老师主讲的机器学习编译相关的课程，回答了机器学习模型是如何从开发状态到部署状态的。整个过程类似源代码到可执行程序的编译过程，因此得名机器学习编译。据他本人所述，这门课是目前全世界第一门主讲机器学习编译的课程。<br><!--more---></p><p>《机器学习编译》是陈天奇老师的公开课。目前在Bilibili上更新视频。<a href="https://space.bilibili.com/1663273796">这里</a>是陈天奇老师在Bilibili的个人主页，如果对课程感兴趣的小伙伴可以免费观摩。</p><p>另外课件地址在这里：<a href="https://mlc.ai/zh/chapter_introduction/">Machine Learning Compilation</a></p><h1 id="一、什么是机器学习编译？学习这门课的意义是什么？"><a href="#一、什么是机器学习编译？学习这门课的意义是什么？" class="headerlink" title="一、什么是机器学习编译？学习这门课的意义是什么？"></a>一、什么是机器学习编译？学习这门课的意义是什么？</h1><p><strong>机器学习编译</strong> (machine learning compilation, MLC) 是指，将机器学习算法从开发阶段，通过变换和优化算法，使其变成部署状态。</p><p>开发阶段：用 PyTorch、TensorFlow 或 JAX 等通用框架编写的模型描述 + 对应的权重文件。</p><p>部署状态：在考虑支撑代码、内存管理、不同语言的开发接口等因素后，成功执行在设备上的状态。</p><p>该过程类似于把源代码转换为可执行文件的程序编译过程，但是二者有比较大的不同。</p><p>首先是目标不同，机器学习编译的目标有：</p><ul><li>集成打包 + 最小化依赖，即将必要的元素组合在一起以用于部署应用程序；</li><li>利用硬件加速，利用硬件本身的特性进行加速；</li><li>通用优化，以最小化内存使用或提高执行效率为目标转换模型执行方式。</li></ul><p>这些目标没有严格的界限。</p><p>其次，这个过程不一定涉及代码生成；例如将开发状态转化为部署形式，可能只是将抽象的模型定义，转化为对某几个预定义的库函数的调用。</p><p>最后，遇到的挑战和解决方案也大不相同。随着硬件和模型种类的增长，机器学习编译难以表示单一稳定的解决方案。</p><p>那么，我们能够从这门课学习到什么？</p><p>对于在从事机器学习工作工程师，机器学习编译提供了以基础的解决问题的方法和工具。它有助于回答我们可以采用什么方法来特定模型的部署和内存效率，如何将优化模型的单个部分的经验推广到更端到端解决方案等一系列问题。</p><p>对于机器学习科学家，学习机器学习编译可以更深入地了解将模型投入生产所需的步骤。机器学习框架本身隐藏了一些技术复杂性，但是当我们尝试开始部署新模型或将模型部署到框架支持不完善的平台时，仍然会面临巨大的挑战。机器学习编译使机器学习算法科学家有机会了解背后的基本原理，并且知晓为什么我的模型的运行速度不及预期，以及如何来使部署更有效。</p><p>最后，学习 MLC 本身很有趣。借助这套现代机器学习编译工具，我们可以进入机器学习模型从高级、代码优化到裸机的各个阶段。端到端 (end to end) 地了解这里发生的事情并使用它们来解决我们的问题。</p><h1 id="二、机器学习编译的关键要素"><a href="#二、机器学习编译的关键要素" class="headerlink" title="二、机器学习编译的关键要素"></a>二、机器学习编译的关键要素</h1><h2 id="1-张量和张量函数"><a href="#1-张量和张量函数" class="headerlink" title="1. 张量和张量函数"></a>1. 张量和张量函数</h2><p>张量 (Tensor) 是执行中最重要的元素。张量是表示神经网络模型执行的输入、输出和中间结果的多维数组。</p><p>张量函数 (Tensor functions) 指接受张量和输出张量的计算序列。</p><p>下面这张图展示了机器学习编译过程中，两种不同形式的张量函数的变换过程。从左边比较抽象的表示形式，转换为右侧较为具体的表示形式。</p><p><img src="https://mlc.ai/zh/_images/mlc-elem-transform.png" alt="机器学习编译过程中的张量函数变换"></p><p>机器学习编译的过程就是是将上图左侧的内容转换为右侧的过程。在不同的场景中，这个过程可以是手动完成的，也可以使用一些自动转换工具，或两者兼而有之。</p><h2 id="2-抽象和实现"><a href="#2-抽象和实现" class="headerlink" title="2. 抽象和实现"></a>2. 抽象和实现</h2><p>上一部分提到了抽象和实现。对于同样的目标，我们会有不同的 抽象表现，但是不同抽象表示有些细节不同。我们会把更细化的抽象表示称为原有抽象表示的一个具体实现。</p><p>抽象和实现可能是所有计算机系统中最重要的关键字。抽象指定“做什么”，实现提供“如何”做。没有具体的界限。根据我们的看法，for 循环本身可以被视为一种抽象，因为它可以使用 python 解释器实现或编译为本地汇编代码。</p><p>MLC 实际上是在相同或不同抽象下转换和组装张量函数的过程。</p><p>本课程会介绍四种不同形式的抽象表示</p><ul><li>计算图的抽象</li><li>张量程序的抽象</li><li>算子库和运行时的抽象</li><li>硬件层面的抽象</li></ul>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>Compilation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】科大讯飞-非标准化疾病诉求的简单分诊挑战赛</title>
    <link href="/2022/06/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E-%E9%9D%9E%E6%A0%87%E5%87%86%E5%8C%96%E7%96%BE%E7%97%85%E8%AF%89%E6%B1%82%E7%9A%84%E7%AE%80%E5%8D%95%E5%88%86%E8%AF%8A%E6%8C%91%E6%88%98%E8%B5%9B/"/>
    <url>/2022/06/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E-%E9%9D%9E%E6%A0%87%E5%87%86%E5%8C%96%E7%96%BE%E7%97%85%E8%AF%89%E6%B1%82%E7%9A%84%E7%AE%80%E5%8D%95%E5%88%86%E8%AF%8A%E6%8C%91%E6%88%98%E8%B5%9B/</url>
    
    <content type="html"><![CDATA[<p>文本分类问题，收集用户问诊信息，需要分别将一段提问就问诊类型（20种）和疾病类型（60种）进行分类。类别包含较多缺失值，trick较多。笔者用到了Tfidf+Logistic回归作为Baseline，然后使用Pytorch训练RoBERTa用作第一版提交模型，最终根据数据的特征，取得了XX（排名）。<br><!--more---></p><p>比赛地址：<br><a href="https://challenge.xfyun.cn/topic/info?type=disease-claims-2022&amp;option=ssgy">https://challenge.xfyun.cn/topic/info?type=disease-claims-2022&amp;option=ssgy</a></p><p>baseline地址：<br><a href="https://mp.weixin.qq.com/s/KiozLF7FaJ_CVx74J3KNWA">https://mp.weixin.qq.com/s/KiozLF7FaJ_CVx74J3KNWA</a></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Datawhale</tag>
      
      <tag>Data Mining</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】MacBook Pro M1Pro Java环境部署和开发准备</title>
    <link href="/2022/05/25/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Java%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E5%92%8C%E5%BC%80%E5%8F%91%E5%87%86%E5%A4%87/"/>
    <url>/2022/05/25/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Java%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E5%92%8C%E5%BC%80%E5%8F%91%E5%87%86%E5%A4%87/</url>
    
    <content type="html"><![CDATA[<p>本文记录了在MacBook Pro M1Pro下部署Java开发环境遇到的问题和解决方法<br><!--more---></p><h2 id="下载并安装JDK"><a href="#下载并安装JDK" class="headerlink" title="下载并安装JDK"></a>下载并安装JDK</h2><p>M1芯片带来的坏处是许多jdk版本不支持arm架构。这里可以选择<code>Zulu JDK</code>，可以进入<a href="https://www.azul.com/downloads/?version=java-8-lts&amp;os=macos&amp;architecture=arm-64-bit&amp;package=jdk">此链接下载</a>并安装指定版本的JDK。</p><p><img src="/2022/05/25/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Java%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E5%92%8C%E5%BC%80%E5%8F%91%E5%87%86%E5%A4%87/2022-05-25-15-46-32.png" alt="安装页面"></p><p>安装完毕后，不必设置环境变量，直接在命令行运行<code>java -version</code>检查是否运行成功：</p><pre><code>% java -versionopenjdk version &quot;1.8.0_332&quot;OpenJDK Runtime Environment (Zulu 8.62.0.19-CA-macos-aarch64) (build 1.8.0_332-b09)OpenJDK 64-Bit Server VM (Zulu 8.62.0.19-CA-macos-aarch64) (build 25.332-b09, mixed mode)</code></pre><h2 id="下载并安装Maven"><a href="#下载并安装Maven" class="headerlink" title="下载并安装Maven"></a>下载并安装Maven</h2><p>可在<a href="https://maven.apache.org/download.cgi">此链接中</a>选择如图所示的链接进行下载压缩包：</p><p><img src="/2022/05/25/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Java%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E5%92%8C%E5%BC%80%E5%8F%91%E5%87%86%E5%A4%87/下载Maven.png" alt></p><p>这是个压缩文件，或者说“绿色版”。然后将其解压到你想要放入的文件夹（我一般移动到<code>/opt</code>目录下）。</p><p>接下来需要我们自己配置环境变量。首先需要确认你的BASH类别。</p><p>查看当前使用的SHELL：</p><pre><code>% echo $SHELL/bin/zsh</code></pre><p>查看本机可用的所有SHELL</p><pre><code>% cat /etc/shells# List of acceptable shells for chpass(1).# Ftpd will not allow users to connect who are not using# one of these shells./bin/bash/bin/csh/bin/dash/bin/ksh/bin/sh/bin/tcsh/bin/zsh</code></pre><p>我这里显示是zsh，则我需要修改<code>~/.zshrc</code><br>如果你的shell是<code>/bin/bash</code>，则需要修改<code>~/.bash_profile</code></p><p>将以下文本加入配置文件末尾：</p><pre><code class="lang-sh">export MAVEN_HOME=/opt/apache-maven-3.8.5export PATH=$PATH:$MAVEN_HOME/bin</code></pre><p>然后立即载入环境变量：</p><pre><code>% source ~/.zshrc</code></pre><p>最后测试是否应用修改：</p><pre><code>% mvn -vApache Maven 3.8.5 (3599d3414f046de2324203b78ddcf9b5e4388aa0)Maven home: /opt/apache-maven-3.8.5Java version: 1.8.0_332, vendor: Azul Systems, Inc., runtime: /Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home/jreDefault locale: zh_CN, platform encoding: UTF-8OS name: &quot;mac os x&quot;, version: &quot;12.3.1&quot;, arch: &quot;aarch64&quot;, family: &quot;mac&quot;</code></pre>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】如何使用树模型预测结构化数据</title>
    <link href="/2022/03/06/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E6%A0%91%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE/"/>
    <url>/2022/03/06/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E6%A0%91%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE/</url>
    
    <content type="html"><![CDATA[<p>TODO</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】Attention和Transformer</title>
    <link href="/2021/08/18/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Attention%E5%92%8CTransformer/"/>
    <url>/2021/08/18/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Attention%E5%92%8CTransformer/</url>
    
    <content type="html"><![CDATA[<p>分享下知识，介绍下我对Attention的理解，然后利用Tensorflow实现一个完整的Transformer模型。</p><!--more---><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="常见的NLP任务"><a href="#常见的NLP任务" class="headerlink" title="常见的NLP任务"></a>常见的NLP任务</h2><p>文本分类，实体抽取等</p><h1 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h1><p>Attention是一种编码机制，最初是用于解决机器翻译（seq2seq任务）的长程遗忘问题。因为基于RNN模型不能很好地应对长文本的翻译，即便是LSTM也受限于序列长度等因素的影响。而Attention可以通过训练一个动态的参数矩阵，决定关注长文本的哪些部分。Attention的一个优点是，相较于LSTM的基于时间的反响传播算法，Attention的计算可以并行化，这为其堆叠比较庞大的模型打下了基础。</p><h1 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h1><p>Attention机制其实用在RNN的机器翻译模型上就已经可以提升机器翻译模型的效果了，于是有人想，不如我们直接把RNN从模型中摘掉，不管encoder和decoder都使用Attention计算单元，性能不就更好了？这也是Attention is all you need 这篇论文的由来。</p><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>Transformer是由Attention模块组成的计算单元。</p><p>在encoder阶段，由6个self-attention模块组成的多层编码器负责编码运算，最后使用一层前馈神经网络汇总（其实就是加权汇总）；在decoder方面，输入不但有编码器传进来的一部分信息，而且还有句子本身。</p><p>Transformer里面使用多种Attention，关注序列的不同层次的信息，这种叫做多头注意力机制。在RNN横行天下的时代，这种堆叠模式带来的庞大训练和推理负担使得难以推动此类模型上线。</p><pre><code class="lang-py">class MultiheadAttention(nn.Module):    # n_heads：多头注意力的数量    # hid_dim：每个词输出的向量维度    def __init__(self, hid_dim, n_heads, dropout):        super(MultiheadAttention, self).__init__()        self.hid_dim = hid_dim        self.n_heads = n_heads        # 强制 hid_dim 必须整除 h        assert hid_dim % n_heads == 0        # 定义 W_q 矩阵        self.w_q = nn.Linear(hid_dim, hid_dim)        # 定义 W_k 矩阵        self.w_k = nn.Linear(hid_dim, hid_dim)        # 定义 W_v 矩阵        self.w_v = nn.Linear(hid_dim, hid_dim)        self.fc = nn.Linear(hid_dim, hid_dim)        self.do = nn.Dropout(dropout)        # 缩放        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads]))    def forward(self, query, key, value, mask=None):        # K: [64,10,300], batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维        # V: [64,10,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维        # Q: [64,12,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维        bsz = query.shape[0]        Q = self.w_q(query)        K = self.w_k(key)        V = self.w_v(value)        # 这里把 K Q V 矩阵拆分为多组注意力，变成了一个 4 维的矩阵        # 最后一维就是是用 self.hid_dim // self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300/6=50        # 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 词，50 表示每组注意力的词的向量长度        # K: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]        # V: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]        # Q: [64,12,300] 拆分多组注意力 -&gt; [64,12,6,50] 转置得到 -&gt; [64,6,12,50]        # 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim //                   self.n_heads).permute(0, 2, 1, 3)        K = K.view(bsz, -1, self.n_heads, self.hid_dim //                   self.n_heads).permute(0, 2, 1, 3)        V = V.view(bsz, -1, self.n_heads, self.hid_dim //                   self.n_heads).permute(0, 2, 1, 3)        # 第 1 步：Q 乘以 K的转置，除以scale        # [64,6,12,50] * [64,6,50,10] = [64,6,12,10]        # attention：[64,6,12,10]        attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale        # 把 mask 不为空，那么就把 mask 为 0 的位置的 attention 分数设置为 -1e10        if mask isnotNone:            attention = attention.masked_fill(mask == 0, -1e10)        # 第 2 步：计算上一步结果的 softmax，再经过 dropout，得到 attention。        # 注意，这里是对最后一维做 softmax，也就是在输入序列的维度做 softmax        # attention: [64,6,12,10]        attention = self.do(torch.softmax(attention, dim=-1))        # 第三步，attention结果与V相乘，得到多头注意力的结果        # [64,6,12,10] * [64,6,10,50] = [64,6,12,50]        # x: [64,6,12,50]        x = torch.matmul(attention, V)        # 因为 query 有 12 个词，所以把 12 放到前面，把 5 和 60 放到后面，方便下面拼接多组的结果        # x: [64,6,12,50] 转置-&gt; [64,12,6,50]        x = x.permute(0, 2, 1, 3).contiguous()        # 这里的矩阵转换就是：把多组注意力的结果拼接起来        # 最终结果就是 [64,12,300]        # x: [64,12,6,50] -&gt; [64,12,300]        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))        x = self.fc(x)        return x# batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维query = torch.rand(64, 12, 300)# batch_size 为 64，有 12 个词，每个词的 Key 向量是 300 维key = torch.rand(64, 10, 300)# batch_size 为 64，有 10 个词，每个词的 Value 向量是 300 维value = torch.rand(64, 10, 300)attention = MultiheadAttention(hid_dim=300, n_heads=6, dropout=0.1)output = attention(query, key, value)## output: torch.Size([64, 12, 300])print(output.shape)</code></pre><h1 id="表示序列中单词顺序的方法"><a href="#表示序列中单词顺序的方法" class="headerlink" title="表示序列中单词顺序的方法"></a>表示序列中单词顺序的方法</h1><p>为了解决这个问题，Transformer 模型对每个输入的向量都添加了一个向量。这些向量遵循模型学习到的特定模式，有助于确定每个单词的位置，或者句子中不同单词之间的距离。这种做法背后的直觉是：将这些表示位置的向量添加到词向量中，得到了新的向量，这些新向量映射到 Q/K/V，然后计算点积得到 attention 时，可以提供有意义的信息。</p><h1 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h1><p>编码器结构中有一个需要注意的细节是：编码器的每个子层（Self Attention 层和 FFNN）都有一个残差连接和层标准化（layer-normalization）。在解码器的子层里面也有层标准化（layer-normalization）。</p><p><img src="https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/0-1-transformer-arc.png" alt="11"></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Attention</tag>
      
      <tag>Transformer&#39;</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读笔记】实现 MapReduce 论文附录 A 的 Word Count 程序</title>
    <link href="/2021/05/07/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91%E5%AE%9E%E7%8E%B0-MapReduce-%E8%AE%BA%E6%96%87%E9%99%84%E5%BD%95-A-%E7%9A%84-Word-Count-%E7%A8%8B%E5%BA%8F/"/>
    <url>/2021/05/07/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91%E5%AE%9E%E7%8E%B0-MapReduce-%E8%AE%BA%E6%96%87%E9%99%84%E5%BD%95-A-%E7%9A%84-Word-Count-%E7%A8%8B%E5%BA%8F/</url>
    
    <content type="html"><![CDATA[<p>给出一个Map和Reduce的具体实现，去除了所有分布式的特性【可能今后会添加】</p><!--more---><p>MapReduce文章最后的附录A有一段C++代码，描述了Map和Reduce函数的编写和使用方法。但是仅靠看代码总是不能深入理解MapReduce的实现细节，阅读其他人的MapReduce学习曲线又太过陡峭。因此我决定自己实现Map函数和Reduce函数，并尽可能使用替代方法将那些没有给出的api实现。</p><p>促使我写这篇文章的另一个原因是，网络上的大部分实现Word Count的文章都是依赖于某某框架的，比如依赖Hadoop。但我觉得过早依赖于某个平台不利于深入理解背后的原理，因此我决定自己实现各种api，体会程序设计者可能遇到的问题。今后在学习分布式系统、分布式框架时，便能够对症下药、有的放矢。</p><p>不过还请各位原谅，这里的Map和Reduce只能运行在单机环境啦。</p><p>下面给出原文中的C++代码：</p><pre><code class="lang-cpp">#include &quot;mapreduce/mapreduce.h&quot;//用户map函数class WordCounter : public Mapper &#123;public:    virtual void Map(const MapInput&amp; input) &#123;        const string&amp; text = input.value();        const int n = text.size();        for (int i = 0; i &lt; n; ) &#123;            //跳过前导空格            while ((i &lt; n) &amp;&amp; isspace(text[i]))                i++;            // 查找单词的结束位置            int start = i;            while ((i &lt; n) &amp;&amp; !isspace(text[i]))                i++;            if (start &lt; i)                Emit(text.substr(start,i-start),&quot;1&quot;);        &#125;    &#125;&#125;;REGISTER_MAPPER(WordCounter);//用户的reduce函数class Adder : public Reducer &#123;    virtual void Reduce(ReduceInput* input) &#123;        //迭代具有相同key的所有条目,并且累加它们的value        int64 value = 0;        while (!input-&gt;done()) &#123;            value += StringToInt(input-&gt;value());            input-&gt;NextValue();        &#125;        //提交这个输入key的综合        Emit(IntToString(value));    &#125;&#125;;REGISTER_REDUCER(Adder);int main(int argc, char** argv) &#123;    ParseCommandLineFlags(argc, argv);    MapReduceSpecification spec;    // 把输入文件列表存入&quot;spec&quot;    for (int i = 1; i &lt; argc; i++) &#123;        MapReduceInput* input = spec.add_input();        input-&gt;set_format(&quot;text&quot;);        input-&gt;set_filepattern(argv[i]);        input-&gt;set_mapper_class(&quot;WordCounter&quot;);    &#125;    //指定输出文件:    // /gfs/test/freq-00000-of-00100    // /gfs/test/freq-00001-of-00100    // ...    MapReduceOutput* out = spec.output();    out-&gt;set_filebase(&quot;/gfs/test/freq&quot;);    out-&gt;set_num_tasks(100);    out-&gt;set_format(&quot;text&quot;);    out-&gt;set_reducer_class(&quot;Adder&quot;);    // 可选操作:在map任务中做部分累加工作,以便节省带宽    out-&gt;set_combiner_class(&quot;Adder&quot;);    // 调整参数: 使用2000台机器,每个任务100MB内存    spec.set_machines(2000);    spec.set_map_megabytes(100);    spec.set_reduce_megabytes(100);    // 运行    MapReduceResult result;    if (!MapReduce(spec, &amp;result)) abort();    // 完成: &#39;result&#39;结构包含计数,花费时间,和使用机器的信息    return 0;&#125;</code></pre><h2 id="抽象基类-Mapper、Reducer"><a href="#抽象基类-Mapper、Reducer" class="headerlink" title="抽象基类 Mapper、Reducer"></a>抽象基类 Mapper、Reducer</h2><p>首先我打算实现<code>WordCounter</code>的父类<code>Mapper</code>和<code>Adder</code>的父类<code>Reducer</code>。</p><pre><code class="lang-cpp">class Mapper &#123;public:    virtual void Map(const MapInput&amp; input) = 0;&#125;;class Reducer &#123;public:    virtual void Reduce(ReduceInput* input) = 0;&#125;;</code></pre><p>简单实现Map和Reduce接口，并把它们设置成纯虚方法。</p><h2 id="WordCounter类"><a href="#WordCounter类" class="headerlink" title="WordCounter类"></a>WordCounter类</h2><pre><code class="lang-cpp">class WordCounter : public Mapper &#123;public:    void Map(const MapInput&amp; input) override &#123;        const string&amp; text = input.value(); // 读取一行文本        const int n = text.size();        for (int i = 0; i &lt; n; ) &#123;            // 跳过行首空白            while ((i &lt; n) &amp;&amp; isspace(text[i])) i++;            // 确定单词的开头和结尾            int start = i;            while ((i &lt; n) &amp;&amp; !isspace(text[i])) i++;            if (start &lt; i)                Emit(text.substr(start, i-start), &quot;1&quot;);        &#125;    &#125;&#125;;</code></pre><p>这一部分相较于原文没什么变化。其主要作用在于分词，然后每个单词组建成一个键值对，以(word, 1)的结构发射出去。发射到哪里呢？我就偷懒直接持久化到本地的消息存储装置了。</p><pre><code class="lang-cpp">static void Emit(const string&amp; key, const string&amp; value) &#123;    mw.put(key, value);&#125;</code></pre><p>mw 是 MiddleWare 的实例，是一个用于保存Emit输出的键值对的全局变量。后续会继续讲解。</p><h2 id="用于保存-Mapper-得到的键值对的-MiddleWare-类"><a href="#用于保存-Mapper-得到的键值对的-MiddleWare-类" class="headerlink" title="用于保存 Mapper 得到的键值对的 MiddleWare 类"></a>用于保存 Mapper 得到的键值对的 MiddleWare 类</h2><pre><code class="lang-cpp">class MiddleWare &#123;private:    vector&lt;pair&lt;string, string&gt;&gt; kv_pairs;    static bool compare_pair(const pair&lt;string, string&gt;&amp; lhs, const pair&lt;string, string&gt;&amp; rhs) &#123;        return lhs.first &lt; rhs.first;    &#125;public:    MiddleWare() = default;    void put(const string&amp; key, const string&amp; value) &#123;        kv_pairs.emplace_back(key, value);    &#125;    vector&lt;pair&lt;string, string&gt;&gt; get() &#123;        std::sort(kv_pairs.begin(), kv_pairs.end(), compare_pair); // 将其按照key相同的一组来排序        return kv_pairs;    &#125;&#125;;MiddleWare mw; // 全局变量：消息队列</code></pre><p>这是我为本地运行顺利而凭空构建出来的类，作用是储存(key, value)对。为了方便使用，内部有get和put方法。其中如果Reducer需要get数据了，那么首先会按照key对这些pair进行排序。这也是与MapReduce的流程相吻合的。</p><p>既然是排序，那么就要定义比较器 compare_pair。我的比较器直接使用string的比较，确保相同key值的pair在相邻位置。</p><h2 id="Adder类"><a href="#Adder类" class="headerlink" title="Adder类"></a>Adder类</h2><pre><code class="lang-cpp">// 用户自定义 Reduce 函数class Adder : public Reducer &#123;public:    void Reduce(ReduceInput* input) override &#123;        // 迭代所有拥有相同key的键值对，把它们的values加起来        int64_t value = 0;        string currentKey = input-&gt;key();        while (!input-&gt;end() &amp;&amp; currentKey == input-&gt;key()) &#123; // 直到下一个键值对的key与当前键值对的key不同为止            value += std::stoi(input-&gt;value());            input-&gt;NextValue(); // 找到下一个拥有相同key的键值对        &#125;        // Emit sum for input-&gt;key()        Emit(to_string(value));    &#125;&#125;;</code></pre><p>与论文中的Adder有逻辑出入，主要变化在把同样key分成不同组的逻辑上，我直接保存了当前组的key。原来论文里是没有这种操作的。</p><h2 id="Map-的输入-MapInput"><a href="#Map-的输入-MapInput" class="headerlink" title="Map 的输入 MapInput"></a>Map 的输入 MapInput</h2><p>观察一下Map的参数里有一个MapInput类型的对象，那么第二步就是新建一个MapInput类。要想跑通Map函数的代码，这个类必须实现value()方法。</p><p>猜测一下，MapInput是Map的输入，而MapReduce框架的输入输出都应该是键值对的形式。因此每个MapInput都应该包含一个key和一个value成员。</p><pre><code class="lang-cpp">class MapInput &#123;private:    string map_value;    string map_key;public:    explicit MapInput(string filename, string text) : map_key(std::move(filename)), map_value(std::move(text)) &#123; &#125;    [[nodiscard]] const string&amp; value() const &#123;        return map_value;    &#125;&#125;;</code></pre><p>MapInput的构造函数接收两个参数，第一个参数是文本文件名，第二个参数是文件的内容。其实第一个参数在我们的程序中没啥作用，但是为了格式的统一，就写上吧。</p><p>explicit修饰构造函数，代表该类的对象禁止发生隐式类型转换，要想转换必须以<strong>明确的(explicit)</strong>方式进行显式类型转换。</p><p>冒号后面的初始化列表中，使用了move特性，避免了函数传参导致的变量复制。</p><p>[[nodiscard]] 含义是该函数的返回值必须被使用，不能丢弃。C++ 17版本新增了几个中括号标识的提示符，当代码不符合要求的时候，编译器也会真的警告。相当于把以前的注释加强了。除[[nodiscard]]之外，还有表示switch语句中不必加break的[[fallthrough]]、变量定义之后没有使用也没关系的标识符[[maybe_unused]]。</p><h2 id="Reduce-的输入-ReduceInput"><a href="#Reduce-的输入-ReduceInput" class="headerlink" title="Reduce 的输入 ReduceInput"></a>Reduce 的输入 ReduceInput</h2><p>ReduceInput的设计就比较麻烦了。首先Reduce函数的输入是ReduceInput的指针，使用到的接口有done()/value()/NextValue()/key()，然后根据Reduce函数的使用方法，感觉ReduceInput像是一个迭代器。</p><pre><code class="lang-cpp">class ReduceInput &#123;private:    vector&lt;pair&lt;string, string&gt;&gt; data;    int currentKey = 0;public:    explicit ReduceInput(vector&lt;pair&lt;string, string&gt;&gt; _data) : data(std::move(_data)) &#123;  &#125;//    bool done() &#123;//        // 直到下一个键值对的key与当前键值对的key不同为止//        // 如果到了末尾，或者下一个key不一样，都是done//        if (currentKey == 0) return false;//        if (end() || data[currentKey].first != data[currentKey-1].first) return true;//        return false;//    &#125;    const string&amp; value() &#123;        return data[currentKey].second;    &#125;    const string&amp; key() &#123;        return data[currentKey].first;    &#125;    void NextValue() &#123;        currentKey++;    &#125;    bool end() &#123;        return currentKey &gt;= data.size();    &#125;&#125;;</code></pre><p>上面是我实现的ReduceInput，偷个懒把所有数据存放到ReduceInput中方便遍历，在真实场景的设计中不会像我这样的。</p><p>此外，Done函数的逻辑是有问题的。关键在于Reduce函数中的这句话：</p><pre><code class="lang-cpp">while (!input-&gt;done()) &#123;    value += StringToInt(input-&gt;value());    input-&gt;NextValue();&#125;</code></pre><p>表面上看起来是希望input作为一个迭代器，当它迭代到key与下一个key不同时，终止迭代（即done返回true表明迭代完成），然而下次迭代的开始还是从这个位置，其结果从程序逻辑上来讲，却又希望返回false。同一个位置，我们希望返回两个不同的值，这显然是说不通的。因此我在Reduce最终实现的主代码部分做了适当的逻辑修改。</p><h2 id="输出结果的单参数Emit"><a href="#输出结果的单参数Emit" class="headerlink" title="输出结果的单参数Emit"></a>输出结果的单参数Emit</h2><p>为了输出方便，最终我定义了单参数的重载Emit，不保存Reducer的计算结果，直接输出：</p><pre><code class="lang-cpp">static void Emit(const string&amp; key) &#123;    cout &lt;&lt; &quot;Sum of values:&quot; &lt;&lt; key &lt;&lt; endl;&#125;</code></pre><h2 id="最后的main"><a href="#最后的main" class="headerlink" title="最后的main"></a>最后的main</h2><pre><code class="lang-cpp">int main(int argc, char* argv[]) &#123;    ifstream in(R&quot;(C:\Users\zyt\CLionProjects\leetcode_2021\lyrics.txt)&quot;);    string content((istreambuf_iterator&lt;char&gt;(in)), istreambuf_iterator&lt;char&gt;());    MapInput minput(&quot;lyrics.txt&quot;, content);    cout &lt;&lt; &quot;minput:\n&quot; &lt;&lt; minput.value() &lt;&lt; endl;    WordCounter wc;    wc.Map(minput);    auto *rinput = new ReduceInput(mw.get());    while (!rinput-&gt;end()) &#123;        cout &lt;&lt; &quot;Key: &quot; &lt;&lt; rinput-&gt;key() &lt;&lt; &quot;\t&quot;;        Adder adder; // 模拟很多 adder        adder.Reduce(rinput);        rinput-&gt;NextValue();    &#125;    return 0;&#125;</code></pre><p>ifstream读入本地文本文档，注意ifstream的参数得是绝对路径（相对路径不知道为什么读取不出来东西）。</p><p>然后WordCount把分词结果的键值对保存在全局变量mw中，使用mw构建ReduceInput，再把ReduceInput输入进Adder里面。</p><p>注意一个Reducer处理一个Group（我把key相同的一组键值对称之为Group），那么我就以While循环来代替啦。</p><p>这就是代码的所有内容了！</p><p><img src="/2021/05/07/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91%E5%AE%9E%E7%8E%B0-MapReduce-%E8%AE%BA%E6%96%87%E9%99%84%E5%BD%95-A-%E7%9A%84-Word-Count-%E7%A8%8B%E5%BA%8F/result.png" alt></p><pre><code class="lang-cpp">#include &quot;stdafx.h&quot;using namespace std;class MiddleWare &#123;private:    vector&lt;pair&lt;string, string&gt;&gt; kv_pairs;    static bool compare_pair(const pair&lt;string, string&gt;&amp; lhs, const pair&lt;string, string&gt;&amp; rhs) &#123;        return lhs.first &lt; rhs.first;    &#125;public:    MiddleWare() = default;    void put(const string&amp; key, const string&amp; value) &#123;        kv_pairs.emplace_back(key, value);    &#125;    vector&lt;pair&lt;string, string&gt;&gt; get() &#123;        std::sort(kv_pairs.begin(), kv_pairs.end(), compare_pair); // 将其按照key相同的一组来排序        return kv_pairs;    &#125;&#125;;MiddleWare mw; // 全局变量：消息队列class MapInput &#123;private:    string map_value;    string map_key;public:    explicit MapInput(string filename, string text) : map_key(std::move(filename)), map_value(std::move(text)) &#123; &#125;    [[nodiscard]] const string&amp; value() const &#123;        return map_value;    &#125;&#125;;class ReduceInput &#123;private:    vector&lt;pair&lt;string, string&gt;&gt; data;    int currentKey = 0;public:    explicit ReduceInput(vector&lt;pair&lt;string, string&gt;&gt; _data) : data(std::move(_data)) &#123;  &#125;//    bool done() &#123;//        // 直到下一个键值对的key与当前键值对的key不同为止//        // 如果到了末尾，或者下一个key不一样，都是done//        if (currentKey == 0) return false;//        if (end() || data[currentKey].first != data[currentKey-1].first) return true;//        return false;//    &#125;    const string&amp; value() &#123;        return data[currentKey].second;    &#125;    const string&amp; key() &#123;        return data[currentKey].first;    &#125;    void NextValue() &#123;        currentKey++;    &#125;    bool end() &#123;        return currentKey &gt;= data.size();    &#125;&#125;;static void Emit(const string&amp; key, const string&amp; value) &#123;    mw.put(key, value);&#125;static void Emit(const string&amp; key) &#123;    cout &lt;&lt; &quot;Sum of values:&quot; &lt;&lt; key &lt;&lt; endl;&#125;class Mapper &#123;public:    virtual void Map(const MapInput&amp; input) = 0;&#125;;class Reducer &#123;public:    virtual void Reduce(ReduceInput* input) = 0;&#125;;class WordCounter : public Mapper &#123;public:    void Map(const MapInput&amp; input) override &#123;        const string&amp; text = input.value(); // 读取一行文本        const int n = text.size();        for (int i = 0; i &lt; n; ) &#123;            // 跳过行首空白            while ((i &lt; n) &amp;&amp; isspace(text[i])) i++;            // 确定单词的开头和结尾            int start = i;            while ((i &lt; n) &amp;&amp; !isspace(text[i])) i++;            if (start &lt; i)                Emit(text.substr(start, i-start), &quot;1&quot;);        &#125;    &#125;&#125;;// 用户自定义 Reduce 函数class Adder : public Reducer &#123;public:    void Reduce(ReduceInput* input) override &#123;        // 迭代所有拥有相同key的键值对，把它们的values加起来        int64_t value = 0;        string currentKey = input-&gt;key();        while (!input-&gt;end() &amp;&amp; currentKey == input-&gt;key()) &#123; // 直到下一个键值对的key与当前键值对的key不同为止            value += std::stoi(input-&gt;value());            input-&gt;NextValue(); // 找到下一个拥有相同key的键值对        &#125;        // Emit sum for input-&gt;key()        Emit(to_string(value));    &#125;&#125;;int main(int argc, char* argv[]) &#123;    ifstream in(R&quot;(C:\Users\zyt\CLionProjects\leetcode_2021\lyrics.txt)&quot;);    string content((istreambuf_iterator&lt;char&gt;(in)), istreambuf_iterator&lt;char&gt;());    MapInput minput(&quot;lyrics.txt&quot;, content);    cout &lt;&lt; &quot;minput:\n&quot; &lt;&lt; minput.value() &lt;&lt; endl;    WordCounter wc;    wc.Map(minput);    auto *rinput = new ReduceInput(mw.get());    while (!rinput-&gt;end()) &#123;        cout &lt;&lt; &quot;Key: &quot; &lt;&lt; rinput-&gt;key() &lt;&lt; &quot;\t&quot;;        Adder adder; // 模拟很多 adder        adder.Reduce(rinput);        rinput-&gt;NextValue();    &#125;    return 0;&#125;</code></pre>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Word Count</tag>
      
      <tag>MapReduce</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】用Hadoop实现Word Count</title>
    <link href="/2021/05/06/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8Hadoop%E5%AE%9E%E7%8E%B0Word-Count/"/>
    <url>/2021/05/06/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8Hadoop%E5%AE%9E%E7%8E%B0Word-Count/</url>
    
    <content type="html"><![CDATA[<p>好像不管什么Hadoop学习都是从Word Count开始，跨越语言的Word Count</p><!--more---><p>Word Count是MapReduce 的经典入门案例，其目标是统计给定一系列文本文件的单词出现次数。</p><p>编程思路：</p><p><code>map</code>阶段：把输入的数据进行分词，并将其组合成键值对的形式，key是单词，value全部标记为1。</p><p><code>shuffle</code> 阶段：经过默认的排序分区分组，key相同的单词会作为一组数据构成新的键值对。</p><p><code>reduce</code>阶段：处理 <code>shuffle</code> 完的一组数据，该组数据就是该单词所有的键值对。</p><p><img src="/2021/05/06/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8Hadoop%E5%AE%9E%E7%8E%B0Word-Count/Word_Count_MR阶段.png" alt></p><h2 id="Map类的编写"><a href="#Map类的编写" class="headerlink" title="Map类的编写"></a>Map类的编写</h2><pre><code class="lang-java">/** * @description: WordCount Mapper类，对应MapTask * @author: ZYT * KEYIN map阶段的输入 * VALUEIN  todo MapReduce 有默认读取数据的组件：TextInputFormat *          todo 逐行读取。k是偏移量（LongWritable），无意义；v是每行的文本内容（Text）。 * KEYOUT 单词类型，Text * VALUEOUT 次数，LongWritable */public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123;    /**     * 每当TextInputFormat返回一个键值对，map就调用一次。     * 根据TextInputFormat的特性，事实上是每一行文本调用一次Map方法。     * @param key     * @param value     * @param context     * @throws IOException     * @throws InterruptedException     */    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;//        super.map(key, value, context);        String text = value.toString();        String[] words = text.split(&quot;\\s+&quot;);        for (String word : words) &#123;            context.write(new Text(word), new LongWritable(1));        &#125;    &#125;&#125;</code></pre><p>优化</p><pre><code class="lang-java">public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123;    private Text outKey = new Text();    private static final LongWritable outValue = new LongWritable(1);    /**     * 每当TextInputFormat返回一个键值对，map就调用一次。     * 根据TextInputFormat的特性，事实上是每一行文本调用一次Map方法。     * @param key     * @param value     * @param context     * @throws IOException     * @throws InterruptedException     */    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;//        super.map(key, value, context);        String text = value.toString();        String[] words = text.split(&quot;\\s+&quot;);        for (String word : words) &#123;            outKey.set(word);            context.write(outKey, outValue);        &#125;    &#125;&#125;</code></pre><h2 id="Reduce类的编写"><a href="#Reduce类的编写" class="headerlink" title="Reduce类的编写"></a>Reduce类的编写</h2><pre><code class="lang-java">public class WordCountReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt; &#123;    private LongWritable outValue = new LongWritable(0);    /**     * todo q: 当map的输出数据来到reduce之后该如何调用？     * 1. 排序所有pair     * 2. 分组pair，key相同的分成一组     * 3. 每一组调用一次reduce     * @param key     * @param values     * @param context     * @throws IOException     * @throws InterruptedException     * 输出key：该组的单词     * 输出value：该组所有次数的迭代器。     */    @Override    protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123;//        super.reduce(key, values, context);        long count = 0;        for (LongWritable value : values) &#123;            count += value.get();        &#125;        outValue.set(count);        context.write(key, outValue);    &#125;&#125;</code></pre><h2 id="Driver类的编写"><a href="#Driver类的编写" class="headerlink" title="Driver类的编写"></a>Driver类的编写</h2><pre><code class="lang-java">/** * 该类是MapReduce程序客户端驱动类。主要是为了构造Job对象，指定各种组件的属性。 * 包括Mapper、Reducer、输入输出类型、数据路径、提交作业等。 */public class WordCountDriver &#123;    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException &#123;        // 创建配置对象        Configuration conf = new Configuration();        // 构建Job作业实例， 参数为Conf、Job名字        Job job = Job.getInstance(conf, WordCountDriver.class.getSimpleName());        // 设置MR程序运行的主类        job.setJarByClass(WordCountDriver.class);        // 设置本次MR程序的Mapper、Reducer类        job.setMapperClass(WordCountMapper.class);        job.setReducerClass(WordCountReducer.class);        // 指定Mapper阶段输出的kv类型        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(LongWritable.class);        // 指定Reducer阶段kv类型，也是最终输出的kv类型        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(LongWritable.class);        // 配置本次作业的输入输出数据路径        // todo: 默认组件 TextInputFormat、TextOutputFormat        FileInputFormat.setInputPaths(job, new Path(args[0]));        FileOutputFormat.setOutputPath(job, new Path(args[1]));        // 提交作业//        job.submit();        // 采用waitForCompletion方式提交job 参数表示是否开启实时追踪作业执行情况的功能        boolean res_flag = job.waitForCompletion(true);        // 退出程序，和job结果进行绑定        System.exit(res_flag ? 0 : 1);    &#125;&#125;</code></pre><p>继承工具类Tool的Driver</p><pre><code class="lang-java">/** * 使用ToolRunner提交MapReduce作业 */public class WordCountDriver_v2 extends Configured implements Tool &#123;    public static void main(String[] args) throws Exception &#123;        // 创建配置对象        Configuration conf = new Configuration();        // 使用ToolRunner提交程序        int status = ToolRunner.run(conf, new WordCountDriver_v2(), args);        // 退出客户端        System.exit(status);    &#125;    @Override    public int run(String[] args) throws Exception &#123;        // 构建Job作业实例， 参数为Conf、Job名字        Job job = Job.getInstance(getConf(), WordCountDriver_v2.class.getSimpleName());        // 设置MR程序运行的主类        job.setJarByClass(WordCountDriver_v2.class);        // 设置本次MR程序的Mapper、Reducer类        job.setMapperClass(WordCountMapper.class);        job.setReducerClass(WordCountReducer.class);        // 指定Mapper阶段输出的kv类型        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(LongWritable.class);        // 指定Reducer阶段kv类型，也是最终输出的kv类型        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(LongWritable.class);        // 配置本次作业的输入输出数据路径        // todo: 默认组件 TextInputFormat、TextOutputFormat        FileInputFormat.setInputPaths(job, new Path(args[0]));        FileOutputFormat.setOutputPath(job, new Path(args[1]));        return job.waitForCompletion(true) ? 0 : 1;    &#125;&#125;</code></pre><h2 id="如何运行MapReduce程序？"><a href="#如何运行MapReduce程序？" class="headerlink" title="如何运行MapReduce程序？"></a>如何运行MapReduce程序？</h2><p>MapReduce程序是单机运行还是分布式运行？</p><p>MapReduce程序需要的运算资源是Hadoop YARN分配还是本机自己分配？</p><p>运行在何种模式，取决于 mapreduce.framwork.name</p><p>yarn: 集群模式</p><p>local: 本地模式</p><p>如果不指定，默认是local模式。</p><p>在 mapred-default.xml 中定义。如果代码中（conf.set）、运行的环境中（mapred-site.xml）有配置，则会覆盖default的配置。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
      <tag>Word Count</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】数据密集性应用系统设计CH1笔记</title>
    <link href="/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E6%80%A7%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1CH1%E7%AC%94%E8%AE%B0/"/>
    <url>/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E6%80%A7%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1CH1%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>第一章：可靠、可扩展和可维护的应用系统<br><!--more---></p><h3 id="数据密集型问题的挑战："><a href="#数据密集型问题的挑战：" class="headerlink" title="数据密集型问题的挑战："></a>数据密集型问题的挑战：</h3><p>数据量，数据复杂度，数据的快速多变性；</p><h3 id="数据密集型应用包含以下模块："><a href="#数据密集型应用包含以下模块：" class="headerlink" title="数据密集型应用包含以下模块："></a>数据密集型应用包含以下模块：</h3><ul><li>数据库；</li><li>高速缓存；</li><li>索引：用户可以按关键字搜索数据井支持各种过滤操作；</li><li>流式处理：持续发送消息至另一个进程，处理采用异步方式；</li><li>批处理：定期处理大量累计数据。</li></ul><h3 id="数据系统"><a href="#数据系统" class="headerlink" title="数据系统"></a>数据系统</h3><p>本章提出<strong>数据系统</strong>的概念，包括<strong>数据库</strong>、<strong>消息队列</strong>和<strong>高速缓存</strong>等不同类型的系统。这样分类的原因有以下几点：</p><ol><li><p>都能将数据保存一段时间。区别在于访问模式不同，以及由于不同访问模式导致的不同性能。</p></li><li><p>新技术、工具拥有多种功能，系统之间的界限变得模糊。比如redis既能<strong>存储</strong>也能作为<strong>消息队列</strong>，kafka作为<strong>消息队列</strong>也能<strong>持久化存储</strong>。</p></li><li><p>系统也需要细分，原有的但各组件无法满足数据处理与存储需求，三分类概念需要进一步分解。例如，即便是一个简单的数据系统，也可能由很多子组件构成：</p></li></ol><p><img src="/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E6%80%A7%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1CH1%E7%AC%94%E8%AE%B0/一种数据系统架构.png" alt></p><p>这个应用包含缓存层（Memcached）和全文索引服务器（Elasticsearch或者Solr）以及数据库。程序员编写应用代码来控制缓存、索引和数据同步操作。</p><h3 id="数据系统的三种关键特性："><a href="#数据系统的三种关键特性：" class="headerlink" title="数据系统的三种关键特性："></a>数据系统的三种关键特性：</h3><ul><li><p>可靠性（Reliability）：当出现意外情况，如软件、硬件故障，人为操作失误等现象时，系统仍可以正常运转的能力。这里的正常运转，是指牺牲部分性能条件下，提供正确的服务。</p></li><li><p>可扩展性（Scalability）：随着问题规模的增长，比如流量、数据量或者任务复杂度，系统以合理方式匹配这种增长的能力。</p></li><li><p>可维护性（Maintainability）：新的开发或者维护人员上手的容易程度，以及适配新场景的能力。</p></li></ul><h3 id="可靠性"><a href="#可靠性" class="headerlink" title="可靠性"></a>可靠性</h3><p>什么是可靠性？即使发生了某些错误，系统仍可以继续正常工作的能力。</p><ul><li>故障或错误（fault）：组件偏离正确规格的现象。</li><li>失效（failure）：系统宕机，无法提供服务。</li></ul><p>为保证可靠性，需要设计容错机制（fault-tolerant）来避免故障引发系统失效，而不是避免故障（当然，避免故障也是提升可靠性的手段）。</p><h3 id="可靠性指标"><a href="#可靠性指标" class="headerlink" title="可靠性指标"></a>可靠性指标</h3><h4 id="失效前平均时间（MTTF）"><a href="#失效前平均时间（MTTF）" class="headerlink" title="失效前平均时间（MTTF）"></a>失效前平均时间（MTTF）</h4><p>是针对不可修复系统而言的，是指系统发生失效前的平均工作（或存储） 时间或工作次数。越高越好。</p><h4 id="平均无故障时间（MTBF）"><a href="#平均无故障时间（MTBF）" class="headerlink" title="平均无故障时间（MTBF）"></a>平均无故障时间（MTBF）</h4><p>是针对可修复系统而言的，指两次相邻失效（故障） 之间的工作时间， 而不是指整个系统的报废时间。越高越好。</p><h4 id="平均修复时间（MTTR）"><a href="#平均修复时间（MTTR）" class="headerlink" title="平均修复时间（MTTR）"></a>平均修复时间（MTTR）</h4><p>是对可修复系统而言的，指从出现故障到修复中间的这段时间。越低越好。</p><h4 id="可用性"><a href="#可用性" class="headerlink" title="可用性"></a>可用性</h4><p>又称为有效性，一般用可用度来定量计算，公式为</p><script type="math/tex; mode=display">A=\frac{\text{MTBF}}{\text{MTBF}+\text{MTTR}}</script><p>可靠性通常低于可用性。对于不可维修系统， 可用度就仅仅决定于且等于可靠度。</p><h3 id="数据系统可能发生哪些故障？如何提升容错能力？"><a href="#数据系统可能发生哪些故障？如何提升容错能力？" class="headerlink" title="数据系统可能发生哪些故障？如何提升容错能力？"></a>数据系统可能发生哪些故障？如何提升容错能力？</h3><h4 id="1-硬件故障"><a href="#1-硬件故障" class="headerlink" title="1. 硬件故障"></a>1. 硬件故障</h4><p>包括硬盘错误、电源错误、网口接触不良等。</p><p>最耐用的硬盘的MTTF为10~50年，因此一个包括10000个磁盘的存储集群中，一天坏一个也是很正常的（假设购买硬盘在时间上是均匀分布）。</p><p>应对方案是添加冗余，比如上述硬盘问题就可以用RAID，电源用双电源，甚至热插拔CPU等。</p><p>以上是从硬件角度解决硬件故障，其实可以通过软件角度解决硬件故障问题。</p><h4 id="2-软件错误"><a href="#2-软件错误" class="headerlink" title="2. 软件错误"></a>2. 软件错误</h4><p>操作系统内核的Bug，系统依赖的服务突然没有响应，某个组件的失控等。</p><p>软件系统的问题有时没有快速解决的办法，只能通过检查依赖假设和系统交互、进行全面测试等方法来预防，或者允许进程重启、评估运行时表现等应对。</p><h4 id="3-人为失误"><a href="#3-人为失误" class="headerlink" title="3. 人为失误"></a>3. 人为失误</h4><p>人是最不可靠的因素。运维人员配置错误往往是系统下线的主要原因。</p><p>这部分的预防以及应对，更加依赖软件工程领域的知识。</p><h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><p>可扩展性是用来描述系统应对负载增加的能力的术语。一个系统添加计算单元越容易，添加计算单元后计算能力的增长越多，就称这个系统的可扩展性越强。</p><h3 id="如何度量系统负载"><a href="#如何度量系统负载" class="headerlink" title="如何度量系统负载"></a>如何度量系统负载</h3><p>每秒请求处理次数、数据库中写入的比例、聊天室的同时活动用户数量、缓存命中率等，</p><p>还要注意分析平均值和峰值对性能的影响</p><h3 id="实例：twitter的两个实现版本"><a href="#实例：twitter的两个实现版本" class="headerlink" title="实例：twitter的两个实现版本"></a>实例：twitter的两个实现版本</h3><p>twitter有两个功能（类比微博，为了方便理解我接下来以微博代替推特），根据其2012年的数据，其功能和负载如下：</p><p>发微博：平均每秒4600条发布申请，峰值12000条推特申请发布。</p><p>收微博：平均每秒300,000条收微博的请求。</p><p>twitter第一个版本使用如下系统设计完成微博的收发：</p><p>发送微博：用户发送的微博插入全局微博集合。</p><p>用户申请查看自己的时间线：  </p><ul><li>遍历所有User的关注对象；</li><li>提取所有关注对象的微博；</li><li>按照发表时间排序并合并。</li></ul><p>随着注册用户变多，负载压力与日俱增，因此采取第二种方法：</p><p>每个用户的时间线维护一个缓存。</p><p>用户发表新的微博：</p><ul><li>查询关注对象；</li><li>插入到每个粉丝的时间线缓存。</li></ul><p>方法二的好处是，用户发布微博时多做一些事情可以加速用户接收微博时的性能。而用户接收微博的请求负载比发送负载高两个数量级。</p><p>Twitter针对那些粉丝量特别多的大V采用方法一，针对粉丝量不太大的绝大多数用户使用方法二。</p><h3 id="如何度量系统性能"><a href="#如何度量系统性能" class="headerlink" title="如何度量系统性能"></a>如何度量系统性能</h3><p><strong>吞吐量</strong></p><p>吞吐量是在一个特定时间段内完成的任务的计数，例如：每秒点击数。</p><p>系统吞吐量几个重要参数：QPS（TPS）、并发数、响应时间</p><p>并发数： 系统同时处理的request/事务数</p><p>TPS：Transactions Per Second（每秒传输的事物处理个数），即服务器每秒处理的事务数。</p><p>QPS：每秒查询率，即对一个特定的查询服务器在规定时间内所处理流量大小。</p><p><strong>响应时间</strong></p><p>从客户端发送请求到接受响应之间的间隔时长。在线系统通常更看重响应时间，批处理系统更关心吞吐量。</p><p>由于每次请求的响应时间服从一个分布，因此我们更关心平均响应时长，或者<strong>百分位数时长</strong>。将响应时长排序，取50百分位数（中位数）比平均数更有意义。其他有意义的百分位数有95、99和99.9百分位数。</p><p>采用较高的响应时间百分位数很重要，因为他们直接影响用户的总体服务体验。95百分位数的响应时间为1.5秒，意味着100个请求中5个请求慢于1.5秒，但是对于电商平台来说，有可能恰恰是这些顾客购买了更多的商品导致访问变慢。</p><p>排队延迟是影响高百分数响应时间的主要延迟来源。因此，如果在性能测试时，负载生成客户端在服务器处理完之前请求后再发送新的请求，就会忽视了排队造成的延迟。</p><h3 id="长尾效应"><a href="#长尾效应" class="headerlink" title="长尾效应"></a>长尾效应</h3><p>一个服务涉及多个不同的后端调用，则最慢的调用会拖累整个服务的响应时间，这种现象称之为长尾效应。</p><p>用户总是需要等待最慢的那个调用完成。因此即便只有很小比例的请求缓慢，也可能由于某一个用户频繁产生这种调用而导致总体变慢。</p><h3 id="如何应对负载增加，提升可扩展性？"><a href="#如何应对负载增加，提升可扩展性？" class="headerlink" title="如何应对负载增加，提升可扩展性？"></a>如何应对负载增加，提升可扩展性？</h3><p>垂直扩展：升级到更强大的机器。</p><p>水平扩展：将负载分布到更多小机器。</p><p>系统设计时，要在这两种扩展中间作取舍。同时要根据不同的吞吐量、请求方式等做针对性优化。</p><h3 id="可维护性"><a href="#可维护性" class="headerlink" title="可维护性"></a>可维护性</h3><p>软件工程师不喜欢读别人留下的代码，不喜欢维护别人开发的老系统，这已经是众所周知的事实。</p><p>但是换个角度，不如说大多数系统在设计之初就没有考虑令后续开发者方便地维护这一个特性。</p><p>为了提升系统的可维护性，减少维护期间的麻烦，可以遵循软件系统的三个设计原则：</p><p>可运维性、简单性、可演化性。</p><h4 id="可运维性"><a href="#可运维性" class="headerlink" title="可运维性"></a>可运维性</h4><p>目标是令运维人员更轻松。之前讨论过，人是系统中最不稳定的因素。因此简化运维人员的操作，使运维人员能够专注于高附加值的任务，能够提升系统的可维护性。</p><p>可以从以下角度提升可运维性：</p><ul><li>提供系统运行时监控工具，方便监控；</li><li>自动化工具；</li><li>避免绑定特定的机器；</li><li>提供良好的文档；</li><li>提供良好的默认配置；</li><li>尝试自我修复等。</li></ul><h4 id="简单性"><a href="#简单性" class="headerlink" title="简单性"></a>简单性</h4><p>简单性是复杂性的反面。而复杂性有各种各样的表现方式，比如模块紧耦合，状态空间膨胀，依赖关系复杂，命名方法混乱，各种性能trick等。</p><p>消除复杂性的最好手段之一是抽象，通过抽象掩盖大量实现细节，提供干净的接口。</p><h4 id="可演化性"><a href="#可演化性" class="headerlink" title="可演化性"></a>可演化性</h4><p>提升可演化性是令系统易于改变的另一种说法。目标是可以轻松地修改数据系统，使其适应不断变化的需求。</p><p>在组织流程方面，敏捷开发模式为适应变化提供了很好的参考。因此敏捷性与可演化性很类似。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Big Data</tag>
      
      <tag>Distributed System</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读笔记】MapReduce: Simplified Data Processing on Large Clusters</title>
    <link href="/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/"/>
    <url>/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/</url>
    
    <content type="html"><![CDATA[<p>网络上描写MapReduce的文章不可胜计，唯倜傥非常之文章留存。</p><!--more---><h2 id="一、MapReduce的概念概括"><a href="#一、MapReduce的概念概括" class="headerlink" title="一、MapReduce的概念概括"></a>一、MapReduce的概念概括</h2><p>MapReduce是Google提出的一个软件架构，用于大规模数据集的并行运算。</p><p>MapReduce是一个编程范式，旨在使用<code>map</code>把大规模的问题分解成子问题，然后利用<code>reduce</code>把子问题的解汇总。这种编程范式特别适合应用于分布式系统。</p><p>要理解<code>map</code>和<code>reduce</code>的操作，最重要是要理解下式：</p><p><img src="/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/map和reduce.png" alt></p><p>k1和v1是原始的输入key和value；<br>list(k2, v2)是<code>map</code>把k1和v1分布式计算后的中间结果集合；<br>reduce(k2, list(v2))是<code>reduce</code>函数根据k2的值来合并v2；<br>最终我们想要得到的结果是list(v2)。</p><h2 id="二、MapReduce的应用场景："><a href="#二、MapReduce的应用场景：" class="headerlink" title="二、MapReduce的应用场景："></a>二、MapReduce的应用场景：</h2><ol><li>WordCount——分布式系统的“Hello World”</li></ol><p><code>map</code>函数输出文档中的每个词以及它的频率(word, 1)，<code>reduce</code>函数负责把所有同样词的频率累加起来。得到每个词出现的频率。这一个应用实现请看这里TODO。</p><ol><li>分布式字符串匹配 grep</li></ol><p><code>map</code>函数输出匹配某个模式的一行，<code>reduce</code>则什么计算也不做，只负责将那一行输出。得到匹配的文本行。</p><ol><li>计算 URL 的访问频率</li></ol><p><code>map</code>函数记录requests，输出(url, 1)，<code>reduce</code>函数将相同url的访问加起来，产生(url, sum_of_count)。</p><ol><li>倒转网络链接关系</li></ol><p>搜索引擎排序算法 pagerank 需要使用爬虫爬取所有页面 source 及页面内部的链接 target 。每个 source 内部可能存在多个 target。遇到门户网站，一个 source 有上千个 target 都不奇怪。</p><p>那么搜索引擎是如何决定哪个 target 比较重要的呢？</p><p>pagerank算法解决了这个问题，它假设如果一个网页被很多其他网页所链接，说明它受到普遍的关注和信赖，那么它的排名就高。同时，每个网页的权重不同，那些权重较大的网页拥有的链接更可靠，这些链接的排名往往更靠前。</p><p>这就需要网络爬虫统计每个链接 target 被哪些 source 引用过这种信息了。但是之前我们获得的是 (source, target)数据对，如何把这一部分数据转换成(target, list(source))呢？这就轮到MapReduce出场了。</p><p><code>map</code>负责输出 (source, target) 对，<code>reduce</code> 负责将相同的 target 归并，生成 (target, list(source))。</p><ol><li>确定每个Host的检索词向量（Term-Vector） <span id="1"></span></li></ol><p>检索词向量是一系列（单词，词频）对，这些数据对能够总结一篇文档或者一系列文章的最重要单词（假设词频越高越重要）。</p><p>一个Host可能有非常多文档，如何确定一个Host的检索词向量？</p><p><code>map</code>负责输出每个输入文档的(host, term-vector)，<code>reduce</code>负责汇总给定host的所有term-vector，丢弃所有低频词，输出最终唯一的(host, term-vector)。</p><ol><li>倒排索引</li></ol><p>什么是正排索引？(document, {keys})这种索引形式，从文档出发，检索关键词。</p><p>不过正排索引在搜索引擎中显然没什么作用，因为我们的应用场景是根据关键词检索到对应的所有文档。因此我们更需要(key, {documents})这种索引形式。</p><p>倒排索引就是关键词到文档的映射。每个关键词都对应着一系列的文档。</p><p><code>map</code>分析每个文档，为每个word生成(word, document)的映射，<code>reduce</code>汇总所有相同word的数据对，输出(word, {documents})（可能还会排序）。</p><h2 id="三、MapReduce的实现"><a href="#三、MapReduce的实现" class="headerlink" title="三、MapReduce的实现"></a>三、MapReduce的实现</h2><p>既然MapReduce这么好，那么究竟该怎么实现呢？</p><p>根据不同的集群以及节点性能，MapReduce有多种不同的实现方式。 </p><p>假设存在以下应用场景：普通配置的PC约1000台，机器之间使用交换机连接，网速为百兆，存储介质为廉价的IDE硬盘。用户希望能够通过向调度系统提交job，自动将job对应的一系列tasks分发到集群的各个节点上。</p><h3 id="3-1-MapReduce执行流程概括"><a href="#3-1-MapReduce执行流程概括" class="headerlink" title="3.1 MapReduce执行流程概括"></a>3.1 MapReduce执行流程概括</h3><p>本节我会综合论文原文的 MapReduce 与 Hadoop MapReduce 的具体实现，给出二者的执行步骤。</p><h4 id="3-1-1-论文中的-MapReduce-执行流程"><a href="#3-1-1-论文中的-MapReduce-执行流程" class="headerlink" title="3.1.1 论文中的 MapReduce 执行流程"></a>3.1.1 论文中的 MapReduce 执行流程</h4><ol><li><p>在map阶段，MapReduce会对要处理的数据进行分片（split）操作，为每一个分片分配一个MapTask任务。将输入分成M部分，每部分的大小一般在16M~64M之间（用户来定义）。输出也分为R部分（？）。然后在各个机器上fork程序副本。</p></li><li><p>选定集群中的一个机器为master节点，负责分发任务；其他节点是worker，负责计算和向master提交任务结果。</p></li><li><p>之前指定了M个map任务和R个reduce任务，master节点给每个空闲的worker分配一个map任务或者一个reduce任务。</p></li><li><p>被分配map任务的worker会读取对应的输入片段，输入用户定义的map函数，输出中间结果，将这些中间结果缓存在内存中。这些中间结果会定期地保存在本地此版中。由partition函数将其分成R部分。worker负责将这些缓存数据对在磁盘中的位置上传给master。</p></li><li><p>master负责收集map worker发送回来的数据对位置，然后把这些位置发送给 reduce worker。当一个reduce worker把这些中间结果读取完毕后，它会首先对这些中间结果排序，这样同样key的中间结果就会相邻了。</p></li></ol><p>key很多种类的情况下，排序是有必要的吗？实践表明，排序是有必要的，因为数据分片后，往往同一key的数据在同一片M中。这体现了数据在空间上的局部性。</p><p>但是如果数据量过大，中间结果过多，我们可能需要外部排序。</p><ol><li><p>reduce worker迭代所有中间结果，由于这些中间信息按照key排序过了，因此很容易获得同样key的所有键值对集合(key, {values})。将这一部分整合key后的信息传递给Reduce函数。Reduce函数的输出被追加到所属分区R的输出文件中。</p></li><li><p>当所有Map和Reduce任务都完成后，master唤醒用户程序，用户程序返回调用结果。</p></li></ol><p>下图是MapReduce论文中的流程概括图。</p><p><img src="/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/流程概括.png" alt></p><h4 id="3-1-2-Hadoop-MapReduce-执行流程"><a href="#3-1-2-Hadoop-MapReduce-执行流程" class="headerlink" title="3.1.2 Hadoop MapReduce 执行流程"></a>3.1.2 Hadoop MapReduce 执行流程</h4><p><img src="/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/MR流程.png" alt></p><ol><li><strong>Map阶段执行过程</strong></li></ol><p>对应流程图的左半部分。</p><ul><li>1) 把输入目录下文件按照一定标准逐个进行<strong>逻辑切片</strong>，形成切片规划</li></ul><p>默认切片大小和块大小是相同的，每个切片由一个MapTask来处理。</p><ul><li>2) 对切片中的数据按照一定规则解析并返回(key,value)对</li></ul><p>如果是文本数据，则调用TextInputFormat类。默认按行读取数据，key是每一行的起始偏移量，value是本行的文本内容。</p><p>key对应的偏移量是什么东西？打开notepad++，底栏的Pos就是当前光标所对应字符的偏移量。</p><p><img src="/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/偏移量.png" alt></p><ul><li>3) 调用Mapper类中的map方法处理数据。</li></ul><p>每读取解析出来的一个(key,value)，调用一次map方法。map方法是用户自己定义的业务逻辑。</p><ul><li><p>4) 按照一定的规则对Map输出的键值对进行分区(partition)。分区的数量就是reduce task的数量。</p></li><li><p>5) Map输出数据写入内存缓冲区，达到一定比例后溢出(spill)到磁盘上。溢出的时候根据key排序(sort)。</p></li><li><p>6) 对所有溢出的文件进行合并(merge)，形成一个文件。</p></li></ul><p>至此，Map阶段结束。map worker并不会将自己的结果发送给reduce，而是会静静地等待reduce worker来主动拉取数据。</p><ol><li><strong>Reduce阶段执行过程</strong></li></ol><p>对应流程图的右半部分。</p><ul><li><p>1) Reduce task会主动从MapTask复制拉取其输出的键值对；</p></li><li><p>2) 把复制到Reduce worker的本地数据全部合并(merge)，再对合并的数据排序。</p></li><li><p>3) 对排序后的键值对调用Reduce方法。</p></li></ul><p>键相等的键值对调用一次reduce方法。</p><p>最后把这些输出的键值对写入到HDFS文件中。</p><h3 id="3-2-Master节点的数据结构"><a href="#3-2-Master节点的数据结构" class="headerlink" title="3.2 Master节点的数据结构"></a>3.2 Master节点的数据结构</h3><p>Master节点会保存每个map任务和reduce任务的执行状态（空闲 idle、正在执行 in-progress，执行完毕 completed），还会给被分配任务了的worker保存一个标注。</p><p>Master还会像一个管道一样储存map任务完成后的中间信息存储位置，并把这些位置信息传输给reduce worker。</p><h3 id="3-3-容错机制"><a href="#3-3-容错机制" class="headerlink" title="3.3 容错机制"></a>3.3 容错机制</h3><h4 id="worker-failure"><a href="#worker-failure" class="headerlink" title="worker failure"></a>worker failure</h4><p>master会周期性地ping每个worker，规定时间内没有返回信息，则master将其标记为fail。master将所有由这个失效的worker做完的（completed）、正在做的（in-progress）<code>map</code>任务标记为初始状态（idle），等待其他worker认领任务。</p><p>worker故障时，对于map任务结果和reduce任务结果的处理方法有所不同。map任务的结果由于需要通知master存储位置，中途中断会导致存储位置丢失，因此失败的map任务需要重新执行；reduce任务的结果存储位置在全局文件系统上，因此不需要再次执行。</p><p>当worker B接手了失败的worker A的task，所有reduce worker都会知晓。因此所有还没有从worker A哪里获得数据的reduce worker会自动向worker B获取数据。</p><h4 id="master-failure"><a href="#master-failure" class="headerlink" title="master failure"></a>master failure</h4><p>首先要有检查点（checkpoint）机制，周期性地将master节点存储的数据保存至磁盘。</p><p>但是由于只有一个master进程，因此master失效后MapReduce运算会中止。由用户亲自检查，根据需要重新执行MapReduce。</p><h4 id="Semantics-in-the-Presence-of-Failures"><a href="#Semantics-in-the-Presence-of-Failures" class="headerlink" title="Semantics in the Presence of Failures"></a>Semantics in the Presence of Failures</h4><p>Semantics，语义。这个词不太好理解，总之本节讨论Failure的出现对程序语义的影响。</p><p>个人理解，对于一个确定性的程序而言，程序的寓意就是程序的执行顺序；但是对于非确定性程序而言，也就是执行多次可能得到不同结果的程序而言，语义是否能保证，直接与最后结果是否正确相关。</p><p>MapReduce保证，当Map和Reduce的操作都是确定性函数（只要相同输入就会得到相同输出的函数），那么MapReduce处理得到的结果也都是确定性的，不论集群内部有没有错误、执行顺序。</p><p>这种强保证是由Map和Reduce中的commit操作的原子性来保证的。</p><p>每个 in-progress task 都将其输出写进私有临时文件中。每个reduce产生一个私有临时文件，每个map产生R个私有临时文件（因为对应R个reduce任务）。</p><p>当map任务完成，map worker发送给master的是那R个临时文件的名称，并标注“我做完了”。master在收到消息后，就将这R个文件名记录在自己的数据结构中。<strong>如果这个时候由于某些错误</strong>，master又收到一遍“我做完了”，master将会忽略。</p><p>当reduce任务完成，reduce worker把临时文件重命名为最终的输出文件名。重命名操作是原子的，即要不就全部重命名成功，要不就一个都不会重命名。这里存在一种语义风险，那就是如果同一个reduce task在多台机器上执行，同一个结果文件有可能被重命名多次。为保证最终文件系统只包含一个reduce任务产生的数据，MapReduce依赖底层文件系统提供的重命名操作（？）。</p><p>坦白说，关于弱语义如何保证这一块儿没看懂，等今后再回来补吧。TODO</p><h3 id="3-4-存储位置"><a href="#3-4-存储位置" class="headerlink" title="3.4 存储位置"></a>3.4 存储位置</h3><p>这一部分的设计要尽可能节约带宽，因为带宽是相当匮乏的资源。</p><p>MapReduce的解决方案是通过GFS (Google File System)将每个文件分成 64 MB的块，然后将每块保存几个副本（通常为3份）在不同的机器上。MapReduce 尽量将这些位置信息保存下来然后尽量将含有某个文件主机的任务分配给它，这样就可以减少网络的传递使用。如果失败，那么将会尝试从靠近输入数据的一个副本主机去启动这个任务。当在一个集群上执行大型的 MapReduce 操作的时候，输入数据一般都是本地读取，减少网络带宽的使用。</p><h3 id="3-5-任务粒度"><a href="#3-5-任务粒度" class="headerlink" title="3.5 任务粒度"></a>3.5 任务粒度</h3><p>理想状况下，M和R应当与worker数目大很多，这样才能提高集群的动态负载均衡能力，并且能加快故障恢复的速度，原因是失效的worker上执行的map任务可以分布到所有其他的worker机器上执行。</p><p>但是M和R也是有限制的，这一部分限制主要是由于master需要执行O(M+R)次调度。</p><p>我们通常会按照这样的比例执行：M=200,000，R=5,000，worker有2,000台。</p><h3 id="3-6-备用任务"><a href="#3-6-备用任务" class="headerlink" title="3.6 备用任务"></a>3.6 备用任务</h3><p>长尾分布现象（或者说“水桶效应”）在MapReduce中也有体现，因为MapReduce计算时间往往取决于其运行速度最慢的worker。</p><p>有一个办法来减少“straggler”（落伍的人），master会在任务快完成时，调用backup进程来解决那些 in-progress 任务。这样，无论是原来的进程还是 backup 进程中的哪个先完成，master都立即将其标记为完成。</p><h2 id="四、MapReduce调优技巧"><a href="#四、MapReduce调优技巧" class="headerlink" title="四、MapReduce调优技巧"></a>四、MapReduce调优技巧</h2><h3 id="4-1-分区函数"><a href="#4-1-分区函数" class="headerlink" title="4.1 分区函数"></a>4.1 分区函数</h3><p>分区函数一般是Hash，<code>Hash(key) % R</code>，这样就能把key分成R份了。但是有的时候我们希望自己定义R的分区方法，比如在第二章的应用场景<a href="#1">Host Term-Vector</a>中，我们希望以Host为分R标准，那么分区函数就可以这么写：<code>hash(Hostname(urlkey)) % R</code>。，这样具有相同的 hostname 的URL将会出现在同一个输出文件中。</p><h3 id="4-2-顺序保证"><a href="#4-2-顺序保证" class="headerlink" title="4.2 顺序保证"></a>4.2 顺序保证</h3><p>在一个分区R中，MapReduce保证所有中间k/v对都是按key排序的。</p><h3 id="4-3-Combiner"><a href="#4-3-Combiner" class="headerlink" title="4.3 Combiner"></a>4.3 Combiner</h3><p>某些任务的中间结果在从map传输到reduce的时候可以先处理一下再传。比如word count应用，中间结果是一堆(word, 1)数据对，这个时候我们利用某个combiner函数，将本地的中间结果合并一下，比如合并相同的100个(word, 1)为(word, 100)，就大量降低了数据传输占用的带宽。</p><p>Combiner函数会在每台执行Map任务的机器上执行一次。通常情况下，Combiner函数和Reduce函数的实现代码是一样的。</p><h3 id="4-4-输入和输出"><a href="#4-4-输入和输出" class="headerlink" title="4.4 输入和输出"></a>4.4 输入和输出</h3><p>MapReduce库支持不同的格式的输入数据。比如文本模式，key是行数，value是该行内容。</p><p>程序员可以定义Reader接口来适应不同的输入类型。程序员需要保证必须能把输入数据切分成数据片段，且这些话宿儒片段能够由单独的Map任务来处理就行了。</p><p>Reader的数据源可能是数据库，可能是文本文件，甚至是内存等。输入Writer同样可以自定义。</p><h3 id="4-5-副作用"><a href="#4-5-副作用" class="headerlink" title="4.5 副作用"></a>4.5 副作用</h3><p>程序员在写Map和Reduce操作的时候，可能会处于方便，定义很多额外功能，比如生成辅助文件等。但应当时刻记住，Map和Reduce操作应当保证原子性和幂等性。</p><p>比如，一个task生成了多个输出文件，但是我们没有原子化多段commit的操作。这就需要程序员自己保证生成多个输出的任务是确定性任务。</p><h3 id="4-6-跳过损坏的纪录"><a href="#4-6-跳过损坏的纪录" class="headerlink" title="4.6 跳过损坏的纪录"></a>4.6 跳过损坏的纪录</h3><p>有时相比于修复不可执行的Bug，跳过该部分引起Bug的Record更加可取。因此，我们希望MapReduce检测到可能引起崩溃的Record时，自动跳过。</p><p>MapReduce如何自动检测这种现象？首先每个worker会通过一个handler来捕获异常，并利用一个全局变量来保存异常序号。worker会在之后发送给master的工作汇报中写上该signal序号（以UDP发送）。master看到该UDP包中存在多次故障，那么将来该worker失败了，master就不会重复执行该task，而是跳过该record。</p><h3 id="4-7-本地执行"><a href="#4-7-本地执行" class="headerlink" title="4.7 本地执行"></a>4.7 本地执行</h3><p>就是说一上来就在成千上万台机器上进行调试是非常棘手的，因此MapReduce开发了在本地计算机上模拟MapReduce任务的项目，方便调试。</p><h3 id="4-8-状态信息"><a href="#4-8-状态信息" class="headerlink" title="4.8 状态信息"></a>4.8 状态信息</h3><p>master内部有一个内置的HTTP服务器，可以用来展示一组状态信息页面。状态页面会显示计算进度，例如：已经完成的任务数量、正在执行的任务数量、输入的字节数、中间数据的字节数、输出的字节数、处理率等等。</p><p>这些页面也包含了指向每个任务的标准差以及生成的标准输出文件的链接。用户可以使用这些数据来预测计算需要多久才能完成，是否需要往该计算中增加更多资源。当计算消耗的时间比预期时间更长的时候，这些页面也可以用来找出为什么执行速度很慢的原因。</p><p>此外，顶层的状态页面会显示那些故障的worker，以及它们故障时正在运行的Map和Reduce任务。这些信息对于调试用户代码中的bug很有帮助。</p><p>这一点HDFS也有类似实现，比如HDFS 在启动完成之后，还会由内部的 Web 服务提供一个查看集群状态的网页：</p><p><a href="http://localhost:50070/">http://localhost:50070/</a></p><p><strong>提供可视化监控界面，是提升分布式系统的可维护性的重要手段</strong>。</p><h3 id="4-9-计数器"><a href="#4-9-计数器" class="headerlink" title="4.9 计数器"></a>4.9 计数器</h3><p>MapReduce内部提供计数器机制，用来统计不同操作发生次数。要想使用计数器，程序员需要创建Counter对象，然后在Map和Reduce函数中以正确的方式增加counter。</p><p>当聚合这些counter的值时，master会去掉那些重复执行的相同map或者reduce操作的次数，以此避免重复计数（之前提到的备用任务和故障后重新执行任务，这两种情况会导致相同的任务被多次执行）。</p><p>有些counter值是由MapReduce库自动维护的，例如已经处理过的输入键值对的数量以及生成的输出键值对的数量。</p><h2 id="五、MapReduce的性能评估"><a href="#五、MapReduce的性能评估" class="headerlink" title="五、MapReduce的性能评估"></a>五、MapReduce的性能评估</h2><h2 id="六、MapReduce使用经验"><a href="#六、MapReduce使用经验" class="headerlink" title="六、MapReduce使用经验"></a>六、MapReduce使用经验</h2><p>本文只关注MapReduce的技术细节，故第五、六节略过。</p><h2 id="七、参考"><a href="#七、参考" class="headerlink" title="七、参考"></a>七、参考</h2><p>Lassen S B . MapReduce: Simplified Data Processing on Large Clusters (work by Jeffrey Dean and Sanjay Ghemawat). </p><p><a href="https://chunlife.top/2020/04/18/Google-MapReduce%E4%B8%AD%E6%96%87%E7%89%88/">https://chunlife.top/2020/04/18/Google-MapReduce%E4%B8%AD%E6%96%87%E7%89%88/</a></p>]]></content>
    
    
    <categories>
      
      <category>paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Big Data</tag>
      
      <tag>Distributed System</tag>
      
      <tag>MapReduce</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】Hadoop介绍和安装</title>
    <link href="/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Hadoop%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%89%E8%A3%85/"/>
    <url>/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Hadoop%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%89%E8%A3%85/</url>
    
    <content type="html"><![CDATA[<p>Hadoop学习笔记第一篇。<br><!--more---></p><h1 id="Hadoop介绍"><a href="#Hadoop介绍" class="headerlink" title="Hadoop介绍"></a>Hadoop介绍</h1><p>Apache Hadoop 软件库是一个框架，允许在<strong>集群服务器</strong>上使用简单的<strong>编程模型</strong>，<strong>对大数据集进行分布式处理</strong>。</p><p>Hadoop 可扩展性强，能从单台服务器扩展到数以千计的服务器；Hadoop 高可用，其代码库自身就能在应用层侦测并处理硬件故障。</p><p>Hadoop 的生态系统不仅包含 Hadoop，而且还包含 HDFS、HBase等基本组件。</p><p><img src="/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Hadoop%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%89%E8%A3%85/Hadoop生态系统.png" alt></p><p><strong>HDFS (Hadoop Distributed File System)</strong></p><p>HDFS是分布式文件系统的一种。HDFS是Hadoop生态系统的基本组成，它将数据保存在计算机集群上。HDFS是HBase等工具的基础。</p><p><strong>MapReduce</strong></p><p>MapReduce是一种分布式计算框架，也是一个分布式、并行处理的编程模型。MapReduce把任务分为<code>map</code>阶段和<code>reduce</code>阶段，<code>map</code>阶段将任务分解成子任务后映射到集群上，<code>reduce</code>将结果化简并整合。</p><p>正是利用了MapReduce的工作特性，Hadoop因此能以并行的方式访问数据，从而实现分布式计算。</p><p>关于MapReduce的论文讲解，请看<a href="https://superlova.github.io/2021/05/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91MapReduce-Simplified-Data-Processing-on-Large-Clusters/">这里</a>。</p><p><strong>HBase</strong></p><p>HBase 是一个建立在 HDFS 之上，面向列的 NoSQL 数据库，用于快速读 / 写大量数据。HBase 使用 Zookeeper 进行管理。</p><p><strong>ZooKeeper</strong></p><p>ZooKeeper 为大型分布式计算提供开源的分布式配置服务、同步服务和命名注册。</p><p>Hadoop 的许多组件依赖于 Zookeeper，它运行在计算机集群中，用于管理 Hadoop 集群。</p><p><strong>Pig</strong></p><p>Pig是一个基于Hadoop的大规模数据分析平台，它为 MapReduce 编程模型提供了一个简单的操作和编程接口。它提供的SQL-LIKE语言叫Pig Latin，该语言的编译器会把类SQL的数据分析请求转换为一系列经过优化处理的MapReduce运算。</p><p><strong>Hive</strong><br>Apache Hive是一个建立在Hadoop架构之上的数据仓库。它能够提供数据的精炼，查询和分析。像 Pig 一样，Hive 作为一个抽象层工具，吸引了很多熟悉 SQL 而不是 Java 编程的数据分析师。</p><p>与Pig的区别在于，Pig是一中编程语言，使用命令式操作加载数据、表达转换数据以及存储最终结果。Pig中没有表的概念。而Hive更像是SQL，使用类似于SQL语法进行数据查询。</p><p><strong>Sqoop</strong></p><p>用于在关系数据库、数据仓库和 Hadoop 之间转移数据。</p><p><strong>Flume</strong></p><p>是一个分布式、可靠、高可用的海量日志采集、聚合和传输的系统，用于有效地收集、聚合和将大量日志数据从许多不同的源移动到一个集中的数据存储（如文本、HDFS、Hbase等）。</p><p><strong>Yarn</strong></p><p>是从Hadoop 2.0版本开始沿用的任务调度和集群资源管理的框架。</p><p><strong>Spark</strong></p><p>一个快速通用的 Hadoop 数据计算引擎，具有简单和富有表达力的编程模型，支持数据 ETL（提取、转换和加载）、机器学习、流处理和图形计算等方面的应用。</p><p>Spark 这一分布式内存计算框架就是脱胎于 Hadoop 体系的，它对 HDFS 、YARN 等组件有了良好的继承，同时也改进了 Hadoop 现存的一些不足。</p><p>下图是Hadoop集群的基本架构。</p><p><img src="/2021/05/04/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Hadoop%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%89%E8%A3%85/Hadoop集群基本架构.png" alt></p><h1 id="Hadoop-可以做什么"><a href="#Hadoop-可以做什么" class="headerlink" title="Hadoop 可以做什么"></a>Hadoop 可以做什么</h1><p>据Hadoop Wiki记载，阿里巴巴使用15个节点组成的Hadoop集群，每个节点拥有8核心、16GB内存和1.4TB存储。阿里巴巴使用这些节点来处理商业数据的排序和组合，应用于交易网站的垂直搜索。</p><p>Ebay拥有32个节点组成的集群，使用Java编写的MapReduce应用，来优化搜索引擎。</p><p>FaceBook使用Hadoop来存储内部日志和结构化数据源副本，并且将其作为数据报告、数据分析和机器学习的数据源。</p><h1 id="Hadoop-不同版本"><a href="#Hadoop-不同版本" class="headerlink" title="Hadoop 不同版本"></a>Hadoop 不同版本</h1><p><strong>关于发行方：</strong></p><p>目前Hadoop发行版非常多，有Intel发行版，华为发行版、Cloudera发行版（CDH）、Hortonworks版本等，所有这些发行版均是基于Apache Hadoop衍生出来的，之所以有这么多的版本，是由于Apache Hadoop的开源协议决定的：任何人可以对其进行修改，并作为开源或商业产品发布/销售。</p><p><strong>关于版本：</strong></p><p>现在最新的Hadoop已经达到3.X了，然而大部分公司使用Hadoop 2.X。又由于Hadoop 2.X与1.X相比有较大变化，因此直接使用2.X是比较合理的选择。</p><p>Hadoop2.0新增了HDFS HA机制，HA增加了standbynamenode进行热备份，解决了1.0的单点故障问题。</p><p>Hadoop2.0新增了HDFS federation，解决了HDFS水平可扩展能力。 </p><p>2.0相比于1.0 新增了YARN框架，Mapreduce的运行环境发生了变化</p><h1 id="Hadoop-安装"><a href="#Hadoop-安装" class="headerlink" title="Hadoop 安装"></a>Hadoop 安装</h1><p>Hadoop有三种安装方式</p><ul><li>单机模式：安装简单，几乎不用做任何配置，但仅限于调试用途。</li><li>伪分布模式：在单节点上同时启动 NameNode、DataNode、JobTracker、TaskTracker、Secondary Namenode 等 5 个进程，模拟分布式运行的各个节点。</li><li>完全分布式模式：正常的 Hadoop 集群，由多个各司其职的节点构成。</li></ul><p>本文介绍 Hadoop 伪分布式模式部署方法，Hadoop 版本为 2.6.1。</p><h2 id="1-设置用户和组"><a href="#1-设置用户和组" class="headerlink" title="1. 设置用户和组"></a>1. 设置用户和组</h2><pre><code>sudo adduser hadoopsudo usermod -G sudo hadoop</code></pre><h2 id="2-安装JDK"><a href="#2-安装JDK" class="headerlink" title="2. 安装JDK"></a>2. 安装JDK</h2><p>不同版本的 Hadoop 对 Java 的版本需求有细微的差别，可以在<a href="https://cwiki.apache.org/confluence/display/HADOOP2/HadoopJavaVersions">这个网站</a>查询 Hadoop 版本与 Java 版本的关系。</p><p>测试jdk是否部署成功：</p><pre><code class="lang-sh">java -version</code></pre><h2 id="3-配置SSH免密码登录"><a href="#3-配置SSH免密码登录" class="headerlink" title="3. 配置SSH免密码登录"></a>3. 配置SSH免密码登录</h2><p>安装和配置 SSH 的目的是为了让 Hadoop 能够方便地运行远程管理守护进程的相关脚本。这些脚本需要用到 sshd 服务。</p><pre><code class="lang-sh">su hadoopcd /home/hadoopssh-keygen -t rsa# 将生成的公钥添加到主机认证记录中。cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys# 为 authorized_keys 文件赋予写权限chmod 600 .ssh/authorized_keys# 尝试登录到本机ssh localhost</code></pre><h2 id="4-下载-Hadoop"><a href="#4-下载-Hadoop" class="headerlink" title="4. 下载 Hadoop"></a>4. 下载 Hadoop</h2><pre><code class="lang-sh">wget https://archive.apache.org/dist/hadoop/common/hadoop-2.6.1/hadoop-2.6.1.tar.gztar zxvf hadoop-2.6.1.tar.gzsudo mv hadoop-2.6.1 /opt/hadoop-2.6.1sudo chown -R hadoop:hadoop /opt/hadoop-2.6.1vim /home/hadoop/.bashrc</code></pre><p>在 /home/hadoop/.bashrc 文件的末尾添加以下内容：</p><pre><code class="lang-sh">export HADOOP_HOME=/opt/hadoop-2.6.1export JAVA_HOME=/usr/lib/jvm/java-8-oracleexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</code></pre><p>在终端中输入 source 命令来激活新添加的环境变量。</p><pre><code class="lang-sh">source /home/hadoop/.bashrc</code></pre><h2 id="5-伪分布式模式配置"><a href="#5-伪分布式模式配置" class="headerlink" title="5. 伪分布式模式配置"></a>5. 伪分布式模式配置</h2><p>Hadoop 还可以以伪分布式模式运行在单个节点上，通过多个独立的 Java 进程来模拟多节点的情况。在初始学习阶段，暂时没有必要耗费大量的资源来创建不同的节点。</p><p>5.1 <strong>打开 core-site.xml 文件:</strong></p><pre><code class="lang-sh">vim /opt/hadoop-2.6.1/etc/hadoop/core-site.xml</code></pre><p>将 configuration 标签的值修改为以下内容：</p><pre><code class="lang-xml">&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;        &lt;value&gt;/home/hadoop/tmp&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>fs.defaultFS 配置项用于指示集群默认使用的文件系统的位置。</p><p>5.2 <strong>打开另一个配置文件 hdfs-site.xml</strong></p><pre><code class="lang-sh">vim /opt/hadoop-2.6.1/etc/hadoop/hdfs-site.xml</code></pre><p>将 configuration 标签的值修改为以下内容：</p><pre><code class="lang-xml">&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;dfs.replication&lt;/name&gt;        &lt;value&gt;1&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>该配置项用于指示 HDFS 中文件副本的数量，默认情况下是 3 份，由于我们在单台节点上以伪分布式的方式部署，所以将其修改为 1 。</p><p>5.3 <strong>编辑 hadoop-env.sh 文件：</strong></p><pre><code class="lang-sh">vim /opt/hadoop-2.6.1/etc/hadoop/hadoop-env.sh</code></pre><p>将其中 <code>export JAVA_HOME</code> 的值修改为 JDK 的实际位置，即 <code>/usr/lib/jvm/java-8-oracle</code> 。</p><p>5.4 <strong>编辑 yarn-site.xml 文件：</strong></p><pre><code class="lang-sh">vim /opt/hadoop-2.6.1/etc/hadoop/yarn-site.xml</code></pre><p>在 configuration 标签内添加以下内容：</p><pre><code class="lang-xml">&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>5.5 <strong>编辑 mapred-site.xml 文件。首先需要从模板复制过来：</strong></p><pre><code class="lang-sh">cp /opt/hadoop-2.6.1/etc/hadoop/mapred-site.xml.template /opt/hadoop-2.6.1/etc/hadoop/mapred-site.xmlvim /opt/hadoop-2.6.1/etc/hadoop/mapred-site.xml</code></pre><p>在 configuration 标签内添加以下内容：</p><pre><code class="lang-xml">&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;        &lt;value&gt;yarn&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h2 id="6-Hadoop-启动测试"><a href="#6-Hadoop-启动测试" class="headerlink" title="6. Hadoop 启动测试"></a>6. Hadoop 启动测试</h2><pre><code class="lang-sh">su -l hadoopvim /home/hadoop/.bashrc</code></pre><p>向<code>.bashrc</code>添加 Java 的环境变量：</p><pre><code class="lang-sh">export JAVA_HOME=/usr/lib/jvm/java-8-oracleexport PATH=$PATH:$JAVA_HOME/bin</code></pre><h1 id="HDFS-的基本使用"><a href="#HDFS-的基本使用" class="headerlink" title="HDFS 的基本使用"></a>HDFS 的基本使用</h1><h2 id="1-初始化HDFS"><a href="#1-初始化HDFS" class="headerlink" title="1. 初始化HDFS"></a>1. 初始化HDFS</h2><pre><code class="lang-sh">hdfs namenode -format</code></pre><p>格式化的操作只需要进行一次即可，不需要多次格式化。每一次格式化 namenode 都会清除 HDFS 分布式文件系统中的所有数据文件。同时，多次格式化容易出现 namenode 和 datanode 不同步的问题。</p><h2 id="2-启动HDFS"><a href="#2-启动HDFS" class="headerlink" title="2. 启动HDFS"></a>2. 启动HDFS</h2><p>HDFS 初始化完成之后，就可以启动 NameNode 和 DataNode 的守护进程。启动之后，Hadoop 的应用（如 MapReduce 任务）就可以从 HDFS 中读写文件。</p><p>在终端中输入以下命令来启动守护进程：</p><pre><code class="lang-sh">start-dfs.sh</code></pre><p>为了确认伪分布式模式下的 Hadoop 已经成功运行，可以利用 Java 的进程查看工具 <code>jps</code> 来查看是否有相应的进程。</p><p>如果执行 jps 发现没有 NameNode 服务进程，可以先检查一下是否执行了 namenode 的初始化操作。如果没有初始化 namenode ，先执行 stop-dfs.sh ,然后执行 hdfs namenode -format ,最后执行 start-dfs.sh 命令，通常来说这样就能够保证这三个服务进程成功启动</p><h2 id="3-查看日志和WebUI"><a href="#3-查看日志和WebUI" class="headerlink" title="3. 查看日志和WebUI"></a>3. 查看日志和WebUI</h2><p>作为大数据领域的学习者，掌握分析日志的能力与学习相关计算框架的能力同样重要。</p><p>Hadoop 的守护进程日志默认输出在安装目录的 log 文件夹中，在终端中输入以下命令进入到日志目录：</p><pre><code class="lang-sh">cd /opt/hadoop-2.6.1/logsls</code></pre><p>HDFS 在启动完成之后，还会由内部的 Web 服务提供一个查看集群状态的网页：</p><p><a href="http://localhost:50070/">http://localhost:50070/</a></p><p>打开网页后，可以在其中查看到集群的概览、DataNode 的状态等信息。</p><h2 id="4-HDFS文件上传测试"><a href="#4-HDFS文件上传测试" class="headerlink" title="4. HDFS文件上传测试"></a>4. HDFS文件上传测试</h2><p>HDFS 运行起来之后，可将其视作一个文件系统。此处进行文件上传的测试，首先需要按照目录层级逐个创建目录，并尝试将 Linux 系统中的一些文件上传到 HDFS 中。</p><pre><code class="lang-sh">cd ~hdfs dfs -mkdir /userhdfs dfs -mkdir /user/hadoop</code></pre><p>如果需要查看创建好的文件夹，可以使用如下命令：</p><pre><code class="lang-sh">hdfs dfs -ls /user</code></pre><p>目录创建成功之后，使用 <code>hdfs dfs -put</code> 命令将本地磁盘上的文件（此处是随意选取的 Hadoop 配置文件）上传到 HDFS 之中。</p><pre><code class="lang-sh">hdfs dfs -put /opt/hadoop-2.6.1/etc/hadoop /user/hadoop/input</code></pre><p>如果要查看上传的文件，可以执行如下命令：</p><pre><code class="lang-sh">hdfs dfs -ls /user/hadoop/input</code></pre><h1 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h1><p>WordCount 是 Hadoop 的 “HelloWorld” 程序。</p><p>绝大多数部署在实际生产环境并且解决实际问题的 Hadoop 应用程序都是基于 WordCount 所代表的 MapReduce 编程模型变化而来。</p><p>在终端中首先启动 YARN 计算服务：</p><pre><code class="lang-sh">start-yarn.sh</code></pre><p>然后输入以下命令以启动任务</p><pre><code class="lang-sh">hadoop jar /opt/hadoop-2.6.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.1.jar wordcount /user/hadoop/input/ output</code></pre><p>上述参数中，关于路径的参数有三个，分别是 jar 包的位置、输入文件的位置和输出结果的存放位置。在填写路径时，应当养成填写绝对路径的习惯。这样做将有利于定位问题和传递工作。</p><p>等待计算完成，然后将 HDFS 上的文件导出到本地目录查看：</p><pre><code class="lang-sh">rm -rf /home/hadoop/outputhdfs dfs -get /user/hadoop/output outputcat output/*</code></pre><p>计算完毕后，如无其他软件需要使用 HDFS 上的文件，则应及时关闭 HDFS 守护进程。</p><p>作为分布式集群和相关计算框架的使用者，应当养成良好的习惯，在每次涉及到集群开启和关闭、软硬件安装和更新的时候，都主动检查相关软硬件的状态。</p><pre><code class="lang-sh">stop-yarn.shstop-dfs.sh</code></pre>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
      <tag>Big Data</tag>
      
      <tag>Distributed System</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】计算二进制中1的个数</title>
    <link href="/2021/05/03/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%AE%A1%E7%AE%97%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0/"/>
    <url>/2021/05/03/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%AE%A1%E7%AE%97%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<p>二进制中1的个数的两种计算方法</p><!--more---><p>第一种解法，<code>n &amp;= (n - 1);</code>这一句消灭了二进制末尾的1。循环次数与二进制中1的个数相同。</p><pre><code class="lang-cpp">int hammingWeight(uint32_t n) &#123;    int count = 0;    while (n != 0) &#123;        n &amp;= (n - 1);        ++count;    &#125;    return count;&#125;</code></pre><p>第二种解法，是把性能挖掘到极致的解法：</p><pre><code class="lang-cpp">size_t hammingWeight(uint64_t V) &#123;    V -= ((V &gt;&gt; 1) &amp; 0x5555555555555555); // 010101010101    V = (V &amp; 0x3333333333333333) + ((V &gt;&gt; 2) &amp; 0x3333333333333333);    return ((V + (V &gt;&gt; 4) &amp; 0xF0F0F0F0F0F0F0F) * 0x101010101010101) &gt;&gt; 56;&#125;</code></pre><p>只依靠位运算，不进行条件判断，方便并行计算。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>leetcode</tag>
      
      <tag>binary</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】不限精度的大整数表示</title>
    <link href="/2021/05/03/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%8D%E9%99%90%E7%B2%BE%E5%BA%A6%E7%9A%84%E6%95%B4%E6%95%B0%E4%B9%98%E6%B3%95/"/>
    <url>/2021/05/03/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%8D%E9%99%90%E7%B2%BE%E5%BA%A6%E7%9A%84%E6%95%B4%E6%95%B0%E4%B9%98%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>两个大整数相乘，如何在不损失精度的前提下计算得到正确的结果？<br><!--more---></p><h1 id="不限精度的整数运算"><a href="#不限精度的整数运算" class="headerlink" title="不限精度的整数运算"></a>不限精度的整数运算</h1><p>前一篇文章我们讨论了两个64位整数的乘法，本篇文章我们讨论高精度数值的加减运算问题。</p><h2 id="大整数存储"><a href="#大整数存储" class="headerlink" title="大整数存储"></a>大整数存储</h2><p>很简单，使用数组即可。注意整数的高位存储在数组的末尾，整数的低位存储在数组的开头。</p><pre><code class="lang-py"># 235813d[0] = 3d[1] = 1d[2] = 8d[3] = 5d[4] = 3d[5] = 2</code></pre><p>这样做的原因是，四则运算一般是从低位向高位计算的，数组则是从0开始遍历的。不过如果数字以字符串存储则不然。比如<code>s=&quot;235813&quot;</code>这个字符串的存储顺序和逻辑顺序恰恰相反。</p><pre><code class="lang-cpp">class BigInt &#123;public:    int d[1000]&#123;&#125;;    int len;    BigInt() &#123;        memset(d, 0, sizeof(d));        len = 0;    &#125;    static BigInt change(std::string str);    static int compare(BigInt a, BigInt b);    static BigInt add(BigInt a, BigInt b);    static BigInt sub(BigInt a, BigInt b);&#125;;</code></pre><p>输入大整数时，一般都以<code>string</code>类型输入，因此需要执行<code>reverse</code>操作，或者直接逆序赋值。</p><h2 id="大整数运算"><a href="#大整数运算" class="headerlink" title="大整数运算"></a>大整数运算</h2><h3 id="加法"><a href="#加法" class="headerlink" title="加法"></a>加法</h3><pre><code class="lang-cpp">BigInt BigInt::add(BigInt a, BigInt b) &#123;    BigInt c;    int carry = 0;    for (int i = 0; i &lt; a.len || i &lt; b.len; ++i) &#123;        int temp = a.d[i] + b.d[i] + carry;        c.d[c.len++] = temp % 10;        carry = temp / 10;    &#125;    if (carry != 0) &#123;        c.d[c.len++] = carry;    &#125;    return c;&#125;</code></pre><h3 id="减法"><a href="#减法" class="headerlink" title="减法"></a>减法</h3><pre><code class="lang-cpp">BigInt BigInt::sub(BigInt a, BigInt b) &#123;    BigInt c;    for (int i = 0; i &lt; a.len || i &lt; b.len; ++i) &#123;        if (a.d[i] &lt; b.d[i]) &#123;            a.d[i+1]--;            a.d[i] += 10; // 借位        &#125;        c.d[c.len++] = a.d[i] - b.d[i];    &#125;    while (c.len - 1 &gt;= 1 &amp;&amp; c.d[c.len - 1] == 0) &#123;        c.len--;    &#125;    return c;&#125;</code></pre><h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><pre><code class="lang-cpp">int BigInt::compare(BigInt a, BigInt b) &#123;    if (a.len &gt; b.len) return 1;    else if (a.len &lt; b.len) return -1;    else &#123;        for (int i = a.len-1; i &gt;= 0; --i) &#123;            if (a.d[i] &gt; b.d[i]) return 1;            else if (a.d[i] &lt; b.d[i]) return -1;        &#125;        return 0;    &#125;&#125;</code></pre><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>算法笔记 5.6 节</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>algorithm</tag>
      
      <tag>leetcode</tag>
      
      <tag>searching</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】在 Clion 中使用 Google Test</title>
    <link href="/2021/05/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9C%A8-Clion-%E4%B8%AD%E4%BD%BF%E7%94%A8-Google-Test/"/>
    <url>/2021/05/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9C%A8-Clion-%E4%B8%AD%E4%BD%BF%E7%94%A8-Google-Test/</url>
    
    <content type="html"><![CDATA[<p>Google Test 是著名的 C++ 单元测试框架。如何在 CLion中使用 Google Test？<br><!--more---></p><h1 id="1-下载和安装"><a href="#1-下载和安装" class="headerlink" title="1. 下载和安装"></a>1. 下载和安装</h1><p>首先去<a href="https://github.com/google/googletest">这个网站</a>下载 google test 最新版本。</p><p><img src="/2021/05/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9C%A8-Clion-%E4%B8%AD%E4%BD%BF%E7%94%A8-Google-Test/readme.png" alt></p><p>笔者写作时， google test 的版本为 1.10。</p><p>然后下载最新版本的压缩文件。</p><p><img src="/2021/05/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9C%A8-Clion-%E4%B8%AD%E4%BD%BF%E7%94%A8-Google-Test/download.png" alt></p><p>解压得到文件夹<code>googletest-release-1.10.0</code>，打开文件夹后，把<code>googletest</code>文件夹复制到你的目标工程目录。</p><p><img src="/2021/05/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9C%A8-Clion-%E4%B8%AD%E4%BD%BF%E7%94%A8-Google-Test/googletestdir.png" alt></p><p><img src="/2021/05/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9C%A8-Clion-%E4%B8%AD%E4%BD%BF%E7%94%A8-Google-Test/putitinproject.png" alt></p><p>还没结束，我们需要修改工程根目录的<code>CMakeLists.txt</code>，以使 google test 生效。注意不要修改<code>googletest/CMakeLists.txt</code>！</p><p>向<code>CMakeLists.txt</code>添加如下代码：</p><pre><code class="lang-sh">add_subdirectory(./googletest)include_directories($&#123;PROJECT_SOURCE_DIR&#125;/src/include ./googletest/include)link_directories($&#123;PROJECT_SOURCE_DIR&#125;/lib $&#123;PROJECT_SOURCE_DIR&#125;/googletest)target_link_libraries($&#123;PROJECT_NAME&#125; gtest)</code></pre><p>如果我们想要使用 google test，就在任意cpp文件添加头<code>#include &quot;gtest/gtest.h&quot;</code></p><h1 id="2-简单的用例"><a href="#2-简单的用例" class="headerlink" title="2. 简单的用例"></a>2. 简单的用例</h1><p>做一个简单的测试，我编写了一个将<code>unsigned long long</code>转为二进制数据的函数<code>ulld_to_b</code>，该函数能将一个无符号64位整数转换为64位长的二进制字符串。</p><pre><code class="lang-cpp">string ulld_to_b(uint64_t i) &#123;    return bitset&lt;64&gt;(i).to_string();&#125;</code></pre><p>现在对其进行测试。测试用例为18，我的预期结果为10010，但是前面应该补零至64位长：</p><pre><code class="lang-cpp">TEST(TestCase, test1) &#123;    EXPECT_STREQ(&quot;10010&quot;, ulld_to_b(18ULL).c_str());&#125;TEST(TestCase, test2) &#123;    EXPECT_STREQ(&quot;00000000000000000000000000010010&quot;, ulld_to_b(18ULL).c_str());&#125;TEST(TestCase, test3) &#123;    EXPECT_STREQ(&quot;0000000000000000000000000000000000000000000000000000000000010010&quot;, ulld_to_b(18ULL).c_str());&#125;</code></pre><p>写完测试用例后，需要改造下<code>main()</code>使其运行所有测试用例：</p><pre><code class="lang-cpp">int main(int argc, char** argv) &#123;    testing::InitGoogleTest(&amp;argc, argv);    return RUN_ALL_TESTS();&#125;</code></pre><p>预期结果为前两个测试用例不通过，后一个测试用例通过。</p><p>编译运行后结果如下，完全符合我们的预期：</p><p><img src="/2021/05/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9C%A8-Clion-%E4%B8%AD%E4%BD%BF%E7%94%A8-Google-Test/results.png" alt></p><p>当然这里的使用是不恰当的，我们应当尽可能令所有单元测试都通过。</p><p>在CLion的代码编辑器中可以方便地查看出错的测试用例：</p><p><img src="/2021/05/01/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9C%A8-Clion-%E4%B8%AD%E4%BD%BF%E7%94%A8-Google-Test/result_wrong.png" alt></p><h1 id="3-批量开关测试用例"><a href="#3-批量开关测试用例" class="headerlink" title="3. 批量开关测试用例"></a>3. 批量开关测试用例</h1><p>在大型工程中，项目上线前需要关闭所有测试用例，逐个删除未免显得笨拙。这个时候我们可以使用宏定义。</p><p>首先将<code>googletest</code>的头文件放在一个统一的头文件<code>stdafx.h</code>中，然后以<code>#ifdef</code>包裹起来：</p><pre><code class="lang-cpp">#ifndef LEETCODE_2021_STDAFX_H#define LEETCODE_2021_STDAFX_H#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;string&gt;#include &lt;map&gt;#include &lt;set&gt;#include &lt;algorithm&gt;#include &lt;stack&gt;#include &lt;memory&gt;#include &lt;bitset&gt;// 开启测试模式#define DEBUG_MODE#ifdef DEBUG_MODE#include &quot;gtest/gtest.h&quot;#endif#endif //LEETCODE_2021_STDAFX_H</code></pre><p>如果我把<code>#define DEBUG_MODE</code>删除的话，<code>#include &quot;gtest/gtest.h&quot;</code>就不会执行了。</p><p>然后回到每个函数实现处，我们在这里写了很多的测试用例，以及修改了<code>main()</code>函数。同样将它们用<code>#ifdef</code>包裹起来：</p><pre><code class="lang-cpp">#ifdef DEBUG_MODETEST(TestCase, test1) &#123;    EXPECT_STREQ(&quot;10010&quot;, ulld_to_b(18ULL).c_str());&#125;TEST(TestCase, test2) &#123;    EXPECT_STREQ(&quot;00000000000000000000000000010010&quot;, ulld_to_b(18ULL).c_str());&#125;TEST(TestCase, test3) &#123;    EXPECT_STREQ(&quot;0000000000000000000000000000000000000000000000000000000000010010&quot;, ulld_to_b(18ULL).c_str());&#125;#endif</code></pre><p>这些测试用例会随着<code>&quot;stdafx.h&quot;</code>中<code>#define DEBUG_MODE</code>语句的删除而失效。</p><p>最后来看看<code>main()</code>函数如何处理：</p><pre><code class="lang-cpp">#ifndef DEBUG_MODEint main() &#123;    /*...*/    return 0;&#125;#elseint main(int argc, char** argv) &#123;    testing::InitGoogleTest(&amp;argc, argv);    return RUN_ALL_TESTS();&#125;#endif</code></pre><p>如果没定义<code>DEBUG_MODE</code>，则执行正常的<code>main()</code>；否则执行所有测试用例的<code>main(int argc, char** argv)</code>。</p><p>在项目需要上线时，把<code>#define DEBUG_MODE</code>那一行删除即可。</p><h1 id="4-测试用例编写"><a href="#4-测试用例编写" class="headerlink" title="4. 测试用例编写"></a>4. 测试用例编写</h1><h2 id="4-1-EXPECT和ASSERT"><a href="#4-1-EXPECT和ASSERT" class="headerlink" title="4.1 EXPECT和ASSERT"></a>4.1 <code>EXPECT</code>和<code>ASSERT</code></h2><p>google test 使用 TEST 宏声明测试用例。TEST() 有两个参数，<code>TestCaseName</code>，<code>TestName</code>。</p><p>我们之前使用了<code>EXPECT_STREQ</code>这个宏，其含义为验证内部的两个参数为相同的字符串。除此之外，还有很多以EXPECT开头的宏，它们功能各不相同，比如<code>EXPECT_EQ</code>这个宏比较两个数字是否相等。</p><p>EXPECT系列和ASSERT系列的区别是，EXPECT失败后，继续往下执行；ASSERT失败则直接终止程序。</p><p>EXPECT和ASSERT的返回是一个流对象，这意味着我们可以在之后使用<code>&lt;&lt;</code>运算符输出额外信息：</p><pre><code class="lang-cpp">for (int i = 0; i &lt; x.size(); ++i) &#123;    EXPECT_EQ(x[i], y[i]) &lt;&lt; &quot;Vectors x and y differ at index &quot; &lt;&lt; i;&#125;</code></pre><h2 id="4-2-各种不同的宏断言"><a href="#4-2-各种不同的宏断言" class="headerlink" title="4.2 各种不同的宏断言"></a>4.2 各种不同的宏断言</h2><p>本节只列出了<code>ASSERT</code>，实际上每个<code>ASSERT</code>对应一个<code>EXPECT</code>版本。</p><p><strong>编写测试用例时，如果有两个参数，注意把待测函数输出放在后面！前面的参数是Ground Truth，也就是答案。</strong></p><h3 id="布尔值断言，只有一个参数，参数只能为true或者false："><a href="#布尔值断言，只有一个参数，参数只能为true或者false：" class="headerlink" title="布尔值断言，只有一个参数，参数只能为true或者false："></a>布尔值断言，只有一个参数，参数只能为true或者false：</h3><pre><code class="lang-cpp">ASSERT_TRUE(condition)ASSERT_FALSE(condition)</code></pre><h3 id="数值断言："><a href="#数值断言：" class="headerlink" title="数值断言："></a>数值断言：</h3><pre><code class="lang-cpp">ASSERT_EQ(v1, v2) // v1 == v2ASSERT_NE(v1, v2) // v1 != v2ASSERT_LT(v1, v2) // v1 &lt; v2ASSERT_LE(v1, v2) // v1 &lt;= v2ASSERT_GT(v1, v2) // v1 &gt; v2ASSERT_GE(v1, v2) // v1 &gt;= v2</code></pre><h3 id="字符串断言"><a href="#字符串断言" class="headerlink" title="字符串断言"></a>字符串断言</h3><p>这里的参数为C风格字符串，因此当参数为<code>string</code>类型时，你需要调用<code>c_str()</code>方法。</p><pre><code class="lang-cpp">ASSERT_STREQ(s1, s2) // s1和s2内容相同ASSERT_STRNE(s1, s2) // s1和s2内容不同ASSERT_STRCASEEQ(s1, s2) // 忽略大小写，s1和s2内容相同ASSERT_STRCASENE(s1, s2) // 忽略大小写，s1和s2内容不同</code></pre><h3 id="断言返回成功或者失败"><a href="#断言返回成功或者失败" class="headerlink" title="断言返回成功或者失败"></a>断言返回成功或者失败</h3><pre><code class="lang-cpp">TEST(ExplicitTest, Demo)&#123;    ADD_FAILURE() &lt;&lt; &quot;Sorry&quot;; // None Fatal Asserton，继续往下执行。    FAIL(); // Fatal Assertion，不往下执行该案例。    SUCCEED();&#125;</code></pre><h3 id="抛出异常的断言"><a href="#抛出异常的断言" class="headerlink" title="抛出异常的断言"></a>抛出异常的断言</h3><pre><code class="lang-cpp">int Foo(int a, int b)&#123;    if (a == 0 || b == 0)    &#123;        throw &quot;don&#39;t do that&quot;;    &#125;    int c = a % b;    if (c == 0)        return b;    return Foo(b, c);&#125;TEST(FooTest, HandleZeroInput)&#123;    EXPECT_ANY_THROW(Foo(10, 0)); // Foo应当抛出异常    EXPECT_THROW(Foo(0, 5), char*); // Foo应当抛出字符串类型的异常    EXPECT_NO_THROW(Foo(1, 1)); // Foo不应该抛出异常&#125;</code></pre><h3 id="浮点数断言"><a href="#浮点数断言" class="headerlink" title="浮点数断言"></a>浮点数断言</h3><pre><code class="lang-cpp">ASSERT_FLOAT_EQ(exp, act) // 检验两个浮点数是否**几乎**相等ASSERT_DOUBLE_EQ(exp, act) // 同上，只不过精度更高ASSERT_NEAR(exp, act, abs_error) // exp和act之间的差值不会超过abs_error</code></pre><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.cnblogs.com/coderzh/archive/2009/04/06/1426755.html">https://www.cnblogs.com/coderzh/archive/2009/04/06/1426755.html</a></p><p><a href="https://blog.csdn.net/zhizhengguan/article/details/110313265">https://blog.csdn.net/zhizhengguan/article/details/110313265</a></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>clion</tag>
      
      <tag>google test</tag>
      
      <tag>unit test</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】大整数相乘 I</title>
    <link href="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/"/>
    <url>/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/</url>
    
    <content type="html"><![CDATA[<p>两个大整数相乘，如何在不损失精度的前提下计算得到正确的结果？<br><!--more---></p><h1 id="unsigned-long-long-整数之间的乘法"><a href="#unsigned-long-long-整数之间的乘法" class="headerlink" title="unsigned long long 整数之间的乘法"></a><code>unsigned long long</code> 整数之间的乘法</h1><h2 id="1-问题引入"><a href="#1-问题引入" class="headerlink" title="1. 问题引入"></a>1. 问题引入</h2><p>如果我们使用 <code>C++</code> 语言，在 64 位系统上，<code>unsigned long long</code> 类型（或者 <code>uint64_t</code> 类型）的变量能够保存的数值范围是 $\left[0,2^{64}\right)$。任何小于该数值的数字都能被正确存储。</p><script type="math/tex; mode=display">2^{64}-1=18,446,744,073,709,551,615</script><p>这个十进制的整数足足有20位，一般情况下我们是不需要考虑溢出问题的。</p><p>但是如果两个<code>uint64_t</code>的变量要做乘法，就必须考虑溢出问题了。因为两个64位整数相乘，其结果可能达到128位，超过了一个变量能存储的数据的极限大小。</p><p>两个<code>uint64_t</code>变量的乘法该如何正确计算？考虑极端情况，两个<code>uint64_t</code>整数所能表达的最大数字<code>UINT_MAX</code>相乘：</p><script type="math/tex; mode=display">(2^{64}-1)\times (2^{64}-1)=2^{128}-2^{65}+1</script><p>这里面最大的数字是$2^{128}$，因此我们最极端情况下有128位的信息需要保存。</p><h2 id="2-问题简化"><a href="#2-问题简化" class="headerlink" title="2. 问题简化"></a>2. 问题简化</h2><p>首先让我们<strong>简化一下问题</strong>。假设我现在只能用一款古董电脑，它只支持保存四位十进制整数的变量，也就是一个变量的保存范围为$[0,9999]$。如何计算两个四位整数相乘？</p><p>还记得乘法是怎么计算的吗？我们应该都学习过小学数学，通过将数字中的每一个字符看作计算单元，然后利用99乘法表就能够口算出结果了。</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/2021-04-29-08-44-58.png" alt></p><p>或者可以每两个字符看成计算的基本单元，这样就需要计算两位数乘法。</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/2021-04-29-08-50-09.png" alt></p><p>同样可以计算得到正确结果。假设每个方框是计算机的一个变量，后一种计算方式可以保证每个变量中保存的元素大小不超过计算机的保存能力（假设计算机保存能力为2位）。</p><p>回到题目本身。该运算的结果达到7位，超过了我们计算机限制的4位，因此可以使用两个变量保存该结果：<code>lo</code>保存结果的低四位，<code>hi</code>保存结果的高四位。</p><p>另外我们使用四个变量将两个乘数拆分成前后两个部分，<code>a</code>保存第一个操作数的高2位，<code>b</code>保存第一个操作数的低2位,<code>c</code>保存第二个操作数的高2位，<code>d</code>保存第二个操作数的低2位。</p><p>为什么要这么做？因为计算机的存储能力为4位，两个四位数字相乘，结果最多能达到八位，所以我们肯定不能容忍两个四位数字相乘；但是两个两位数字相乘，结果最多达到四位，是可以被当前的计算机所存储的。所以我们需要把四位数字拆分成前后两个部分。</p><p>抽象一下计算过程，以下不考虑进位。</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/2021-04-29-09-07-00.png" alt></p><p>第一步，计算b与d的乘积，得到结果<code>bd</code>和进位k_1：</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/计算b与d的乘积.png" alt></p><p>第二步，计算a与d的乘积，并且加上bd的进位k_1，得到结果ad和进位k_2，注意这个k_2最终会与ac求和：</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/计算a与d的乘积.png" alt></p><p>第三步，计算b与c的乘积，得到结果bc和进位k_3：</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/计算b与c的乘积.png" alt></p><p>第四步，计算a与c的乘积，并且加上bc的进位k_3，得到结果ac和进位k_4，注意这个k_4最终会成为高位hi的最大的部分，因此ac得到的k_4没必要和ac分离开：</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/计算a与c的乘积.png" alt></p><p>第五步，低位结果计算，首先将ad和bc加起来，得到ad+bc以及可能存在的进位k_5；然后将ad+bc向左移位两格，再加上bd，就是低位结果lo。</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/低位结果计算.png" alt></p><p>第六步，高位结果计算，将ac和进位k_5加起来，注意此时的ac包含了进位k_4，并没有分离出去。我们还需要加上计算ad得到的k_2，因此hi=ac+k_2+k_5。</p><p><img src="/2021/04/28/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E4%B8%A4%E6%95%B0%E7%9B%B8%E4%B9%98/高位结果计算.png" alt></p><h2 id="3-示例"><a href="#3-示例" class="headerlink" title="3. 示例"></a>3. 示例</h2><p>举一个具体点的例子：如何利用该电脑正确计算<code>1213*1214</code>？</p><script type="math/tex; mode=display">1213\times 1214=1,472,582</script><pre><code>a = 12b = 13c = 12d = 14</code></pre><ol><li>计算b与d的乘积，得到结果<code>bd</code>和进位k_1：<br>```<br>b <em> d = 13 </em> 14 = 182</li></ol><hr><p>k_1 = 1<br>bd = 82</p><pre><code>2. 计算a与d的乘积，并且加上bd的进位k_1，得到结果ad和进位k_2</code></pre><p>a <em> d + k_1 = 12 </em> 14 + 1 = 169<br>k_2 = 1<br>ad = 69</p><pre><code>3. 计算b与c的乘积，得到结果bc和进位k_3</code></pre><p>b <em> c = 13 </em> 12 = 156<br>k_3 = 1<br>bc = 56</p><pre><code>4. 计算a与c的乘积，并且加上bc的进位k_3，得到结果ac。此时不用分离进位，因此没有k_4。</code></pre><p>a <em> c + k_3 = 12 </em> 12 + 1 = 145<br>ac = 145</p><pre><code>5. 低位结果计算，首先将ad和bc加起来，得到ad+bc以及可能存在的进位k_5；然后将ad+bc向左移位两格，再加上bd，就是低位结果lo</code></pre><p>ad + bc = 125<br>k_5 = 1<br>adbc = 25<br>lo = adbc &lt;&lt; 2 + bd = 2500 + 82 = 2582</p><pre><code>6. 高位结果计算，将ac和进位k_5加起来，注意此时的ac包含了进位k_4，并没有分离出去。我们还需要加上计算ad得到的k_2，因此hi=ac+k_2+k_5</code></pre><p>hi = ac + k_2 + k_5 = 145 + 1 + 1 = 147</p><pre><code>最终结果的高位部分为147，低位部分为2582，与答案1472582相同。## 迁移到64位整数相乘以上我们讨论的都是四位整数相乘的问题，其实64位二进制数相乘也是一样的道理。首先我们将两个64位乘数OP1和OP2各自分成两半32位整数，也就是a/b/c/d，然后按照上述算法计算，即可得到结果的高64位和低64位。算法如下：```cppvoid mul_64_64_to_128(uint64_t op1, uint64_t op2, uint64_t *hi, uint64_t *lo) &#123;    // 取abcd    uint64_t a, b, c, d;    b = (op1 &amp; 0xFFFFFFFF);    a = (op1 &gt;&gt; 32);    d = (op2 &amp; 0xFFFFFFFF);    c = (op2 &gt;&gt; 32);    // bd    uint64_t k_1 = b * d;    uint64_t bd = (k_1 &amp; 0xFFFFFFFF);    k_1 &gt;&gt;= 32;    // ad    uint64_t k_2 = a * d + k_1;    uint64_t ad = (k_2 &amp; 0xFFFFFFFF);    k_2 &gt;&gt;= 32;    // bc    uint64_t k_3 = b * c;    uint64_t bc = (k_3 &amp; 0xFFFFFFFF);    k_3 &gt;&gt;= 32;    // ac    uint64_t ac = a * c + k_3;    // lo    uint64_t k_5 = ad + bc;    uint64_t ad_bc = (k_5 &amp; 0xFFFFFFFF);    k_5 &gt;&gt;= 32;    *lo = (ad_bc &lt;&lt; 32) + bd;    // hi    *hi = ac + k_2 + k_5;&#125;</code></pre><p>验证一下结果：</p><pre><code class="lang-cpp">int main() &#123;    uint64_t op1 = 1e12;    uint64_t op2 = 1e12;    uint64_t hi = 0ULL, lo = 0ULL;    mul_64_64_to_128(op1, op2, &amp;hi, &amp;lo);    cout &lt;&lt; hi &lt;&lt; &quot;\n&quot; &lt;&lt; lo &lt;&lt; endl;    return 0;&#125;&gt;&gt;&gt;542102003764205206896640Process finished with exit code 0</code></pre><p>乍一看结果，<code>1e12*1e12</code>结果怎么不是<code>1e24</code>呢？其实这是由于结果是十进制导致不直观，实际上我们的结果是正确的。我们比较下二进制就可以了。</p><p>首先定义一个函数<code>ulld_to_b</code>，将<code>uint64_t</code>变量转换为64位的二进制字符串：</p><pre><code class="lang-cpp">string ulld_to_b(uint64_t i) &#123;    return bitset&lt;64&gt;(i).to_string();&#125;</code></pre><p>然后查询<code>1e24</code>的二进制表示为：</p><pre><code>00000000000000000000000000000000000000000000000011010011110000100001101111001110110011001110110110100001000000000000000000000000</code></pre><p>该数值是我在<a href="https://www.sojson.com/hexconvert.html">这个网站</a>上查询的。</p><p>然后编写测试用例：</p><pre><code class="lang-cpp">TEST(TestCase, test1) &#123;    uint64_t op1 = 1e12;    uint64_t op2 = 1e12;    uint64_t hi = 0ULL, lo = 0ULL;    mul_64_64_to_128(op1, op2, &amp;hi, &amp;lo);    string res = &quot;&quot;;    res = ulld_to_b(hi) + ulld_to_b(lo);    cout &lt;&lt; res &lt;&lt; endl;    string ground_truth = &quot;00000000000000000000000000000000000000000000000011010011110000100001101111001110110011001110110110100001000000000000000000000000&quot;;    EXPECT_STREQ(ground_truth.c_str(), res.c_str());&#125;int main(int argc, char** argv) &#123;    testing::InitGoogleTest(&amp;argc, argv);    return RUN_ALL_TESTS();&#125;</code></pre><p>没问题，通过了。当然，仅通过一个测试用例不算证明程序的正确性，勤劳的你可以多用几个测试用例试试。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.codeproject.com/Tips/618570/UInt-Multiplication-Squaring">https://www.codeproject.com/Tips/618570/UInt-Multiplication-Squaring</a></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>algorithm</tag>
      
      <tag>leetcode</tag>
      
      <tag>searching</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】Docker综合实践</title>
    <link href="/2021/04/24/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Docker%E7%BB%BC%E5%90%88%E5%AE%9E%E8%B7%B5/"/>
    <url>/2021/04/24/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Docker%E7%BB%BC%E5%90%88%E5%AE%9E%E8%B7%B5/</url>
    
    <content type="html"><![CDATA[<p>Datawhale Docker学习笔记第六篇<br><!--more---></p><p>在没有学习docker之前，部署项目都是直接启动文件，比如java项目就是java –jar xxxx.jar的方式，python项目就是python xxxx.py。如果采用docker的方式去部署这些项目，一般有两种方式，以jar包项目为例</p><h1 id="方式一、挂载部署"><a href="#方式一、挂载部署" class="headerlink" title="方式一、挂载部署"></a>方式一、挂载部署</h1><p>这种方式类似于常规部署，通过数据卷的方式将宿主机的jar包挂载到容器中，然后执行jar包的jdk选择容器中的而非采用本地的。</p><ol><li>将jar包上传到服务器的指定目录，比如/root/docker/jar。</li></ol><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9yobtp2tj30o007njt5.jpg" alt></p><ol><li>通过docker pull openjdk:8命令获取镜像</li></ol><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9yord00xj30o006bju3.jpg" alt></p><ol><li>编写docker-compose.yml文件</li></ol><pre><code class="lang-yaml">version:&#39;3.0&#39;services:  java:    image: docker.io/openjdk    restart:always    container_name: myopenjdk    ports:      - 8080:8001    volumes:      - /root/docker/jar/xxxx.jar:/root:z      - /etc/localtime:/etc/localtime    environment:      - TZ=&quot;Asia/Shanghai&quot;    entrypoint: java -jar /root/xxxx.jar    mynetwork:      ipv4_address: 192.168.1.13networks:  mynetwork:   ipam:     config:      - subnet: 192.168.1.0/24</code></pre><p>参数解释：</p><ul><li><p>build 指定dockerfile所在文件夹的路径 context指定dockerfile文件所在路径 dockerfile指定文件的具体名称</p></li><li><p>container_name 指定容器名称</p></li><li><p>volumes 挂载路径  z是用来设置selinux，或者直接在linux通过命令临时关闭或者永久关闭</p></li><li><p>ports 暴露端口信息</p></li><li><p>networks是用来给容器设置固定的ip</p></li></ul><ol><li>执行命令docker-compose up –d启动jar包, 可以通过docker ps查看容器是否在运行，需要注意的是默认查看所有运行中的容器，如果想查看所有容器，需要添加参数-a</li></ol><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9yp7mo2wj30o0057aay.jpg" alt></p><ol><li><p>注意如果容器启动失败或者状态异常，可以通过docker logs查看日志</p></li><li><p>通过docker inspect myopenjdk查看容器详细信息，可以看到容器ip已经设置成功</p></li></ol><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9ypl8wgdj30o009wabq.jpg" alt></p><ol><li>然后在虚拟机中打开浏览器输入jar包项目的访问地址，就可以看到运行的项目，需要注意访问端口是映射过的端口而非项目实际端口</li></ol><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9yq1vynoj30o00dytaj.jpg" alt></p><h2 id="方式二、构建镜像部署"><a href="#方式二、构建镜像部署" class="headerlink" title="方式二、构建镜像部署"></a>方式二、构建镜像部署</h2><ol><li><p>将jar包上传到服务器的指定目录，比如/root/docker/jar。</p></li><li><p>在该目录下创建Dockerfile文件，通过vim等编辑工具在Dockerfile中编辑以下内容</p></li></ol><pre><code class="lang-dockerfile">FROM java:8MAINTAINER YHFLABEL description=”learn docker”ADD xxx.jarEXPOSE 8001ENTRYPOINT [“java”,”-jar”,”xxxx.jar”]</code></pre><p>参数解释：</p><ul><li><p>FROM java:8 指定所创建镜像的基础镜像</p></li><li><p>MAINTAINER yhf 指定作者为yhf</p></li><li><p>LABEL 为生成的镜像添加元数据标签信息</p></li><li><p>ADD xxxx.jar 添加内容到镜像</p></li><li><p>EXPOSE 8080 声明镜像内服务监听的端口</p></li><li><p>ENTRYPOINT 指定镜像的默认入口命令，支持两种格式ENTRYPOINT[“java”,”-jar”,”xxxx.jar”]；ENTRYPOINT java –jar xxxx.jar。注意每个dokcerfile中只能有一个ENTRYPOINT，如果指定多个只有最后一个生效。</p></li></ul><ol><li><p>Dockerfile构建完成以后可以通过命令docker build构建镜像，然后再运行容器，这里咱们用docker-compose命令直接编排构建镜像和运行容器。</p></li><li><p>编写docker-compose.yml文件</p><p>```yaml<br>version: ‘3’</p></li></ol><p>services:</p><p>  java_2:<br>    restart: always<br>    image: yhfopenjdk:latest<br>    container_name: myopenjdk<br>    ports:</p><pre><code>  - 8080:8001volumes:  - /etc/localtime:/etc/localtimeenvironment:  - TZ=&quot;Asia/Shanghai&quot;entrypoint: java -jar /root/datawhale-admin-1.0.0.jarnetworks:  mynetwork:     ipv4_address: 192.168.1.13</code></pre><p>networks:<br>  mynetwork:<br>   ipam:<br>     config:</p><pre><code>  - subnet: 192.168.1.0/24</code></pre><p> ```</p><p>参数解释同方式一：</p><ol><li>执行docker-compose up –d直接启动基于文件构建的自定义镜像，如果镜像不存在会自动构建，如果已存在那么直接启动。如果想重新构建镜像，则执行docker-compose build。如果想在执行compose文件的时候重构，则执行docker-compose up –d –build。</li></ol><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9yqi1lv0j30o007ign6.jpg" alt></p><p>此使通过dockerfile文件构建的镜像已经创建</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9yqxa7t1j30o005qdi5.jpg" alt></p><p>通过镜像运行的容器已经正常启动，可以通过docker ps查看容器是否在运行，需要注意的是默认查看所有运行中的容器，如果想查看所有容器，需要添加参数-a</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9yra1jg1j30o0013q33.jpg" alt></p><ol><li>在浏览器中输入访问路径可以看到项目已经正常运行</li></ol><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp9yrmnnzzj30o00d975v.jpg" alt></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】Docker Compose</title>
    <link href="/2021/04/21/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Docker-Compose/"/>
    <url>/2021/04/21/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Docker-Compose/</url>
    
    <content type="html"><![CDATA[<p>Datawhale Docker学习笔记第五篇<br><!--more---></p><h1 id="什么是docker-compose"><a href="#什么是docker-compose" class="headerlink" title="什么是docker compose"></a>什么是docker compose</h1><p>要实现一个 Web 项目，除了 Web 服务容器本身，往往还需要再加上后端的数据库服务容器，甚至还包括负载均衡容器等，这些需要多个容器相互配合来完成。</p><p>Docker Compose 允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。</p><p>下面是一个 docker-compose.yml 示例：</p><p><img src="/2021/04/21/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Docker-Compose/2021-04-21-23-54-09.png" alt></p><p>里面多了image、ports、networks等标签。</p><h1 id="如何使用docker-compose"><a href="#如何使用docker-compose" class="headerlink" title="如何使用docker compose"></a>如何使用docker compose</h1><p>在Compose 中有两个重要的概念：</p><p><strong>服务 (service)</strong>：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。</p><p><strong>项目 (project)</strong>：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。</p><p>Compose的默认管理对象是<strong>项目</strong>，也就是通过docker-compose.yml定义的一组服务集合，通过一些命令来对项目中的一组容器进行便捷地生命周期管理。</p><p>下面我们来看一个真实的场景，在该场景下我们是通过Python来写一个能够记录页面访问次数的 web 网站。</p><h2 id="web-应用示例"><a href="#web-应用示例" class="headerlink" title="web 应用示例"></a>web 应用示例</h2><p><strong>app.py</strong></p><pre><code class="lang-py">from flask import Flaskfrom redis import Redisimport osimport socketapp = Flask(__name__)redis = Redis(host=os.environ.get(&#39;REDIS_HOST&#39;, &#39;127.0.0.1&#39;), port=6379)@app.route(&#39;/&#39;)def hello():    redis.incr(&#39;hits&#39;)    return &#39;Hello Container World! I have been seen %s times and my hostname is %s.\n&#39; % (redis.get(&#39;hits&#39;),socket.gethostname())if __name__ == &quot;__main__&quot;:    app.run(host=&quot;0.0.0.0&quot;, port=5000, debug=True)</code></pre><p><strong>Dockerfile</strong></p><pre><code class="lang-docker">FROM python:2.7COPY . /appWORKDIR /appRUN pip install flask redisEXPOSE 5000CMD [ &quot;python&quot;, &quot;app.py&quot; ]</code></pre><p><strong>docker-compose.yml</strong></p><pre><code class="lang-yaml">version: &quot;3&quot;services:  redis:    image: redis  web:    build:      context: .      dockerfile: Dockerfile    ports:      - &quot;5000:5000&quot;    environment:      REDIS_HOST: redis</code></pre><p><strong>运行 compose 项目</strong></p><p><code>$ docker-compose up -d</code></p><p>此时访问本地 5000 端口<a href="http://localhost:5000，每次刷新页面，计数就会加">http://localhost:5000，每次刷新页面，计数就会加</a> 1。</p><h1 id="docker-compose基本使用"><a href="#docker-compose基本使用" class="headerlink" title="docker compose基本使用"></a>docker compose基本使用</h1><h2 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h2><p>在创建好docker-compose.yml文件后，可以通过下面这个命令将文件中定义的容器都启动起来，在docker compose中我们更习惯于将每一个容器叫做service。</p><pre><code>docker-compose up</code></pre><p>命令后会自动接一个默认值-f docker-compose.yml，也就是默认是使用docker-compose.yml文件的。我们也可以给文件起名为docke-test.yml，这样在使用时指定文件名，但是为了符合规范，还是统一为docker-compose.yml。</p><pre><code>docker-compose up -f docer-test.yml</code></pre><p>但是直接通过这种方式的话会直接将启动时的输出打印到终端，所以我们常会加上-d参数。</p><pre><code>docker-compose up -d</code></pre><h2 id="查看服务状态"><a href="#查看服务状态" class="headerlink" title="查看服务状态"></a>查看服务状态</h2><p>接下来可以查看一下我们创建的service状态</p><pre><code>docker-compose ps</code></pre><p>要是想要查看所有service的状态可以使用-a参数：</p><pre><code>docker-compose ps -a</code></pre><h2 id="停止或删除服务"><a href="#停止或删除服务" class="headerlink" title="停止或删除服务"></a>停止或删除服务</h2><p>如何停止已经运行的services呢，可以使用以下两个命令</p><pre><code>docker-compose stopdocker-compose down</code></pre><p>其中stop是直接停止services，而down则会停止并删除创建的service，volume和network。</p><h2 id="进入服务"><a href="#进入服务" class="headerlink" title="进入服务"></a>进入服务</h2><p>有些情况下我们还需要进入容器来执行一些命令，可以通过如下方式进入容器</p><pre><code>docker-compose exec mysql bash</code></pre><p>exec后面接的就是我们要进入具体的service的名字，名字后面就是我们要执行的命令。</p><h2 id="查看服务输出日志"><a href="#查看服务输出日志" class="headerlink" title="查看服务输出日志"></a>查看服务输出日志</h2><p>有些情况下一些服务可能无法正常启动，这时可以使用命令查看日志并定位发生错误的原因</p><pre><code>docker-compose logs</code></pre>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>央行工作论文读后感</title>
    <link href="/2021/04/21/%E5%A4%AE%E8%A1%8C%E5%B7%A5%E4%BD%9C%E8%AE%BA%E6%96%87%E8%AF%BB%E5%90%8E%E6%84%9F/"/>
    <url>/2021/04/21/%E5%A4%AE%E8%A1%8C%E5%B7%A5%E4%BD%9C%E8%AE%BA%E6%96%87%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
    
    <content type="html"><![CDATA[<p>央行（中国人民银行）在2021年3月26日发表了一篇工作论文《关于我国人口转型的认识和应对之策》。论文一经发表便引起了全网的大讨论，新浪微博上的相关点击量达到3.2亿次。然而网络上的消息难免囿于其碎片话叙事风格，使人产生偏见。因此笔者抱着批判性学习的心态，拜读了该篇论文，并试图梳理主要内容，最后就相关观点提出自己的评论。<br><!--more---></p><p>声明：本文并非学术文章，而是代表个人见解的社会评论；针对的也仅限于该篇文章，而不能涵盖文章所涉及的所有相关议题。本文写作目的仅出于学习和交流。本文作者并非研究人口与社会问题的专业人士，因此观点未免存在疏漏和幼稚之处，望读者海涵。</p><p>这篇工作论文的四名作者均是中国人民银行下属研究机构的经济学博士。作者还向当局提出放开计划生育、控制房价、重视储蓄和投资等建议。</p><p>所有的中国人民银行工作论文可在如下网站查看：<br><a href="http://www.pbc.gov.cn/yanjiuju/124427/133100/index.html">http://www.pbc.gov.cn/yanjiuju/124427/133100/index.html</a></p><h1 id="论文原文内容摘录"><a href="#论文原文内容摘录" class="headerlink" title="论文原文内容摘录"></a>论文原文内容摘录</h1><p>以下将《关于我国人口转型的认识和应对之策》简称为“论文”。灰色引用部分和图表为原文内容，其余部分为本人评注。</p><h2 id="一、人口转型的规律"><a href="#一、人口转型的规律" class="headerlink" title="一、人口转型的规律"></a>一、人口转型的规律</h2><p>该部分讨论了经济社会发展和出生率与死亡率的关系问题。</p><blockquote><p>经济社会的发展将导致出生率和死亡率下降，但是死亡率会先于出生率下降，因此论文将人口增长状态分为四个阶段：低增长（I）、加速增长（II）、增长减缓（III）和恢复低增长（IV）。</p></blockquote><p><img src="/2021/04/21/%E5%A4%AE%E8%A1%8C%E5%B7%A5%E4%BD%9C%E8%AE%BA%E6%96%87%E8%AF%BB%E5%90%8E%E6%84%9F/2021-04-21-12-02-48.png" alt></p><blockquote><p>发达经济体已经位于IV阶段，而大多数发展中经济体位于II或III阶段。</p></blockquote><p>什么是人口红利？人口红利是指生育率下降带来的劳动人口占比提升的现象。在人口红利时期，因为抚养子女和老人的压力变小，更多的人口能够从事经济发展有关的工作，因此这个地区相较于其他地区增长快。具体表现为居民储蓄率高、投资高等。</p><p>人口红利是一个地区发展的机会，但是能否利用这个机会，则有赖于社会和政治条件所创造的生产性就业和投资环境。</p><p>劳动适龄人口占多数的时期是短暂的（约50年），这一部分劳动人口终究会老去并成为负担，届时新一代人将成为社会发展的主力。这种人口红利之后的现象称之为人口转型。</p><p>什么是中等收入陷阱？经济水平长期处于中等收入水平的国家，经济社会发展长期停滞在该水平，受限于各种因素而无法跻身富裕国家行列，看起来好像陷入了一个“陷阱”之中。与“中等收入陷阱”类似的概念还有“贫困陷阱”。</p><h2 id="二、国外应对人口转型的经验和教训"><a href="#二、国外应对人口转型的经验和教训" class="headerlink" title="二、国外应对人口转型的经验和教训"></a>二、国外应对人口转型的经验和教训</h2><p>发达经济体早在工业革命时期就已经积累了一定的人口转型方面的经验和教训。具体表现在：</p><blockquote><p>教训：</p><ul><li>没料到会出现人口转型；</li><li>低估了人口对经济增长的作用；</li><li>低估了老龄化和少子化的危害。</li></ul></blockquote><p>前两点不言自明。而对于第三点，论文认为体现在：</p><blockquote><p>1）发达国家其一高估了教育和科技的作用；2）高估了鼓励生育的效果；3）高估了养老保障政策和养老产业的作用。</p><p>经验：</p><ul><li>资本扩张</li><li>移民</li></ul></blockquote><p>论文提到了这两点是十分行之有效的做法。但这就意味着要求国家发展到国际产业链的顶端，要“当大哥”才能获得这份利益。</p><h2 id="三、我们该如何应对人口转型？"><a href="#三、我们该如何应对人口转型？" class="headerlink" title="三、我们该如何应对人口转型？"></a>三、我们该如何应对人口转型？</h2><h3 id="3-1-目前问题很严峻"><a href="#3-1-目前问题很严峻" class="headerlink" title="3.1 目前问题很严峻"></a>3.1 目前问题很严峻</h3><p>论文指出，一方面我们同样面临人口红利即将结束的人口转型问题，</p><p><img src="/2021/04/21/%E5%A4%AE%E8%A1%8C%E5%B7%A5%E4%BD%9C%E8%AE%BA%E6%96%87%E8%AF%BB%E5%90%8E%E6%84%9F/2021-04-22-08-42-49.png" alt></p><p>另一方面我们的问题比发达国家更为严重，我们“未富先老”；而且由于计划生育政策，我们的生育率下降得更快。在可以预见的将来，经济发展停滞，甚至是物价疲软、资产价格通缩、高负债等现象都可能出现。</p><p>选取美国和印度两个典型的国家进行比较，美国由于技术移民和资本扩张，人口结构问题并不是很突出。而印度正处于人口红利初期，具体可看下图：</p><p><img src="/2021/04/21/%E5%A4%AE%E8%A1%8C%E5%B7%A5%E4%BD%9C%E8%AE%BA%E6%96%87%E8%AF%BB%E5%90%8E%E6%84%9F/2019年中印美人口结构比较图.png" alt></p><p>预计2050年，中印美三国的人口结构如下图所示：</p><p><img src="/2021/04/21/%E5%A4%AE%E8%A1%8C%E5%B7%A5%E4%BD%9C%E8%AE%BA%E6%96%87%E8%AF%BB%E5%90%8E%E6%84%9F/2050年预计中印美人口结构比较图.png" alt></p><blockquote><p>如果说过去四十年我国缩小与美国的差距，靠的是廉价的劳动力和巨大的人口红利，那未来三十年要靠什么？这值得我们深思。</p></blockquote><h3 id="3-2-应对方案"><a href="#3-2-应对方案" class="headerlink" title="3.2 应对方案"></a>3.2 应对方案</h3><blockquote><p>用调控经济的思路去应对人口转型，多半事与愿违。所以一定要有提前量，并且搞得多多的，这样才不至于积重难返；一定要坚持，这样才有成效。</p><h4 id="3-2-1-全面放开生育。"><a href="#3-2-1-全面放开生育。" class="headerlink" title="3.2.1 全面放开生育。"></a>3.2.1 全面放开生育。</h4></blockquote><p>这应当是解决人口结构问题的大前提。</p><blockquote><h4 id="3-2-2-重视储蓄。"><a href="#3-2-2-重视储蓄。" class="headerlink" title="3.2.2 重视储蓄。"></a>3.2.2 重视储蓄。</h4></blockquote><p>老龄化会导致储蓄率下降。未雨绸缪不仅适用于个人，而且对于国家而言同样重要。从其仅次于第一点就能看出，储蓄率对解决人口结构问题至关重要。事实上储蓄率是国家和政府主导投资的核心，是经济发展的直接动力源之一。</p><blockquote><h4 id="3-2-3-重视投资。"><a href="#3-2-3-重视投资。" class="headerlink" title="3.2.3 重视投资。"></a>3.2.3 重视投资。</h4></blockquote><p>重视对我国中西部的投资、对亚非拉地区的投资，以及对科技的投资。</p><blockquote><h4 id="3-2-4-推进养老制度从即时支付型向储蓄型转移。"><a href="#3-2-4-推进养老制度从即时支付型向储蓄型转移。" class="headerlink" title="3.2.4 推进养老制度从即时支付型向储蓄型转移。"></a>3.2.4 推进养老制度从即时支付型向储蓄型转移。</h4></blockquote><p>我国的养老保险制度由三大支柱组成。第一支柱是基本养老保险，即人们常说的养老金；第二支柱即企业年金和职业年金；第三支柱包括个人储蓄性养老保险和商业养老保险。第三支柱以个人主导，工作时有一部分钱税前缴纳，退休取的时候再征税。</p><blockquote><p>削弱第一支柱的养老金，增强个人储蓄保险和商保的占比。这两者最大的区别，就是前者是政府兜底，而后者是市场化产品，政府不会补贴。</p></blockquote><p><a href="https://www.163.com/dy/article/G55MIAN2053907LI.html">个人储蓄养老保险制度</a>参考这个网站。</p><p>不仅如此，论文还提到应该早点降低养老金、延长退休年龄，越早改革越好。用通俗的话来说，就是“别指望政府给你养老”。</p><blockquote><p>我国延迟退休可早做的一个原因在于，<strong>我国有社会主义的优越性，集体主义精神更强有利于避免发达国家延迟退休中的社会动荡；老一辈的人更能忍让、更能吃苦。</strong></p></blockquote><p>这一点经不起推敲，而且越读越让人感觉味道不对。</p><p>很喜欢王小波的一句话：“声称‘东方和西方的思考方式不一样’的人，其目的都是在掩饰一些自己也觉得不体面的事情。都是同一个物种，怎么会思想方式不同呢？”</p><p>而且，总不能因为班里同学好欺负，就挑软柿子捏，还说这是“软柿子的思想境界高”吧。</p><p>制定政策如果真的是为人民服务的，就应该接收人民群众的监督、承受一部分人的意见，并耐心回复和解答，这才是人民民主专政。</p><p>综上，笔者认为这一点不如不说。</p><blockquote><h4 id="3-2-5-促进教育和科技进步。"><a href="#3-2-5-促进教育和科技进步。" class="headerlink" title="3.2.5 促进教育和科技进步。"></a>3.2.5 促进教育和科技进步。</h4><p>给创新以兜底的社保；把握创新与监管的平衡；控房价；重视母亲和少儿的教育；重视基础教育；重视理工科教育，<strong>东南亚国家掉入中等收入陷阱原因之一是文科生太多</strong>；</p></blockquote><p><img src="/2021/04/21/%E5%A4%AE%E8%A1%8C%E5%B7%A5%E4%BD%9C%E8%AE%BA%E6%96%87%E8%AF%BB%E5%90%8E%E6%84%9F/控房价原文.png" alt></p><p>笔者非常赞成关于控房价相关的论述。不仅如此，笔者认为控制房价还有助于提升居民储蓄率，进一步提升经济发展水平。</p><p>文科与理科的大讨论，不是一两句话能说明白的。简单讲，在没有明确“文科”和“理科”的定义之前，擅自对文理进行评价，是不妥当的。</p><p>当然，单纯以毕业生就业情况来看，以自动化、电子、机械、计算机等专业为首的理工科专业的确要比以语言、传媒、艺术等专业要方便就业。</p><p>但是就社会贡献来看，文科尤其是社会科学对社会的贡献要不亚于理工科学。毕竟我们国家的国策就是在马克思主义的指导下建立的。因此在概念模糊的时候不宜“捧一踩一”。</p><blockquote><p>老话讲，<strong>爹戳戳一个，娘戳戳一窝</strong>，所以教育好母亲比教育其他人事半功倍。事实上，教育母亲不是一句空话，可以从很多具体的小工作做起。比如，有发达国家就给每个育龄妇女分发孕前、孕中、孕后、育儿知识的小册子（政府组织专家编写，简单易懂，免费提供），而不是让大家去市场上买厚厚的、良莠不齐的、五花八门的生育指南。这样花钱不多，又成效显著。</p></blockquote><p>出自俗语“兵熊熊一个,将熊熊一窝”。</p><p>重视对母亲的教育，其实还是囿于传统文化的固有成见，即女性应该承担更多哺育孩子的任务。</p><p>如若从制度角度平衡两性之间的家庭职责，并令政府承担一部分新生儿抚养职能，比如给男性以产假、设立新生儿的政府统一抚养等，比单纯教育母亲要人性化得多。</p><blockquote><p>教育和科技进步周期长、见效慢、不可控，因此我国当前还是要以人、财、物的储蓄为本，教育和科技为辅。</p><p>不一定要建那么多学校、养那么多老师，可以用互联网技术让优质教育资源共享。</p></blockquote><p>这一点确实值得提倡。因为中国东西部教育资源分配不均现象，可以被互联网技术轻易解决。给贫困山区孩子配备一块显示器，然后去北京四中网校下载相应视频，孩子就能享受一流名师的教育教学了。</p><p>然而教育资源可不仅仅是课堂资源，生理指导、心理干预、学习习惯的形成等都需要老师手把手地进行指导。</p><p>今后老师的职能也许会从单纯的课堂教学，逐渐转化为学习、生活等方面的导师，引导学生们的发展。</p><p>这里减少老师和学校数目的倡议，应当是为了应对今后一段时间内新生儿数目减少所导致的需求降低。互联网技术的发展与老师数目的需求之间并无直接关系。</p><h1 id="简评"><a href="#简评" class="headerlink" title="简评"></a>简评</h1><h2 id="1-文笔"><a href="#1-文笔" class="headerlink" title="1. 文笔"></a>1. 文笔</h2><p>从行文上来看，本片工作论文与其说是一篇论文，不如说是一篇由央行背书的社会评论。从其观点的严谨程度到行文的文笔上都不够“学术”，更加口语化。</p><p>例如在第三章第二小节中的一段关于人口与经济的关系的论述如下所示：</p><ul><li>人口与经济的关系<strong>似乎有点像</strong>物价。过快增长不好，萎缩更糟糕，<strong>也许就像</strong>温和的通胀一样，适度、可持续的人口增长才有利于经济社会发展。</li></ul><p>再比如论文第14页的脚注中，关于日本“失去的二十年”的相关注解：</p><ul><li><strong>有人说</strong>，日本失去的二十年的主要原因是日本泡沫经济，而非人口老龄化。……（即便是上世纪三十年代的大萧条，持续时间即使算满，<strong>也就</strong>十年）。<strong>还有人说</strong>，……。 </li></ul><p>并且“有人说”之后也没有跟随参考文献，不由得让人怀疑该“人”的存在性。如此种种不一而足。</p><h2 id="2-储蓄率如何提升？"><a href="#2-储蓄率如何提升？" class="headerlink" title="2. 储蓄率如何提升？"></a>2. 储蓄率如何提升？</h2><p>本节参考自<a href="https://zhuanlan.zhihu.com/p/95800733">《居民储蓄率与经济的关系》</a>一文。</p><p>论文中提到要“高度警惕和防止储蓄率过快下降的趋势”，并列举了一组数字：我国国民储蓄率从 2010 年的 47.8%，下降到 2018 年的 44.4%，8 年下降了 3.4 个百分点，其中居民储蓄率降幅更大，下降 7.3 个百分点。</p><p>储蓄率下降过快，会使得人口红利期积累下来的财富得不到保存。</p><p>虽然论文在解决方案的第二点就提到了“提升储蓄率”作为解决老龄化问题的方案，但是既没有讨论储蓄率下降的原因，也没有讨论储蓄率如何提升。</p><p>本节将讨论三个问题：</p><ol><li>储蓄率是什么？对经济发展重要吗？</li><li>储蓄率的影响因素有哪些？</li><li>既然储蓄率对国家如此重要，如何提升储蓄率？</li></ol><h3 id="2-1-居民储蓄率和经济之间的关系"><a href="#2-1-居民储蓄率和经济之间的关系" class="headerlink" title="2.1 居民储蓄率和经济之间的关系"></a>2.1 居民储蓄率和经济之间的关系</h3><p>首先需要明确，经济增长的三驾马车是消费、净出口和投资，其中投资依赖于本国储蓄率和外债。</p><p>储蓄率是指用于最终消费后的余额占所有可支配收入的比率。我国的国民储蓄中，居民储蓄是主要部分。</p><p>改革开放以来，中国经济的发展一直依赖于政府主导的大规模投资和大量出口。投资的钱怎么来？银行，银行的钱来自哪里？居民的高储蓄率。</p><p>说白了，你在银行里的存款越多，对政府的投资贡献越大。说不定高铁的一块铁轨、一个地铁站就用了你的一部分存款。</p><p>因此中国的投资大部分依赖于居民储蓄率，储蓄率的下降对经济发展有较大负面影响。</p><h3 id="2-2-老龄化和居民储蓄率之间的关系"><a href="#2-2-老龄化和居民储蓄率之间的关系" class="headerlink" title="2.2 老龄化和居民储蓄率之间的关系"></a>2.2 老龄化和居民储蓄率之间的关系</h3><p>储蓄率和人口结构是有关系的，只不过关系不那么明显。</p><p>人的一生中，消费和储蓄模式会发生变化。</p><p>一个人从事劳动的年份，他们的收入大于消费，由此产生的盈余用于抚养子女以及存入银行。</p><p>而老年人一般将其收入的大部分花在住房和社会服务方面，另外保健和长期护理方面的需求可能会上升，娱乐、交通运输方面的需求可能会下降。</p><p>因此，理论上，当社会中有收入的青壮年逐步减少、没有收入的老人越来越多时，居民的储蓄率就会下降。</p><p>不过储蓄率往往受其他因素影响，因此老龄化与居民储蓄率之间的关系并不是绝对相关的。</p><h3 id="2-3-影响居民储蓄率的其他原因"><a href="#2-3-影响居民储蓄率的其他原因" class="headerlink" title="2.3 影响居民储蓄率的其他原因"></a>2.3 影响居民储蓄率的其他原因</h3><p>虽然老龄化会对居民储蓄率产生一定程度的影响，但还有很多其他因素影响人们的储蓄行为，比如：</p><ul><li>收入水平、资产价值和分配情况</li><li>对未来发展的估计</li><li>税率</li><li>现行养恤金制度</li><li>老年医疗和临终关怀等</li></ul><p>其中居民存款的增长与否与居民收入有很大关系。当经济向好时，居民收入有大幅增长，存款就增大；当经济下行时，居民收入增长较缓，存款增长就放缓。</p><p>既然老龄化和居民收入等原因都会影响居民储蓄率，那么究竟什么是中国居民储蓄率快速下降的主要原因？</p><h3 id="2-4-储蓄率降低的主要原因"><a href="#2-4-储蓄率降低的主要原因" class="headerlink" title="2.4 储蓄率降低的主要原因"></a>2.4 储蓄率降低的主要原因</h3><blockquote><p>“房价太高，再怎么放开生育我都不会生的。”</p></blockquote><p>由于近年教育、医疗、房产等的价格的上升，居民在以上几项当中的消费额增加，致使居民储蓄率下降。特别是房地产在家庭资产配置当中占有非常大的比例，已经大大超过了存款。</p><p>据央行一季度金融统计数据新闻发布会消息，2020年居民杠杆率为 45.7% ，比2019年上升了 7.1% 。</p><p><a href="http://finance.sina.com.cn/zl/china/2021-04-18/zl-ikmxzfmk7496559.shtml">http://finance.sina.com.cn/zl/china/2021-04-18/zl-ikmxzfmk7496559.shtml</a></p><p>居民部门杠杆率增幅较大主要是受房地产市场影响。数据显示，在全部居民债务中，占最大比例的即是居民中长期消费贷款（主要是住房按揭贷款），占到了全部居民贷款的65%。</p><p><img src="/2021/04/21/%E5%A4%AE%E8%A1%8C%E5%B7%A5%E4%BD%9C%E8%AE%BA%E6%96%87%E8%AF%BB%E5%90%8E%E6%84%9F/2021-04-23-13-16-09.png" alt></p><p>让我们再说得明白些。居民买房的首付使用了六个钱包，这六个钱包中或许还包含私人借债，因此实际居民借债率可能远高于这个数字。</p><p>中国居民已经逐渐由资金供给方变为资金的需求方。高房价掏空了年轻人们的钱包，房贷车贷负债累累，谈何储蓄？</p><h3 id="2-5-如何令居民储蓄率上升？"><a href="#2-5-如何令居民储蓄率上升？" class="headerlink" title="2.5 如何令居民储蓄率上升？"></a>2.5 如何令居民储蓄率上升？</h3><p>2.5 和 2.6 参考自<a href="https://www.thepaper.cn/newsDetail_forward_11420531">《从养老负担和储蓄率之间的关系，深入解析中国“老龄化》</a>一文。</p><p>目前我国的储蓄中，有较大的份额是来自政府和企业，这是国民经济分配失衡所造成的。调整分配，使居民的收入份额上升，对缓解养老压力十分关键。同时，调整政府支出结构，增加公共物品供给，使居民用于这类产品的支出降下来，增加他们的净财富，有助于他们提高养老保障能力。</p><p>在高储蓄率下，之所以仍然面临巨大的养老压力，与过高的收入分配差距有关，低收入人群的养老风险较高的，他们需要为养老而储蓄，但是恰恰是他们的收入水平过低，无力储蓄。所以对养老问题并不能从问题本身来谈论。应对措施也应该更具有综合性和整体性，调节收入分配尤其重要。</p><h3 id="2-6-如何提升养老金缴费比率？"><a href="#2-6-如何提升养老金缴费比率？" class="headerlink" title="2.6 如何提升养老金缴费比率？"></a>2.6 如何提升养老金缴费比率？</h3><p>目前中国的养老金缴费率比较低。中国的非正规就业规模较大，这部分就业者主要是中低收入人群，他们之所以不愿意参保，原因在于缴纳养老金并不符合他们的利益最大化目标。</p><p>有两个比较明显的直接原因。</p><p>一是中国近年来的通货膨胀率较高，这就意味着未来的领取的养老金现值也较少；</p><p>二是中国的城市住房价格快速上涨，住房投资的回报率较高，这也促使劳动者更愿意领取较高的工资而不太看重养老金，因为当前领取工资收入，可以尽早积累起足够的资金用于买房或者其他投资，而养老金只能到退休以后才能领取。</p><p>高通胀和资产价格膨胀使人们增强了流动性偏好，而缴纳养老金对个人来说意味着增加了流动性较低的资产，劳动者更愿意选择哪怕是略微高一些的工资，而不是一个较低的工资加上一份养老保险。</p><p>因而在这种情况下，强制缴纳养老保险的规定往往难以得到落实，企业和雇员会努力采取措施规避这种规定。即使政府出台严格的规定，强制企业缴纳养老保险，个人和企业也都有很强的动机来规避。</p><p>当然，社会养老保障的参保率提高缓慢的原因是多方面的，例如缴费在地区间不可携带，高通胀和资产回报较高可能是一个重要原因。因此，治理通胀和维持房价稳定有助于养老体制的完善。</p><h2 id="3-文理之争"><a href="#3-文理之争" class="headerlink" title="3. 文理之争"></a>3. 文理之争</h2><p>“文科生太多”的说法在网络上引发轩然大波。话题“#文科生太多会影响国家发展吗”在中国社交媒体微博上的阅读量超过3.2亿次，大部分网友对此持批评态度。</p><p><img src="/2021/04/21/%E5%A4%AE%E8%A1%8C%E5%B7%A5%E4%BD%9C%E8%AE%BA%E6%96%87%E8%AF%BB%E5%90%8E%E6%84%9F/文科生太多.png" alt></p><p>众所周知，我国理科生与文科生素来不睦，前者蔑视后者没逻辑，后者鄙夷前者没文化。</p><p>不过在网上的论战中，理科生底气更足一些。毕竟社会上一直流传着“学好数理化、走遍天下都不怕”的说法。相对地，文科一直被认为“无用”。</p><p>2020年，一名来自湖南省的高考状元因在大学选择了考古专业，再度在社交媒体引发争论。有网友认为考古专业“没钱途”，规劝她换专业。</p><p>但是人们在讨论文科和理科的时候，是否能够确切说出什么是“文科”，什么是“理科”呢？</p><p>关于文科生和理科生的定义，建议看下<a href="https://matters.news/@xiemeng/%E4%BB%8E%E7%BF%BB%E8%AF%91%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%9A%84%E8%A7%92%E5%BA%A6%E7%9C%8B%E6%96%87%E7%A7%91%E7%94%9F%E4%B8%8E%E7%90%86%E7%A7%91%E7%94%9F%E7%9A%84%E4%BA%92%E7%9B%B8%E9%84%99%E5%A4%B7-bafyreicrdy7y4fnmjktwnsxv6ljmkiq3ga4yqo5lvjzn3ng74fog32a6jm">这篇文章的介绍</a>，相当全面。</p><p>透过现象看本质，现实情况是一些中国大学文科毕业生确实面临“就业难”和“薪资低”的困境。</p><p>加上2019年国务院发布的职业教育改革方案，能够看出国家其实是想解决一些行业人口过剩但高级技工人才短缺的问题。所以问题不在于文理之争，而在于人才分配不均。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>人口结构问题已成为中国未来发展所必须面对的问题。央行论文提出了一揽子解决方法，包括解除生育限制、提升储蓄率、稳房价和鼓励创新等；但也包含一些本人所不理解的观点，如储蓄率与人口结构的关系，关于“吃苦”的论述，以及“爹戳戳一个，娘戳戳一窝”的观点等。笔者就上述部分观点进行了深入调研和思辩。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>胡翠, 许召元. 人口老龄化对储蓄率影响的实证研究[J]. 经济学 (季刊), 2014, 13(4).</p><p>United Nations. Department of Economic and Social Affairs. World Economic and Social Survey 2007: Development in an ageing world[M]. UN, 2007.</p><p>一些参考文章链接已直接在原文列出。</p>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>middle income trap</tag>
      
      <tag>social sciences</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】Docker网络</title>
    <link href="/2021/04/18/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Docker%E7%BD%91%E7%BB%9C/"/>
    <url>/2021/04/18/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Docker%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<p>Datawhale Docker学习笔记第四篇<br><!--more---></p><h1 id="Docker-基础网络介绍"><a href="#Docker-基础网络介绍" class="headerlink" title="Docker 基础网络介绍"></a>Docker 基础网络介绍</h1><h2 id="外部访问容器"><a href="#外部访问容器" class="headerlink" title="外部访问容器"></a>外部访问容器</h2><p>容器中可以运行一些网络应用，要让外部也可以访问这些应用，可以通过-P或-p参数来指定端口映射。</p><p>当使用-P标记时，Docker会随机映射一个端口到内部容器开放的网络端口。 使用docker container ls可以看到，本地主机的 32768 被映射到了容器的 80 端口。此时访问本机的 32768 端口即可访问容器内 NGINX 默认页面。</p><pre><code>$ docker run -d -P nginx:alpine$ docker container ls -lCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                   NAMESfae320d08268        nginx:alpine        &quot;/docker-entrypoint.…&quot;   24 seconds ago      Up 20 seconds       0.0.0.0:32768-&gt;80/tcp   bold_mcnulty</code></pre><p>同样的，可以通过docker logs命令来查看访问记录。</p><pre><code>$ docker logs fa172.17.0.1 - - [25/Aug/2020:08:34:04 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0&quot; &quot;-&quot;</code></pre><p>-p则可以指定要映射的端口，并且，在一个指定端口上只可以绑定一个容器。支持的格式有<code>ip:hostPort:containerPort | ip::containerPort | hostPort:containerPort</code>.</p><h2 id="容器互联"><a href="#容器互联" class="headerlink" title="容器互联"></a>容器互联</h2><p>下面先创建一个新的 Docker网络。</p><pre><code>$ docker network create -d bridge my-net</code></pre><p>-d参数指定Docker网络类型，有bridge overlay,其中overlay网络类型用于Swarm mode，在本小节中你可以忽略它。</p><p>运行一个容器并连接到新建的my-net网络</p><pre><code>$ docker run -it --rm --name busybox1 --network my-net busybox sh</code></pre><p>打开新的终端，再运行一个容器并加入到 my-net网络</p><pre><code>$ docker run -it --rm --name busybox2 --network my-net busybox sh</code></pre><p>再打开一个新的终端查看容器信息</p><pre><code>$ docker container lsCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMESb47060aca56b        busybox             &quot;sh&quot;                11 minutes ago      Up 11 minutes                           busybox28720575823ec        busybox             &quot;sh&quot;                16 minutes ago      Up 16 minutes                           busybox1</code></pre><p>下面通过 ping来证明busybox1容器和busybox2容器建立了互联关系。 在busybox1容器输入以下命令</p><pre><code>/ # ping busybox2PING busybox2 (172.19.0.3): 56 data bytes64 bytes from 172.19.0.3: seq=0 ttl=64 time=0.072 ms64 bytes from 172.19.0.3: seq=1 ttl=64 time=0.118 ms</code></pre><p>用ping来测试连接busybox2容器，它会解析成 172.19.0.3。 同理在busybox2容器执行ping busybox1，也会成功连接到。</p><pre><code>/ # ping busybox1PING busybox1 (172.19.0.2): 56 data bytes64 bytes from 172.19.0.2: seq=0 ttl=64 time=0.064 ms64 bytes from 172.19.0.2: seq=1 ttl=64 time=0.143 ms</code></pre><p>这样，busybox1 容器和 busybox2 容器建立了互联关系。</p><p>Docker Compose 如果你有多个容器之间需要互相连接，推荐使用DockerCompose。</p><h2 id="配置DNS"><a href="#配置DNS" class="headerlink" title="配置DNS"></a>配置DNS</h2><p>如何自定义配置容器的主机名和 DNS 呢？秘诀就是Docker利用虚拟文件来挂载容器的 3个相关配置文件。</p><p>在容器中使用 mount命令可以看到挂载信息：</p><pre><code>$ mount/dev/disk/by-uuid/1fec...ebdf on /etc/hostname type ext4 .../dev/disk/by-uuid/1fec...ebdf on /etc/hosts type ext4 ...tmpfs on /etc/resolv.conf type tmpfs ...</code></pre><p>这种机制可以让宿主主机 DNS 信息发生更新后，所有Docker容器的 DNS 配置通过 /etc/resolv.conf文件立刻得到更新。</p><p>配置全部容器的 DNS ，也可以在 /etc/docker/daemon.json 文件中增加以下内容来设置。</p><pre><code>&#123;  &quot;dns&quot; : [    &quot;114.114.114.114&quot;,    &quot;8.8.8.8&quot;  ]&#125;</code></pre><p>这样每次启动的容器 DNS 自动配置为 114.114.114.114 和8.8.8.8。使用以下命令来证明其已经生效。</p><pre><code>$ docker run -it --rm ubuntu:18.04  cat etc/resolv.confnameserver 114.114.114.114nameserver 8.8.8.8</code></pre><p>如果用户想要手动指定容器的配置，可以在使用docker run命令启动容器时加入如下参数： -h HOSTNAME或者—hostname=HOSTNAME设定容器的主机名，它会被写到容器内的/etc/hostname 和 /etc/hosts。但它在容器外部看不到，既不会在docker container ls中显示，也不会在其他的容器的/etc/hosts看到。</p><p>—dns=IP_ADDRESS添加 DNS 服务器到容器的/etc/resolv.conf中，让容器用这个服务器来解析所有不在 /etc/hosts 中的主机名。</p><p>—dns-search=DOMAIN设定容器的搜索域，当设定搜索域为.example.com时，在搜索一个名为host的主机时，DNS 不仅搜索 host，还会搜索host.example.com。</p><p><strong>注意：</strong>如果在容器启动时没有指定最后两个参数，Docker会默认用主机上的/etc/resolv.conf来配置容器。</p><h1 id="Docker的网络模式"><a href="#Docker的网络模式" class="headerlink" title="Docker的网络模式"></a>Docker的网络模式</h1><p>可以通过docker network ls查看网络，默认创建三种网络。</p><pre><code>[root@localhost ~]# docker network lsNETWORK ID          NAME                DRIVER              SCOPE688d1970f72e        bridge              bridge              local885da101da7d        host                host                localf4f1b3cf1b7f        none                null                local</code></pre><p>常见网络的含义：</p><div class="table-container"><table><thead><tr><th>网络模式</th><th>简介</th></tr></thead><tbody><tr><td>Bridge</td><td>为每一个容器分配、设置 IP 等，并将容器连接到一个 docker0 虚拟网桥，默认为该模式。</td></tr><tr><td>Host</td><td>容器将不会虚拟出自己的网卡，配置自己的 IP 等，而是使用宿主机的 IP 和端口。</td></tr><tr><td>None</td><td>容器有独立的 Network namespace，但并没有对其进行任何网络设置，如分配 veth pair 和网桥连接，IP 等。</td></tr><tr><td>Container</td><td>新创建的容器不会创建自己的网卡和配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。</td></tr></tbody></table></div><h2 id="Bridge-模式"><a href="#Bridge-模式" class="headerlink" title="Bridge 模式"></a>Bridge 模式</h2><p>当Docker进程启动时，会在主机上创建一个名为docker0的虚拟网桥，此主机上启动的Docker容器会连接到这个虚拟网桥上，附加在其上的任何网卡之间都能自动转发数据包。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中。从docker0子网中分配一个 IP 给容器使用，并设置 docker0 的 IP 地址为容器的默认网关。在主机上创建一对虚拟网卡veth pair设备，Docker 将 veth pair 设备的一端放在新创建的容器中，并命名为eth0（容器的网卡），另一端放在主机中，以vethxxx这样类似的名字命名，并将这个网络设备加入到 docker0 网桥中。可以通过brctl show命令查看。</p><h2 id="Host-模式"><a href="#Host-模式" class="headerlink" title="Host 模式"></a>Host 模式</h2><p>host 网络模式需要在创建容器时通过参数 —net host 或者 —network host 指定；<br>采用 host 网络模式的 Docker Container，可以直接使用宿主机的 IP 地址与外界进行通信，若宿主机的 eth0 是一个公有 IP，那么容器也拥有这个公有 IP。同时容器内服务的端口也可以使用宿主机的端口，无需额外进行 NAT 转换；<br>host 网络模式可以让容器共享宿主机网络栈，这样的好处是外部主机与容器直接通信，但是容器的网络缺少隔离性。</p><h2 id="None-模式"><a href="#None-模式" class="headerlink" title="None 模式"></a>None 模式</h2><p>none 网络模式是指禁用网络功能，只有 lo 接口 local 的简写，代表 127.0.0.1，即 localhost 本地环回接口。在创建容器时通过参数 —net none 或者 —network none 指定；<br>none 网络模式即不为 Docker Container 创建任何的网络环境，容器内部就只能使用 loopback 网络设备，不会再有其他的网络资源。可以说 none 模式为 Docke Container 做了极少的网络设定，但是俗话说得好“少即是多”，在没有网络配置的情况下，作为 Docker 开发者，才能在这基础做其他无限多可能的网络定制开发。这也恰巧体现了 Docker 设计理念的开放。</p><h2 id="Container-模式"><a href="#Container-模式" class="headerlink" title="Container 模式"></a>Container 模式</h2><p>Container 网络模式是 Docker 中一种较为特别的网络的模式。在创建容器时通过参数 —net container:已运行的容器名称|ID 或者 —network container:已运行的容器名称|ID 指定；<br>处于这个模式下的 Docker 容器会共享一个网络栈，这样两个容器之间可以使用 localhost 高效快速通信。</p><p>Container 网络模式即新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样两个容器除了网络方面相同之外，其他的如文件系统、进程列表等还是隔离的。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】Docker数据管理、数据卷和挂载主机目录</title>
    <link href="/2021/04/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Docker%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E3%80%81%E6%95%B0%E6%8D%AE%E5%8D%B7%E5%92%8C%E6%8C%82%E8%BD%BD%E4%B8%BB%E6%9C%BA%E7%9B%AE%E5%BD%95/"/>
    <url>/2021/04/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Docker%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E3%80%81%E6%95%B0%E6%8D%AE%E5%8D%B7%E5%92%8C%E6%8C%82%E8%BD%BD%E4%B8%BB%E6%9C%BA%E7%9B%AE%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<p>Datawhale Docker学习笔记第三篇<br><!--more---></p><h1 id="数据卷"><a href="#数据卷" class="headerlink" title="数据卷"></a>数据卷</h1><ul><li>创建数据卷<pre><code>docker volume create datawhale</code></pre>查看所有的数据卷<pre><code>docker volume ls</code></pre></li><li>启动一个挂载数据卷的容器</li></ul><p>在用 docker run 命令的时候，使用 —mount 标记来将数据卷挂载到容器里。在一次 docker run 中可以挂载多个 数据卷。</p><ul><li>查看数据卷的具体信息</li></ul><p>在主机里使用以下命令可以查看 web 容器的信息</p><pre><code>docker inspect web</code></pre><ul><li><p>删除数据卷</p><pre><code>docker volume rm datawhale  #datawhale为卷名</code></pre><p>无主的数据卷可能会占据很多空间，要清理请使用以下命令</p><pre><code>docker volume prune</code></pre><h1 id="挂载主机目录"><a href="#挂载主机目录" class="headerlink" title="挂载主机目录"></a>挂载主机目录</h1></li><li><p>挂载一个主机目录作为数据卷</p><pre><code>docker run -d -P \  --name web \  --mount type=bind,source=/src/webapp,target=/usr/share/nginx/html \  nginx:alpine</code></pre><p>使用 —mount 标记可以指定挂载一个本地主机的目录到容器中去。</p></li><li><p>查看数据卷的具体信息</p></li></ul><p>在主机里使用以下命令可以查看 web 容器的信息</p><pre><code>docker inspect web</code></pre><ul><li>挂载一个本地主机文件作为数据卷</li></ul><p>—mount 标记也可以从主机挂载单个文件到容器中</p><pre><code>docker run --rm -it \   --mount type=bind,source=$HOME/.bash_history,target=/root/.bash_history \   ubuntu:18.04 \   bash</code></pre>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】如何求出任意日期是星期几？</title>
    <link href="/2021/04/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E6%B1%82%E5%87%BA%E4%BB%BB%E6%84%8F%E6%97%A5%E6%9C%9F%E6%98%AF%E6%98%9F%E6%9C%9F%E5%87%A0%EF%BC%9F/"/>
    <url>/2021/04/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E6%B1%82%E5%87%BA%E4%BB%BB%E6%84%8F%E6%97%A5%E6%9C%9F%E6%98%AF%E6%98%9F%E6%9C%9F%E5%87%A0%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<p>已知某年月日，如何得到这一天是星期几？<br>已知两日期的年月日，如何得到这两日期之间相差多少天？<br>如此种种问题，均可以利用蔡勒公式解决。</p><!--more---><h1 id="一、已知日期y年m月d日，如何求解该日期是周几？"><a href="#一、已知日期y年m月d日，如何求解该日期是周几？" class="headerlink" title="一、已知日期y年m月d日，如何求解该日期是周几？"></a>一、已知日期y年m月d日，如何求解该日期是周几？</h1><h2 id="1-问题转化"><a href="#1-问题转化" class="headerlink" title="1. 问题转化"></a>1. 问题转化</h2><p>该问题容易转换为计算1年1月1日与y年m月d日之间的差值。如何计算两个日期之间的差值？</p><p>不难想到，首先求解前y年的天数$w_1$，然后求解从y年1月1日到y年m月1日之间的天数$w_2$，最后求解m月1日到m月d日之间的天数（即为d天），将这三个值相加即可得到结果$w$。</p><p><img src="/2021/04/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E6%B1%82%E5%87%BA%E4%BB%BB%E6%84%8F%E6%97%A5%E6%9C%9F%E6%98%AF%E6%98%9F%E6%9C%9F%E5%87%A0%EF%BC%9F/计算方法.png" alt></p><p>得到天数$w$之后，将值与7取余，$w\mod 7 == 1$ 就是星期一， $w\mod 7 == 2$ 就是星期二，$w\mod 7 == 0$ 就是星期天。</p><h2 id="2-判断闰年"><a href="#2-判断闰年" class="headerlink" title="2. 判断闰年"></a>2. 判断闰年</h2><p>这个问题的难点在于年和月的计算。一年有365天，闰年是366天（2月中多一天）。如何判断y是闰年还是平年？</p><p>按照我们一直以来学习的知识，闰年按照如下方法进行计算：</p><p>普通闰年：公历年份是4的倍数，且不是100的倍数的，为闰年（如2004年、2020年等就是闰年）。</p><p>世纪闰年：公历年份是整百数的，必须是400的倍数才是闰年（如1900年不是闰年，2000年是闰年）。</p><p>则y年对应的天数 $w_1$ 可以这样计算：</p><script type="math/tex; mode=display">w_1 = 365\times y + \frac{y}{4} - \frac{y}{100} + \frac{y}{400}</script><h2 id="3-计算月份对应的天数"><a href="#3-计算月份对应的天数" class="headerlink" title="3. 计算月份对应的天数"></a>3. 计算月份对应的天数</h2><p>每个月的天数是不同的，并且根据当年是平年还是闰年，1月到m月之间的日期也有所不同，不能一概而论。难道我们需要使用各种情况分析，通过嵌套if语句，最后得到一个丑陋但能用的怪物吗？不必！下面两种情况都能在 O(1) 的空间和时间复杂度内计算得到 y年1月1日到y年m月1日之间的天数。</p><h3 id="3-1-平凡解法：储存12个月对应的天数表"><a href="#3-1-平凡解法：储存12个月对应的天数表" class="headerlink" title="3.1 平凡解法：储存12个月对应的天数表"></a>3.1 平凡解法：储存12个月对应的天数表</h3><p>最容易想到的当然是将平年的12个月对应的天数存在数组中，闰年的另外保存12个月的天数。每次计算，直接取对应下标的值即可。</p><pre><code class="lang-python">def compute_date_normal(y, m, d):    isleap = False    if (y % 4 == 0 and y % 100 != 0) or y % 400 == 0:        isleap = True    y = y - 1  # 计算经过了完整的多少年    yd = 365 * y + y // 4 - y // 100 + y // 400    month_arr = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]    leap_month_arr = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]    if isleap:        md = sum(leap_month_arr[:m - 1])    else:        md = sum(month_arr[:m - 1])    # print(yd, md, d)    return yd + md + d</code></pre><h3 id="3-2-蔡勒公式：24个参数-—-gt-3个参数"><a href="#3-2-蔡勒公式：24个参数-—-gt-3个参数" class="headerlink" title="3.2 蔡勒公式：24个参数 —&gt; 3个参数"></a>3.2 蔡勒公式：24个参数 —&gt; 3个参数</h3><p>有没有更优雅、更酷的方法来改进该算法？有的！我们可以使用一个神奇的公式：蔡勒公式。</p><p>克里斯蒂安·蔡勒（Julius Christian Johannes Zeller，1822-1899）是一名德国数学家。蔡勒公式（Zeller’s congruence），是一种计算任何一天是一星期中哪一天的算法，由德国数学家克里斯提安·蔡勒发表。蔡勒公式的特点是不用判断闰年平年，不用判断大小月，直接在O(1)的复杂度下解决日期和星期的转换问题。很神奇是吧？</p><p>具体地，蔡勒公式计算的是从公元1年1月1日到某日期过了多少天。天数$w$的计算方法如下所示：</p><script type="math/tex; mode=display">w = 365\times y + \frac{y}{4} - \frac{y}{100} + \frac{y}{400} + \frac{153\times m-457}{5} + d - 306</script><p>其中y为某一年，m为该年中的某一月，d为该月中的某一天。</p><hr><p>好吧，你肯定会问 153、457和306这三个参数是从哪里来的了。</p><p>不要以为蔡勒公式中的前半部分还是$w_1$的解法，事实上蔡勒公式把y年全年都给算进去了！也就是说，蔡勒公式的前半部分$w_1$多计算了y年m月d日到y年12月31日这段日期。</p><p>接下来计算这段日期的长度。但是蔡勒公式不急着做这件事，而是计算了y年12月31日到y年3月1日之间经过了多少天。后半部分计算月份的分式我们将其命名为$w_2$：</p><script type="math/tex; mode=display">w_2=M_3+M_4+\cdots+M_12=31+30+\cdots+31=306</script><p>上式中$M_n$为n月的天数。</p><hr><p>为什么选择3月1日作为起始呢？因为3月到12月对于平年和闰年来说都是固定的，因此该数值也是固定的，为306天。</p><p>但是如果我们计算1月1日到12月31日的长度的话，就会因为2月的不稳定性而需要分类讨论了。</p><p>由此，只要我们计算的月份不是1月和2月，那么我们不必在乎这一年到底是闰年还是平年。</p><p>那么如果我计算的日期的确是1月或2月的某日怎么办呢？要想避免讨论平年闰年，无论如何都要避过计算2月！</p><p>蔡勒公式采取的方案是，不计算到y年12月31日，而是少计算一年(1.01.01)—(y-1.12.31)，再加上12个月。由此，原来的1月就变成了13月，原来的2月就变成了14月。</p><hr><p>下面将是蔡勒公式中最难以理解的部分，那就是计算从y年3月1日到4月（X月）1日之间经过了多少天这一部分。</p><p>我们首先看一个统计表，这个表列出了3月1日到各个月份的首日（X月1日）之间经过了多少天：</p><div class="table-container"><table><thead><tr><th style="text-align:center">月份</th><th style="text-align:center">该月天数</th><th style="text-align:center">该月首天与3/1的差值</th></tr></thead><tbody><tr><td style="text-align:center">3</td><td style="text-align:center">31</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">30</td><td style="text-align:center">31</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">31</td><td style="text-align:center">61</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">30</td><td style="text-align:center">92</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">31</td><td style="text-align:center">122</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">31</td><td style="text-align:center">153</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">30</td><td style="text-align:center">184</td></tr><tr><td style="text-align:center">10</td><td style="text-align:center">31</td><td style="text-align:center">214</td></tr><tr><td style="text-align:center">11</td><td style="text-align:center">30</td><td style="text-align:center">245</td></tr><tr><td style="text-align:center">12</td><td style="text-align:center">31</td><td style="text-align:center">275</td></tr><tr><td style="text-align:center">13</td><td style="text-align:center">31</td><td style="text-align:center">306</td></tr><tr><td style="text-align:center">14</td><td style="text-align:center">X</td><td style="text-align:center">337</td></tr></tbody></table></div><p>我们想找到一个简单的映射关系，能把月份给换算成该月首天与3月1日之间的差值：$f(m)$。</p><p>细心观察我们可以发现，除了个位之外，该差值是一个等差数列，公差为30天。而个位的变化为1,1,2,2,3,4,4,5,5,6,7.</p><p>存不存在一个线性函数，能够拟合个位的变化呢？答案是有的，那就是：</p><script type="math/tex; mode=display">\lfloor\frac{m\times 3-7}{5}\rfloor</script><p>左右括号是向下取整。其实我现在也没能找到该式与该问题之间的逻辑关系，但是该式恰好能够拟合差值的个位数变化，因此我也就“拿来主义”了。</p><p>至此我们已经得到月份和差值之间的函数关系：</p><script type="math/tex; mode=display">w_3=30\times(m-3)+\lfloor(m\times3-7)/5\rfloor</script><script type="math/tex; mode=display">=⌊(153×m-457)/5⌋</script><p>在公式的最后，将日期d加上，我们就得到了最终的蔡勒公式。</p><p>下面是蔡勒公式的具体实现。可以看到，代码的确短了很多，而且参数也从24个变成了3个。</p><pre><code class="lang-python">def zellers(y, m, d):    if m &lt; 3:        y -= 1        m += 12    return 365*y + y//4 - y//100 + y//400 + (153*m-457)//5 + d – 306def test_zellers():    w = zellers(2021,1,7)    print((w%7+7)%7)</code></pre><h1 id="二、奇思妙想"><a href="#二、奇思妙想" class="headerlink" title="二、奇思妙想"></a>二、奇思妙想</h1><p>知其然，更要知其所以然。我写这篇文章的目的，其实是因为我碰到了一道笔试题，在解题过程中，我无意发现了一些有趣的解法，但是网上的文章又不能解答我的疑问，所以我就想通过自己的调研，解答自己的问题。如果这个问题的回答能帮到你，那就再好不过了。</p><h2 id="1-为什么闰年这样计算"><a href="#1-为什么闰年这样计算" class="headerlink" title="1. 为什么闰年这样计算"></a>1. 为什么闰年这样计算</h2><p>闰年是为了弥补因人为历法规定的年度天数365日和实际公转时间365.25日的差距而设立的。即是，每四年便会累积1日 ，所以四年便会有一次闰年。多出来的一天为2月29日。</p><p>其实闰年的计算方法一直是有争议的，因为一年的公转日纪年误差会随着年份的累计而不断扩大，最终必须以一天的的方式扩展开来。</p><p>“四年一闰，百年不闰，四百年再闰”，这就造成了在一些特殊年份，会出现八年一闰的现象。</p><p>闰年这个东西其实也是舶来品，因为中国传统纪年方式是按照农历纪年的，我们会在一年之中添加一个月，称之为闰月。</p><p>农历作为阴阳历的一种，每月的天数依照月亏而定，一年的时间以12个月为基准，平年比一回归年少约11天。为了合上地球围绕太阳运行周期即回归年，每隔2到3年，增加一个月，增加的这个月为闰月。闰月加到哪个月，以农历历法规则推断，主要依照与农历的二十四节气相符合来确定。在加有闰月的那一年有13个月，历年长度为383至385日，这一年也称为闰年。如2004年猴年的农历中，有两个二月，通常称为前二月和后二月（即闰月）。</p><p>而现在使用公元纪年，则是格里高利历所规定。</p><h2 id="2-公元纪年"><a href="#2-公元纪年" class="headerlink" title="2. 公元纪年"></a>2. 公元纪年</h2><p>公元是公历纪元的简称，是一个被当今国际社会最广泛地使用的纪年标准。其源自于西方国家使用的基督纪年，以当时认定的耶稣出生年为纪年的开始。第一年被称作基督元年（耶稣出生的那一年）。后来，由于西方文化的强势，西历纪元成为世界通用标准。</p><p>1911 年 10 月 10 日辛亥革命后，湖北军政府使用黄帝纪年。次年 （1912年），中华民国临时政府决定采用国际通用的公元历法做为国历，但纪元部分沿用中国独自的纪年传统，称民国纪年。</p><p>1949年9月27日，中国人民政治协商会议第一届全体会议决议，同年10月1日成立的中华人民共和国放弃使用民国纪年，改采用世界通用的公元纪年制度[5]，大陆地区改称“公元”，以昭明其是“国际共同”，避免“西方独用”的歧义。</p><h2 id="2-公元1年1月1日是星期几？"><a href="#2-公元1年1月1日是星期几？" class="headerlink" title="2. 公元1年1月1日是星期几？"></a>2. 公元1年1月1日是星期几？</h2><p>如果你以为你的程序特别厉害了，那我可要考考你了：公元1年1月1日是星期几？</p><p>按照上文分析，应该是周一。但是知乎上的回答指出，这样计算是不对的：</p><p><img src="/2021/04/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E6%B1%82%E5%87%BA%E4%BB%BB%E6%84%8F%E6%97%A5%E6%9C%9F%E6%98%AF%E6%98%9F%E6%9C%9F%E5%87%A0%EF%BC%9F/知乎回答.png" alt></p><p>教皇格里戈八世在1582年2月24日颁布法令，永远抹去了1582年10月5日到1582年10月14日。历史上从来不曾有过这10天。1582年10月4日是星期四，它的第二天是1582年10月15日星期五。因此考虑到这10天的影响再去推算，公元元年1月1日就是星期六了。</p><p>这方面还是华为的手机日历比较厉害：</p><p><img src="/2021/04/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E6%B1%82%E5%87%BA%E4%BB%BB%E6%84%8F%E6%97%A5%E6%9C%9F%E6%98%AF%E6%98%9F%E6%9C%9F%E5%87%A0%EF%BC%9F/华为.png" alt></p><p><img src="/2021/04/16/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%A6%82%E4%BD%95%E6%B1%82%E5%87%BA%E4%BB%BB%E6%84%8F%E6%97%A5%E6%9C%9F%E6%98%AF%E6%98%9F%E6%9C%9F%E5%87%A0%EF%BC%9F/华为2.png" alt></p><p>可以清楚地看到，从1582年10月5日到1582年10月14日的十天被永远抹去了。</p><p>华为，牛！</p><h2 id="变化问题"><a href="#变化问题" class="headerlink" title="变化问题"></a>变化问题</h2><p>假设存在平行世界，在这个世界中一年的每个月都比我们的世界少一天，除此之外该平行世界的平年闰年、大小月都与我们所处的世界一致。如何计算任意日期是该世界的星期几？</p><p>这个时候蔡勒公式就不太好使了，起码你一时半会儿想不出参数如何变化的。既然时间复杂度和空间复杂度都是O(1)，那我们不妨直接用传统的方法来计算：</p><pre><code class="lang-py">def compute_date_shrink(y, m, d):    isleap = False    if (y % 4 == 0 and y % 100 != 0) or y % 400 == 0:        isleap = True    y = y - 1  # 计算经过了完整的多少年    yd = 353 * y + y // 4 - y // 100 + y // 400    month_arr = [30, 27, 30, 29, 30, 29, 30, 30, 29, 30, 29, 30]    leap_month_arr = [30, 28, 30, 29, 30, 29, 30, 30, 29, 30, 29, 30]    if isleap:        md = sum(leap_month_arr[:m - 1])    else:        md = sum(month_arr[:m - 1])    # print(yd, md, d)    return yd + md + d</code></pre><p>其实我自己也实现了蔡勒公式的变种方法，但是解释起来过于复杂。由于虚拟历法没有现实参照，我也不知道我写的程序对不对，只能把程序放在最后，供大家参考。</p><pre><code class="lang-py">def zellers_shrinked(y, m, d):    if m &lt; 3:        y -= 1        m += 12    return 353 * y + y // 4 - y // 100 + y // 400 + (148 * m - 447) // 5 + d - 295</code></pre>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>algorithm</tag>
      
      <tag>zeller</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】Docker镜像与容器</title>
    <link href="/2021/04/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Docker%E9%95%9C%E5%83%8F%E4%B8%8E%E5%AE%B9%E5%99%A8/"/>
    <url>/2021/04/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Docker%E9%95%9C%E5%83%8F%E4%B8%8E%E5%AE%B9%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<p>Datawhale Docker学习笔记第二篇<br><!--more---></p><h1 id="Docker镜像"><a href="#Docker镜像" class="headerlink" title="Docker镜像"></a>Docker镜像</h1><ul><li>获取镜像</li></ul><p>docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签]</p><ul><li>列出镜像</li></ul><p>docker image ls</p><ul><li>删除本地镜像</li></ul><p>docker image rm [选项] &lt;镜像1&gt; [&lt;镜像2&gt; …]</p><ul><li>Dockerfile构建镜像</li></ul><p>如何可以生成 image 文件？如果你要推广自己的软件，势必要自己制作 image 文件。</p><p>这就需要用到 Dockerfile 文件。它是一个文本文件，用来配置 image。Docker 根据 该文件生成二进制的 image 文件。</p><p>在项目的根目录下，新建一个文本文件 Dockerfile</p><p>入下面的内容。</p><p>FROM node:8.4<br>COPY . /app<br>WORKDIR /app<br>RUN npm install —registry=<a href="https://registry.npm.taobao.org">https://registry.npm.taobao.org</a><br>EXPOSE 3000</p><p>上面代码一共五行，含义如下。</p><p>FROM node:8.4：该 image 文件继承官方的 node image，冒号表示标签，这里标签是8.4，即8.4版本的 node。<br>COPY . /app：将当前目录下的所有文件（除了.dockerignore排除的路径），都拷贝进入 image 文件的/app目录。<br>WORKDIR /app：指定接下来的工作路径为/app。<br>RUN npm install：在/app目录下，运行npm install命令安装依赖。注意，安装后所有的依赖，都将打包进入 image 文件。<br>EXPOSE 3000：将容器 3000 端口暴露出来， 允许外部连接这个端口。</p><p>有了 Dockerfile 文件以后，就可以使用docker image build命令创建 image 文件了</p><h1 id="Docker容器"><a href="#Docker容器" class="headerlink" title="Docker容器"></a>Docker容器</h1><p>容器是独立运行的一个或一组应用，以及它们的运行态环境。</p><ul><li>新建并启动容器</li></ul><p>使用 ubuntu 输出一个 “Hello World”，之后终止容器。</p><p>docker run ubuntu:18.04 /bin/echo ‘Hello world’<br>Hello world</p><p>启动一个 bash 终端，允许用户进行交互</p><p>docker run -t -i ubuntu:18.04 /bin/bash<br>root@af8bae53bdd3:/#</p><p>其中，-t 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上， -i 则让容器的标准输入保持打开。</p><p>当利用 docker run 来创建容器时，Docker 在后台运行的标准操作包括：</p><p>检查本地是否存在指定的镜像，不存在就从registry下载<br>利用镜像创建并启动一个容器<br>分配一个文件系统，并在只读的镜像层外面挂载一层可读写层<br>从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去<br>从地址池配置一个 ip 地址给容器<br>执行用户指定的应用程序<br>执行完毕后容器被终止</p><ul><li>启动已终止的容器</li></ul><p>可以利用 docker container start 命令，直接将一个已经终止（exited）的容器启动运行。</p><ul><li>停止容器</li></ul><p>docker stop可以停止运行的容器。理解：容器在docker host中实际上是一个进程，docker stop命令本质上是向该进程发送一个SIGTERM信号。如果想要快速停止容器，可使用docker kill命令，其作用是向容器进程发送SIGKILL信号。</p><p>docker ps 列出容器，默认列出只在运行的容器；加-a可以显示所有的容器</p><ul><li>重启容器</li></ul><p>对于已经处于停止状态的容器，可以通过docker start重新启动。docker start会保留容器的第一次启动时的所有参数。docker restart可以重启容器，其作用就是依次执行docker stop和docker start。</p><ul><li>后台运行容器</li></ul><p>添加 -d 参数来实现后台运行容器。在使用 -d 参数时，容器启动后会进入后台，启动完容器之后会停在host端；某些时候需要进入容器进行操作，包括使用 docker attach 命令或 docker exec 命令</p><p>docker exec 后边可以跟多个参数，这里主要说明 -i -t 参数。</p><p>只用 -i 参数时，由于没有分配伪终端，界面没有我们熟悉的 Linux 命令提示符，但命令执行结果仍然可以返回。</p><p>当 -i -t 参数一起使用时，则可以看到我们熟悉的 Linux 命令提示符。</p><p>attach和exec的区别<br>attach和exec的区别： （1）attach直接进入容器启动命令的终端，不会启动新的进程； （2）exec则是在容器中打开新的终端，并且可以启动新的进程； （3）如果想直接在终端中查看命令的输出，用attach，其他情况使用exec；</p><ul><li>删除容器</li></ul><p>可以使用 docker container rm 来删除一个处于终止状态的容器。</p><p>docker container rm trusting_newton<br>trusting_newton</p><p>如果要删除一个运行中的容器，可以添加 -f 参数。Docker 会发送 SIGKILL 信号给容器。</p><p>用 docker container ls -a 命令可以查看所有已经创建的包括终止状态的容器，如果数量太多要一个个删除可能会很麻烦，用下面的命令可以清理掉所有处于终止状态的容器。</p><p>docker container prune</p><p>批量删除所有已经退出的容器</p><p>docker rm -v $(docker ps -aq -f status=exited)</p><ul><li>导出容器</li></ul><p>如果要导出本地某个容器，可以使用 docker export 命令。</p><p>这样将导出容器快照到本地文件。</p><ul><li>导入容器</li></ul><p>可以使用 docker import 从容器快照文件中再导入为镜像</p><p>$ cat ubuntu.tar | docker import - test/ubuntu:v1.0<br>$ docker image ls<br>REPOSITORY          TAG                 IMAGE ID            CREATED              VIRTUAL SIZE<br>test/ubuntu         v1.0                9d37a6082e97        About a minute ago   171.3 MB</p><p>用户既可以使用 docker load 来导入镜像存储文件到本地镜像库，也可以使用 docker import 来导入一个容器快照到本地镜像库。这两者的区别在于容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入时可以重新指定标签等元数据信息。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】Docker的安装</title>
    <link href="/2021/04/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Docker%E7%9A%84%E5%AE%89%E8%A3%85/"/>
    <url>/2021/04/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Docker%E7%9A%84%E5%AE%89%E8%A3%85/</url>
    
    <content type="html"><![CDATA[<p>Datawhale Docker学习笔记第一篇<br><!--more---></p><p>为了今后的方便，我选择将Docker安装在实验室的电脑上，服务器的操作系统为 Ubuntu 18.04 LTS 。整个安装过程参考这篇文章(<a href="https://vuepress.mirror.docker-practice.com/install/ubuntu)。">https://vuepress.mirror.docker-practice.com/install/ubuntu)。</a></p><p>我曾在一款老旧的笔记本电脑上尝试过安装 Docker ，但是最终失败了，原因是 Docker 不支持 32 位的操作系统。震惊！ Docker竟然不支持 32位操作系统！</p><p>由于我没有安装过老版本，因此不需要执行卸载旧版本的语句。直接执行</p><pre><code class="lang-sh">$ sudo apt-get update$ sudo apt-get install \    apt-transport-https \    ca-certificates \    curl \    gnupg \    lsb-release</code></pre><p>这里还出现了一些小插曲，当我执行完上面的 install 语句后，我与服务器建立的 ssh 连接断掉了，之后我试图重新连接居然提示密码错误。最后我重启虚拟机，修改 /etc/ssh/sshd_config 中的 PermitRootLogin 字段为 yes 解决了该问题。</p><p>添加软件源的 GPG 密钥，下载并安装，这一部分不再赘述。</p><pre><code class="lang-sh">$ curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg$ echo \  &quot;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \  $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null$ sudo apt-get update$ sudo apt-get install docker-ce docker-ce-cli containerd.io</code></pre><p>之后启动 Docker 服务：</p><pre><code class="lang-sh">$ sudo systemctl enable docker$ sudo systemctl start docker</code></pre><p>激动人心的时刻到了，测试下 Docker 是否安装成功，执行一个 Hello World 看看：</p><pre><code class="lang-sh">$ sudo docker run --rm hello-world</code></pre><p><img src="/2021/04/13/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Docker%E7%9A%84%E5%AE%89%E8%A3%85/2021-04-13-00-48-07.png" alt></p><p>大功告成！收工睡觉~</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的2020年终总结</title>
    <link href="/2021/01/25/%E6%88%91%E7%9A%842020%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"/>
    <url>/2021/01/25/%E6%88%91%E7%9A%842020%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>迟来的总结<br><!--more---></p><p>2020年终于过去了。回首2020，我的成绩不多，教训不少；成功不多，失败不少；经验不多，教训不少；目标列得多，完成的少。</p><h1 id="关于此博客"><a href="#关于此博客" class="headerlink" title="关于此博客"></a>关于此博客</h1><p>在2020年初我给自己定下来的目标是发50篇博客。现在看来，到8月底已经62篇，算是超额完成任务。</p><p>不过自八月底开始我进入创作沉寂期，这是因为进入新学期之后，大量的重要又紧急的事情扑面而来，写博客只能无限期推迟。</p><p>但我心中还是有着写作的欲望的。通过写作，我的语言表达能力得到了极大的锻炼，很多知识也在整理博客的过程中进一步地消化和吸收。</p><p>我写博客的初衷是找一个僻静的地方，释放自己的分享欲望，同时不会打扰到我的熟人朋友们。我从来不指望自己写的内容能够被很多人发现和转发。</p><p>一开始了解到博客还是在“面向对象设计”这门课上听黄哥说的，于是我开通了CSDN账号，开通了自己的公众号，但都不了了之。后来FTK介绍github上可以搭建个人博客，我便试着按照教程搭起来了。</p><p>我对此博客的未来愿景是，首先改造博客的排版布局，其次改造博客的知识架构。目前博客给人的第一观感十分美好，但是仅限于第一印象。因为当我一旦想要通过博客查询自己之前的博文时，文章结构混乱的弊病便暴露出来。光是一个IMDB数据集的介绍就写了三篇，每次写作时都忘记了自己上次已经写过了。博客应当首先是用来看的，而不是用来写的。</p><h1 id="关于论文"><a href="#关于论文" class="headerlink" title="关于论文"></a>关于论文</h1><p>论文、论文，我一整年都在忙论文，无论是文献阅读还是组会汇报，我都努力做那个最用功、最认真的那一个人。但是事与愿违，这三年我不但没能产出论文，甚至被逼到连毕业都成问题。</p><p>是什么导致事情演变成现在这个样子？我也很难说。我倾向于从自己找原因，下面是我总结的几个原因：</p><ol><li>自控力差到了极点</li></ol><p>我的自控能力真的差到可怕。疫情期间我隔离在家，父母已经尽可能为我提供了安静的学习环境。然而我在家里的8个月，最常做的一件事就是熬夜。</p><p>为什么熬夜？我们每周四都要进行工作汇报，通常情况下周四当天下午、周五一直到周二，我都不会忙学术，每天不是瞎忙就是看视频、玩游戏。等到周三当天我迫于压力，只能写点东西做做样子，随后迫于实在做不完了，就开始熬夜。</p><p>这八个月里面鬼知道我熬了几次夜？伴随着作息紊乱而来的，是我的情绪和身体都出现了问题。我的工作效率逐渐降低，思维再也没有以前敏捷，简单的问题需要思考很长时间，这就形成了一个恶性循环，导致我的工作效率大大下降，从而逼迫我熬更多的夜。另外很多任务赶不上工期，这直接导致我开学之后工作量激增。</p><p>然而这一切明明是可以避免的。控制自己把该做的事情做好，起码投入到自己该做的事情上，这明明是一个学生、一个人的本分，但我却没有做到。</p><ol><li>不会规划事情的主次和轻重缓急</li></ol><p>我喜欢做计划，但并不喜欢按照计划执行。相反地，我喜欢先做那些突如其来的工作、更有意思的工作，而把重要且紧急的工作抛诸脑后。要知道像是写博客这种事情能够给我带来很多成就感，而我的课题则已经很久没有进展。这也就是为什么学姐劝我“不要把一件事情的战线拉得特别长”，我现在理解了。一鼓作气，再而衰，三而竭。</p><h1 id="关于未来"><a href="#关于未来" class="headerlink" title="关于未来"></a>关于未来</h1><p>2020我经历了比较大的几个事情，一个是姥姥去世，一个是抑郁症，一个是女友分手。我已经振作起来了，接下来我会试着在毕业前冲一冲论文，算是给自己一个交代；同时刷一刷题，在春招的时候找一份好工作！</p><p>2021年的几个硬指标：</p><ol><li><p>博客100篇</p></li><li><p>上传10个B站视频</p></li><li><p>引体向上可以一口气20个，俯卧撑一口气100个</p></li><li><p>发表论文</p></li><li><p>NLP的工作</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】机器学习——支持向量机SVM</title>
    <link href="/2020/12/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/"/>
    <url>/2020/12/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/</url>
    
    <content type="html"><![CDATA[<p>什么是SVM？SVM是如何用于分类的？为什么求解对偶问题？核函数的原理是什么？<br><!--more---></p><h1 id="什么是支持向量机（Support-Vector-Machine-SVM）"><a href="#什么是支持向量机（Support-Vector-Machine-SVM）" class="headerlink" title="什么是支持向量机（Support Vector Machine, SVM）"></a>什么是支持向量机（Support Vector Machine, SVM）</h1><p>支持向量机是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机。通过核技巧，支持向量机可以处理非线性分类问题。</p><p>支持向量机的学习算法是求解凸二次规划的最优化算法。</p><p>由于算法确定超平面时需要使用距离超平面最近的几个训练样本点，这些样本被称为“支持向量”，支持向量机由此得名。</p><p>假设训练样本<br><img src="/2020/12/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/2020-12-29-20-56-41.png" alt></p><p>则超平面可以这样表示<br><img src="/2020/12/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/2020-12-29-20-56-54.png" alt></p><p>这也就意味着<br><img src="/2020/12/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/2020-12-29-20-57-06.png" alt></p><p>选取那些离超平面最近的两个不同类别的数据点，将他们带入超平面方程，发现<br><img src="/2020/12/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/2020-12-29-20-57-21.png" alt><br><img src="/2020/12/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/2020-12-29-20-57-25.png" alt></p><p>则两个点到超平面距离之和为<br><img src="/2020/12/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/2020-12-29-20-57-37.png" alt></p><p>这是由于点到直线的距离公式为<br><img src="/2020/12/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/2020-12-29-20-57-49.png" alt></p><p>这几个令分子为1的数据点，就被称作支持向量。</p><h1 id="支持向量机如何分类"><a href="#支持向量机如何分类" class="headerlink" title="支持向量机如何分类"></a>支持向量机如何分类</h1><p>支持向量机的学习算法的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。当数据线性可分时，存在无数个分离超平面能将实例正确分开。感知机利用误分类最小的策略，而支持向量机则利用间隔最大化求得最优超平面，这时解是唯一的。</p><p>选取那些支持向量，即距离超平面最近的两个样本点，计算它们到超平面的距离2/||w||<br>通过优化2/||w||使之最大化，便可使得边距扩大。为了寻找超平面 $ F(x)=wx+b $ ，支持向量机试图求解这个优化问题：<br><img src="/2020/12/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/2020-12-29-20-59-04.png" alt></p><p>可以转化为<br><img src="/2020/12/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/2020-12-29-20-54-11.png" alt></p><p>高等数学告诉我们，求解带有限制条件的优化问题，拉格朗日乘子法是经常用于考虑的方法。<br>问题转化为：<br><img src="/2020/12/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/2020-12-29-20-59-33.png" alt></p><p>也就是说，将带N个约束条件的二次优化问题，转换为无约束优化问题。<br>这里注意限制条件变成了1-。。。这是因为拉格朗日乘数法要求参数必须大于等于零，限制条件需要相遇等于零。</p><p>由此，问题变成了$\min<em>{w,b}\max</em>{\lambda}\mathcal{L}(w,b,\lambda)$<br>先把$\lambda$视作参数，将$w,b$视作常数，求$\mathcal{L}$的最大值。</p><p>$\min<em>{w,b} \max</em>{\lambda} \mathcal{L}(w,b,\lambda)$和原问题是等价的。</p><h1 id="为什么求解对偶问题"><a href="#为什么求解对偶问题" class="headerlink" title="为什么求解对偶问题"></a>为什么求解对偶问题</h1><p>由于满足KKT条件，进而将原问题转化为对偶问题，即更换求解max和min 的次序。<br><img src="/2020/12/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/2020-12-29-21-11-01.png" alt></p><p>可以看到，原问题的限制条件中包含较为复杂的线性不等式，这是阻碍我们进行优化的问题所在；而其对偶问题不但简化了限制条件，而且包含$\sum{\lambda_iy_i}=0$很容易消去，极大降低了求解难度。</p><h1 id="核函数的原理"><a href="#核函数的原理" class="headerlink" title="核函数的原理"></a>核函数的原理</h1><p>计算两个向量在隐式映射过后的空间中的内积的函数叫做核函数（Kernel Function）<br><img src="/2020/12/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/2020-12-29-21-16-23.png" alt></p><p><img src="/2020/12/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/2020-12-29-21-27-34.png" alt></p><p><img src="/2020/12/29/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/2020-12-29-21-27-49.png" alt></p><p>核函数目的：把原坐标系里线性不可分的数据用Kernel投影到另一个空间，尽量使得数据在新的空间里线性可分。</p><p>核函数的特点：</p><p>1）核函数的引入避免了“维数灾难”，大大减小了计算量。而输入空间的维数n对核函数矩阵无影响，因此，核函数方法可以有效处理高维输入。</p><p>2）无需知道非线性变换函数$\phi$的形式和参数.</p><p>3）核函数的形式和参数的变化会隐式地改变从输入空间到特征空间的映射，进而对特征空间的性质产生影响，最终改变各种核函数方法的性能。</p><p>4）核函数方法可以和不同的算法相结合，形成多种不同的基于核函数技术的方法，且这两部分的设计可以单独进行，并可以为不同的应用选择不同的核函数和算法。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>《机器学习》周志华</p><p>《统计学习方法》李航</p><p>《机器学习基础》梅尔亚·莫里等</p><p>SVM入门（七）为何需要核函数：<a href="http://www.blogjava.net/zhenandaci/archive/2009/03/06/258288.html">http://www.blogjava.net/zhenandaci/archive/2009/03/06/258288.html</a></p><p>如何通俗地讲解对偶问题？尤其是拉格朗日对偶lagrangian duality？知乎回答：<a href="https://www.zhihu.com/question/58584814">https://www.zhihu.com/question/58584814</a></p><p>为什么支持向量机要用拉格朗日对偶算法来解最大化间隔问题？知乎回答：<a href="https://www.zhihu.com/question/36694952">https://www.zhihu.com/question/36694952</a></p><p>简易解说拉格朗日对偶（Lagrange duality）：<a href="https://www.cnblogs.com/90zeng/p/Lagrange_duality.html">https://www.cnblogs.com/90zeng/p/Lagrange_duality.html</a></p><p>拉格朗日对偶性：<a href="https://www.cnblogs.com/nxf-rabbit75/p/11453355.html">https://www.cnblogs.com/nxf-rabbit75/p/11453355.html</a></p><p>机器学习白板推导：<a href="https://www.bilibili.com/video/BV1aE411o7qd">https://www.bilibili.com/video/BV1aE411o7qd</a></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
      <tag>SVM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】Ubuntu 18.04 安装Go的踩坑指南</title>
    <link href="/2020/10/02/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ubuntu-18-04-%E5%AE%89%E8%A3%85Go%E7%9A%84%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97/"/>
    <url>/2020/10/02/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Ubuntu-18-04-%E5%AE%89%E8%A3%85Go%E7%9A%84%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<p>Go安装中遇到的坑<br><!--more---></p><p>gvm是第三方开发的Go多版本管理工具，利用gvm下载和安装go。</p><p>执行以下代码时，你应该确保自己安装有curl</p><pre><code>bash &lt; &lt;(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer)</code></pre><p>在执行上述curl时，我的ubuntu 18.04报错</p><p>curl: (7) Failed to connect to <a href="http://raw.githubusercontent.com/">raw.githubusercontent.com</a> port 443: Connection refused</p><p>经过<a href="[https://github.com/hawtim/blog/issues/10](https://github.com/hawtim/blog/issues/10">这个</a>)帖子的指引，我在hosts中添加了如下几行：</p><p>199.232.68.133 raw.githubusercontent.com</p><p>199.232.68.133 user-images.githubusercontent.com</p><p>199.232.68.133 avatars2.githubusercontent.com</p><p>199.232.68.133 avatars1.githubusercontent.com</p><p>curl便可以正常下载了。</p><p>安装完成gvm后我们就可以安装go了：</p><pre><code>gvm install go1.15.2gvm use go1.15.2</code></pre><p>这个时候出现错误</p><p>zyt@ubuntu:~$ gvm install go1.15.2<br>Installing go1.15.2…</p><ul><li>Compiling…<br>/home/zyt/.gvm/scripts/install: line 84: go: command not found<br>ERROR: Failed to compile. Check the logs at /home/zyt/.gvm/logs/go-go1.15.2-compile.log<br>ERROR: Failed to use installed version</li></ul><p>经查询，Go版本在1.5以上，需要在指令最后加上-B</p><p><a href="https://github.com/moovweb/gvm#a-note-on-compiling-go-15">https://github.com/moovweb/gvm#a-note-on-compiling-go-15</a></p><p>zyt@ubuntu:~$ gvm install go1.15.2 -B<br>Installing go1.15.2 from binary source</p><pre><code>gvm use go1.15.2export GOROOT_BOOTSTRAP=$GOROOTgvm install go1.5zyt@ubuntu:~$ gvm listgvm gos (installed)=&gt; go1.15.2zyt@ubuntu:~$ gvm listallgvm gos (available)</code></pre>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Golang</tag>
      
      <tag>安装</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】leetcode打卡：查找算法2</title>
    <link href="/2020/08/27/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%952/"/>
    <url>/2020/08/27/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%952/</url>
    
    <content type="html"><![CDATA[<p>见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。<br><!--more---></p><h2 id="1-两数之和"><a href="#1-两数之和" class="headerlink" title="1 两数之和"></a>1 两数之和</h2><pre><code class="lang-cpp">class Solution &#123;public:    vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) &#123;        if (nums.empty()) return &#123;&#125;;        unordered_map&lt;int, int&gt; hash;        vector&lt;int&gt; res;        // construct dict        for (int i = 0; i &lt; nums.size(); ++i) &#123;            hash[nums[i]] = i;        &#125;        for (int i = 0; i &lt; nums.size(); ++i) &#123;            int temp = target - nums[i];            auto iter = hash.find(temp);            if (iter != hash.end() &amp;&amp; iter-&gt;second != i) &#123;                res.push_back(i);                res.push_back(iter-&gt;second);                break;            &#125;        &#125;        return res;    &#125;&#125;;</code></pre><p>思想：用hash把待查数组保存起来，这样再次查找的时间就是O(1)了。</p><h2 id="15-三数之和"><a href="#15-三数之和" class="headerlink" title="15 三数之和"></a>15 三数之和</h2><pre><code class="lang-py">class Solution:    def threeSum(self, nums: List[int]) -&gt; List[List[int]]:        res = []        size = len(nums)        if size &lt; 3: return res        nums.sort()        for i in range(size-2):            if i &gt; 0 and nums[i] == nums[i-1]: continue            j = i + 1            k = size - 1            while j &lt; k:                ans = nums[i] + nums[j] + nums[k]                if (ans &gt; 0): k = k - 1                elif (ans &lt; 0): j = j + 1                else:                    res.append([nums[i], nums[j], nums[k]])                    while j &lt; size and nums[j] == nums[j-1]: j += 1                    k -= 1                    while k &gt;= 0 and nums[k] == nums[k+1]: k -= 1        return res</code></pre><p>思想：用三个下标<code>i,j,k</code>遍历所有可能。首先排序，然后不断缩小i、j和k的区间。</p><h2 id="16-最接近的三数之和"><a href="#16-最接近的三数之和" class="headerlink" title="16 最接近的三数之和"></a>16 最接近的三数之和</h2><pre><code class="lang-cpp">class Solution &#123;public:    int threeSumClosest(vector&lt;int&gt;&amp; nums, int target) &#123;        const int size = nums.size();        if (size &lt;= 3) return std::accumulate(nums.begin(), nums.end(), 0); // 0是累加的初值        std::sort(nums.begin(), nums.end());        int result = nums[0] + nums[1] + nums[2]; // 初值        for (int i = 0; i &lt; size - 2; ++i) &#123;            int j = i + 1;            int k = size - 1;            while (j &lt; k) &#123;                int temp = nums[i] + nums[j] + nums[k];                if (std::abs(target - temp) &lt; std::abs(target - result)) &#123;                    result = temp;                &#125;                if (result == target) &#123; // 直接找到了                    return result;                &#125;                if (temp &gt; target) &#123;                    --k; // temp太大，需要缩小右边界                &#125; else &#123;                    ++j; // temp太小，需要缩小左边界                &#125;            &#125;        &#125;        return result;    &#125;&#125;;</code></pre><p>思路：还是利用三个下标<code>i,j,k</code>遍历全部数组。中途不断保存和target最近的temp值。</p><h2 id="18-四数之和"><a href="#18-四数之和" class="headerlink" title="18 四数之和"></a>18 四数之和</h2><pre><code class="lang-cpp">class Solution &#123;public:    vector&lt;vector&lt;int&gt;&gt; fourSum(vector&lt;int&gt;&amp; nums, int target) &#123;        sort(nums.begin(),nums.end());        vector&lt;vector&lt;int&gt; &gt; res;        if(nums.size()&lt;4) return res;        int a,b,c,d,_size=nums.size();        for(a=0;a&lt;_size-3;a++)&#123;            if(a&gt;0&amp;&amp;nums[a]==nums[a-1]) continue;      //确保nums[a] 改变了            for(b=a+1;b&lt;_size-2;b++)&#123;                if(b&gt;a+1&amp;&amp;nums[b]==nums[b-1])continue;   //确保nums[b] 改变了                c=b+1,d=_size-1;                while(c&lt;d)&#123;                    if(nums[a]+nums[b]+nums[c]+nums[d]&lt;target)                        c++;                    else if(nums[a]+nums[b]+nums[c]+nums[d]&gt;target)                        d--;                    else&#123;                        res.push_back(&#123;nums[a],nums[b],nums[c],nums[d]&#125;);                        while(c&lt;d&amp;&amp;nums[c+1]==nums[c])      //确保nums[c] 改变了                            c++;                        while(c&lt;d&amp;&amp;nums[d-1]==nums[d])      //确保nums[d] 改变了                            d--;                        c++;                        d--;                    &#125;                &#125;            &#125;        &#125;        return res;    &#125;&#125;;</code></pre><p>思路：四指针。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>algorithm</tag>
      
      <tag>leetcode</tag>
      
      <tag>Datawhale</tag>
      
      <tag>search</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】数据挖掘实践——总结与反思</title>
    <link href="/2020/08/25/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%80%BB%E7%BB%93%E4%B8%8E%E5%8F%8D%E6%80%9D/"/>
    <url>/2020/08/25/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%80%BB%E7%BB%93%E4%B8%8E%E5%8F%8D%E6%80%9D/</url>
    
    <content type="html"><![CDATA[<!--more---><p>这次组队学习确实是自己不太擅长的领域。借着上次NLP组队学习的冲劲，本以为想在不熟悉的领域也能至少学点东西，但确实是小看了这次学习的难度。</p><p>回头来看，组队学习进入到特征工程的时候，我已经完全一脸懵逼了。我很想跟上大家的节奏，但是确实一个是没有时间，另一个是差太多了，已经进入了恐慌区，脑海里也是拒绝的心态。我知道这次组队学习已经彻底失败了。</p><p>唉！内心充满了挫败感。这次组队学习唯一的收获就是，获得了这次比赛的Baseline。自此之后我就要投入到秋招了，我曾参加过CV赛事、NLP赛事，再加上这个结构化赛事，我的比赛经历算是圆满了，接下来的日子就是研究Baseline、好好准备简历了，也该收收心了。</p><p>祝自己好运。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Datawhale</tag>
      
      <tag>Data Mining</tag>
      
      <tag>Time Series Analysis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】数据挖掘实践——建模预测</title>
    <link href="/2020/08/24/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E5%BB%BA%E6%A8%A1%E9%A2%84%E6%B5%8B/"/>
    <url>/2020/08/24/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E5%BB%BA%E6%A8%A1%E9%A2%84%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<p>我也是加把劲骑士！<br><!--more---></p><pre><code class="lang-py">import matplotlib.pyplot as pltimport pandas as pdimport numpy as npfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dense, Add, Dropout, Flatten, TimeDistributedfrom tensorflow.keras.models import Model, Sequentialfrom tensorflow.keras import activations, optimizers, regularizersfrom tensorflow.keras.callbacks import EarlyStoppingimport tensorflow.keras.backend as kbfrom tensorflow import kerasfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.metrics import mean_squared_error, mean_absolute_errorfrom time import *user_balance = pd.read_csv(&#39;Purchase Redemption Data/user_balance_table.csv&#39;)df_tmp = user_balance.groupby([&#39;report_date&#39;])[&#39;total_purchase_amt&#39;, &#39;total_redeem_amt&#39;].sum()df_tmp.index = pd.to_datetime(df_tmp.index, format=&#39;%Y%m%d&#39;)holidays = (&#39;20130813&#39;, &#39;20130902&#39;, &#39;20131001&#39;, &#39;20131111&#39;, &#39;20130919&#39;, &#39;20131225&#39;, &#39;20140101&#39;, &#39;20140130&#39;, &#39;20140131&#39;,           &#39;20140214&#39;, &#39;20140405&#39;, &#39;20140501&#39;, &#39;20140602&#39;, &#39;20140802&#39;, &#39;20140901&#39;, &#39;20140908&#39;)def create_features(timeindex):    n = len(timeindex)    features = np.zeros((n, 4))    features[:, 0] = timeindex.day.values/31    features[:, 1] = timeindex.month.values/12    features[:, 2] = timeindex.weekday.values/6    for i in range(n):        if timeindex[i].strftime(&#39;%Y%m%d&#39;) in holidays:            features[i, 3] = 1    return featuresfeatures = create_features(df_tmp.index)september = pd.to_datetime([&#39;201409%02d&#39; % i for i in range(1, 31)])features_sep = create_features(september)scaler_pur = MinMaxScaler()scaler_red = MinMaxScaler()data_pur = scaler_pur.fit_transform(df_tmp.values[:, 0:1])data_red = scaler_red.fit_transform(df_tmp.values[:, 1:2])def create_dataset(data, back, forward=30):    n_samples = len(data) - back - forward + 1    X, Y = np.zeros((n_samples, back, data.shape[-1])), np.zeros((n_samples, forward, data.shape[-1]))    for i in range(n_samples):        X[i, ...] = data[i:i+back, :]        Y[i, ...] = data[i+back:i+back+forward, :]    return X, Ydef build_cnn(X_trn, lr, n_outputs, dropout_rate):    inputs = Input(X_trn.shape[1:])    z = Conv1D(64, 14, padding=&#39;valid&#39;, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)(inputs)#     z = MaxPooling1D(2)(z)    z = Conv1D(128, 7, padding=&#39;valid&#39;, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)(z)    z = MaxPooling1D(2)(z)    z = Conv1D(256, 3, padding=&#39;valid&#39;, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)(z)    z = Conv1D(256, 3, padding=&#39;valid&#39;, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)(z)    z = MaxPooling1D(2)(z)    z = Flatten()(z)    z = Dropout(dropout_rate)(z)    z = Dense(128, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)(z)    z = Dropout(dropout_rate)(z)    z = Dense(84, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)(z)    outputs = Dense(n_outputs)(z)    model = Model(inputs=inputs, outputs=outputs)    adam = optimizers.Adam(lr=lr)    model.compile(loss=&#39;mse&#39;, optimizer=adam, metrics=[&#39;mae&#39;])    model.summary()    return modelback = 60forward = 30X_pur_data, Y_pur_data = create_dataset(data_pur, back, forward)X_red_data, Y_red_data = create_dataset(data_red, back, forward)X_features, Y_features = create_dataset(features, back, forward)Y_features = np.concatenate((Y_features, np.zeros((Y_features.shape[0], back-forward, Y_features.shape[-1]))), axis=1)# X_pur, X_red = np.concatenate((X_pur_data, X_features, Y_features), axis=-1), np.concatenate((X_red_data, X_features, Y_features), axis=-1)# X_pur_trn, X_pur_val, X_red_trn, X_red_val = X_pur[:-forward, ...], X_pur[-1:, ...], X_red[:-forward, ...], X_red[-1:, ...]# Y_pur_trn, Y_pur_val, Y_red_trn, Y_red_val = Y_pur_data[:-forward, ...], Y_pur_data[-1:, ...], Y_red_data[:-forward, ...], Y_red_data[-1:, ...]Y_fea_sep = np.concatenate((features_sep, np.zeros((back-forward, features_sep.shape[-1]))), axis=0)# X_pur_tst = np.concatenate((data_pur[-back:, :], features[-back:, :], Y_fea_sep), axis=-1)[None, ...]# X_red_tst = np.concatenate((data_red[-back:, :], features[-back:, :], Y_fea_sep), axis=-1)[None, ...]X = np.concatenate((X_pur_data, X_red_data, X_features, Y_features), axis=-1)Y = np.concatenate((Y_pur_data, Y_red_data), axis=1)X_trn, X_val, Y_trn, Y_val = X[:-forward, ...], X[-1:, ...], Y[:-forward, ...], Y[-1:, ...]X_tst = np.concatenate((data_pur[-back:, :], data_red[-back:, :], features[-back:, :], Y_fea_sep), axis=-1)[None, ...]cnn_pur = build_cnn(X_trn, lr=0.0008, n_outputs=2*forward, dropout_rate=0.5)history = cnn_pur.fit(X_trn, Y_trn, batch_size=32, epochs=1000, verbose=2,                       validation_data=(X_val, Y_val),                     callbacks=[EarlyStopping(monitor=&#39;val_mae&#39;, patience=200, restore_best_weights=True)])plt.figure(figsize=(8, 5))plt.plot(history.history[&#39;mae&#39;], label=&#39;train mae&#39;)plt.plot(history.history[&#39;val_mae&#39;], label=&#39;validation mae&#39;)plt.ylim([0, 0.2])plt.legend()plt.show()def plot_prediction(y_pred, y_true):    plt.figure(figsize=(16,4))    plt.plot(np.squeeze(y_pred), label=&#39;prediction&#39;)    plt.plot(np.squeeze(y_true), label=&#39;true&#39;)    plt.legend()    plt.show()    print(&#39;MAE: %.3f&#39; % mean_absolute_error(np.squeeze(y_pred), np.squeeze(y_true)))pred = cnn.predict(X_val)plot_prediction(pred, Y_val)history = cnn.fit(X, Y, batch_size=32, epochs=500, verbose=2,                     callbacks=[EarlyStopping(monitor=&#39;mae&#39;, patience=30, restore_best_weights=True)])plt.figure(figsize=(8, 5))plt.plot(history.history[&#39;mae&#39;], label=&#39;train mae&#39;)plt.legend()plt.show()print(cnn.evaluate(X, Y, verbose=2))pred_tst = cnn.predict(X_tst)pur_sep = scaler_pur.inverse_transform(pred_tst[:, :forward].transpose())red_sep = scaler_red.inverse_transform(pred_tst[:, forward:].transpose())test_user = pd.DataFrame(&#123;&#39;report_date&#39;: [20140900 + i for i in range(1, 31)]&#125;)test_user[&#39;pur&#39;] = pur_sep.astype(&#39;int&#39;)test_user[&#39;red&#39;] = red_sep.astype(&#39;int&#39;)test_user.to_csv(&#39;submission.csv&#39;, encoding=&#39;utf-8&#39;, index=None, header=None)from google.colab import filesfiles.download(&quot;submission.csv&quot;)</code></pre>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Datawhale</tag>
      
      <tag>Data Mining</tag>
      
      <tag>Time Series Analysis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】数据挖掘实践——特征工程</title>
    <link href="/2020/08/24/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    <url>/2020/08/24/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<p>我也是加把劲骑士！<br><!--more---></p><p>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。由此可见，特征工程在机器学习中占有相当重要的地位。在实际应用当中，可以说特征工程是机器学习成功的关键。</p><p>那特征工程是什么？</p><p>特征工程是利用数据领域的相关知识来创建能够使机器学习算法达到最佳性能的特征的过程。</p><p>特征工程又包含了 Data PreProcessing（数据预处理）、Feature Extraction（特征提取）、Feature Selection（特征选择）和 Feature construction（特征构造）等子问题，本章内容主要讨论特征构造的方法。</p><p>创造新的特征是一件十分困难的事情，需要丰富的专业知识和大量的时间。机器学习应用的本质基本上就是特征工程。<br>——Andrew Ng</p><p>对于时间型数据来说，即可以把它转换成连续值，也可以转换成离散值。</p><p>1.连续值时间特征<br>持续时间（单页浏览时长）；间隔时间；上次购买/点击离现在的时长；产品上线到现在经过的时长；2.离散值时间特征<br>1）时间特征拆解</p><p>年；月；日；时；分；数；一天中的第几分钟；星期几；一年中的第几天；一年中的第几个周；一天中哪个时间段：凌晨、早晨、上午、中午、下午、傍晚、晚上、深夜；一年中的哪个季度；</p><p>2）时间特征判断<br>是否闰年；是否月初；是否月末；是否季节初；是否季节末；是否年初；是否年尾；是否周末；是否公共假期；是否营业时间；两个时间间隔之间是否包含节假日/特殊日期；</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Datawhale</tag>
      
      <tag>Data Mining</tag>
      
      <tag>Time Series Analysis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】leetcode打卡：查找算法</title>
    <link href="/2020/08/24/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/"/>
    <url>/2020/08/24/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。<br><!--more---></p><h2 id="leetcode-35-搜索插入位置"><a href="#leetcode-35-搜索插入位置" class="headerlink" title="leetcode 35 搜索插入位置"></a>leetcode 35 搜索插入位置</h2><p>给定一个无重复元素的排序数组和一个目标值，在数组中找到目标值，并返回其索引。如果目标值不存在于数组中，返回它将会被按顺序插入的位置。</p><p>思路：简单的二分搜索。注意边界条件。注意初始化条件是<code>L = 0, R = nums.size()</code>。</p><p>代码：</p><pre><code class="lang-cpp">class Solution &#123;public:    int searchInsert(vector&lt;int&gt;&amp; nums, int target) &#123;        int L = 0, R = nums.size();        if (nums.empty() || target &lt; nums[L]) return 0;        if (nums[R-1] &lt; target) return R;        while (L &lt;= R) &#123;            int i = (L + R) / 2;            if (nums[i] &lt; target) &#123;                L = i + 1;            &#125; else if (target &lt; nums[i]) &#123;                R = i - 1;            &#125; else if (target == nums[i])&#123;                return i;            &#125; //else         &#125;        return L;    &#125;&#125;;</code></pre><h2 id="leetcode-202-快乐数"><a href="#leetcode-202-快乐数" class="headerlink" title="leetcode 202 快乐数"></a>leetcode 202 快乐数</h2><p>先分享一下直观的解法</p><p>检查一个数是快乐数，就不断执行<code>n=compute(n)</code>这一步，然后检查n是否为1就行了。但是一旦一个数不是快乐数，则必定是陷于某个数字循环中。比如2这个非快乐数，它的计算过程如下：</p><pre><code>241637588914542204 &lt;- 注意这里的4已经出现过</code></pre><p>我的思路很简单。只需将出现过的n都保存在一个字典中，如果新计算的n已经存在于字典中了，那就意味着陷入了计算循环，非快乐数。</p><pre><code class="lang-cpp">class Solution &#123;public:    bool isHappy(int n) &#123;        unordered_map&lt;int, int&gt; hash;        while (n != 1) &#123;            n = compute(n);            auto iter = hash.find(n);            if (iter != hash.end()) return false;            ++hash[n];        &#125;        return true;    &#125;    int compute(int n) &#123;        int res = 0, bit = 0;        while (n) &#123;            bit = n % 10;            n = n / 10;            res += bit * bit;         &#125;        return res;    &#125;&#125;;</code></pre><p>这个问题还可以转化为检测链表是否存在环路的问题。就可以使用快慢指针法。<code>compute</code>函数不变，只需把主函数部分变成：</p><pre><code class="lang-cpp">class Solution &#123;public:    bool isHappy(int n) &#123;        int slow = n;        int fast = compute(n);        while (slow != fast &amp;&amp; fast != 1) &#123;            slow = compute(slow);            fast = compute(compute(fast));        &#125;        return fast == 1;    &#125;    int compute(int n) &#123;        int res = 0, bit = 0;        while (n) &#123;            bit = n % 10;            n = n / 10;            res += bit * bit;         &#125;        return res;    &#125;&#125;;</code></pre><h2 id="leetcode-205-同构字符串"><a href="#leetcode-205-同构字符串" class="headerlink" title="leetcode 205 同构字符串"></a>leetcode 205 同构字符串</h2><p>将两个字符串翻译为数字，最后比较数字是否相同即可。</p><pre><code class="lang-cpp">class Solution &#123;public:    bool isIsomorphic(string s, string t) &#123;        s = translate(s);        t = translate(t);        return s == t;    &#125;    string translate(string s) &#123;        int count = 0;        unordered_map&lt;char, int&gt; hash;        string res = &quot;&quot;;        for (auto c : s) &#123;            auto iter = hash.find(c);            if (iter != hash.end()) res += std::to_string(iter-&gt;second);            else hash[c] = count++;        &#125;        return res;    &#125;&#125;;</code></pre><h2 id="leetcode-242-有效的字母异位词"><a href="#leetcode-242-有效的字母异位词" class="headerlink" title="leetcode 242 有效的字母异位词"></a>leetcode 242 有效的字母异位词</h2><p>总体思路还是哈希表，保存两个字符串出现的字符类别和次数，如若相等则true。</p><p>可以进一步优化，即使用一个哈希表，遍历s的时候构建哈希，遍历t的时候删减对应哈希的元素，如果哈希表的数值低于0，就说明为false。</p><p>万一删减不到零呢？其实这种情况是不会出现的，因为我们在循环伊始，检查两字符串的长度必须相同。</p><pre><code class="lang-cpp">class Solution &#123;public:    bool isAnagram(string s, string t) &#123;        if (s.size() != t.size()) return false;        vector&lt;int&gt; table(26, 0);        for (auto c : s) &#123;            ++table[c - &#39;a&#39;];        &#125;        for (auto c : t) &#123;            --table[c - &#39;a&#39;];            if (table[c - &#39;a&#39;] &lt; 0) return false;        &#125;        return true;    &#125;&#125;;</code></pre><h2 id="leetcode-290-单词规律"><a href="#leetcode-290-单词规律" class="headerlink" title="leetcode 290 单词规律"></a>leetcode 290 单词规律</h2><p>还是将其翻译成中间表示，然后比较中间表示是否同一。</p><pre><code class="lang-cpp">#include&lt;regex&gt;#include &lt;iterator&gt;class Solution &#123;public:    bool wordPattern(string pattern, string str) &#123;        vector&lt;string&gt; str_array;        std::regex r(&quot;\\s+&quot;);        std::sregex_token_iterator pos(str.cbegin(), str.cend(), r, -1); // -1代表你对正则表达式匹配的内容不感兴趣        std::sregex_token_iterator end;        for (; pos != end; ++pos) &#123;            str_array.push_back(*pos);        &#125;        if (pattern.size() != str_array.size()) return false;        unordered_map&lt;char, int&gt; hash_char;        unordered_map&lt;string, int&gt; hash_string;        for (int i = 0; i &lt; pattern.size(); ++i) &#123;            auto iter_char = hash_char.find(pattern[i]);            auto iter_string = hash_string.find(str_array[i]);            if (iter_char != hash_char.end() &amp;&amp; iter_string != hash_string.end()) &#123;                if (iter_char-&gt;second != iter_string-&gt;second) return false;            &#125; else if (iter_char == hash_char.end() &amp;&amp; iter_string == hash_string.end()) &#123;                hash_char[pattern[i]] = i;                hash_string[str_array[i]] = i;            &#125; else return false;        &#125;        return true;    &#125;&#125;;</code></pre><h2 id="leetcode-349-两个数组的交集"><a href="#leetcode-349-两个数组的交集" class="headerlink" title="leetcode 349 两个数组的交集"></a>leetcode 349 两个数组的交集</h2><p>显然是用hash。</p><pre><code class="lang-cpp">class Solution &#123;public:    vector&lt;int&gt; intersection(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123;        if (nums1.empty() || nums2.empty()) return &#123;&#125;;        unordered_map&lt;int, int&gt; hash;        vector&lt;int&gt; res;        for (auto c : nums1) &#123;            hash[c] = 1;        &#125;        for (auto c : nums2) &#123;            auto iter = hash.find(c);            if (iter != hash.end()) hash[c] = 0;        &#125;        for (auto iter = hash.begin(); iter != hash.end(); ++iter) &#123;            if (iter-&gt;second == 0) res.push_back(iter-&gt;first);        &#125;        return res;    &#125;&#125;;</code></pre><h2 id="leetcode-350-两个数组的交集-II"><a href="#leetcode-350-两个数组的交集-II" class="headerlink" title="leetcode 350 两个数组的交集 II"></a>leetcode 350 两个数组的交集 II</h2><p>带重复元素了。由于hash本身就可以记录每个元素出现的次数，那么我们每当发现一个元素，执行的不是<code>hash[nums[i]] = 1</code>，而是<code>hash[nums[i]]++</code>。</p><pre><code class="lang-cpp">class Solution &#123;public:    vector&lt;int&gt; intersect(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123;        if (nums1.empty() || nums2.empty()) return &#123;&#125;;        unordered_map&lt;int, int&gt; hash;        vector&lt;int&gt; res;        for (int i = 0; i &lt; nums1.size(); ++i) &#123;            ++hash[nums1[i]];        &#125;        for (int i = 0; i &lt; nums2.size(); ++i) &#123;            auto iter = hash.find(nums2[i]);            if (iter != hash.end() &amp;&amp; iter-&gt;second != 0) &#123;                res.push_back(nums2[i]);                --iter-&gt;second;            &#125;        &#125;        return res;    &#125;&#125;;</code></pre><h2 id="leetcode-451-根据字符出现频率排序"><a href="#leetcode-451-根据字符出现频率排序" class="headerlink" title="leetcode 451 根据字符出现频率排序"></a>leetcode 451 根据字符出现频率排序</h2><pre><code class="lang-cpp">class Solution &#123;public:    string frequencySort(string s) &#123;        if (s.empty()) return &quot;&quot;;        unordered_map&lt;char, int&gt; hash;        for (auto c : s) &#123;            ++hash[c];        &#125;        sort(s.begin(), s.end(), [&amp;hash](char lhs, char rhs) &#123;            return hash[lhs] &gt; hash[rhs] || (hash[lhs] == hash[rhs] &amp;&amp; lhs &lt; rhs);        &#125;);        return s;    &#125;&#125;;</code></pre><h2 id="leetcode-540-有序数组中的单一元素"><a href="#leetcode-540-有序数组中的单一元素" class="headerlink" title="leetcode 540 有序数组中的单一元素"></a>leetcode 540 有序数组中的单一元素</h2><pre><code class="lang-cpp">class Solution &#123;public:    int singleNonDuplicate(vector&lt;int&gt;&amp; nums) &#123;        if (nums.size() == 1) return nums[0];        return helper(nums, 0, nums.size()-1);    &#125;    int helper(vector&lt;int&gt;&amp; nums, int start, int end) &#123;        if (end == start) return nums[start];        int mid = start + (end - start) / 2;        if (nums[mid-1] == nums[mid]) &#123; // 中点左边相同，须删除中点和左边元素            int left_len = mid - start - 1;            int right_len = end - mid;            if (left_len % 2 != 0) &#123; // 如果删除后左边长度为奇数则递归左边                return helper(nums, start, mid-2);            &#125; else &#123; // 如果删除后右边长度为奇数则递归右边                return helper(nums, mid+1, end);            &#125;        &#125; else if (nums[mid] == nums[mid+1]) &#123;  // 中点右边相同，须删除中点和右边元素            int left_len = mid - start;            int right_len = end - mid - 1;            if (left_len % 2 != 0) &#123; // 如果删除后左边长度为奇数则递归左边                return helper(nums, start, mid-1);            &#125; else &#123; // 如果删除后右边长度为奇数则递归右边                return helper(nums, mid+2, end);            &#125;        &#125; else return nums[mid];    &#125;&#125;;</code></pre><h2 id="leetcode-410-分割数组的最大值"><a href="#leetcode-410-分割数组的最大值" class="headerlink" title="leetcode 410 分割数组的最大值"></a>leetcode 410 分割数组的最大值</h2>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>algorithm</tag>
      
      <tag>leetcode</tag>
      
      <tag>Datawhale</tag>
      
      <tag>searching</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】leetcode打卡：动态规划</title>
    <link href="/2020/08/23/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    <url>/2020/08/23/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    
    <content type="html"><![CDATA[<p>见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。<br><!--more---></p><h2 id="leetcode-198-打家劫舍"><a href="#leetcode-198-打家劫舍" class="headerlink" title="leetcode 198 打家劫舍"></a>leetcode 198 打家劫舍</h2><p>动态规划</p><p>设置<code>dp[i]</code>为前i个元素中打劫所得最高金额</p><p>构建状态转移方程：</p><pre><code class="lang-cpp">dp[i] = max(dp[i-1], dp[i-2]+nums[i]);</code></pre><p>边界条件：</p><pre><code class="lang-cpp">dp[0] = nums[0];dp[1] = max(nums[0], nums[1]);</code></pre><p>代码：</p><pre><code class="lang-cpp">class Solution &#123;public:    int rob(vector&lt;int&gt;&amp; nums) &#123;        if (nums.empty()) return 0;        if (nums.size() == 1) return nums[0];        vector&lt;int&gt; dp(nums.size(), 0);        dp[0] = nums[0];        dp[1] = max(nums[0], nums[1]);        for (int i = 2; i &lt; nums.size(); ++i) &#123;            dp[i] = max(dp[i-1], dp[i-2]+nums[i]);        &#125;        return dp[nums.size()-1];    &#125;&#125;;</code></pre><h2 id="leetcode-674-最长连续递增子序列"><a href="#leetcode-674-最长连续递增子序列" class="headerlink" title="leetcode 674 最长连续递增子序列"></a>leetcode 674 最长连续递增子序列</h2><p>不必使用动态规划，直接一遍遍历，碰到<code>nums[i] &lt; nums[i+1]</code>就递增计数器，保留计数器最大值即可：</p><pre><code class="lang-cpp">class Solution &#123;public:    int findLengthOfLCIS(vector&lt;int&gt;&amp; nums) &#123;        if (nums.empty()) return 0;        int count = 1;        int maxCount = 1;        for (int i = 0; i &lt; nums.size() - 1; ++i) &#123;            if (nums[i] &lt; nums[i+1]) &#123;                ++count;                maxCount = max(count, maxCount);            &#125; else &#123;                count = 1;            &#125;        &#125;        return maxCount;    &#125;&#125;;</code></pre><p>后来我悟了，这就是动态规划，只不过我利用<code>maxCount</code>来代替了dp数组。我真是个天才（误）！</p><h2 id="leetcode-5-最长回文子串"><a href="#leetcode-5-最长回文子串" class="headerlink" title="leetcode 5 最长回文子串"></a>leetcode 5 最长回文子串</h2><p>本来是想定义dp[i][j]，表达字符串从i到j的子串中的最长回文子串的。后来想想这样定义不合适，不如把dp定义为一个bool数组，用来标记从i到j子串是否为回文串即可。</p><p>状态转移关系：dp[i][j] = dp[i+1][j-1] &amp;&amp; (s[i] == s[j])</p><p>即s[i:j]为回文串的条件为s[i+1][j-1]为回文串，且s[i] == s[j]</p><p>边界条件：<br>dp[i][j] = true if i == j<br>dp[i][j] = false if i &gt; j<br>dp[i][i+1] = (s[i] == s[i+1])</p><p>最后遍历所有dp[i][j]=true的项，返回最长的子串即可</p><p>需要注意的一点：我们在遍历双层循环的时候，应该j在外，i在内。想想为什么？如果循环结构是这样的：</p><pre><code class="lang-cpp">for (int i = 0; i &lt; s.size(); ++i) &#123;    for (int j = i+1; j &lt; s.size(); ++j) &#123;        // TODO    &#125;&#125;</code></pre><p>那 i、j的变化为：<br>0,0<br>0,1<br>0,2<br>0,3-&gt;这里就不对了，因为dp[0][3]需要用到dp[1][2]的值。而i=1时的所有dp都还没求呢。</p><p>代码：</p><pre><code class="lang-cpp">class Solution &#123;public:    string longestPalindrome(string s) &#123;        if (s.empty()) return &quot;&quot;;        int size = s.size();        vector&lt;vector&lt;bool&gt;&gt; dp(size, vector&lt;bool&gt;(size, false));        string ans = &quot;&quot;;        for (int j = 0; j &lt; size; ++j) &#123;            for (int i = 0; i &lt;= j; ++i) &#123;                if (j == i) dp[i][j] = true;                else if (j == i+1) dp[i][j] = (s[i] == s[j]);                else dp[i][j] = (dp[i+1][j-1]) &amp;&amp; (s[i] == s[j]);                if (dp[i][j] &amp;&amp; ans.size() &lt; j-i+1) ans = s.substr(i, j-i+1);            &#125;        &#125;        return ans;    &#125;&#125;;</code></pre><h2 id="leetcode-213-打家劫舍2"><a href="#leetcode-213-打家劫舍2" class="headerlink" title="leetcode 213 打家劫舍2"></a>leetcode 213 打家劫舍2</h2><p>打家劫舍升级版，贼不能同时打劫头尾。</p><p>也好办，拆分成两个动态规划，一个规定不能打劫nums[0]，另一个规定不能打劫nums[size-1]，最后返回更大的那个即可。</p><p>代码：</p><pre><code class="lang-cpp">class Solution &#123;public:    int rob(vector&lt;int&gt;&amp; nums) &#123;        if (nums.empty()) return 0;        int size = nums.size();        if (size == 1) return nums[0];        if (size == 2) return max(nums[0], nums[1]);        vector&lt;int&gt; dp_robfirst(size, 0);        vector&lt;int&gt; dp_roblast(size, 0);        dp_robfirst[0] = nums[0];        dp_robfirst[1] = max(nums[0], nums[1]);        for (int i = 2; i &lt; size-1; ++i) &#123;            dp_robfirst[i] = max(dp_robfirst[i-1], dp_robfirst[i-2] + nums[i]);        &#125;        dp_roblast[0] = 0;        dp_roblast[1] = nums[1];        for (int i = 2; i &lt; size; ++i) &#123;            dp_roblast[i] = max(dp_roblast[i-1], dp_roblast[i-2] + nums[i]);        &#125;        return max(dp_robfirst[size-2], dp_roblast[size-1]);    &#125;&#125;;</code></pre><h2 id="leetcode-516-最长回文子序列"><a href="#leetcode-516-最长回文子序列" class="headerlink" title="leetcode 516 最长回文子序列"></a>leetcode 516 最长回文子序列</h2><p>这次的dp含义为从i到j子串中最长的回文序列长度。</p><p>转移方程：</p><p>dp[i][j] = dp[i+1][j-1] if s[i] == s[j]<br>dp[i][j] = max(dp[i+1][j], dp[i][j-1]) if s[i] != s[j]</p><p>注意，i从大遍历到小，j从小遍历到大。最后返回dp[0][size-1]</p><p>边界条件：dp[i][j] = 1 if i == j</p><p>代码：</p><pre><code class="lang-cpp">class Solution &#123;public:    int longestPalindromeSubseq(string s) &#123;        if (s.empty()) return 0;        int size = s.size();        vector&lt;vector&lt;int&gt;&gt; dp(size, vector&lt;int&gt;(size, 0));        for (int i = size-1; i &gt;= 0; --i) &#123;            for (int j = i; j &lt; size; ++j) &#123;                if (i == j) dp[i][j] = 1;                else if (s[i] == s[j]) dp[i][j] = dp[i+1][j-1] + 2;                else dp[i][j] = max(dp[i+1][j], dp[i][j-1]);            &#125;        &#125;        return dp[0][size-1];    &#125;&#125;;</code></pre><h2 id="leetcode-72-编辑距离"><a href="#leetcode-72-编辑距离" class="headerlink" title="leetcode 72 编辑距离"></a>leetcode 72 编辑距离</h2><p>代码：</p><pre><code class="lang-cpp">class Solution &#123;public:    int minDistance(string word1, string word2) &#123;        int M = word1.size();        int N = word2.size();        // if (word1.empty() || word2.empty()) return abs(M-N);        vector&lt;vector&lt;int&gt;&gt; dp(M+1, vector&lt;int&gt;(N+1, 0));        //initial        for (int i = 0; i &lt;= M; ++i) &#123;            dp[i][0] = i;        &#125;        for (int i = 0; i &lt;= N; ++i) &#123;            dp[0][i] = i;        &#125;        //dp        for (int i = 1; i &lt;= M; ++i) &#123;            for (int j = 1; j &lt;= N; ++j) &#123;                if (word1[i-1] == word2[j-1]) &#123;                    dp[i][j] = min(dp[i - 1][j - 1], 1 + dp[i - 1][j]);                    dp[i][j] = min(dp[i][j], 1 + dp[i][j - 1]);                &#125; else &#123;                    dp[i][j] = min(dp[i - 1][j - 1], dp[i - 1][j]);                    dp[i][j] = 1 + min(dp[i][j], dp[i][j - 1]);                &#125;            &#125;        &#125;        return dp[M][N];    &#125;&#125;;</code></pre>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>algorithm</tag>
      
      <tag>leetcode</tag>
      
      <tag>Datawhale</tag>
      
      <tag>Dynamic Programming</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】数据挖掘实践——时间序列模型</title>
    <link href="/2020/08/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/"/>
    <url>/2020/08/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p>不要停下来啊！<br><!--more---><br>AR模型是自回归模型，AutoRegression的简称。是一种较为朴素的时间序列数据处理方法，利用同一变量的前n期来预测本期的变量的数值，并且假定为线性关系。这种分析方法中，自变量不是其他的影响因素，而是变量本身的历史数据，利用xt-n来预测xt，因此被称为自回归。</p><p>MA模型：移动平均模型将序列{xt}表示为白噪声的线性加权。</p><p>在一个平稳的随机过程中，如果既有自回归的特性，又有移动平均过程的特性，则需要对两个模型进行混合使用，也即是较为普遍的ARMA模型，一般记为ARMA(p,q)。</p><p>ARIMA(p,d,q)是差分自回归移动平均模型，是运用最为广泛的一种时间序列分析模型。p,q的意义不变，其中d的含义是将时间序列化为平稳时间序列所做的差分次数。</p><p>在建立时间序列的模型后，我们要对时间序列数据进行多重检验，以确定该数据符合我们的统计学上的分析准则。主要的检验要观察数据的自相关性，阶数识别和单位根检验（ADF检验）。对于ARMA模型来说，最难的步骤是进行阶数的识别。ADF检验是时间序列中最为重要的检验之一，帮助我们准确判断时间序列数据是否平稳，是为后期的Johansen 检验、Granger 检验等的基础。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Datawhale</tag>
      
      <tag>Data Mining</tag>
      
      <tag>Time Series Analysis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】数据挖掘实践——时间序列规则</title>
    <link href="/2020/08/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E8%A7%84%E5%88%99/"/>
    <url>/2020/08/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E8%A7%84%E5%88%99/</url>
    
    <content type="html"><![CDATA[<p>不要停下来啊！<br><!--more---></p><p>时间序列基本规则法<br>提取时间序列的周期性特征进行预测，参考：时间序列规则法快速入门<br>观察序列，当序列存在周期性时，可以用线性回归-利用时间特征做线性回归做为baseline</p><p>时间序列是指将同一统计指标的数值按其发生的时间先后顺序排列而成的数列。</p><p>时间序列分析的主要目的是根据已有的历史数据对未来进行预测。与面板数据不同，面板数据侧重于同一时间点不同样本的数值，而时间序列侧重于同一统计指标在时间的不同点的数值。时间序列有两个重要指标，一个是资料所属的时间，另一个是时间上的统计指标数值。时间序列可以描述社会经济现象在不同时间的发展状态和过程，也可以根据历史数据进行合理的未来推测。</p><p>一般地，我们认为一个随机游走的变量会服从正态分布。</p><p>提取时间的周期性特点做为特征，此时训练集每条样本为”时间特征-&gt;目标值”，时间序列的依赖关系被剔除，不需要严格依赖滑窗截取训练样本。常见是将时间用0-1哑变量表达，有以下若干种特征：</p><ul><li>将星期转化为了0-1变量，从周一至周天，独热编码共7个变量</li><li>将节假日转化为0-1变量，视具体节假日数目，可简单分为两类，”有假日”-“无假日”，独热编码共2个变量；或赋予不同编码值，如区分国庆、春节、劳动节等使用1、2、3表示</li><li>将月初转化为0-1变量，简单分两类表示为”是月初”-“非月初”，共2个特征</li><li>类似的月中、月初可以转化为0-1变量</li><li>控制时间粒度，区分是weekday or weekend</li></ul><p>按列提取中位数是一种简单而有效的提取周期因子的方法。中位数十分鲁棒，不受极端值的影响。但中位数损失了很多信息。实践中，可以在此基础上进一步优化。比如可以提取一个均值和一个中位数，然后将均值和中位数融合。融合的比例按照测试集的表现来确定。也可以根据与预测周的时间距离来赋予不同的权重。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Datawhale</tag>
      
      <tag>Data Mining</tag>
      
      <tag>Time Series Analysis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】数据挖掘实践——数据探索和分析</title>
    <link href="/2020/08/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%86%E6%9E%90/"/>
    <url>/2020/08/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>我也是加把劲骑士！<br><!--more---></p><pre><code class="lang-py">file_names = glob.glob(&#39;Purchase Redemption Data/*.csv&#39;)file_names[&#39;Purchase Redemption Data/mfd_bank_shibor.csv&#39;, &#39;Purchase Redemption Data/comp_predict_table.csv&#39;, &#39;Purchase Redemption Data/user_profile_table.csv&#39;, &#39;Purchase Redemption Data/mfd_day_share_interest.csv&#39;, &#39;Purchase Redemption Data/user_balance_table.csv&#39;]</code></pre><h2 id="1-用户信息表-user-profile-table"><a href="#1-用户信息表-user-profile-table" class="headerlink" title="1. 用户信息表 user_profile_table"></a>1. 用户信息表 user_profile_table</h2><p>我们总共随机抽取了约 3 万用户。</p><p>其中部分用户在 2014 年 9 月份第一次出现，这部分用户只在测试数据中（真的太鉴了这个）。</p><p>因此用户信息表是约 2.8 万 个用户的基本数据，在原始数据的基础上处理后，主要包含了用户的性别、城市和星座。</p><p><img src="/2020/08/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%86%E6%9E%90/2020-08-20-23-43-55.png" alt></p><h2 id="2-用户申购赎回数据表-user-balance-table"><a href="#2-用户申购赎回数据表-user-balance-table" class="headerlink" title="2. 用户申购赎回数据表 user_balance_table"></a>2. 用户申购赎回数据表 user_balance_table</h2><p>里面有 20130701 至 20140831 申购和赎回信息、以及所有的子类目信息。</p><p>数据经过脱敏处理。脱敏之后的数据，基本保持了原数据趋势。</p><p>数据主要包括用户操作时间和操作记录，其中操作记录包括申购和赎回两个部分。</p><p>金额的单位是分，即 0.01 元人民币。 如果用户今日消费总量为0，即consume_amt=0，则四个字类目为空。</p><div class="table-container"><table><thead><tr><th>字段名称</th><th>含义</th></tr></thead><tbody><tr><td>report_date</td><td>日期</td></tr><tr><td>tBalance</td><td>今日余额</td></tr><tr><td>yBalance</td><td>昨日余额</td></tr><tr><td>total_purchase_amt</td><td>今日总购买量 = 直接购买 + 收益</td></tr><tr><td>direct_purchase_amt</td><td>今日直接购买量</td></tr><tr><td>purchase_bal_amt</td><td>今日支付宝余额购买量</td></tr><tr><td>purchase_bank_amt</td><td>今日银行卡购买量</td></tr><tr><td>total_redeem_amt</td><td>今日总赎回量 = 消费 + 转出</td></tr><tr><td>consume_amt</td><td>今日消费总量</td></tr><tr><td>transfer_amt</td><td>今日转出总量</td></tr><tr><td>tftobal_amt</td><td>今日转出到支付宝余额总量</td></tr><tr><td>tftocard_amt</td><td>今日转出到银行卡总量</td></tr><tr><td>share_amt</td><td>今日收益</td></tr><tr><td>category1</td><td>今日类目 1 消费总额</td></tr><tr><td>category2</td><td>今日类目 2 消费总额</td></tr><tr><td>category3</td><td>今日类目 3 消费总额</td></tr><tr><td>category4</td><td>今日类目 4 消费总额</td></tr></tbody></table></div><p><img src="/2020/08/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%86%E6%9E%90/2020-08-20-23-47-13.png" alt></p><p>注 1 ：上述的数据都是经过脱敏处理的，收益为重新计算得到的，计算方法按照简化后的计算方式处理，具体计算方式在下节余额宝收益计算方式中描述。</p><p>注 2 ：脱敏后的数据保证了今日余额 = 昨日余额 + 今日申购 - 今日赎回，不会出现负值。</p><h2 id="3-收益率表-mfd-day-share-interest"><a href="#3-收益率表-mfd-day-share-interest" class="headerlink" title="3. 收益率表 mfd_day_share_interest"></a>3. 收益率表 mfd_day_share_interest</h2><p>收益表为余额宝在 14 个月内的收益率表 。</p><p><img src="/2020/08/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%86%E6%9E%90/2020-08-20-23-47-55.png" alt></p><div class="table-container"><table><thead><tr><th>列名</th><th>含义</th></tr></thead><tbody><tr><td>mfd_date</td><td>日期</td></tr><tr><td>mfd_daily_yield</td><td>万份收益，即 1 万块钱的收益。</td></tr><tr><td>mfd_7daily_yield</td><td>七日年化收益率（ % ）</td></tr></tbody></table></div><h2 id="4-上海银行间同业拆放利率（Shibor）表-df-mfd-bank-shibor"><a href="#4-上海银行间同业拆放利率（Shibor）表-df-mfd-bank-shibor" class="headerlink" title="4. 上海银行间同业拆放利率（Shibor）表 df_mfd_bank_shibor"></a>4. 上海银行间同业拆放利率（Shibor）表 df_mfd_bank_shibor</h2><p>银行间拆借利率表是 14 个月期间银行之间的拆借利率（皆为年化利率）</p><p><img src="/2020/08/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%86%E6%9E%90/2020-08-20-23-48-31.png" alt></p><div class="table-container"><table><thead><tr><th>列名</th><th>类型</th><th>含义</th><th>示例</th></tr></thead><tbody><tr><td>mfd_date</td><td>String</td><td>日期</td><td>20140102</td></tr><tr><td>Interest_O_N</td><td>Double</td><td>隔夜利率（%）</td><td>2.8</td></tr><tr><td>Interest_1_W</td><td>Double</td><td>1周利率（%）</td><td>4.25</td></tr><tr><td>Interest_2_W</td><td>Double</td><td>2周利率（%）</td><td>4.9</td></tr><tr><td>Interest_1_M</td><td>Double</td><td>1个月利率（%）</td><td>5.04</td></tr><tr><td>Interest_3_M</td><td>Double</td><td>3个月利率（%）</td><td>4.91</td></tr><tr><td>Interest_6_M</td><td>Double</td><td>6个月利率（%）</td><td>4.79</td></tr><tr><td>Interest_9_M</td><td>Double</td><td>9个月利率（%）</td><td>4.76</td></tr><tr><td>Interest_1_Y</td><td>Double</td><td>1年利率（%）</td><td>4.78</td></tr></tbody></table></div><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><h2 id="收益计算方式"><a href="#收益计算方式" class="headerlink" title="收益计算方式"></a>收益计算方式</h2><p>本赛题的余额宝收益方式，主要基于实际余额宝收益计算方法，但是进行了一定的简化，此处计算简化的地方如下：</p><p>首先，收益计算的时间不再是会计日，而是自然日，以 0 点为分隔，如果是 0 点之前转入或者转出的金额算作昨天的，如果是 0 点以后转入或者转出的金额则算作今天的。</p><p>然后，收益的显示时间，即实际将第一份收益打入用户账户的时间为如下表格，以周一转入周三显示为例，如果用户在周一存入 10000 元，即 1000000 分，那么这笔金额是周一确认，周二是开始产生收益，用户的余额还是 10000 元，在周三将周二产生的收益打入到用户的账户中，此时用户的账户中显示的是 10001.1 元，即 1000110 分。其他时间的计算按照表格中的时间来计算得到。</p><div class="table-container"><table><thead><tr><th>转入时间</th><th>首次显示收益时间</th></tr></thead><tbody><tr><td>周一</td><td>周三</td></tr><tr><td>周二</td><td>周四</td></tr><tr><td>周三</td><td>周五</td></tr><tr><td>周四</td><td>周六</td></tr><tr><td>周五</td><td>下周二</td></tr><tr><td>周六</td><td>下周三</td></tr><tr><td>周天</td><td>下周三</td></tr></tbody></table></div><h2 id="提交格式"><a href="#提交格式" class="headerlink" title="提交格式"></a>提交格式</h2><div class="table-container"><table><thead><tr><th>字段</th><th>类型</th><th>含义</th><th>示例</th></tr></thead><tbody><tr><td>report_date</td><td>bigint</td><td>日期</td><td>20140901</td></tr><tr><td>purchase</td><td>bigint</td><td>申购总额</td><td>40000000</td></tr><tr><td>redeem</td><td>bigint</td><td>赎回总额</td><td>30000000</td></tr></tbody></table></div><p><img src="/2020/08/20/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%86%E6%9E%90/2020-08-20-23-49-17.png" alt></p><p>每一行数据是一天对申购、赎回总额的预测值， 2014 年 9 月每天一行数据，共 30 行数据。 Purchase 和 redeem 都是金额数据，精确到分，而不是精确到元。</p><p>评分数据格式要求与“选手结果数据样例文件”一致，结果表命名为：tc_comp_predict_table， 字段之间以逗号为分隔符</p><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><p>评估指标的设计主要期望选手对未来 30 天内每一天申购和赎回的总量数据预测的越准越好，同时考虑到可能存在的多种情况。</p><p>譬如有些选手在 30 天中 29 天预测都是非常精准的但是某一天预测的结果可能误差很大，而有些选手在 30 天中每天的预测都不是很精准误差较大，如果采用绝对误差则可能导致前者的成绩比后者差，而在实际业务中可能更倾向于前者。</p><p>所以最终选用积分式的计算方法：每天的误差选用相对误差来计算，然后根据用户预测申购和赎回的相对误差，通过得分函数映射得到一个每天预测结果的得分，将 30 天内的得分汇总，然后结合实际业务的倾向，对申购赎回总量预测的得分情况进行加权求和，得到最终评分。具体的操作如下：</p><p>1) 计算所有用户在测试集上每天的申购及赎回总额与实际情况总额的误差。</p><p><img src="https://gtms02.alicdn.com/tps/i2/TB1UMhaHVXXXXbWXpXXmP_sPXXX-372-168.png" alt></p><p>2) 申购预测得分与 Purchasei 相关，赎回预测得分与 Redeemi 相关 , 误差与得分之间的计算公式不公布，但保证该计算公式为单调递减的，即误差越小，得分越高，误差与大，得分越低。当第 i 天的申购误差 Purchasei =0 ，这一天的得分为 10 分；当 Purchasei &gt; 0.3 ，其得分为 0 。</p><p>3) 最后公布总积分 = 申购预测得分 <em>45%+ 赎回预测得分 </em>55% 。 </p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Datawhale</tag>
      
      <tag>Data Mining</tag>
      
      <tag>Time Series Analysis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】Fuzzing学习笔记3——灰盒Fuzzing</title>
    <link href="/2020/08/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%E2%80%94%E2%80%94%E7%81%B0%E7%9B%92Fuzzing/"/>
    <url>/2020/08/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%E2%80%94%E2%80%94%E7%81%B0%E7%9B%92Fuzzing/</url>
    
    <content type="html"><![CDATA[<p>灰盒变异的模糊测试。<br><!--more---></p><p>普通fuzzing：从0开始构造测试用例<br>突变fuzzing：从seed开始构造测试用例<br>灰盒fuzzing：有引导的突变fuzzing</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>上回书说到，AFL是突变fuzzer，通过把种子字符串进行些微修改，得到变异体；此外AFL还会将一个种子的前半部分与另一个种子的后半部分连接，形成变异体。</p><p>AFL也是个灰盒fuzzer，这是由于AFL需要使用程序内部信息（即覆盖率）。AFL不是白盒，因为AFL没有对程序进行约束求解、程序分析之类的，只是简单获取了一个覆盖率。如果生成的样本能够提升覆盖率，那么就将这个样本添加进种子队列以供下次突变使用（这就意味着突变体有重复突变的可能）。</p><p>AFL计算覆盖率的方法，是通过在每个分支的跳转指令后执行一段标记代码。这样就可以做到，监控每个输入导致的激活分支，以及每个分支被激活的大概频率。注入代码这个环节通常在编译时完成。对于Python，可以在未经处理的情况下执行覆盖率信息收集。</p><h2 id="突变算法和种子"><a href="#突变算法和种子" class="headerlink" title="突变算法和种子"></a>突变算法和种子</h2><p>引入Mutator类。Mutator类是个过程类，包装了对输入inp的突变方法。</p><pre><code class="lang-py">class Mutator(object):    def __init__(self):        self.mutators = [            self.delete_random_character,            self.insert_random_character,            self.flip_random_character    def insert_random_character(self,s):        &quot;&quot;&quot;Returns s with a random character inserted&quot;&quot;&quot;        pos = random.randint(0, len(s))        random_character = chr(random.randrange(32, 127))        return s[:pos] + random_character + s[pos:]    def delete_random_character(self,s):        &quot;&quot;&quot;Returns s with a random character deleted&quot;&quot;&quot;        if s == &quot;&quot;:            return self.insert_random_character(s)        pos = random.randint(0, len(s) - 1)        return s[:pos] + s[pos + 1:]    def flip_random_character(self,s):        &quot;&quot;&quot;Returns s with a random bit flipped in a random position&quot;&quot;&quot;        if s == &quot;&quot;:            return self.insert_random_character(s)        pos = random.randint(0, len(s) - 1)        c = s[pos]        bit = 1 &lt;&lt; random.randint(0, 6)        new_c = chr(ord(c) ^ bit)        return s[:pos] + new_c + s[pos + 1:]    def mutate(self, inp):        &quot;&quot;&quot;Return s with a random mutation applied&quot;&quot;&quot;        mutator = random.choice(self.mutators)        return mutator(inp)</code></pre><p>使用Mutator是只需实例化Mutator，然后调用mutate()方法即可。</p><pre><code class="lang-py">Mutator().mutate(&quot;good&quot;)&#39;cood&#39;</code></pre><h2 id="精力分配（Power-Schedule）"><a href="#精力分配（Power-Schedule）" class="headerlink" title="精力分配（Power Schedule）"></a>精力分配（Power Schedule）</h2><p>模糊测试是一种执行很慢的测试方法。既然并不是每个测试用例种子都值得分配同样的精力，那么试图发现那些更令人感兴趣的种子就是理所当然的选择了。</p><p>我们把一个种子从种群中被选中的可能性称为种子的能量（energy）。我们希望优先突变和执行那些更有希望发现待测程序错误的种子，不希望在无进步的种子身上浪费精力。</p><p>决定种子能量分配的算法称为“功率表”（Power Schedule）。AFL的功率表会将更多的能量分配给那些长度较短、执行速度较快、覆盖率增加较多的种子。</p><p>由此，每个种子需要额外维护其能量。构建Seed类如下：</p><pre><code class="lang-py">class Seed(object):        def __init__(self, data):        &quot;&quot;&quot;Set seed data&quot;&quot;&quot;        self.data = data    def __str__(self):        &quot;&quot;&quot;Returns data as string representation of the seed&quot;&quot;&quot;        return self.data    __repr__ = __str__</code></pre><p>下面是功率表PowerSchedule类的定义：</p><pre><code class="lang-py">class PowerSchedule(object):        def assignEnergy(self, population):        &quot;&quot;&quot;Assigns each seed the same energy&quot;&quot;&quot;        for seed in population:            seed.energy = 1    def normalizedEnergy(self, population):        &quot;&quot;&quot;Normalize energy&quot;&quot;&quot;        energy = list(map(lambda seed: seed.energy, population))        sum_energy = sum(energy)  # Add up all values in energy        norm_energy = list(map(lambda nrg: nrg/sum_energy, energy))        return norm_energy    def choose(self, population):        &quot;&quot;&quot;Choose weighted by normalized energy.&quot;&quot;&quot;        import numpy as np        self.assignEnergy(population)        norm_energy = self.normalizedEnergy(population)        seed = np.random.choice(population, p=norm_energy)        return seed</code></pre><h2 id="灰盒fuzzing与黑盒fuzzing的比较"><a href="#灰盒fuzzing与黑盒fuzzing的比较" class="headerlink" title="灰盒fuzzing与黑盒fuzzing的比较"></a>灰盒fuzzing与黑盒fuzzing的比较</h2><p>首先定义不使用coverage的黑盒fuzzer，MutationFuzzer 类：</p><pre><code class="lang-py">class MutationFuzzer(Fuzzer):    def __init__(self, seeds, mutator, schedule):        self.seeds = seeds        self.mutator = mutator        self.schedule = schedule        self.inputs = []        self.reset()    def reset(self):        &quot;&quot;&quot;Reset the initial population and seed index&quot;&quot;&quot;        self.population = list(map(lambda x: Seed(x), self.seeds))        self.seed_index = 0    def create_candidate(self):        &quot;&quot;&quot;Returns an input generated by fuzzing a seed in the population&quot;&quot;&quot;        seed = self.schedule.choose(self.population)        # Stacking: Apply multiple mutations to generate the candidate        candidate = seed.data        trials = min(len(candidate), 1 &lt;&lt; random.randint(1,5))        for i in range(trials):            candidate = self.mutator.mutate(candidate)        return candidate    def fuzz(self):        &quot;&quot;&quot;Returns first each seed once and then generates new inputs&quot;&quot;&quot;        if self.seed_index &lt; len(self.seeds):            # Still seeding            self.inp = self.seeds[self.seed_index]            self.seed_index += 1        else:            # Mutating            self.inp = self.create_candidate()        self.inputs.append(self.inp)        return self.inp</code></pre><p>MutationFuzzer 是由一组初始种子、一个突变器和一个功率表构成的。在整个模糊化过程中，它维护着一个名为population的种子语料库。create_candidate对某个种子执行多次突变，fuzz先试图返回正常种子，随后返回突变种子。</p><p>population_coverage 是预先定义好的覆盖率计算库，返回（all_coverage，cumulative_coverage）。其中all_coverage是所有输入所覆盖的语句集，cumulative_coverage是随着执行输入数量的增加而覆盖的语句数量。</p><p>下面是GrayBox fuzzing的实现：</p><pre><code class="lang-py">class GreyboxFuzzer(MutationFuzzer):        def reset(self):        &quot;&quot;&quot;Reset the initial population, seed index, coverage information&quot;&quot;&quot;        super().reset()        self.coverages_seen = set()        self.population = [] # population is filled during greybox fuzzing    def run(self, runner):        &quot;&quot;&quot;Run function(inp) while tracking coverage.           If we reach new coverage,           add inp to population and its coverage to population_coverage        &quot;&quot;&quot;        result, outcome = super().run(runner)        new_coverage = frozenset(runner.coverage())        if new_coverage not in self.coverages_seen:            # We have new coverage            seed = Seed(self.inp)            seed.coverage = runner.coverage()            self.coverages_seen.add(new_coverage)            self.population.append(seed)        return (result, outcome)</code></pre><p>经过计算，分别得到覆盖率变化趋势blackbox_coverage和greybox_coverage，可视化如下：</p><p><img src="/2020/08/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%E2%80%94%E2%80%94%E7%81%B0%E7%9B%92Fuzzing/2020-08-20-00-42-27.png" alt></p><p>可以看到，灰盒fuzzing的覆盖率增长明显比黑盒要好。</p><h2 id="增强后的灰盒fuzzer"><a href="#增强后的灰盒fuzzer" class="headerlink" title="增强后的灰盒fuzzer"></a>增强后的灰盒fuzzer</h2><p>通过修改功率表PowerSchedule，使那些能激活不寻常的path的input具有更高的energy。不寻常的path指的是激活次数比较小。</p><p>有多种方法计算一个种子的能量。上述的要求形式化为具体定义即为</p><script type="math/tex; mode=display">e(s)=\frac{1}{f(p(s))^a}</script><p>其中$s$是种子<br>$p(s)$为$s$激活的path<br>$f(p)$返回path激活的次数<br>$a$是给定的超参数<br>$e(s)$是种子$s$被分配的能量</p><p>下面是按照此思想设置的PowerSchedule：</p><pre><code class="lang-py">class AFLFastSchedule(PowerSchedule):     def __init__(self, exponent):        self.exponent = exponent    def assignEnergy(self, population):        &quot;&quot;&quot;Assign exponential energy inversely proportional to path frequency&quot;&quot;&quot;        for seed in population:            seed.energy = 1 / (self.path_frequency[getPathID(seed.coverage)] ** self.exponent)</code></pre><p>改进的灰盒Fuzzer：</p><pre><code class="lang-py">class CountingGreyboxFuzzer(GreyboxFuzzer):    def reset(self):        &quot;&quot;&quot;Reset path frequency&quot;&quot;&quot;        super().reset()        self.schedule.path_frequency = &#123;&#125;    def run(self, runner):        &quot;&quot;&quot;Inform scheduler about path frequency&quot;&quot;&quot;        result, outcome = super().run(runner)        path_id = getPathID(runner.coverage())        if not path_id in self.schedule.path_frequency:            self.schedule.path_frequency[path_id] = 1        else:            self.schedule.path_frequency[path_id] += 1        return(result, outcome)</code></pre><p>覆盖率变化如图所示</p><p><img src="/2020/08/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%E2%80%94%E2%80%94%E7%81%B0%E7%9B%92Fuzzing/2020-08-20-00-52-08.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>三篇Fuzzing文章到此为止。</p><p>Fuzzing 的方法是通过大量生成input，来找出被测程序的错误的方法。</p><p>Fuzzing的关键点之一在于input生成方法，其二在于input的排序方法，其三在于软件内部信息的获取和应用。</p><p>如果input完全是自己构建的，那么这种方法称之为generational fuzzing</p><p>如果input是通过原始种子略微修改后得到的，那么这种fuzzing为Mutational fuzzing。</p><p>如果Mutator可以经过一定的程序信息的引导，那么这叫做GrayBox Fuzzing，比如覆盖率引导的模糊测试</p><p>如果Seed经过Power Schedule的精力分配，随后Mutator根据Seed的精力大小排序，那么这种方法称之为Boosted GrayBox Fuzzing</p><p>使用到的类：</p><p>Runner：待测程序的基类</p><p>Fuzzer：模糊测试器的基类</p><p>Seed：测试用例种子的基类</p><p>PowerSchedule：功率表的基类</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Fuzzing</tag>
      
      <tag>Testing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】Fuzzing学习笔记2——基于变异的Fuzzing</title>
    <link href="/2020/08/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E5%8F%98%E5%BC%82%E7%9A%84Fuzzing/"/>
    <url>/2020/08/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E5%8F%98%E5%BC%82%E7%9A%84Fuzzing/</url>
    
    <content type="html"><![CDATA[<p>基于变异的模糊测试。<br><!--more---></p><p><a href="https://www.fuzzingbook.org/html/MutationFuzzer.html">https://www.fuzzingbook.org/html/MutationFuzzer.html</a></p><p>大多数随机生成的输入在语法上都是无效的，程序很快就会检测并拒绝这些输入，这样便达不到深入测试Runner内部的目的。因此我们必须试图生成有效的输入。</p><p>本节我们介绍Mutation Fuzzing，这种基于变异的方法对现有输入进行小的更改，这些更改可能仍使输入保持有效，但仍会表现出新的行为。 </p><p>要对字符串形式的输入进行变异（Mutate），具体来说，就是执行随机插入字符、删除字符、修改字符等操作。Mutational fuzzing的特点是基于一个有效的原始输入，与之前凭空捏造出来一个input的generational fuzzing不同。</p><p>随机删除</p><pre><code class="lang-py">def delete_random_character(s):    &quot;&quot;&quot;Returns s with a random character deleted&quot;&quot;&quot;    if s == &quot;&quot;:        return s    pos = random.randint(0, len(s) - 1)    # print(&quot;Deleting&quot;, repr(s[pos]), &quot;at&quot;, pos)    return s[:pos] + s[pos + 1:]seed_input = &quot;A quick brown fox&quot;for i in range(10):    x = delete_random_character(seed_input)    print(repr(x))&#39;A uick brown fox&#39;&#39;A quic brown fox&#39;&#39;A quick brown fo&#39;&#39;A quic brown fox&#39;&#39;A quick bown fox&#39;&#39;A quick bown fox&#39;&#39;A quick brown fx&#39;&#39;A quick brown ox&#39;&#39;A quick brow fox&#39;&#39;A quic brown fox&#39;</code></pre><p>随机插入</p><pre><code class="lang-py">def insert_random_character(s):    &quot;&quot;&quot;Returns s with a random character inserted&quot;&quot;&quot;    pos = random.randint(0, len(s))    random_character = chr(random.randrange(32, 127))    # print(&quot;Inserting&quot;, repr(random_character), &quot;at&quot;, pos)    return s[:pos] + random_character + s[pos:]for i in range(10):    print(repr(insert_random_character(seed_input)))&#39;A quick brvown fox&#39;&#39;A quwick brown fox&#39;&#39;A qBuick brown fox&#39;&#39;A quick broSwn fox&#39;&#39;A quick brown fvox&#39;&#39;A quick brown 3fox&#39;&#39;A quick brNown fox&#39;&#39;A quick brow4n fox&#39;&#39;A quick brown fox8&#39;&#39;A equick brown fox&#39;</code></pre><p>随机替换</p><pre><code class="lang-py">def flip_random_character(s):    &quot;&quot;&quot;Returns s with a random bit flipped in a random position&quot;&quot;&quot;    if s == &quot;&quot;:        return s    pos = random.randint(0, len(s) - 1)    c = s[pos]    bit = 1 &lt;&lt; random.randint(0, 6)    new_c = chr(ord(c) ^ bit)    # print(&quot;Flipping&quot;, bit, &quot;in&quot;, repr(c) + &quot;, giving&quot;, repr(new_c))    return s[:pos] + new_c + s[pos + 1:]for i in range(10):    print(repr(flip_random_character(seed_input)))&#39;A quick bRown fox&#39;&#39;A quici brown fox&#39;&#39;A&quot;quick brown fox&#39;&#39;A quick brown$fox&#39;&#39;A quick bpown fox&#39;&#39;A quick brown!fox&#39;&#39;A 1uick brown fox&#39;&#39;@ quick brown fox&#39;&#39;A quic+ brown fox&#39;&#39;A quick bsown fox&#39;</code></pre><p>只要我们有一些原始输入，这些输入是有效的，那么我们基于原始输入的变异也应该是有效的。</p><p>多重变异</p><p>假设我们这里有个方法mutate()，能对字符串执行变异操作。那么连续变异50次，输入会变成什么样子？</p><pre><code class="lang-py">seed_input = &quot;http://www.google.com/search?q=fuzzing&quot;mutations = 50inp = seed_inputfor i in range(mutations):    if i % 5 == 0:        print(i, &quot;mutations:&quot;, repr(inp))    inp = mutate(inp)0 mutations: &#39;http://www.google.com/search?q=fuzzing&#39;5 mutations: &#39;http:/L/www.googlej.com/seaRchq=fuz:ing&#39;10 mutations: &#39;http:/L/www.ggoWglej.com/seaRchqfu:in&#39;15 mutations: &#39;http:/L/wwggoWglej.com/seaR3hqf,u:in&#39;20 mutations: &#39;htt://wwggoVgle&quot;j.som/seaR3hqf,u:in&#39;25 mutations: &#39;htt://fwggoVgle&quot;j.som/eaRd3hqf,u^:in&#39;30 mutations: &#39;htv://&gt;fwggoVgle&quot;j.qom/ea0Rd3hqf,u^:i&#39;35 mutations: &#39;htv://&gt;fwggozVle&quot;Bj.qom/eapRd[3hqf,u^:i&#39;40 mutations: &#39;htv://&gt;fwgeo6zTle&quot;Bj.\&#39;qom/eapRd[3hqf,tu^:i&#39;45 mutations: &#39;htv://&gt;fwgeo]6zTle&quot;BjM.\&#39;qom/eaR[3hqf,tu^:i&#39;</code></pre><p>可以看到变异体已经几乎无法识别了。我们通过多次变异，获得了更加多样的输入。</p><p>MutationFuzzer的实现</p><pre><code class="lang-py">class MutationFuzzer(Fuzzer):    def __init__(self, seed, min_mutations=2, max_mutations=10):        self.seed = seed        self.min_mutations = min_mutations        self.max_mutations = max_mutations        self.reset()    def reset(self):        self.population = self.seed        self.seed_index = 0    def mutate(self, inp):        return mutate(inp)    def create_candidate(self):        candidate = random.choice(self.population)        trials = random.randint(self.min_mutations, self.max_mutations)        for i in range(trials):            candidate = self.mutate(candidate)        return candidate    def fuzz(self):        if self.seed_index &lt; len(self.seed):            # Still seeding            self.inp = self.seed[self.seed_index]            self.seed_index += 1        else:            # Mutating            self.inp = self.create_candidate()        return self.inp</code></pre><p><code>create_candidate()</code>随机选取种子<code>candidate</code>，然后将这个种子随机突变<code>trials</code>次，返回经过多次突变的<code>candidate</code>。</p><p><code>fuzz()</code>方法一开始返回的是未经突变的种子样本，当种子挑选完毕后，返回突变样本。这样可以确保每次调用fuzz()，得到的输出是不一样的。</p><p>Mutational Fuzzing成功的关键在于引导这些突变的方法—即保留那些特别有价值的样本。</p><p>覆盖率引导</p><p>我们可以利用被测程序来引导测试用例生成。以前我们只是收集程序执行成功或者失败的信息，现在我们可以收集多点信息，比如运行时代码覆盖率。</p><p>利用覆盖率引导变异的Fuzzing，最成功的实践是<a href="http://lcamtuf.coredump.cx/afl/">American fuzzy loop</a>，即AFL。</p><p>AFL会生成“成功”的测试用例。AFL认为，所谓“成功”是指找到了一条新的程序执行路径。AFL不断地突变新路径的输入，如果产生了新的路径，输入会保留下来。</p><p>为了获得程序运行时的覆盖率信息，我们需要重新定义Runner。FunctionRunner类负责包装一个被测函数。</p><pre><code class="lang-py">class FunctionRunner(Runner):    def __init__(self, function):        &quot;&quot;&quot;Initialize.  `function` is a function to be executed&quot;&quot;&quot;        self.function = function    def run_function(self, inp):        return self.function(inp)    def run(self, inp):        try:            result = self.run_function(inp)            outcome = self.PASS        except Exception:            result = None            outcome = self.FAIL        return result, outcome</code></pre><p>而FunctionCoverageRunner在此基础上增加了覆盖率计算模块<code>Coverage</code>。</p><pre><code class="lang-py">class FunctionCoverageRunner(FunctionRunner):    def run_function(self, inp):        with Coverage() as cov:            try:                result = super().run_function(inp)            except Exception as exc:                self._coverage = cov.coverage()                raise exc        self._coverage = cov.coverage()        return result    def coverage(self):        return self._coverage</code></pre><p>下面改写Fuzzer类。</p><pre><code class="lang-py">class MutationCoverageFuzzer(MutationFuzzer):    def reset(self):        super().reset()        self.coverages_seen = set()        # Now empty; we fill this with seed in the first fuzz runs        self.population = []    def run(self, runner):        &quot;&quot;&quot;Run function(inp) while tracking coverage.           If we reach new coverage,           add inp to population and its coverage to population_coverage        &quot;&quot;&quot;        result, outcome = super().run(runner)        new_coverage = frozenset(runner.coverage())        if outcome == Runner.PASS and new_coverage not in self.coverages_seen:            # We have new coverage            self.population.append(self.inp)            self.coverages_seen.add(new_coverage)        return result</code></pre><p><code>MutationCoverageFuzzer</code>内部保存测试用例队列<code>population</code>和覆盖率队列<code>coverages_seen</code>。如果fuzz的input产生了新的coverage，则将该input添加到population中，并将该coverage添加到coverage_seen中。</p><p>由此，我们得到的population中的每个input都能够使得程序产生不同的coverage，这背后可能是程序的不同执行路径，也就增加了inputs的多样性。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Fuzzing</tag>
      
      <tag>Testing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】Fuzzing学习笔记1——认识Fuzzing的基本单元</title>
    <link href="/2020/08/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E8%AE%A4%E8%AF%86Fuzzing%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8D%95%E5%85%83/"/>
    <url>/2020/08/19/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E8%AE%A4%E8%AF%86Fuzzing%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8D%95%E5%85%83/</url>
    
    <content type="html"><![CDATA[<p>Fuzzing又称模糊测试。<br><!--more---></p><p><a href="https://www.fuzzingbook.org/html/Fuzzer.html">https://www.fuzzingbook.org/html/Fuzzer.html</a></p><h2 id="Fuzzing测试是什么？"><a href="#Fuzzing测试是什么？" class="headerlink" title="Fuzzing测试是什么？"></a>Fuzzing测试是什么？</h2><blockquote><p>Create random inputs, and see if they break things.</p></blockquote><p>说得简单、纯粹点，Fuzzing是一种软件测试方法，通过不断生成不同的输入，使被测程序崩溃、出错，以此改进程序本身的软件测试方法。由于Fuzzing的核心在于生成软件测试用例，因此这种方法又被称为<strong>生成软件测试</strong>。</p><h2 id="构建第一个fuzzer"><a href="#构建第一个fuzzer" class="headerlink" title="构建第一个fuzzer"></a>构建第一个fuzzer</h2><pre><code class="lang-py">import randomdef fuzzer(max_length=100, char_start=32, char_range=32):    &quot;&quot;&quot;A string of up to `max_length` characters       in the range [`char_start`, `char_start` + `char_range`]&quot;&quot;&quot;    string_length = random.randrange(0, max_length + 1)    out = &quot;&quot;    for i in range(0, string_length):        out += chr(random.randrange(char_start, char_start + char_range))    return outfuzzer()4$)&gt;(,-&amp;!$25;&gt;6=27= 5)9?300(.466&amp;(&#39;$*,,1:8&#39; ,$/99&gt;&#39;*=(</code></pre><p>此fuzzer的作用是生成一堆随机字符。要想只生成26个字母，那么</p><pre><code class="lang-py">fuzzer(100, ord(&#39;a&#39;), 26)ueffzgwltwmspvmowihhtjmgsixofnvntnqmr</code></pre><h2 id="Fuzzing关键的两个部件："><a href="#Fuzzing关键的两个部件：" class="headerlink" title="Fuzzing关键的两个部件："></a>Fuzzing关键的两个部件：</h2><h3 id="Fuzzer类"><a href="#Fuzzer类" class="headerlink" title="Fuzzer类"></a>Fuzzer类</h3><p><code>Fuzzer</code>，是所有<code>fuzzer</code>的基类，<code>RandomFuzzer</code> 是其简单实现。<code>Fuzzer</code> 的<code>fuzz()</code>接口返回一个字符串，字符串内容是根据不同实现逻辑而构造出来的。</p><p>比如Fuzzer的实现RandomFuzzer，其<code>fuzz()</code>就是随机生成的字符串。</p><pre><code class="lang-py">&gt;&gt;&gt; random_fuzzer = RandomFuzzer(min_length=10, max_length=20, char_start=65, char_range=26)&gt;&gt;&gt; random_fuzzer.fuzz()&#39;XGZVDDPZOOW&#39;</code></pre><p>Fuzzer的run()接口负责运行一个Runner对象。</p><p>下面是Fuzzer的代码架构：</p><pre><code class="lang-py">class Fuzzer(object):    def __init__(self):        pass    def fuzz(self):        &quot;&quot;&quot;Return fuzz input&quot;&quot;&quot;        return &quot;&quot;    def run(self, runner=Runner()):        &quot;&quot;&quot;Run `runner` with fuzz input&quot;&quot;&quot;        return runner.run(self.fuzz())    def runs(self, runner=PrintRunner(), trials=10):        &quot;&quot;&quot;Run `runner` with fuzz input, `trials` times&quot;&quot;&quot;        # Note: the list comprehension below does not invoke self.run() for subclasses        # return [self.run(runner) for i in range(trials)]        outcomes = []        for i in range(trials):            outcomes.append(self.run(runner))        return outcomes</code></pre><p>此时Fuzzer基类的fuzz()接口还没有功能。派生类RandomFuzzer则实现了fuzz()：</p><pre><code class="lang-py">class RandomFuzzer(Fuzzer):    def __init__(self, min_length=10, max_length=100,                 char_start=32, char_range=32):        &quot;&quot;&quot;Produce strings of `min_length` to `max_length` characters           in the range [`char_start`, `char_start` + `char_range`]&quot;&quot;&quot;        self.min_length = min_length        self.max_length = max_length        self.char_start = char_start        self.char_range = char_range    def fuzz(self):        string_length = random.randrange(self.min_length, self.max_length + 1)        out = &quot;&quot;        for i in range(0, string_length):            out += chr(random.randrange(self.char_start,                                        self.char_start + self.char_range))        return out</code></pre><p>有了RandomFuzzer，我们可以生成一些随机的字符串了。</p><pre><code class="lang-py">random_fuzzer = RandomFuzzer(min_length=20, max_length=20)for i in range(10):    print(random_fuzzer.fuzz())&#39;&gt;23&gt;33)(&amp;&quot;09.377.*3*+:5 ? (?1$4&lt;&gt;!?3&gt;.&#39;4+3/(3 (0%!&gt;!(+9%,#$/51$2964&gt;;)2417&lt;9&quot;2&amp;907.. !7:&amp;--&quot;=$7&#39;,7*(5=5&#39;.!*+&amp;&gt;&quot;)6%9)=,/?:&amp;5) &quot;;.0!=6&gt;3+&gt;)=,6&amp;,?:!#2))- ?:)=63&#39;-,)9#839%)?&amp;(0&lt;6(&quot;*;)4?!(49+8=-&#39;&amp;499%?&lt; &#39;</code></pre><h3 id="Runner类"><a href="#Runner类" class="headerlink" title="Runner类"></a>Runner类</h3><p><code>Runner</code>，是所有待测程序的基类。一个Fuzzer 与一个Runner搭配。</p><p>Runner类含有run(input)接口，负责接收input并执行，返回 (result, outcome)，result是Runner在运行时的信息和细节，而outcom代表着这次运行的结果。</p><p>运行结果为枚举对象 outcome，含义为程序运行结果，有(PASS, FAIL, or UNRESOLVED)三种可能。</p><ul><li>Runner.PASS：测试通过，run()输出正确。</li><li>Runner.FAIL：测试失败，结果错误。</li><li>Runner.UNRESOLVED：没有输出，这一般代表runner无法应对输入而崩溃。</li></ul><p>Runner的大体架构如下：</p><pre><code class="lang-py">class Runner(object):    # Test outcomes    PASS = &quot;PASS&quot;    FAIL = &quot;FAIL&quot;    UNRESOLVED = &quot;UNRESOLVED&quot;    def __init__(self):        &quot;&quot;&quot;Initialize&quot;&quot;&quot;        pass    def run(self, inp):        &quot;&quot;&quot;Run the runner with the given input&quot;&quot;&quot;        return (inp, Runner.UNRESOLVED)</code></pre><p>想要实现其他Runner，只需继承Runner即可。</p><pre><code class="lang-py">class PrintRunner(Runner):    def run(self, inp):        &quot;&quot;&quot;Print the given input&quot;&quot;&quot;        print(inp)        return (inp, Runner.UNRESOLVED)</code></pre><pre><code>p = PrintRunner()(result, outcome) = p.run(&quot;Some input&quot;)Some input</code></pre><p>对于PrintRunner，我们无法验证其结果，因此通通返回UNRESOLVED。</p><p>下面是一个Runner的派生类ProgramRunner的代码，此架构代表了大多数程序。</p><pre><code class="lang-py">class ProgramRunner(Runner):    def __init__(self, program):        &quot;&quot;&quot;Initialize.  `program` is a program spec as passed to `subprocess.run()`&quot;&quot;&quot;        self.program = program    def run_process(self, inp=&quot;&quot;):        &quot;&quot;&quot;Run the program with `inp` as input.  Return result of `subprocess.run()`.&quot;&quot;&quot;        return subprocess.run(self.program,                              input=inp,                              stdout=subprocess.PIPE,                              stderr=subprocess.PIPE,                              universal_newlines=True)    def run(self, inp=&quot;&quot;):        &quot;&quot;&quot;Run the program with `inp` as input.  Return test outcome based on result of `subprocess.run()`.&quot;&quot;&quot;        result = self.run_process(inp)        if result.returncode == 0:            outcome = self.PASS        elif result.returncode &lt; 0:            outcome = self.FAIL        else:            outcome = self.UNRESOLVED        return (result, outcome)</code></pre><h2 id="Fuzzing-实例"><a href="#Fuzzing-实例" class="headerlink" title="Fuzzing 实例"></a>Fuzzing 实例</h2><pre><code class="lang-py">cat = ProgramRunner(program=&quot;cat&quot;)cat.run(&quot;hello&quot;)random_fuzzer = RandomFuzzer(min_length=20, max_length=20)random_fuzzer.runs(cat, 10)[(CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&#39;3976%%&amp;+%6=(1)3&amp;3:&lt;9&#39;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&#39;33$#42$ 11=*%$20=&lt;.-&#39;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&#39;&quot;?&lt;\&#39;#8 &lt;/:*%9.--\&#39;97!&#39;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&quot;/0-#(03/!#60&#39;+6&gt;&amp;&amp;72&quot;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&quot;=,+:,6&#39;5:950+&gt;&lt;3(*()&quot;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&quot; 379+0?&#39;%3137=2:4605&quot;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&quot;02&gt;!$&lt;/&#39;*81.#&lt;/22&gt;+:&quot;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&quot;=-&lt;&#39;3-#88*%&amp;*9&lt; +1&amp;&amp;&quot;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&#39;2;;0=3&amp;6=8&amp;30&amp;&lt;-;?*;&#39;, stderr=&#39;&#39;),  &#39;PASS&#39;), (CompletedProcess(args=&#39;cat&#39;, returncode=0, stdout=&#39;/#05=*3($&gt;::#7!0=12+&#39;, stderr=&#39;&#39;),  &#39;PASS&#39;)]</code></pre>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Fuzzing</tag>
      
      <tag>Testing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】leetcode打卡：分治算法</title>
    <link href="/2020/08/19/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/"/>
    <url>/2020/08/19/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91leetcode%E6%89%93%E5%8D%A1%EF%BC%9A%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。<br><!--more---></p><h2 id="leetcode-50-Pow-x-n"><a href="#leetcode-50-Pow-x-n" class="headerlink" title="leetcode 50 Pow(x,n)"></a>leetcode 50 Pow(x,n)</h2><p><strong>题目描述</strong></p><p>实现 pow(x, n) ，即计算 x 的 n 次幂函数。</p><p><strong>算法描述</strong>：</p><p>Pow算法有快速幂实现方法。</p><p>快速幂，二进制取幂（Binary Exponentiation，也称平方法），是一个在 $O(\log(n))$ 的时间内计算 $a^n$ 的小技巧，而暴力的计算需要 $O(n)$ 的时间。而这个技巧也常常用在非计算的场景，因为它可以应用在任何具有结合律的运算中。其中显然的是它可以应用于模意义下取幂、矩阵幂等运算。</p><p>计算a的n次方表示将n个a连乘在一起。然而当a和n太大的时候，这种方法就不太适用了。</p><p>不过我们知道，$a^{b+c}=a^b\cdot a^c$，$a^{2b}=(a^b)^2$。</p><p>快速幂的想法是，我们将取幂的任务按照指数的 <strong>二进制表示</strong> 来分割成更小的任务。</p><p>我们将 n 表示为 2 进制，举一个例子：</p><p>$3^{13}=3^{(1101)_2}=3^8\cdot 3^4\cdot 3^1$</p><p>因此只需把n转化成二进制，然后分解成对应的权值即可简化计算。</p><p>为什么这样能简化计算？因为n的二进制形式长度最长只有$O(\log(n))$。原问题被我们转化成了形式相同的子问题的乘积。</p><p><strong>实现</strong>：</p><pre><code class="lang-cpp">class Solution &#123;public:    double myPow(double x, int n) &#123;        if (n &lt; 0) &#123;            return 1 / myPow(x, -n);        &#125;        double base = x;        double res = 1.0;        for (; n != 0; n &gt;&gt;= 1) &#123;            if (n &amp; 0x1) res *= base;            base *= base;        &#125;        return res;    &#125;&#125;;</code></pre><p>上面的代码在循环的过程中将二进制位为 1 时对应的幂累乘到答案中。</p><blockquote><p><a href="https://oi-wiki.org/">https://oi-wiki.org/</a></p></blockquote><h2 id="leetcode-53-最大子序和"><a href="#leetcode-53-最大子序和" class="headerlink" title="leetcode 53 最大子序和"></a>leetcode 53 最大子序和</h2><p><strong>题目描述</strong></p><p>给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。</p><p><strong>算法描述</strong>：</p><p>给定一个数组，下标从start到end，即[start,end]。要求这其中的最大连续子数组之和。</p><p>分解成如下子问题：查找[start,mid]中的最大连续子数组之和，查找[mid,end]中的最大连续子数组之和，最后比较二者哪个更大。</p><p>但是最大连续子数组可能是跨越mid的数组，所以递归的时候要额外计算mid及其周围元素之和的最大值，用此值与前面两个区间的值比较。</p><p><strong>实现</strong></p><pre><code class="lang-cpp">class Solution &#123;public:    int maxSubArray(vector&lt;int&gt;&amp; nums) &#123;        return find(nums, 0, nums.size()-1);    &#125;    int find(vector&lt;int&gt;&amp; nums, int start, int end) &#123;        if (start == end) return nums[start];        if (start &gt; end) return INT_MIN;        int mid = start + (end - start) / 2;        int left_max = 0, right_max = 0, ml = 0, mr = 0;        left_max = find(nums, start, mid-1);        right_max = find(nums, mid+1, end);        for (int i = mid-1, sum = 0; i &gt;= start; --i) &#123;            sum += nums[i];            if (sum &gt; ml) ml = sum;        &#125;        for (int i = mid+1, sum = 0; i &lt;= end; ++i) &#123;            sum += nums[i];            if (sum &gt; mr) mr = sum;        &#125;        return max(max(left_max, right_max), ml + mr + nums[mid]);    &#125;&#125;;</code></pre><p>在代码中，<code>left_max</code>为[start,mid)区间内的最大连续子数组和，<code>right_max</code>为(mid,end]区间内的最大连续子数组和。</p><p>而跨越中心mid的计算方法，则是通过两个for循环，从mid开始一个往前遍历得到最大值<code>ml</code>，一个往后遍历得到<code>mr</code>，最后得到<code>ml + mr + nums[mid]</code>即可。</p><p>结果为三者的最大值。</p><blockquote><p><a href="https://www.bilibili.com/video/BV19t411k7jR">https://www.bilibili.com/video/BV19t411k7jR</a></p></blockquote><h2 id="leetcode-169-多数元素"><a href="#leetcode-169-多数元素" class="headerlink" title="leetcode 169 多数元素"></a>leetcode 169 多数元素</h2><p><strong>题目描述</strong></p><p>给定一个大小为 n 的数组，找到其中的多数元素。多数元素是指在数组中出现次数大于 ⌊ n/2 ⌋ 的元素。</p><p>你可以假设数组是非空的，并且给定的数组总是存在多数元素。</p><p><strong>算法描述</strong>：</p><p>如果数 a 是数组 nums 的众数，如果我们将 nums 分成两部分，那么 a 必定是至少一部分的众数。</p><p>这样一来，我们就可以使用分治法解决这个问题：将数组分成左右两部分，分别求出左半部分的众数 a1 以及右半部分的众数 a2，随后在 a1 和 a2 中选出正确的众数。</p><p><strong>实现</strong></p><p>遍历法：</p><pre><code class="lang-cpp">class Solution &#123;public:    int majorityElement(vector&lt;int&gt;&amp; nums) &#123;        int res = -1;        int count = 0;        for (auto c : nums) &#123;            if (!count) &#123;                res = c;            &#125;            if (res == c) &#123;                ++count;            &#125; else &#123;                --count;            &#125;        &#125;        return res;    &#125;&#125;;</code></pre><p>分治法：</p><pre><code class="lang-cpp">class Solution &#123;public:    int majorityElement(vector&lt;int&gt;&amp; nums) &#123;        return majorityElement_inrange(nums, 0, nums.size()-1);    &#125;private:    int majorityElement_inrange(vector&lt;int&gt;&amp; nums, int lo, int hi) &#123;        if (lo == hi) return nums[lo];        if (lo &gt; hi) return -1;        int mid = lo + (hi - lo) / 2;        int left_maj = majorityElement_inrange(nums, lo, mid);        int right_maj = majorityElement_inrange(nums, mid+1, hi);        return (count_in_range(nums, lo, hi, left_maj) &gt; count_in_range(nums, lo, hi, right_maj)) ? left_maj : right_maj;    &#125;    int count_in_range(vector&lt;int&gt;&amp; nums, int lo, int hi, int val) &#123;        int count = 0;        for (int i = lo; i &lt;= hi; ++i) &#123;            if (nums[i] == val) count++;        &#125;        return count;    &#125;&#125;;</code></pre><blockquote><p><a href="https://leetcode-cn.com/problems/majority-element/solution/duo-shu-yuan-su-by-leetcode-solution/">https://leetcode-cn.com/problems/majority-element/solution/duo-shu-yuan-su-by-leetcode-solution/</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>algorithm</tag>
      
      <tag>leetcode</tag>
      
      <tag>Datawhale</tag>
      
      <tag>divide-and-conquer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】使用tf.data对预处理过程优化</title>
    <link href="/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8tf-data%E5%AF%B9%E9%A2%84%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E4%BC%98%E5%8C%96/"/>
    <url>/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8tf-data%E5%AF%B9%E9%A2%84%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<p>本文是关于 <code>tf.data</code> 介绍的第二篇，主要介绍一些数据预处理方面的优化方法，诸如并行化预处理映射函数、使用缓存等。<br><!--more---></p><p>构建一个机器学习模型时，由于数据预处理过程不能使用GPU进行加速，因此格外耗时。背后的原因可能是CPU、网络或者缓存等复杂的因素。因此要研究如何提升数据预处理的效率，首先需要控制实验的变量。想实现这一点，构造一个虚假的数据集比较可行。</p><p>通过构建一个虚假的数据集，从<code>tf.data.Dataset</code>继承的类，称为<code>ArtificialDataset</code>。该数据集模拟三件事：</p><ol><li>生成<code>num_samples</code>样本（默认为3）</li><li>在第一个模拟打开文件的项目之前睡眠一段时间</li><li>在产生每个项目以模拟从文件读取数据之前先休眠一段时间</li></ol><pre><code class="lang-py">class ArtificialDataset(tf.data.Dataset):    def _generator(num_samples):        # Opening the file        time.sleep(0.03)        for sample_idx in range(num_samples):            # Reading data (line, record) from the file            time.sleep(0.015)            yield (sample_idx,)    def __new__(cls, num_samples=3):        return tf.data.Dataset.from_generator(            cls._generator,            output_types=tf.dtypes.int64,            output_shapes=(1,),            args=(num_samples,)        )</code></pre><p>构建 <code>benchmark</code> ，通过模拟训练的方式，计算该数据预处理模式的耗时：</p><pre><code class="lang-py">def benchmark(dataset, num_epochs=2):    start_time = time.perf_counter()    for epoch_num in range(num_epochs):        for sample in dataset:            # Performing a training step            time.sleep(0.01)    tf.print(&quot;Execution time:&quot;, time.perf_counter() - start_time)</code></pre><p>我们先来不加任何优化地运行一次benchmark：</p><pre><code class="lang-py">benchmark(ArtificialDataset())Execution time: 0.33306735700000445</code></pre><p>此时模型的执行时间图如图所示：<br><img src="https://www.tensorflow.org/guide/images/data_performance/naive.svg" alt="Naive"></p><p>时间消耗是这样的：先是打开文件，然后从文件中获取数据项，然后使用数据进行训练。这种执行方式，当数据进行预处理，模型就空闲；当模型开始训练，管道又空闲下来了。预处理和训练这两部分明显可以重叠。</p><p><code>tf.data</code> API提供了<code>tf.data.Dataset.prefetch</code>转换。它可以用于将数据生成时间与数据消耗时间分开。转换使用后台线程和内部缓冲区预取元素。要预取的元素数量应等于（或可能大于）单个训练步骤消耗的批次数量。将预取的元素数量设置为<code>tf.data.experimental.AUTOTUNE</code> ，这将提示<code>tf.data</code>运行时在运行时动态调整值。</p><pre><code class="lang-py">benchmark(    ArtificialDataset()    .prefetch(tf.data.experimental.AUTOTUNE))Execution time: 0.20504431599999862</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/prefetched.svg" alt="Prefetched"></p><p>时间有了明显优化，因为数据的生产和消费有了些许重叠。</p><p>在实际工作中，输入数据可以远程存储在其他计算机上。在本地和远程存储之间存在以下差异：</p><ol><li>到达第一个字节的时间：从远程存储读取文件的第一个字节所花费的时间要比从本地存储中读取文件的时间长几个数量级。</li><li>读取吞吐量：虽然远程存储通常提供较大的聚合带宽，但是读取单个文件可能只能使用此带宽的一小部分。</li></ol><p>此外，一旦将原始字节加载到内存中，可能还需要对数据进行反序列化和/或解密，这需要进行额外的计算。不管数据是本地存储还是远程存储，都存在这种开销，但是<strong>如果数据没有有效地预取，则在远程情况下会更糟</strong>。</p><p>可以使用<code>tf.data.Dataset.interleave</code>转换来<strong>并行化数据加载步骤</strong>， <code>cycle_length</code> 表明可以一起处理的数据集数量， <code>num_parallel_calls</code> 则是并行度。</p><pre><code class="lang-py">benchmark(    tf.data.Dataset.range(2)    .interleave(        ArtificialDataset,        num_parallel_calls=tf.data.experimental.AUTOTUNE    )Execution time: 0.18243273299958673</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/parallel_interleave.svg" alt="Parallel interleave"></p><p>该图可以显示<code>interleave</code>变换的行为，从两个可用的数据集中获取样本。这次，两个数据集的读取并行进行，从而减少了全局数据处理时间</p><h2 id="并行预处理操作"><a href="#并行预处理操作" class="headerlink" title="并行预处理操作"></a>并行预处理操作</h2><p>在准备数据时，可能需要对输入元素进行预处理。可以使用<code>tf.data.Dataset.map(f)</code>转换，其含义为将某个转换<code>f</code>作用于数据集<code>Dataset</code>中的每个元素。这里有个很重要的前提条件，由于输入元素彼此独立，因此预处理可以跨多个CPU内核并行化。因此<code>map</code>转换也提供<code>num_parallel_calls</code>参数来指定并行度。关于并行度的选择上，<code>map</code>转换支持<code>tf.data.experimental.AUTOTUNE</code>，而不必人工定义。</p><p>首先定义伪操作：</p><pre><code class="lang-py">def mapped_function(s):    # Do some hard pre-processing    tf.py_function(lambda: time.sleep(0.03), [], ())    return s</code></pre><p>我们来测试伪操作，此时没有任何并行优化：</p><pre><code class="lang-py">benchmark(    ArtificialDataset()    .map(mapped_function))Execution time: 0.4592052289999913</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/sequential_map.svg" alt="Sequential mapping"></p><p>现在，使用相同的预处理功能，但将其并行应用于多个样本。</p><pre><code class="lang-py">benchmark(    ArtificialDataset()    .map(        mapped_function,        num_parallel_calls=tf.data.experimental.AUTOTUNE    ))Execution time: 0.3045882669994171</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/parallel_map.svg" alt="Parallel mapping"></p><p>可以在图上看到预处理步骤重叠，从而减少了单次迭代的总时间。</p><p><code>tf.data.Dataset.cache</code>转换可以在内存中或本地存储上缓存数据集。这样可以避免在每个epoch执行某些重复性操作（例如打开文件和读取数据）。</p><pre><code class="lang-py">benchmark(    ArtificialDataset()    .map(  # Apply time consuming operations before cache        mapped_function    ).cache(    ),    5)Execution time: 0.3795637040002475</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/cached_dataset.svg" alt="Cached dataset"></p><p>第一个epoch执行一次cache之前的转换（例如文件打开和数据读取）。下一个epoch将重用cache转换所缓存的数据。</p><p>这里涉及到一个<code>map</code>和<code>cache</code>操作谁先谁后的问题。有一个原则，如果<code>map</code>操作很复杂、昂贵，那么先<code>map</code>再<code>cache</code>，下次不用<code>map</code>了。如果<code>cache</code>过大而无法放入缓冲区，则先<code>cache</code>后<code>map</code>，或者试图采用一些数据预处理方法以减少资源使用。</p><h2 id="向量化数据预处理操作"><a href="#向量化数据预处理操作" class="headerlink" title="向量化数据预处理操作"></a>向量化数据预处理操作</h2><p>所谓向量化，即使得<code>mapping</code>操作能够一次处理一<code>batch</code>数据。这样做肯定可以加速，因为避免了繁杂的数据读取时间。对用户定义的函数进行向量化处理，并且对数据集应用<code>batch</code>转换再进入<code>mapping</code>。在某种情况下，这个做法非常有用。</p><p>首先定义一个数据集操作<code>increment</code>，负责把每个元素的值+1。另外之前的例子里面使用了毫秒级别的<code>sleep</code>操作，这会掩盖我们优化的结果。这次我们把它拿掉。</p><p>下面是未经向量化优化的<code>increment</code>操作耗时：</p><pre><code class="lang-py">fast_dataset = tf.data.Dataset.range(10000)def fast_benchmark(dataset, num_epochs=2):    start_time = time.perf_counter()    for _ in tf.data.Dataset.range(num_epochs):        for _ in dataset:            pass    tf.print(&quot;Execution time:&quot;, time.perf_counter() - start_time)def increment(x):    return x+1fast_benchmark(    fast_dataset    # Apply function one item at a time    .map(increment)    # Batch    .batch(256))Execution time: 0.7625284370005829</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/scalar_map.svg" alt="Scalar map"></p><p>与之对比，经过向量化后，耗时明显减少：</p><pre><code class="lang-py">fast_benchmark(    fast_dataset    .batch(256)    # Apply function on a batch of items    # The tf.Tensor.__add__ method already handle batches    .map(increment))Execution time: 0.04735958700075571</code></pre><p><img src="https://www.tensorflow.org/guide/images/data_performance/vectorized_map.svg" alt="Vectorized map"></p><h2 id="减少内存占用"><a href="#减少内存占用" class="headerlink" title="减少内存占用"></a>减少内存占用</h2><p>许多转换（包括interleave ， prefetch和shuffle ）各自维护内部缓冲区。如果传递给map转换的用户定义函数更改了元素的大小，则映射转换的顺序以及缓冲元素的转换会影响内存使用。</p><p>通常，我们建议选择导致内存占用减少的顺序，除非需要不同的顺序才能提高性能。</p><p>对于缓存，我们建议除非转换后的数据难以保存到缓冲区，否则一律先<code>map</code>再<code>cache</code>。如果你有两个<code>map</code>，其中一个比较耗时<code>time_consuming_mapping</code>，另一个比较耗内存<code>memory_consuming_mapping</code>，那么其实你可以将其拆分成两部分；</p><pre><code class="lang-py">dataset.map(time_consuming_mapping).cache().map(memory_consuming_mapping)</code></pre><p>这样，耗时部分仅在第一个epoch执行，并且避免了使用过多的缓存空间。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>使用<code>tf.data</code>，并采用合理的优化手段，就能让你的数据预处理过程节约很多时间。这些手段有：</p><ul><li>使用<code>prefetch</code>转换可以使生产者和消费者的工作重叠。</li><li>使用<code>interleave</code>变换并行化数据读取变换。</li><li>通过设置<code>num_parallel_calls</code>参数来并行化<code>map</code>转换 。</li><li>在第一个epoch使用<code>cache</code>转换将数据缓存在内存中</li><li>向量化传递给<code>map</code>转换的用户定义函数</li><li>应用<code>interleave</code> ， <code>prefetch</code>和<code>shuffle</code>转换时， 逐渐减少内存使用 。</li></ul>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tensorflow</tag>
      
      <tag>preprocessing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】tf.data基本使用</title>
    <link href="/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91tf-data%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"/>
    <url>/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91tf-data%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>使用 <code>tf.data</code> API 可以轻松处理大量数据，支持多样化的数据格式，还可以方便执行复杂的转换。本文介绍了不同类别源数据转化为 <code>tf.data.Dataset</code> 的方法，以及 <code>Dataset</code> 常见的预处理方法。<br><!--more---></p><h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>最近几年我们在机器学习的加速计算领域取得了一些突破。虽然我们进行指标运算和矩阵运算所需的时间大大减少了，但是提供数据加速的CPU却没能跟上相应的步伐，这就成为了预处理中的瓶颈。我们本以为可以通过构建更复杂的模型来减少对硬件的需求，但是CPU的效率还是取决于他们拥有多少RAM。</p><p><img src="/2020/08/15/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91tf-data%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/2020-08-15-22-34-14.png" alt></p><p>对于一些数据集很大的问题，解决问题的方法除了提升CPU的性能之外，还有对数据集进行分批次处理，但是分批预处理需要不断进行训练和合并操作，这对预处理增加了难度。<code>tf.data</code> 可以帮助解决数据集过大造成的预处理瓶颈问题。</p><p><code>tf.data</code> 是tensorflow的数据预处理框架。它速度快、灵活且易于使用。</p><p>假设你的数据以TFRecord格式存在磁盘，需要将这些数据读取、处理并训练模型，可以先通过<code>TFRecordDataset</code> 开始处理这些数据：</p><pre><code class="lang-py">dataset = tf.data.TFRecordDataset(&#39;.../*.tfrecord&#39;)</code></pre><p>然后执行一些数据预处理操作，这个过程可能很消耗资源：</p><pre><code class="lang-py">dataset = dataset.map(expensive_preprocess)</code></pre><p>随后你需要打乱数据，以降低模型训练过程中过拟合的可能性：</p><pre><code class="lang-py">dataset = dataset.shuffle(buffer_size=1024)</code></pre><p>然后我们需要分批次，以令模型加速计算</p><pre><code class="lang-py">dataset = dataset.batch(batch_size=32)</code></pre><p>最终要完成pipeline搭建，这样可以保证模型在运行一个batch的数据时，另一批数据进行预处理以提升效率。</p><pre><code class="lang-py">dataset = dataset.prefetch()</code></pre><p>将数据输入到模型，我们可以开始训练了。</p><pre><code class="lang-py">model = tf.keras.Model(...)model.fit(dataset)</code></pre><p>上面就是数据从读取到处理到训练的全部流程，称之为管道（pipeline）。</p><p>处理大量原始数据，要经过多次函数变换，这些函数变换都是可重用的。使用<code>tf.data</code>将这些变换整理成管道，一方面可以简化复杂输入的预处理过程，另一方面，由于<code>Dataset</code> 对象可迭代，可以执行分批处理。使用<code>tf.data.Dataset</code>可以方便地整合操作、构造数据集。</p><p>有两种方法构造可供训练使用的<code>Dataset</code>数据集：</p><ol><li>从文件、内存中直接构建<code>Dataset</code></li><li>从其他<code>Dataset</code>中转化</li></ol><p>如果打算从内存中读取数据构建 <code>Dataset</code> ，有 <code>tf.data.Dataset.from_tensors()</code> 和 <code>tf.data.Dataset.from_tensor_slices()</code> 可供选择；如果打算从 <code>TFRecord</code> 格式的文件中读取数据，可以调用 <code>tf.data.TFRecordDataset()</code> 。</p><p>当 <code>Dataset</code> 对象构建好了之后，通过使用 <code>Dataset.map()</code> 为其中每个元素施加变换、使用 <code>Dataset.batch()</code> 为整批元素添加变换等等对数据进行预处理。</p><h2 id="从内存或文件中构造Dataset"><a href="#从内存或文件中构造Dataset" class="headerlink" title="从内存或文件中构造Dataset"></a>从内存或文件中构造<code>Dataset</code></h2><h3 id="从内存中的array构造Dataset"><a href="#从内存中的array构造Dataset" class="headerlink" title="从内存中的array构造Dataset"></a>从内存中的array构造<code>Dataset</code></h3><p>如果你的所有数据都在内存中，那么最简单构造 <code>Dataset</code> 的方式就是，先将其利用 <code>tf.Tensor</code> 转成tensor，后使用<code>Dataset.from_tensor_slices()</code>。</p><pre><code class="lang-python">train, test = tf.keras.datasets.fashion_mnist.load_data()images, labels = trainimages = images/255dataset = tf.data.Dataset.from_tensor_slices((images, labels))dataset</code></pre><h3 id="从生成器构造Dataset"><a href="#从生成器构造Dataset" class="headerlink" title="从生成器构造Dataset"></a>从生成器构造<code>Dataset</code></h3><p>你也可以利用 <code>Dataset.from_generator</code> 从Python的生成器来构造 <code>Dataset</code> ，比如从 <code>preprocessing.image.ImageDataGenerator</code> 构造 <code>Dataset</code>。但这种方法受制于Python的GIL，因此效率不会太高。</p><p>首先下载花朵图片数据集，一共3670张花朵图片，分成五个类别。</p><pre><code class="lang-python">flowers = tf.keras.utils.get_file(    &#39;flower_photos&#39;,    &#39;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&#39;,    untar=True)</code></pre><p>利用 <code>preprocessing.image.ImageDataGenerator</code> 定义数据增强操作，然后将其套用到花朵数据集上。</p><pre><code class="lang-python">img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)images, labels = next(img_gen.flow_from_directory(flowers))</code></pre><p>最后构造 <code>Dataset</code> 。</p><pre><code class="lang-python">ds = tf.data.Dataset.from_generator(    img_gen.flow_from_directory, args=[flowers],     output_types=(tf.float32, tf.float32),     output_shapes=([32,256,256,3], [32,5]))ds&lt;FlatMapDataset shapes: ((32, 256, 256, 3), (32, 5)), types: (tf.float32, tf.float32)&gt;</code></pre><h3 id="从TFRecord格式文件构造Dataset"><a href="#从TFRecord格式文件构造Dataset" class="headerlink" title="从TFRecord格式文件构造Dataset"></a>从<code>TFRecord</code>格式文件构造<code>Dataset</code></h3><p>有些时候数据不在内存中，而是以特定格式存在磁盘上，比如 <code>TFRecord</code> 格式。这种情况我们可以使用 <code>tf.data.TFRecordDataset</code> 作为数据管道的一部分。</p><pre><code class="lang-python"># Creates a dataset that reads all of the examples from two files.fsns_test_file = tf.keras.utils.get_file(&quot;fsns.tfrec&quot;, &quot;https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001&quot;)dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])</code></pre><p><code>tf.data.TFRecordDataset</code> API中，<code>filenames</code> 的输入很灵活，既可以是字符串，表明一个文件；也可以是字符串列表，表明多个文件。</p><h3 id="从-txt格式文件构造Dataset"><a href="#从-txt格式文件构造Dataset" class="headerlink" title="从.txt格式文件构造Dataset"></a>从<code>.txt</code>格式文件构造<code>Dataset</code></h3><p>如果是 <code>.txt</code> 格式，那么采用 <code>tf.data.TextLineDataset</code> 也可转成 <code>Dataset</code>。</p><pre><code class="lang-python">directory_url = &#39;https://storage.googleapis.com/download.tensorflow.org/data/illiad/&#39;file_names = [&#39;cowper.txt&#39;, &#39;derby.txt&#39;, &#39;butler.txt&#39;]file_paths = [    &#39;cowper.txt&#39;,&#39;derby.txt&#39;,&#39;butler.txt&#39;]dataset = tf.data.TextLineDataset(file_paths)</code></pre><p>看一下第一个文件的前5行：</p><pre><code class="lang-python">for line in dataset.take(5):  print(line.numpy())b&quot;\xef\xbb\xbfAchilles sing, O Goddess! Peleus&#39; son;&quot;b&#39;His wrath pernicious, who ten thousand woes&#39;b&quot;Caused to Achaia&#39;s host, sent many a soul&quot;b&#39;Illustrious into Ades premature,&#39;b&#39;And Heroes gave (so stood the will of Jove)&#39;</code></pre><p>前五行都是 <code>cowper.txt</code> 中的。如果我们希望生成的 <code>Dataset</code> 能够轮流选取三个文件中的元素，可以在构造之初，使用 <code>Dataset.interleave</code> ，并设置 <code>cycle_length</code>：</p><pre><code class="lang-python">files_ds = tf.data.Dataset.from_tensor_slices(file_paths)lines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length=3)for i, line in enumerate(lines_ds.take(9)):  if i % 3 == 0:    print()  print(i, line.numpy())0 b&quot;\xef\xbb\xbfAchilles sing, O Goddess! Peleus&#39; son;&quot;1 b&quot;\xef\xbb\xbfOf Peleus&#39; son, Achilles, sing, O Muse,&quot;2 b&#39;\xef\xbb\xbfSing, O goddess, the anger of Achilles son of Peleus, that brought&#39;3 b&#39;His wrath pernicious, who ten thousand woes&#39;4 b&#39;The vengeance, deep and deadly; whence to Greece&#39;5 b&#39;countless ills upon the Achaeans. Many a brave soul did it send&#39;6 b&quot;Caused to Achaia&#39;s host, sent many a soul&quot;7 b&#39;Unnumbered ills arose; which many a soul&#39;8 b&#39;hurrying down to Hades, and many a hero did it yield a prey to dogs and&#39;</code></pre><p>有的时候我们不希望录入文件的第一行，或者只要文件中满足要求的特定行，可以分别使用 <code>Dataset.skip()</code> 和 <code>Dataset.filter()</code> 。比如下面的泰坦尼克数据集，去掉第一行后，筛选生存下来的人。</p><pre><code class="lang-python">titanic_file = tf.keras.utils.get_file(&quot;train.csv&quot;, &quot;https://storage.googleapis.com/tf-datasets/titanic/train.csv&quot;)titanic_lines = tf.data.TextLineDataset(titanic_file)for line in titanic_lines.take(10):  print(line.numpy())b&#39;survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone&#39;b&#39;0,male,22.0,1,0,7.25,Third,unknown,Southampton,n&#39;b&#39;1,female,38.0,1,0,71.2833,First,C,Cherbourg,n&#39;b&#39;1,female,26.0,0,0,7.925,Third,unknown,Southampton,y&#39;b&#39;1,female,35.0,1,0,53.1,First,C,Southampton,n&#39;b&#39;0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y&#39;b&#39;0,male,2.0,3,1,21.075,Third,unknown,Southampton,n&#39;b&#39;1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n&#39;b&#39;1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n&#39;b&#39;1,female,4.0,1,1,16.7,Third,G,Southampton,n&#39;def survived(line):  return tf.not_equal(tf.strings.substr(line, 0, 1), &quot;0&quot;)survivors = titanic_lines.skip(1).filter(survived)for line in survivors.take(10):  print(line.numpy())b&#39;1,female,38.0,1,0,71.2833,First,C,Cherbourg,n&#39;b&#39;1,female,26.0,0,0,7.925,Third,unknown,Southampton,y&#39;b&#39;1,female,35.0,1,0,53.1,First,C,Southampton,n&#39;b&#39;1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n&#39;b&#39;1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n&#39;b&#39;1,female,4.0,1,1,16.7,Third,G,Southampton,n&#39;b&#39;1,male,28.0,0,0,13.0,Second,unknown,Southampton,y&#39;b&#39;1,female,28.0,0,0,7.225,Third,unknown,Cherbourg,y&#39;b&#39;1,male,28.0,0,0,35.5,First,A,Southampton,y&#39;b&#39;1,female,38.0,1,5,31.3875,Third,unknown,Southampton,n&#39;</code></pre><h3 id="从csv格式构造Dataset"><a href="#从csv格式构造Dataset" class="headerlink" title="从csv格式构造Dataset"></a>从<code>csv</code>格式构造<code>Dataset</code></h3><p>除了<code>TFRecord</code>和<code>txt</code>格式，还有<code>csv</code>格式也很流行。<code>csv</code>格式能够以纯文本保存表格数据。<code>pandas</code>的<code>to_csv</code>是将<code>csv</code>搬运到内存的良好工具。</p><pre><code class="lang-python">titanic_file = tf.keras.utils.get_file(&quot;train.csv&quot;, &quot;https://storage.googleapis.com/tf-datasets/titanic/train.csv&quot;)df = pd.read_csv(titanic_file, index_col=None)df.head()titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))for feature_batch in titanic_slices.take(1):  for key, value in feature_batch.items():    print(&quot;  &#123;!r:20s&#125;: &#123;&#125;&quot;.format(key, value))</code></pre><p>当然<code>tf.data</code>强大之处在于可以处理<code>pandas</code>处理不了的文件大小。<code>experimental.make_csv_dataset</code>函数是用于读取csv文件集的高层接口，它可以自动推导每个<code>column</code>的文件类型。</p><pre><code class="lang-python">titanic_batches = tf.data.experimental.make_csv_dataset(    titanic_file, batch_size=4,    label_name=&quot;survived&quot;)</code></pre><p>查看第一个<code>batch</code>的内容。</p><pre><code class="lang-python">for feature_batch, label_batch in titanic_batches.take(1):  print(&quot;&#39;survived&#39;: &#123;&#125;&quot;.format(label_batch))  print(&quot;features:&quot;)  for key, value in feature_batch.items():    print(&quot;  &#123;!r:20s&#125;: &#123;&#125;&quot;.format(key, value))</code></pre><p>如果只需要<code>csv</code>的某一列，那么可以使用<code>select_columns</code>参数。</p><pre><code class="lang-python">titanic_batches = tf.data.experimental.make_csv_dataset(    titanic_file, batch_size=4,    label_name=&quot;survived&quot;, select_columns=[&#39;class&#39;, &#39;fare&#39;, &#39;survived&#39;])for feature_batch, label_batch in titanic_batches.take(1):  print(&quot;&#39;survived&#39;: &#123;&#125;&quot;.format(label_batch))  for key, value in feature_batch.items():    print(&quot;  &#123;!r:20s&#125;: &#123;&#125;&quot;.format(key, value))&#39;survived&#39;: [1 1 1 0]features:  &#39;sex&#39;               : [b&#39;female&#39; b&#39;female&#39; b&#39;male&#39; b&#39;female&#39;]  &#39;age&#39;               : [35. 31. 45. 28.]  &#39;n_siblings_spouses&#39;: [0 1 0 8]  &#39;parch&#39;             : [0 1 0 2]  &#39;fare&#39;              : [512.3292  20.525    8.05    69.55  ]  &#39;class&#39;             : [b&#39;First&#39; b&#39;Third&#39; b&#39;Third&#39; b&#39;Third&#39;]  &#39;deck&#39;              : [b&#39;unknown&#39; b&#39;unknown&#39; b&#39;unknown&#39; b&#39;unknown&#39;]  &#39;embark_town&#39;       : [b&#39;Cherbourg&#39; b&#39;Southampton&#39; b&#39;Southampton&#39; b&#39;Southampton&#39;]  &#39;alone&#39;             : [b&#39;y&#39; b&#39;n&#39; b&#39;y&#39; b&#39;n&#39;]</code></pre><p>还有一个底层的<code>experimental.CsvDataset</code>类，它可以更精细的控制读取<code>csv</code>的过程。不支持列类型推断。</p><pre><code class="lang-python">titanic_types  = [tf.int32, tf.string, tf.float32, tf.int32, tf.int32, tf.float32, tf.string, tf.string, tf.string, tf.string] dataset = tf.data.experimental.CsvDataset(titanic_file, titanic_types , header=True)for line in dataset.take(10):  print([item.numpy() for item in line])[0, b&#39;male&#39;, 22.0, 1, 0, 7.25, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 38.0, 1, 0, 71.2833, b&#39;First&#39;, b&#39;C&#39;, b&#39;Cherbourg&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 26.0, 0, 0, 7.925, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;y&#39;][1, b&#39;female&#39;, 35.0, 1, 0, 53.1, b&#39;First&#39;, b&#39;C&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][0, b&#39;male&#39;, 28.0, 0, 0, 8.4583, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Queenstown&#39;, b&#39;y&#39;][0, b&#39;male&#39;, 2.0, 3, 1, 21.075, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 27.0, 0, 2, 11.1333, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 14.0, 1, 0, 30.0708, b&#39;Second&#39;, b&#39;unknown&#39;, b&#39;Cherbourg&#39;, b&#39;n&#39;][1, b&#39;female&#39;, 4.0, 1, 1, 16.7, b&#39;Third&#39;, b&#39;G&#39;, b&#39;Southampton&#39;, b&#39;n&#39;][0, b&#39;male&#39;, 20.0, 0, 0, 8.05, b&#39;Third&#39;, b&#39;unknown&#39;, b&#39;Southampton&#39;, b&#39;y&#39;]</code></pre><p><code>CsvDataset</code>还可以指定每列的默认值，供元素为空时填充。</p><p>在Colab中，直接书写<code>csv</code>文件：</p><pre><code class="lang-bash">%%writefile missing.csv1,2,3,4,2,3,41,,3,41,2,,41,2,3,,,,</code></pre><p>设置每列默认值：</p><pre><code class="lang-python"># Creates a dataset that reads all of the records from two CSV files, each with# four float columns which may have missing values.record_defaults = [999,999,999,999]dataset = tf.data.experimental.CsvDataset(&quot;missing.csv&quot;, record_defaults)dataset = dataset.map(lambda *items: tf.stack(items))for line in dataset:  print(line.numpy())[1 2 3 4][999   2   3   4][  1 999   3   4][  1   2 999   4][  1   2   3 999][999 999 999 999]</code></pre><p>你也可以选择删除<code>header</code>，或者指定某列输出</p><pre><code class="lang-python"># Creates a dataset that reads all of the records from two CSV files with# headers, extracting float data from columns 2 and 4.record_defaults = [999, 999] # Only provide defaults for the selected columnsdataset = tf.data.experimental.CsvDataset(&quot;missing.csv&quot;, record_defaults, select_cols=[1, 3])dataset = dataset.map(lambda *items: tf.stack(items))for line in dataset:  print(line.numpy())[2 4][2 4][999   4][2 4][  2 999][999 999]</code></pre><h3 id="从文件夹中的每个文件构造Dataset"><a href="#从文件夹中的每个文件构造Dataset" class="headerlink" title="从文件夹中的每个文件构造Dataset"></a>从文件夹中的每个文件构造<code>Dataset</code></h3><p>如果每个单独的文件都是一个数据项（比如图片数据集），这样的数据集如何整理？</p><pre><code class="lang-python">flowers_root = tf.keras.utils.get_file(    &#39;flower_photos&#39;,    &#39;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&#39;,    untar=True)flowers_root = pathlib.Path(flowers_root)list_ds = tf.data.Dataset.list_files(str(flowers_root/&#39;*/*&#39;))for f in list_ds.take(5):  print(f.numpy())b&#39;/root/.keras/datasets/flower_photos/dandelion/8720503800_cab5c62a34.jpg&#39;b&#39;/root/.keras/datasets/flower_photos/dandelion/16510864164_3afa8ac37f.jpg&#39;b&#39;/root/.keras/datasets/flower_photos/tulips/7136973281_b2a935ce20.jpg&#39;b&#39;/root/.keras/datasets/flower_photos/sunflowers/14623719696_1bb7970208_n.jpg&#39;b&#39;/root/.keras/datasets/flower_photos/dandelion/4560663938_3557a1f831.jpg&#39;</code></pre><p>通过使用<code>tf.io.read_file</code>读取数据，并从路径中提取<code>label</code>，返回<code>(image, label)</code>数据对。</p><pre><code class="lang-python">def process_path(file_path):  label = tf.strings.split(file_path, os.sep)[-2]  return tf.io.read_file(file_path), labellabeled_ds = list_ds.map(process_path)for image_raw, label_text in labeled_ds.take(1):  print(repr(image_raw.numpy()[:100]))  print()  print(label_text.numpy())b&#39;\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x01\x03|\x03|\x00\x00\xff\xe2\x0cXICC_PROFILE\x00\x01\x01\x00\x00\x0cHLino\x02\x10\x00\x00mntrRGB XYZ \x07\xce\x00\x02\x00\t\x00\x06\x001\x00\x00acspMSFT\x00\x00\x00\x00IEC sRGB\x00\x00\x00\x00\x00\x00&#39;b&#39;sunflowers&#39;</code></pre><h2 id="批处理数据集元素"><a href="#批处理数据集元素" class="headerlink" title="批处理数据集元素"></a>批处理数据集元素</h2><p>批处理的最简单形式是将数据集的<code>n</code>个连续元素堆叠为单个元素。</p><p>batched_dataset = dataset.batch(4)</p><BatchDataset shapes: ((none,), (none,)), types: (tf.int64, tf.int64)><p>最后一个批次可能未满，使用<code>drop_remainder</code>参数忽略最后一批，使得<code>shape</code>完整：</p><p>batched_dataset = dataset.batch(7, drop_remainder=True)<br>batched_dataset</p><BatchDataset shapes: ((7,), (7,)), types: (tf.int64, tf.int64)><p>许多模型（例如序列模型）都可以使用大小可变（例如长度不同的序列）的输入数据。通过<code>Dataset.padded_batch</code>可以将不同长度的<code>tensor</code>转换成一个<code>batch</code>。</p><p>在多个<code>epochs</code>的情况下，需要重复迭代数据集，最简单的方法是将数据集重复<code>epochs</code>遍。可以使用 <code>Dataset.repeat()</code>完成。</p><p>原有的<code>titanic_lines</code>数据集中的数据数量为 628</p><pre><code class="lang-python">count = 0for data in titanic_lines:    count += 1    # print(data.numpy())print(count)</code></pre><p>titanic_lines数据集经过<code>repeat</code>之后数目变为原来的两倍，1256</p><pre><code class="lang-python">count = 0for data in titanic_lines.repeat(2):    count += 1    # print(data.numpy())print(count)</code></pre><p>将数据集打散的方法 <code>Dataset.shuffle()</code> 通过维护一个固定大小的缓冲区来实现。</p><pre><code class="lang-py">dataset = tf.data.Dataset.zip((counter, lines))dataset = dataset.shuffle(buffer_size=100)dataset = dataset.batch(20)</code></pre><p>由于<code>buffer_size</code>为100，而批大小为20，因此第一批不包含索引大于120的元素。</p><p>在实际使用中，<code>repeat</code>操作、<code>batch</code>操作和<code>shuffle</code>操作经常一起混用，但是一定要注意操作的先后顺序。</p><h2 id="预处理数据"><a href="#预处理数据" class="headerlink" title="预处理数据"></a>预处理数据</h2><p><code>Dataset.map(f)</code> 通过函数 <code>f</code> 对数据集执行变换，<code>f</code> 必须以单个Tensor为输入，单个Tensor为输出（这里指的单个tensor，其意思是由原始数据和标签组成的数据对）。</p><p>假设我们定义了单个图像变换函数 <code>parse_image</code> ，只需 <code>images_ds = list_ds.map(parse_image)</code> 即可对 <code>image_ds</code> 数据集中的所有图片执行变换了。</p><p>假设我们需要将照片随机旋转，可以定义函数，然后使用map将其应用于数据集的所有图片上。</p><pre><code class="lang-py">import scipy.ndimage as ndimagedef random_rotate_image(image):  image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False)  return imageimage, label = next(iter(images_ds))image = random_rotate_image(image)show(image, label)</code></pre><p><img src="https://www.tensorflow.org/guide/data_files/output__wEyL7bS9S6t_1.png" alt></p><p>这里使用的是 <code>scipy</code> 中的旋转函数，需套用 <code>tf.py_function()</code> 才能在Tensorflow的eager_mode里面使用。</p><p>接下来使用 <code>Dataset.map</code> ：</p><pre><code class="lang-py">def tf_random_rotate_image(image, label):  im_shape = image.shape  [image,] = tf.py_function(random_rotate_image, [image], [tf.float32])  image.set_shape(im_shape)  return image, label</code></pre><p>在函数内部不但要注意输入和返回值是<code>(image, label)</code>，而且需要描述数据的<code>shape</code>和<code>type</code>，方便调试。</p><h2 id="时间序列数据窗口化"><a href="#时间序列数据窗口化" class="headerlink" title="时间序列数据窗口化"></a>时间序列数据窗口化</h2><p>时间序列数据的标签有所不同，一般以下一时刻的输入数据为标签，对未来进行一步一步的密集预测。比如：</p><pre><code class="lang-py">range_ds = tf.data.Dataset.range(100000)batches = range_ds.batch(10, drop_remainder=True)def dense_1_step(batch):  # Shift features and labels one step relative to each other.  return batch[:-1], batch[1:]predict_dense_1_step = batches.map(dense_1_step)for features, label in predict_dense_1_step.take(3):  print(features.numpy(), &quot; =&gt; &quot;, label.numpy())[0 1 2 3 4 5 6 7 8]  =&gt;  [1 2 3 4 5 6 7 8 9][10 11 12 13 14 15 16 17 18]  =&gt;  [11 12 13 14 15 16 17 18 19][20 21 22 23 24 25 26 27 28]  =&gt;  [21 22 23 24 25 26 27 28 29]</code></pre><p>如果要预测整个时间窗口而不是固定的偏移量，比如</p><pre><code class="lang-py">batches = range_ds.batch(15, drop_remainder=True)def label_next_5_steps(batch):  return (batch[:-5],   # Take the first 5 steps          batch[-5:])   # take the remainderpredict_5_steps = batches.map(label_next_5_steps)for features, label in predict_5_steps.take(3):  print(features.numpy(), &quot; =&gt; &quot;, label.numpy())[0 1 2 3 4 5 6 7 8 9]  =&gt;  [10 11 12 13 14][15 16 17 18 19 20 21 22 23 24]  =&gt;  [25 26 27 28 29][30 31 32 33 34 35 36 37 38 39]  =&gt;  [40 41 42 43 44]</code></pre><p>或者一个批次的标签和下个批次的输入有重叠：</p><pre><code class="lang-py">feature_length = 10label_length = 5features = range_ds.batch(feature_length, drop_remainder=True)labels = range_ds.batch(feature_length).skip(1).map(lambda labels: labels[:-5])predict_5_steps = tf.data.Dataset.zip((features, labels))for features, label in predict_5_steps.take(3):  print(features.numpy(), &quot; =&gt; &quot;, label.numpy())[0 1 2 3 4 5 6 7 8 9]  =&gt;  [10 11 12 13 14][10 11 12 13 14 15 16 17 18 19]  =&gt;  [20 21 22 23 24][20 21 22 23 24 25 26 27 28 29]  =&gt;  [30 31 32 33 34]</code></pre><p>有更方便的方法，那就是使用<code>Dataset.window</code>方法</p><pre><code class="lang-py">def make_window_dataset(ds, window_size, shift, stride):  windows = ds.window(window_size, shift=shift, stride=stride)  def sub_to_batch(sub):    return sub.batch(window_size, drop_remainder=True)  windows = windows.flat_map(sub_to_batch)  return windows</code></pre><p><code>Dataset.window(window_size, shift=shift, stride=stride)</code> 中的window_size代表窗口大小，即每个batch的元素个数；shift代表每次窗口移动的距离；stride代表选择元素的间隔</p><pre><code class="lang-py">ds = make_window_dataset(range_ds, window_size=10, shift = 5, stride=3)for example in ds.take(10):  print(example.numpy())[ 0  3  6  9 12 15 18 21 24 27][ 5  8 11 14 17 20 23 26 29 32][10 13 16 19 22 25 28 31 34 37][15 18 21 24 27 30 33 36 39 42][20 23 26 29 32 35 38 41 44 47][25 28 31 34 37 40 43 46 49 52][30 33 36 39 42 45 48 51 54 57][35 38 41 44 47 50 53 56 59 62][40 43 46 49 52 55 58 61 64 67][45 48 51 54 57 60 63 66 69 72]</code></pre><p>提取这些数据的标签方法：</p><pre><code class="lang-py">dense_labels_ds = ds.map(dense_1_step)for inputs,labels in dense_labels_ds.take(3):  print(inputs.numpy(), &quot;=&gt;&quot;, labels.numpy())[ 0  3  6  9 12 15 18 21 24] =&gt; [ 3  6  9 12 15 18 21 24 27][ 5  8 11 14 17 20 23 26 29] =&gt; [ 8 11 14 17 20 23 26 29 32][10 13 16 19 22 25 28 31 34] =&gt; [13 16 19 22 25 28 31 34 37]</code></pre><h2 id="重采样"><a href="#重采样" class="headerlink" title="重采样"></a>重采样</h2><p>有一些数据集，不同类别的数据分布不均匀。这个时候需要对那些不足的类别进行重采样。</p><p>给定信用卡欺诈（二分类）数据集，下面首先检查数据集中不同类别的占比</p><pre><code class="lang-py">def count(counts, batch):  features, labels = batch  class_1 = labels == 1  class_1 = tf.cast(class_1, tf.int32)  class_0 = labels == 0  class_0 = tf.cast(class_0, tf.int32)  counts[&#39;class_0&#39;] += tf.reduce_sum(class_0)  counts[&#39;class_1&#39;] += tf.reduce_sum(class_1)  return countscounts = creditcard_ds.take(10).reduce(    initial_state=&#123;&#39;class_0&#39;: 0, &#39;class_1&#39;: 0&#125;,    reduce_func = count)counts = np.array([counts[&#39;class_0&#39;].numpy(),                   counts[&#39;class_1&#39;].numpy()]).astype(np.float32)fractions = counts/counts.sum()print(fractions)[0.9953 0.0047]</code></pre><p>偏差很大，这样训练的二分类器只需全预测为正类，即可达到99.53%的正确率。</p><p>重采样数据集的一种方法是使用<code>sample_from_datasets</code> 。当每个类都有单独的<code>data.Dataset</code>时，此方法更适用。</p><p>正类和反类分别构建 <code>Dataset</code></p><pre><code class="lang-py">negative_ds = (  creditcard_ds    .unbatch()    .filter(lambda features, label: label==0)    .repeat())positive_ds = (  creditcard_ds    .unbatch()    .filter(lambda features, label: label==1)    .repeat())</code></pre><p>要使用<code>tf.data.experimental.sample_from_datasets</code>传递数据集以及每个数据集的权重</p><pre><code class="lang-py">balanced_ds = tf.data.experimental.sample_from_datasets(    [negative_ds, positive_ds], [0.5, 0.5]).batch(10)for features, labels in balanced_ds.take(10):  print(labels.numpy())[0 1 1 0 0 1 0 1 0 0][0 1 0 1 1 0 0 1 1 0][1 1 1 0 0 1 1 0 1 1][0 0 0 0 1 1 0 0 1 0][0 1 1 0 0 0 1 0 0 0][1 1 0 0 0 0 0 0 0 0][0 1 0 1 1 0 0 1 1 1][0 0 1 0 1 0 1 0 1 1][1 0 1 1 0 1 0 0 1 0][0 0 0 1 1 1 1 0 1 1]</code></pre><p>现在数据集就平衡了。</p><p>上述<code>experimental.sample_from_datasets</code>方法的一个问题是，每个类需要一个单独的<code>tf.data.Dataset</code>。</p><p>可以将<code>data.experimental.rejection_resample</code>函数应用于数据集，它仅加载一次，通过将多余元素将从数据集中删除以实现平衡。</p><p><code>data.experimental.rejection_resample</code>采用<code>class_func</code>参数，用于标记每个数据集元素所属的类别。</p><p>由于<code>Dataset</code>已经是<code>(features, label)</code>标记好的状态，因此只需</p><pre><code class="lang-py">def class_func(features, label):  return label</code></pre><p>重采样器输入的数据不能为batch后的Dataset，必须经过unbatch。重采样器还需要目标分布，以及可选的初始分布估计。最后经过map中的函数，直接删除掉extra_label即可。</p><pre><code class="lang-py">resampler = tf.data.experimental.rejection_resample(    class_func, target_dist=[0.5, 0.5], initial_dist=fractions)resample_ds = creditcard_ds.unbatch().apply(resampler).batch(10)balanced_ds = resample_ds.map(lambda extra_label, features_and_label: features_and_label)</code></pre><h2 id="数据集迭代器的checkpoint"><a href="#数据集迭代器的checkpoint" class="headerlink" title="数据集迭代器的checkpoint"></a>数据集迭代器的checkpoint</h2><p>没想到吧？不只是模型能使用checkpoint，Dataset的处理过程也可以使用checkpoint。如果您有一个很大的数据集，并且不想在每次重新启动时都从头开始，则这可能很有用。但是请注意，迭代器检查点可能很大，因为诸如shuffle和prefetch需要迭代器中的缓冲元素。</p><p>下面是示例：</p><pre><code class="lang-py">range_ds = tf.data.Dataset.range(20)iterator = iter(range_ds)ckpt = tf.train.Checkpoint(step=tf.Variable(0), iterator=iterator)manager = tf.train.CheckpointManager(ckpt, &#39;/tmp/my_ckpt&#39;, max_to_keep=3)print([next(iterator).numpy() for _ in range(5)])[0, 1, 2, 3, 4]save_path = manager.save()print([next(iterator).numpy() for _ in range(5)])[5, 6, 7, 8, 9]ckpt.restore(manager.latest_checkpoint)print([next(iterator).numpy() for _ in range(5)])[5, 6, 7, 8, 9]</code></pre><h2 id="在Keras中使用tf-data"><a href="#在Keras中使用tf-data" class="headerlink" title="在Keras中使用tf.data"></a>在Keras中使用<code>tf.data</code></h2><p>数据集的处理：</p><pre><code class="lang-py">train, test = tf.keras.datasets.fashion_mnist.load_data()images, labels = trainimages = images/255.0labels = labels.astype(np.int32)fmnist_train_ds = tf.data.Dataset.from_tensor_slices((images, labels))fmnist_train_ds = fmnist_train_ds.shuffle(5000).batch(32)</code></pre><p>模型构建：</p><pre><code class="lang-py">model = tf.keras.Sequential([  tf.keras.layers.Flatten(),  tf.keras.layers.Dense(10)])model.compile(optimizer=&#39;adam&#39;,              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),               metrics=[&#39;accuracy&#39;])</code></pre><p>模型训练：</p><pre><code class="lang-py">model.fit(fmnist_train_ds, epochs=2)</code></pre><p>如果在fit过程中你要对Dataset进行repeat，只需指定每个epochs使用的数据个数，然后不给repeat指定参数，数据集就会变成无限个，一定会满足epochs的要求。</p><pre><code class="lang-py">model.fit(fmnist_train_ds.repeat(), epochs=2, steps_per_epoch=20)</code></pre><p>同理，evaluate时也是一样的</p><pre><code class="lang-py">loss, accuracy = model.evaluate(fmnist_train_ds.repeat(), steps=10)print(&quot;Loss :&quot;, loss)print(&quot;Accuracy :&quot;, accuracy)Loss : 0.3501795828342438Accuracy : 0.8968750238418579</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>如果在使用机器学习算法解决问题过程中，数据预处理是主要问题的话，采用合适的数据组织手段可以帮助解决问题。 <code>tf.data</code> 能够快速处理大量数据，并将各个来源的数据归一化成合适的 <code>Dataset</code> 格式。</p><p>你可以对构建好的 <code>tf.data.Dataset</code> 做预处理操作，比如随机打乱、分批次、规划时间窗口、重采样等等。</p><p>经过处理后的 <code>Dataset</code> 对象可以直接输入到keras进行训练。</p></BatchDataset></BatchDataset>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tensorflow</tag>
      
      <tag>preprocessing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】使用TensorBoard分析模型性能</title>
    <link href="/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8TensorBoard%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD/"/>
    <url>/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8TensorBoard%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD/</url>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第六篇，讲述的是如何使用TensorBoard分析模型的性能，优化模型的资源消耗。<br><!--more---></p><p>TensorBoard可以监控模型的各个组分运行过程中的时间消耗和资源消耗，并根据这些数据对模型下一步优化提出建议。</p><p>首先我们安装性能分析的插件</p><pre><code class="lang-bash">!pip install -U tensorboard_plugin_profile</code></pre><p>定义TensorBoard的回调函数（数据预处理和模型定义略去不表），注意这里新的参数<code>profile_batch</code>只监控第500到520之间的20个Batch，避免监控过多导致模型运行效率过低。</p><pre><code class="lang-python"># Create a TensorBoard callbacklogs = &quot;logs/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)tboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs,                        histogram_freq=1,                        profile_batch=&#39;500,520&#39;)model.fit(ds_train,    epochs=2,    validation_data=ds_test,    callbacks=[tboard_callback])</code></pre><p>打开TensorBoard：</p><pre><code class="lang-bash"># Load the TensorBoard notebook extension.%load_ext tensorboard# Launch TensorBoard and navigate to the Profile tab to view performance profile%tensorboard --logdir=logs</code></pre><p><img src="/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8TensorBoard%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD/2020-08-14-16-47-10.png" alt></p><p>有非常多有用的信息，比如每个batch消耗的时间都花在哪里了：</p><p><img src="/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8TensorBoard%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD/2020-08-14-17-09-32.png" alt></p><p>还有针对耗时的改进意见：</p><p><img src="/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8TensorBoard%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD/2020-08-14-17-09-56.png" alt></p><p>有耗时最长的10大操作：</p><p><img src="/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8TensorBoard%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD/2020-08-14-17-10-27.png" alt></p><p>有性能监控选项，查看CPU活动和GPU活动。根据一般经验，始终保持设备（GPU / TPU）处于活动状态是我们的优化目标。</p><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/profiler_trace_viewer_bad_ip.png" alt></p><p>查看事件跟踪，可以看到<code>tf_data_iterator_get_next</code> 操作在CPU上运行时GPU不活动。该操作负责处理输入数据并将其发送到GPU进行训练。因此我们的优化方法可以是使用tf.data API优化输入管道，缓存训练数据集并预取数据，以确保始终有可供GPU处理的数据。</p><pre><code class="lang-python"> (ds_train, ds_test), ds_info = tfds.load(    &#39;mnist&#39;,    split=[&#39;train&#39;, &#39;test&#39;],    shuffle_files=True,    as_supervised=True,    with_info=True,)ds_train = ds_train.map(normalize_img)ds_train = ds_train.batch(128)ds_train = ds_train.cache()ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)ds_test = ds_test.map(normalize_img)ds_test = ds_test.batch(128)ds_test = ds_test.cache()ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)model.fit(ds_train,        epochs=2,        validation_data=ds_test,        callbacks = [tboard_callback])%tensorboard --logdir=logs</code></pre><p>跟踪查看器显示<code>tf_data_iterator_get_next</code>操作执行得更快。因此，GPU获得了稳定的数据流以进行训练，并通过模型训练获得了更好的利用率。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tensorflow</tag>
      
      <tag>TensorBoard</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】通过TensorBoard可视化词嵌入空间</title>
    <link href="/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%A9%BA%E9%97%B4/"/>
    <url>/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%A9%BA%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第五篇，讲述的是如何使用TensorBoard对词嵌入空间可视化。<br><!--more---></p><p>TensorBoard可以将词嵌入空间二维化，方便我们理解词嵌入空间的含义。</p><pre><code class="lang-python">from tensorboard.plugins import projector</code></pre><p>引入IMDb数据集。tensorflow_datasets是tensorflow的官方数据集库。</p><pre><code class="lang-python">import tensorflow_datasets as tfds(train_data, test_data), info = tfds.load(    &quot;imdb_reviews/subwords8k&quot;,    split=(tfds.Split.TRAIN, tfds.Split.TEST),    with_info=True,    as_supervised=True,)encoder = info.features[&quot;text&quot;].encoder# shuffle and pad the data.train_batches = train_data.shuffle(1000).padded_batch(    10, padded_shapes=((None,), ()))test_batches = test_data.shuffle(1000).padded_batch(    10, padded_shapes=((None,), ()))train_batch, train_labels = next(iter(train_batches))</code></pre><p>训练词嵌入模型</p><pre><code class="lang-python"># Create an embedding layerembedding_dim = 16embedding = tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim)# Train this embedding as part of a keras modelmodel = tf.keras.Sequential(    [        embedding, # The embedding layer should be the first layer in a model.        tf.keras.layers.GlobalAveragePooling1D(),        tf.keras.layers.Dense(16, activation=&quot;relu&quot;),        tf.keras.layers.Dense(1),    ])# Compile modelmodel.compile(    optimizer=&quot;adam&quot;,    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),    metrics=[&quot;accuracy&quot;],)# Train modelhistory = model.fit(    train_batches, epochs=1, validation_data=test_batches, validation_steps=20)</code></pre><p>将数据保存成方便TensorBoard读取的形式</p><pre><code class="lang-python"># Set up a logs directory, so Tensorboard knows where to look for fileslog_dir=&#39;/logs/imdb-example/&#39;if not os.path.exists(log_dir):    os.makedirs(log_dir)# Save Labels separately on a line-by-line manner.with open(os.path.join(log_dir, &#39;metadata.tsv&#39;), &quot;w&quot;) as f:  for subwords in encoder.subwords:    f.write(&quot;&#123;&#125;\n&quot;.format(subwords))  # Fill in the rest of the labels with &quot;unknown&quot;  for unknown in range(1, encoder.vocab_size - len(encoder.subwords)):    f.write(&quot;unknown #&#123;&#125;\n&quot;.format(unknown))# Save the weights we want to analyse as a variable. Note that the first# value represents any unknown word, which is not in the metadata, so# we will remove that value.weights = tf.Variable(model.layers[0].get_weights()[0][1:])# Create a checkpoint from embedding, the filename and key are# name of the tensor.checkpoint = tf.train.Checkpoint(embedding=weights)checkpoint.save(os.path.join(log_dir, &quot;embedding.ckpt&quot;))# Set up configconfig = projector.ProjectorConfig()embedding = config.embeddings.add()# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`embedding.tensor_name = &quot;embedding/.ATTRIBUTES/VARIABLE_VALUE&quot;embedding.metadata_path = &#39;metadata.tsv&#39;projector.visualize_embeddings(log_dir, config)</code></pre><pre><code class="lang-python">%tensorboard --logdir /logs/imdb-example/</code></pre><p>你可以看到2维、3维空间中的Embedding，搜索某个单词在词嵌入空间中的位置，甚至可以采取不同的可视化方法：<br><img src="/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%A9%BA%E9%97%B4/2020-08-13-19-45-25.png" alt></p><p>打开该工具的时候我的电脑很卡。。。希望大家注意。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tensorflow</tag>
      
      <tag>TensorBoard</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】通过TensorBoard调整超参数</title>
    <link href="/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E8%B0%83%E6%95%B4%E8%B6%85%E5%8F%82%E6%95%B0/"/>
    <url>/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E8%B0%83%E6%95%B4%E8%B6%85%E5%8F%82%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第四篇，讲述的是如何借助TensorBoard调整模型的超参数。<br><!--more---></p><p>TensorBoard中的HParams仪表板是比较新颖的工具包，提供了多种调节超参数的工具，并且该工具还在不断更新中。</p><pre><code class="lang-python">from tensorboard.plugins.hparams import api as hp</code></pre><p>用过sklearn进行机器学习模型调参的同学应该体验过交叉验证调参的方法。通过提供许多不同的超参数选项，<code>GridSearchCV</code>将训练多个模型，并取性能最优的模型超参数。</p><pre><code class="lang-python">from sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import make_pipelineX_train, X_test, y_train, y_test = train_test_split(df_train[&#39;text&#39;][:10000], df_train[&#39;label&#39;][:10000], random_state=0)pipe_logis = make_pipeline(TfidfVectorizer(min_df=5, ngram_range=(1,3)), LogisticRegression())param_grid = &#123;&#39;logisticregression__C&#39;: [0.001, 0.01, 0.1, 1, 10]&#125;grid = GridSearchCV(pipe_logis, param_grid, cv=5)grid.fit(X_train, y_train)print(&quot;Best params:\n&#123;&#125;\n&quot;.format(grid.best_params_))print(&quot;Best cross-validation score: &#123;:.2f&#125;&quot;.format(grid.best_score_))print(&quot;Test-set score: &#123;:.2f&#125;&quot;.format(grid.score(X_test, y_test)))</code></pre><p>在上面的程序中，grid中包含一个需要调节的超参数，即逻辑回归的C值。候选C值有5个，因此grid在fit过程中会训练五个模型，每个模型执行5次交叉验证（因为fit中cv参数为5）。</p><p>HParams也是采用类似的方法找超参数。首先我们定义候选超参数的变化范围。我们选择三个参数进行网格搜索，分别是Dense层的Unit数目、dropout的比例和优化器，每个超参数都有两种选择，因此一共需要训练八个模型。</p><p>最终模型的评价标准以Accuracy为准。具体代码如下所示：</p><pre><code class="lang-python">HP_NUM_UNITS = hp.HParam(&#39;num_units&#39;, hp.Discrete([16, 32]))HP_DROPOUT = hp.HParam(&#39;dropout&#39;, hp.RealInterval(0.1, 0.2))HP_OPTIMIZER = hp.HParam(&#39;optimizer&#39;, hp.Discrete([&#39;adam&#39;, &#39;sgd&#39;]))METRIC_ACCURACY = &#39;accuracy&#39;</code></pre><p>设置write句柄，这已经是传统艺能了。</p><pre><code class="lang-python">with tf.summary.create_file_writer(&#39;logs/hparam_tuning&#39;).as_default():  hp.hparams_config(    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],    metrics=[hp.Metric(METRIC_ACCURACY, display_name=&#39;Accuracy&#39;)],  )</code></pre><p>接下来我们定义待训练模型。模型本身非常简单，而且只训练一个epoch，这是考虑到要消耗平时八倍的时间而采取的tradeoff。</p><pre><code class="lang-python">def train_test_model(hparams):  model = tf.keras.models.Sequential([    tf.keras.layers.Flatten(),    tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=tf.nn.relu),    tf.keras.layers.Dropout(hparams[HP_DROPOUT]),    tf.keras.layers.Dense(10, activation=tf.nn.softmax),  ])  model.compile(      optimizer=hparams[HP_OPTIMIZER],      loss=&#39;sparse_categorical_crossentropy&#39;,      metrics=[&#39;accuracy&#39;],  )  model.fit(x_train, y_train, epochs=1) # Run with 1 epoch to speed things up for demo purposes  _, accuracy = model.evaluate(x_test, y_test)  return accuracy</code></pre><p>进行训练并记录模型输出；</p><pre><code class="lang-python">def run(run_dir, hparams):  with tf.summary.create_file_writer(run_dir).as_default():    hp.hparams(hparams)  # record the values used in this trial    accuracy = train_test_model(hparams)    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)</code></pre><p>之后我们进行网格搜索（其实就是遍历每种可能。搜索方法完全是自己定义的，你也可以使用随机搜索方法）：</p><pre><code class="lang-python">session_num = 0for num_units in HP_NUM_UNITS.domain.values:  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):    for optimizer in HP_OPTIMIZER.domain.values:      hparams = &#123;          HP_NUM_UNITS: num_units,          HP_DROPOUT: dropout_rate,          HP_OPTIMIZER: optimizer,      &#125;      run_name = &quot;run-%d&quot; % session_num      print(&#39;--- Starting trial: %s&#39; % run_name)      print(&#123;h.name: hparams[h] for h in hparams&#125;)      run(&#39;logs/hparam_tuning/&#39; + run_name, hparams)      session_num += 1</code></pre><p>最后使用TensorBoard进行可视化：</p><pre><code class="lang-bash">%tensorboard --logdir logs/hparam_tuning</code></pre><p><img src="https://www.tensorflow.org/tensorboard/images/hparams_table.png?raw=1" alt></p><p>我们可以通过TensorBoard发现很多有趣的现象：比如在本模型中，adam优化器比sgd要好等等。</p><p><img src="/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E9%80%9A%E8%BF%87TensorBoard%E8%B0%83%E6%95%B4%E8%B6%85%E5%8F%82%E6%95%B0/2020-08-13-19-19-41.png" alt></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tensorflow</tag>
      
      <tag>TensorBoard</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】用TensorBoard生成模型图</title>
    <link href="/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%9B%BE/"/>
    <url>/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%9B%BE/</url>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第三篇，讲述的是如何令TensorBoard生成模型架构图。<br><!--more---></p><p>TensorBoard不但可以展示存在的图片和张量，还可以生成图片，诸如模型图等。通过TensorBoard的GRAPHS选项卡，可以快速查看模型结构的预览图，并确保其符合设计预期。</p><p>比如我们定义模型如下：</p><pre><code class="lang-python">model = keras.models.Sequential([    keras.layers.Flatten(input_shape=(28, 28)),    keras.layers.Dense(32, activation=&#39;relu&#39;),    keras.layers.Dropout(0.2),    keras.layers.Dense(10, activation=&#39;softmax&#39;)])model.compile(    optimizer=&#39;adam&#39;,    loss=&#39;sparse_categorical_crossentropy&#39;,    metrics=[&#39;accuracy&#39;])</code></pre><p><img src="/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%9B%BE/2020-08-13-13-31-51.png" alt></p><p>通过TensorBoard中的GRAPHS选项卡，我们看到执行图。图是倒置的，数据从下到上流动，因此与代码相比是上下颠倒的。</p><p>可以更改Tag，选择Keras，选择左边的Conceptual Graph查看概念图，双击Sequential，得到概念图。概念图更像是代码。</p><p><img src="/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%9B%BE/2020-08-13-13-35-03.png" alt></p><p>有的时候我们希望得到计算图，研究数据经过了何种计算。比如下面这个函数：</p><pre><code class="lang-python"># Sample data for your function.x = tf.random.uniform((3, 3))y = tf.random.uniform((3, 3))# The function to be traced.@tf.functiondef my_func(x, y):  # A simple hand-rolled layer.  return tf.nn.relu(tf.matmul(x, y))# z = my_func(x, y)</code></pre><p>我们希望得到它的计算图。首先需要使用<code>@tf.function</code>修饰被监控的函数，然后使用<code>tf.summary.trace_on()</code>在<code>z = my_func(x, y)</code>函数运行之前开始记录。</p><p><img src="https://as2.bitinn.net/uploads/legacy/og/cistioqrt008t8q5nh9gtx9og.1200.jpg" alt="Trace On"></p><pre><code class="lang-python">tf.summary.trace_on(graph=True, profiler=True)z = my_func(x, y)</code></pre><p>定义日志目录名称和文件写入句柄，这些都是刻在DNA里的操作：</p><pre><code class="lang-python"># Set up logging.stamp = datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)logdir = &#39;logs/func/%s&#39; % stampwriter = tf.summary.create_file_writer(logdir)</code></pre><p>最后执行记录：</p><pre><code class="lang-python"># Call only one tf.function when tracing.with writer.as_default():  tf.summary.trace_export(      name=&quot;my_func_trace&quot;,      step=0,      profiler_outdir=logdir)</code></pre><pre><code class="lang-bash">%tensorboard --logdir logs/func</code></pre><p><img src="/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%9B%BE/2020-08-13-13-47-57.png" alt></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tensorflow</tag>
      
      <tag>TensorBoard</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】用TensorBoard展示图片</title>
    <link href="/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E5%B1%95%E7%A4%BA%E5%9B%BE%E7%89%87/"/>
    <url>/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%94%A8TensorBoard%E5%B1%95%E7%A4%BA%E5%9B%BE%E7%89%87/</url>
    
    <content type="html"><![CDATA[<p>这是TensorBoard笔记的第二篇，讲述的是如何令TensorBoard展示外界已有的图片和Tensor。<br><!--more---></p><p>我们可以利用TensorBoard展示图片类数据，或者通过tf.summary将张量类数据转化成图片。下面是对Fashion-MNIST数据集中部分图片的可视化：</p><h3 id="可视化单个图片"><a href="#可视化单个图片" class="headerlink" title="可视化单个图片"></a>可视化单个图片</h3><pre><code class="lang-python">import tensorflow as tffrom tensorflow import kerasfashion_mnist = keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = \    fashion_mnist.load_data()</code></pre><p>数据集中每个图像的形状都是2阶张量形状（28、28），分别表示高度和宽度</p><p>但是， tf.summary.image()期望包含(batch_size, height, width, channels)的4级张量。因此，张量需要重塑。</p><pre><code class="lang-python">img = np.reshape(train_images[0], (-1, 28, 28, 1))</code></pre><p>使用<code>tf.summary.image</code>将其转化为tensor，并利用TensorBoard可视化：</p><pre><code class="lang-python"> # Clear out any prior log data.!rm -rf logs# Sets up a timestamped log directory.logdir = &quot;logs/train_data/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)# Creates a file writer for the log directory.file_writer = tf.summary.create_file_writer(logdir)# Using the file writer, log the reshaped image.with file_writer.as_default():  tf.summary.image(&quot;Training data&quot;, img, step=0)</code></pre><p>转化后的图片被<code>tf.summary.create_file_writer</code>输出到logdir里面了。使用TensorBoard看看：</p><pre><code class="lang-bash">%tensorboard --logdir logs/train_data</code></pre><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_single.png" alt></p><p>加载的过程可能有点慢，注意留足够的内存以免标签页崩溃。</p><p>你也可以使用左边的滑动条调节亮度、对比度和大小。</p><h3 id="可视化多张图片"><a href="#可视化多张图片" class="headerlink" title="可视化多张图片"></a>可视化多张图片</h3><p>调整<code>tf.summary.image</code>里面的参数<code>max_outputs</code>：</p><pre><code class="lang-python"> with file_writer.as_default():    # Don&#39;t forget to reshape.    images = np.reshape(train_images[0:25], (-1, 28, 28, 1))    tf.summary.image(&quot;25 training data examples&quot;, images, max_outputs=25, step=0)</code></pre><pre><code class="lang-bash">%tensorboard --logdir logs/train_data</code></pre><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_multiple.png" alt></p><h3 id="可视化其他格式的图片"><a href="#可视化其他格式的图片" class="headerlink" title="可视化其他格式的图片"></a>可视化其他格式的图片</h3><p>有些图片不是tensor或者numpy.array，而是由诸如opencv、matplotlib生成的png图像，我们需要将其转化为tensor。</p><p>由于matplotlib适合生成复杂的数据图，因此先利用其他库生成图片，随后利用<code>tf.summary.image</code>将其转化为一个tensor再可视化，是一个比较方便的选择。</p><p>matplotlib生成数据集可视化：</p><pre><code class="lang-python"> # Clear out prior logging data.!rm -rf logs/plotslogdir = &quot;logs/plots/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)file_writer = tf.summary.create_file_writer(logdir)def plot_to_image(figure):  &quot;&quot;&quot;Converts the matplotlib plot specified by &#39;figure&#39; to a PNG image and  returns it. The supplied figure is closed and inaccessible after this call.&quot;&quot;&quot;  # Save the plot to a PNG in memory.  buf = io.BytesIO()  plt.savefig(buf, format=&#39;png&#39;)  # Closing the figure prevents it from being displayed directly inside  # the notebook.  plt.close(figure)  buf.seek(0)  # Convert PNG buffer to TF image  image = tf.image.decode_png(buf.getvalue(), channels=4)  # Add the batch dimension  image = tf.expand_dims(image, 0)  return imagedef image_grid():  &quot;&quot;&quot;Return a 5x5 grid of the MNIST images as a matplotlib figure.&quot;&quot;&quot;  # Create a figure to contain the plot.  figure = plt.figure(figsize=(10,10))  for i in range(25):    # Start next subplot.    plt.subplot(5, 5, i + 1, title=class_names[train_labels[i]])    plt.xticks([])    plt.yticks([])    plt.grid(False)    plt.imshow(train_images[i], cmap=plt.cm.binary)  return figure</code></pre><p>尔后，利用<code>tf.summary.image</code>转化：</p><pre><code class="lang-python"># Prepare the plotfigure = image_grid()# Convert to image and logwith file_writer.as_default():  tf.summary.image(&quot;Training data&quot;, plot_to_image(figure), step=0)</code></pre><p>最后，利用TensorBoard可视化：</p><pre><code class="lang-bash">%tensorboard --logdir logs/plots</code></pre><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_arbitrary.png" alt></p><h3 id="在图片分类器中使用TensorBoard"><a href="#在图片分类器中使用TensorBoard" class="headerlink" title="在图片分类器中使用TensorBoard"></a>在图片分类器中使用TensorBoard</h3><p>之前我们通过TensorBoard了解了Fashion-MNIST数据集的概要，但是TensorBoard的功能不止于此。</p><p>首先构建分类模型：</p><pre><code class="lang-python">model = keras.models.Sequential([    keras.layers.Flatten(input_shape=(28, 28)),    keras.layers.Dense(32, activation=&#39;relu&#39;),    keras.layers.Dense(10, activation=&#39;softmax&#39;)])model.compile(    optimizer=&#39;adam&#39;,     loss=&#39;sparse_categorical_crossentropy&#39;,    metrics=[&#39;accuracy&#39;])</code></pre><p>我们想使用<a href="https://en.wikipedia.org/wiki/Confusion_matrix">混淆矩阵</a>详细了解分类器对测试数据的性能。因此接下来定义一个函数，专门计算混淆矩阵。具体来说，</p><ol><li>使用model.predict预测该epoch的所有测试用例的标签，得到<code>test_pred</code></li><li>调用<code>sklearn.metrics.confusion_matrix</code>直接计算混淆矩阵</li><li>使用<code>matplotlib</code>将混淆矩阵可视化</li><li>将<code>matplotlib</code>生成的图片转为tensor，最后变成log储存</li></ol><p>下面是前两步所需的操作：</p><pre><code class="lang-python">def log_confusion_matrix(epoch, logs):    # Use the model to predict the values from the validation dataset.    test_pred_raw = model.predict(test_images)    test_pred = np.argmax(test_pred_raw, axis=1)    # Calculate the confusion matrix.    cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)    # Log the confusion matrix as an image summary.    figure = plot_confusion_matrix(cm, class_names=class_names)    cm_image = plot_to_image(figure)    # Log the confusion matrix as an image summary.    with file_writer_cm.as_default():    tf.summary.image(&quot;Confusion Matrix&quot;, cm_image, step=epoch)</code></pre><p>下面是第三步所需的可视化函数：</p><pre><code class="lang-python">def plot_confusion_matrix(cm, class_names):    &quot;&quot;&quot;    Returns a matplotlib figure containing the plotted confusion matrix.    Args:    cm (array, shape = [n, n]): a confusion matrix of integer classes    class_names (array, shape = [n]): String names of the integer classes    &quot;&quot;&quot;    figure = plt.figure(figsize=(8, 8))    plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=plt.cm.Blues)    plt.title(&quot;Confusion matrix&quot;)    plt.colorbar()    tick_marks = np.arange(len(class_names))    plt.xticks(tick_marks, class_names, rotation=45)    plt.yticks(tick_marks, class_names)    # Normalize the confusion matrix.    cm = np.around(cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis], decimals=2)    # Use white text if squares are dark; otherwise black.    threshold = cm.max() / 2.    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):    color = &quot;white&quot; if cm[i, j] &gt; threshold else &quot;black&quot;    plt.text(j, i, cm[i, j], horizontalalignment=&quot;center&quot;, color=color)    plt.tight_layout()    plt.ylabel(&#39;True label&#39;)    plt.xlabel(&#39;Predicted label&#39;)    return figure</code></pre><p>下面是第四步所需的tensor转化和储存函数以及其他回调函数：</p><pre><code class="lang-bash"># Clear out prior logging data.!rm -rf logs/image</code></pre><pre><code class="lang-python">logdir = &quot;logs/image/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)# Define the basic TensorBoard callback.tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)file_writer_cm = tf.summary.create_file_writer(logdir + &#39;/cm&#39;)</code></pre><p>让我们开始训练：</p><pre><code class="lang-bash"># Start TensorBoard.%tensorboard --logdir logs/image</code></pre><pre><code class="lang-python"># Train the classifier.model.fit(    train_images,    train_labels,    epochs=5,    verbose=0, # Suppress chatty output    callbacks=[tensorboard_callback, cm_callback],    validation_data=(test_images, test_labels),)</code></pre><p>请注意，此时我先调用的TensorBoard，然后开始的训练，并且我设置了verbose=0，意味着信息完全通过TensorBoard动态展示。训练过程中你就可以看到参数的变化。</p><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_accuracy.png" alt></p><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/images_cm.png" alt></p><p>你还可以看到，随着训练的进行，矩阵是如何发生变化的：沿着对角线的正方形会逐渐变暗，而矩阵的其余部分趋向于0和白色。这意味着分类器正在不断改进。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tensorflow</tag>
      
      <tag>TensorBoard</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】TensorBoard简介</title>
    <link href="/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91TensorBoard%E7%AE%80%E4%BB%8B/"/>
    <url>/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91TensorBoard%E7%AE%80%E4%BB%8B/</url>
    
    <content type="html"><![CDATA[<p>简单介绍了TensorBoard的用途和使用方法。<br><!--more---></p><h2 id="TensorBoard简介"><a href="#TensorBoard简介" class="headerlink" title="TensorBoard简介"></a>TensorBoard简介</h2><p>TensorBoard是Google开发的模型内部参数跟踪和可视化的调试工具。在Tensorflow中，用TensorBoard可以监控模型的各种指标的变化（如acc、loss的动态变化），可以将模型结构可视化，可以可视化词嵌入空间，可以分析模型性能，可以分析数据集的公平性等等，是一个非常强大且非常简单的工具。</p><p>TensorBoard核心就是回调函数和可视化操作面板。通过编写回调函数获取模型信息，通过命令行启动TensorBoard图形化界面。</p><p>TensorBoard的回调函数API为：</p><pre><code class="lang-python">tf.keras.callbacks.TensorBoard(    log_dir=&#39;logs&#39;, histogram_freq=0, write_graph=True, write_images=False,    update_freq=&#39;epoch&#39;, profile_batch=2, embeddings_freq=0,    embeddings_metadata=None, **kwargs)</code></pre><div class="table-container"><table><thead><tr><th>参数</th><th>含义</th></tr></thead><tbody><tr><td>log_dir</td><td>模型的信息保存目录</td></tr><tr><td>histogram_freq</td><td>模型激活和参数信息记录的频率，每隔几个epochs记录一次</td></tr><tr><td>write_graph</td><td>是否保存模型图文件</td></tr><tr><td>write_images</td><td>是否保存模型参数可视化图</td></tr><tr><td>update_freq</td><td>模型loss和其他metrics的记录频率，每隔几个batch更新一次</td></tr><tr><td>profile_batch</td><td>指定性能分析时使用的批次</td></tr><tr><td>embeddings_freq</td><td>embedding 层更新的频率</td></tr></tbody></table></div><p>在Colab中使用TensorBoard需输入：</p><pre><code class="lang-bash">%load_ext tensorboard</code></pre><p>为了跟踪模型训练过程，需要在模型的<code>fit</code>过程中添加回调函数</p><pre><code class="lang-python">tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)</code></pre><p>其中<code>log_dir</code>为你想储存log的目录，在教程中，<code>log_dir=&quot;logs/fit/&quot; + datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)</code>。</p><pre><code class="lang-python">model.fit(x=x_train,           y=y_train,           epochs=5,           validation_data=(x_test, y_test),           callbacks=[tensorboard_callback])</code></pre><p>在<code>log_dir</code>中生成了一系列的日志文件：</p><p><img src="/2020/08/14/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91TensorBoard%E7%AE%80%E4%BB%8B/2020-08-13-12-39-02.png" alt></p><p>这些日志文件可以通过TensorBoard解析：</p><pre><code class="lang-shell">%tensorboard --logdir logs/fit</code></pre><p>在命令行中， 运行不带“％”的相同命令。结果如下：</p><p><img src="https://raw.githubusercontent.com/tensorflow/tensorboard/master/docs/images/quickstart_model_fit.png" alt></p><p>在TensorBoard面板上，有四个选项卡：</p><ul><li><strong>Scalars</strong> 显示损失和指标在每个时期如何变化</li><li><strong>Graphs</strong> 可帮助您可视化模型</li><li><strong>Distributions</strong> 和 <strong>Histograms</strong> 显示张量随时间的分布</li></ul><p>如果使用tensorflow原生API训练模型，也可以利用<code>tf.summary</code>记录log，然后利用TensorBoard可视化。具体流程如下：</p><ol><li>使用 <code>tf.summary.create_file_writer()</code> 创建文件编写器；</li><li>使用 <code>tf.summary.scalar()</code> 记录感兴趣的指标</li><li>将 <code>LearningRateScheduler</code> 回调传递给 <code>Model.fit()</code></li><li>使用命令行<code>tensorboard --logdir logs/fit</code>打开可视化界面</li></ol>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tensorflow</tag>
      
      <tag>TensorBoard</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】新闻文本分类之注意力机制与预训练模型</title>
    <link href="/2020/08/04/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <url>/2020/08/04/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p>见多了优秀的文章，再写博客的时候就会感叹自己的学识浅薄。<br><!--more---></p><p>本章介绍了Bert的原理和使用，具体包括pretrain和finetune两部分。</p><h2 id="基于深度学习的文本分类"><a href="#基于深度学习的文本分类" class="headerlink" title="基于深度学习的文本分类"></a>基于深度学习的文本分类</h2><h4 id="Transformer原理"><a href="#Transformer原理" class="headerlink" title="Transformer原理"></a>Transformer原理</h4><p>当用神经网络来处理大量的输入信息时，可以借鉴人脑的注意力机制，只选择一些关键的信息输入进行处理，来提高神经网络的效率。注意力机制可以单独使用，但更多地用作神经网络中的一个组件。基于循环神经网络的序列到序列模型的一个缺点是无法并行计算，为了提高并行计算效率以及捕捉长距离的依赖关系，我们可以使用自注意力模型（Self-Attention Model）来建立一个全连接的网络结构。</p><p>Transformer模型是一个基于多头自注意力的序列到序列模型，包含编码器和解码器两部分。</p><p><img src="https://img-blog.csdnimg.cn/20200714211046668.png" alt></p><h3 id="基于Bert的文本分类"><a href="#基于Bert的文本分类" class="headerlink" title="基于Bert的文本分类"></a>基于Bert的文本分类</h3><p>分成Pretrain和Fine-Tune两部分。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Datawhale</tag>
      
      <tag>Classification</tag>
      
      <tag>attention</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】新闻文本分类之深度学习Word2Vec</title>
    <link href="/2020/07/31/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Word2Vec/"/>
    <url>/2020/07/31/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Word2Vec/</url>
    
    <content type="html"><![CDATA[<p>谦虚使人进步，骄傲使人落后。<br><!--more---></p><p>最近课题内容较多时间紧张，同时我认为学习资料非常翔实，遂转发以做笔记。之后会在文章最后总结自己的思路和经验。</p><h1 id="Task5-基于深度学习的文本分类2"><a href="#Task5-基于深度学习的文本分类2" class="headerlink" title="Task5 基于深度学习的文本分类2"></a>Task5 基于深度学习的文本分类2</h1><p>在上一章节，我们通过FastText快速实现了基于深度学习的文本分类模型，但是这个模型并不是最优的。在本章我们将继续深入。</p><h2 id="基于深度学习的文本分类"><a href="#基于深度学习的文本分类" class="headerlink" title="基于深度学习的文本分类"></a>基于深度学习的文本分类</h2><p>本章将继续学习基于深度学习的文本分类。</p><h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>学习Word2Vec的使用和基础原理</li><li>学习使用TextCNN、TextRNN进行文本表示</li><li>学习使用HAN网络结构完成文本分类</li></ul><h3 id="文本表示方法-Part3"><a href="#文本表示方法-Part3" class="headerlink" title="文本表示方法 Part3"></a>文本表示方法 Part3</h3><h4 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h4><p>本节通过word2vec学习词向量。word2vec模型背后的基本思想是对出现在上下文环境里的词进行预测。对于每一条输入文本，我们选取一个上下文窗口和一个中心词，并基于这个中心词去预测窗口里其他词出现的概率。因此，word2vec模型可以方便地从新增语料中学习到新增词的向量表达，是一种高效的在线学习算法（online learning）。</p><p>word2vec的主要思路：通过单词和上下文彼此预测，对应的两个算法分别为：</p><ul><li><p>Skip-grams (SG)：预测上下文</p></li><li><p>Continuous Bag of Words (CBOW)：预测目标单词</p></li></ul><p>另外提出两种更加高效的训练方法：</p><ul><li><p>Hierarchical softmax</p></li><li><p>Negative sampling</p></li></ul><p><strong>1. Skip-grams原理和网络结构</strong></p><p>Word2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Gram是给定input word来预测上下文。而CBOW是给定上下文，来预测input word。</p><p><img src="https://img-blog.csdnimg.cn/20200714210354729.png" alt="skip_grams"></p><p>Word2Vec模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。</p><p>Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。</p><p>Skip-grams过程</p><p>假如我们有一个句子“The dog barked at the mailman”。</p><ol><li><p>首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；</p></li><li><p>有了input word以后，我们再定义一个叫做skip_window的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设置skip_window=2，那么我们最终获得窗口中的词（包括input word在内）就是[‘The’, ‘dog’，’barked’, ‘at’]。skip_window=2代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小span=2x2=4。另一个参数叫num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当skip_window=2，num_skips=2时，我们将会得到两组 (input word, output word) 形式的训练数据，即 (‘dog’, ‘barked’)，(‘dog’, ‘the’)。</p></li><li><p>神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词作为input word的output word的可能性。这句话有点绕，我们来看个例子。第二步中我们在设置skip_window和num_skips=2的情况下获得了两组训练数据。假如我们先拿一组数据 (‘dog’, ‘barked’) 来训练神经网络，那么模型通过学习这个训练样本，会告诉我们词汇表中每个单词当’dog’作为input word时，其作为output word的可能性。</p></li></ol><p>也就是说模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。例如：如果我们向神经网络模型中输入一个单词“Soviet“，那么最终模型的输出概率中，像“Union”， ”Russia“这种相关词的概率将远高于像”watermelon“，”kangaroo“非相关词的概率。因为”Union“，”Russia“在文本中更大可能在”Soviet“的窗口中出现。</p><p>我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。下面的图中给出了一些我们训练样本的例子。我们选定句子“The quick brown fox jumps over lazy dog”，设定我们的窗口大小为2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。</p><p><img src="https://img-blog.csdnimg.cn/20200721190035764.png" alt="1"></p><p><img src="https://img-blog.csdnimg.cn/20200714210519939.png" alt="2"></p><p>我们的模型将会从每对单词出现的次数中习得统计结果。例如，我们的神经网络可能会得到更多类似（“Soviet“，”Union“）这样的训练样本对，而对于（”Soviet“，”Sasquatch“）这样的组合却看到的很少。因此，当我们的模型完成训练后，给定一个单词”Soviet“作为输入，输出的结果中”Union“或者”Russia“要比”Sasquatch“被赋予更高的概率。</p><p>PS：input word和output word都会被我们进行one-hot编码。仔细想一下，我们的输入被one-hot编码以后大多数维度上都是0（实际上仅有一个位置为1），所以这个向量相当稀疏，那么会造成什么结果呢。如果我们将一个1 x 10000的向量和10000 x 300的矩阵相乘，它会消耗相当大的计算资源，为了高效计算，它仅仅会选择矩阵中对应的向量中维度值为1的索引行：</p><p><img src="https://img-blog.csdnimg.cn/20200714205344406.png" alt></p><p><strong>2. Skip-grams训练</strong></p><p>由上部分可知，Word2Vec模型是一个超级大的神经网络（权重矩阵规模非常大）。例如：我们拥有10000个单词的词汇表，我们如果想嵌入300维的词向量，那么我们的输入-隐层权重矩阵和隐层-输出层的权重矩阵都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。百万数量级的权重矩阵和亿万数量级的训练样本意味着训练这个模型将会是个灾难</p><p>解决方案：</p><ul><li><p>将常见的单词组合（word pairs）或者词组作为单个“words”来处理</p></li><li><p>对高频次单词进行抽样来减少训练样本的个数</p></li><li><p>对优化目标采用“negative sampling”方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担</p></li></ul><p><em>2.1 Word pairs and “phases”</em></p><p>一些单词组合（或者词组）的含义和拆开以后具有完全不同的意义。比如“Boston Globe”是一种报刊的名字，而单独的“Boston”和“Globe”这样单个的单词却表达不出这样的含义。因此，在文章中只要出现“Boston Globe”，我们就应该把它作为一个单独的词来生成其词向量，而不是将其拆开。同样的例子还有“New York”，“United Stated”等。</p><p>在Google发布的模型中，它本身的训练样本中有来自Google News数据集中的1000亿的单词，但是除了单个单词以外，单词组合（或词组）又有3百万之多。</p><p><em>2.2 对高频词抽样</em></p><p>在上一部分中，对于原始文本为“The quick brown fox jumps over the laze dog”，如果使用大小为2的窗口，那么我们可以得到图中展示的那些训练样本。</p><p><img src="https://img-blog.csdnimg.cn/20200714210458879.png" alt="1"></p><p>但是对于“the”这种常用高频单词，这样的处理方式会存在下面两个问题：</p><ol><li><p>当我们得到成对的单词训练样本时，(“fox”, “the”) 这样的训练样本并不会给我们提供关于“fox”更多的语义信息，因为“the”在每个单词的上下文中几乎都会出现</p></li><li><p>由于在文本中“the”这样的常用词出现概率很大，因此我们将会有大量的（”the“，…）这样的训练样本，而这些样本数量远远超过了我们学习“the”这个词向量所需的训练样本数</p></li></ol><p>Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。</p><p>ωi 是一个单词，Z(ωi) 是 ωi 这个单词在所有语料中出现的频次，例如：如果单词“peanut”在10亿规模大小的语料中出现了1000次，那么 Z(peanut) = 1000/1000000000 = 1e - 6。</p><p>P(ωi) 代表着保留某个单词的概率：</p><p><img src="https://img-blog.csdnimg.cn/20200714205456898.png" alt></p><p><em>2.3 Negative sampling</em></p><p>训练一个神经网络意味着要输入训练样本并且不断调整神经元的权重，从而不断提高对目标的准确预测。每当神经网络经过一个训练样本的训练，它的权重就会进行一次调整。</p><p>所以，词典的大小决定了我们的Skip-Gram神经网络将会拥有大规模的权重矩阵，所有的这些权重需要通过数以亿计的训练样本来进行调整，这是非常消耗计算资源的，并且实际中训练起来会非常慢。</p><p>负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。</p><p>当我们用训练样本 ( input word: “fox”，output word: “quick”) 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的词典大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为“negative” word。</p><p>当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。</p><p>PS: 在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。</p><p>我们使用“一元模型分布（unigram distribution）”来选择“negative words”。个单词被选作negative sample的概率跟它出现的频次有关，出现频次越高的单词越容易被选作negative words。</p><p>每个单词被选为“negative words”的概率计算公式：</p><p><img src="https://img-blog.csdnimg.cn/20200714205545327.png" alt></p><p>其中 f(ωi)代表着单词出现的频次，而公式中开3/4的根号完全是基于经验的。</p><p>在代码负采样的代码实现中，unigram table有一个包含了一亿个元素的数组，这个数组是由词汇表中每个单词的索引号填充的，并且这个数组中有重复，也就是说有些单词会出现多次。那么每个单词的索引在这个数组中出现的次数该如何决定呢，有公式，也就是说计算出的负采样概率*1亿=单词在表中出现的次数。</p><p>有了这张表以后，每次去我们进行负采样时，只需要在0-1亿范围内生成一个随机数，然后选择表中索引号为这个随机数的那个单词作为我们的negative word即可。一个单词的负采样概率越大，那么它在这个表中出现的次数就越多，它被选中的概率就越大。</p><p><strong>3. Hierarchical Softmax</strong></p><p><em>3.1 霍夫曼树</em></p><p>输入：权值为(w1,w2,…wn)的n个节点</p><p>输出：对应的霍夫曼树</p><ol><li><p>将(w1,w2,…wn)看做是有n棵树的森林，每个树仅有一个节点</p></li><li><p>在森林中选择根节点权值最小的两棵树进行合并，得到一个新的树，这两颗树分布作为新树的左右子树。新树的根节点权重为左右子树的根节点权重之和</p></li><li><p>将之前的根节点权值最小的两棵树从森林删除，并把新树加入森林</p></li><li><p>重复步骤 2 和 3 直到森林里只有一棵树为止</p></li></ol><p>下面我们用一个具体的例子来说明霍夫曼树建立的过程，我们有(a，b，c，d，e，f)共6个节点，节点的权值分布是(16，4，8，6，20，3)。</p><p> 首先是最小的b和f合并，得到的新树根节点权重是7.此时森林里5棵树，根节点权重分别是16，8，6，20，7。此时根节点权重最小的6，7合并，得到新子树，依次类推，最终得到下面的霍夫曼树。</p><p><img src="https://img-blog.csdnimg.cn/20200714210647687.png" alt="3"></p><p>那么霍夫曼树有什么好处呢？一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？一般对于一个霍夫曼树的节点（根节点除外），可以约定左子树编码为0，右子树编码为1。如上图，则可以得到c的编码是00。</p><p>  在word2vec中，约定编码方式和上面的例子相反，即约定左子树编码为1，右子树编码为0，同时约定左子树的权重不小于右子树的权重。</p><p>更多原理可参考：<a href="https://blog.csdn.net/lzw66666/article/details/78934893">霍夫曼树原理</a></p><p><em>3.2Hierarchical Softmax过程</em></p><p>为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。</p><p>霍夫曼树的建立：</p><ul><li><p>根据标签（label）和频率建立霍夫曼树（label出现的频率越高，Huffman树的路径越短）</p></li><li><p>Huffman树中每一叶子结点代表一个label</p></li></ul><p><img src="https://img-blog.csdnimg.cn/20200714205623583.png" alt="4"></p><p>如上图所示：</p><p><img src="https://img-blog.csdnimg.cn/20200714205711676.png" alt></p><p><img src="https://img-blog.csdnimg.cn/20200714205759860.png" alt></p><p>注意：此时的theta是一个待定系数，它是由推导最大似然之后求解得到迭代式子。</p><p><img src="https://img-blog.csdnimg.cn/20200714205841871.png" alt></p><p><strong>使用gensim训练word2vec</strong></p><pre><code>from gensim.models.word2vec import Word2Vecmodel = Word2Vec(sentences, workers=num_workers, size=num_features)</code></pre><p><strong>参考：</strong></p><ol><li><p><a href="http://www.hankcs.com/nlp/word-vector-representations-word2vec.html">CS224n笔记2 词的向量表示：word2vec</a></p></li><li><p><a href="http://www.52nlp.cn/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E4%BA%8C%E8%AE%B2%E8%AF%8D%E5%90%91%E9%87%8F">斯坦福大学深度学习与自然语言处理第二讲：词向量 </a></p></li><li><p><a href="https://www.cnblogs.com/iloveai/p/cs224d-lecture3-note.html">(Stanford CS224d) Deep Learning and NLP课程笔记（三）：GloVe与模型的评估</a></p></li><li><p><a href="http://www.cnblogs.com/pinard/p/7249903.html">http://www.cnblogs.com/pinard/p/7249903.html</a></p></li><li><p><a href="https://blog.csdn.net/yinkun6514/article/details/79218736">https://blog.csdn.net/yinkun6514/article/details/79218736</a></p></li><li><p><a href="https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html">https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html</a></p></li></ol><h4 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h4><p>TextCNN利用CNN（卷积神经网络）进行文本特征抽取，不同大小的卷积核分别抽取n-gram特征，卷积计算出的特征图经过MaxPooling保留最大的特征值，然后将拼接成一个向量作为文本的表示。</p><p>这里我们基于TextCNN原始论文的设定，分别采用了100个大小为2,3,4的卷积核，最后得到的文本向量大小为100*3=300维。</p><p><img src="https://img-blog.csdnimg.cn/20200714205932720.jpeg" alt></p><h4 id="TextRNN"><a href="#TextRNN" class="headerlink" title="TextRNN"></a>TextRNN</h4><p>TextRNN利用RNN（循环神经网络）进行文本特征抽取，由于文本本身是一种序列，而LSTM天然适合建模序列数据。TextRNN将句子中每个词的词向量依次输入到双向双层LSTM，分别将两个方向最后一个有效位置的隐藏层拼接成一个向量作为文本的表示。</p><p><img src="https://img-blog.csdnimg.cn/20200714210806492.png" alt="5"></p><h3 id="基于TextCNN、TextRNN的文本表示"><a href="#基于TextCNN、TextRNN的文本表示" class="headerlink" title="基于TextCNN、TextRNN的文本表示"></a>基于TextCNN、TextRNN的文本表示</h3><h4 id="TextCNN-1"><a href="#TextCNN-1" class="headerlink" title="TextCNN"></a>TextCNN</h4><ul><li>模型搭建</li></ul><pre><code>self.filter_sizes = [2, 3, 4]  # n-gram windowself.out_channel = 100self.convs = nn.ModuleList([nn.Conv2d(1, self.out_channel, (filter_size, input_size), bias=True) for filter_size in self.filter_sizes])</code></pre><ul><li>前向传播</li></ul><pre><code>pooled_outputs = []for i in range(len(self.filter_sizes)):    filter_height = sent_len - self.filter_sizes[i] + 1    conv = self.convs[i](batch_embed)    hidden = F.relu(conv)  # sen_num x out_channel x filter_height x 1    mp = nn.MaxPool2d((filter_height, 1))  # (filter_height, filter_width)    # sen_num x out_channel x 1 x 1 -&gt; sen_num x out_channel    pooled = mp(hidden).reshape(sen_num, self.out_channel)    pooled_outputs.append(pooled)</code></pre><h4 id="TextRNN-1"><a href="#TextRNN-1" class="headerlink" title="TextRNN"></a>TextRNN</h4><ul><li>模型搭建</li></ul><pre><code>input_size = config.word_dimsself.word_lstm = LSTM(    input_size=input_size,    hidden_size=config.word_hidden_size,    num_layers=config.word_num_layers,    batch_first=True,    bidirectional=True,    dropout_in=config.dropout_input,    dropout_out=config.dropout_hidden,)</code></pre><ul><li>前向传播</li></ul><pre><code>hiddens, _ = self.word_lstm(batch_embed, batch_masks)  # sent_len x sen_num x hidden*2hiddens.transpose_(1, 0)  # sen_num x sent_len x hidden*2if self.training:    hiddens = drop_sequence_sharedmask(hiddens, self.dropout_mlp)</code></pre><h3 id="使用HAN用于文本分类"><a href="#使用HAN用于文本分类" class="headerlink" title="使用HAN用于文本分类"></a>使用HAN用于文本分类</h3><p><a href="https://link.zhihu.com/?target=http%3A//www.aclweb.org/anthology/N16-1174">Hierarchical Attention Network for Document Classification</a>(HAN)基于层级注意力，在单词和句子级别分别编码并基于注意力获得文档的表示，然后经过Softmax进行分类。其中word encoder的作用是获得句子的表示，可以替换为上节提到的TextCNN和TextRNN，也可以替换为下节中的BERT。</p><p><img src="https://img-blog.csdnimg.cn/20200714210015326.png" alt="Sequence Intent Classification Using Hierarchical Attention..."></p><h3 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h3><p>本章介绍了Word2Vec的使用，以及TextCNN、TextRNN的原理和训练，最后介绍了用于长文档分类的HAN。</p><h3 id="本章作业"><a href="#本章作业" class="headerlink" title="本章作业"></a>本章作业</h3><ul><li>尝试通过Word2Vec训练词向量</li><li>尝试使用TextCNN、TextRNN完成文本表示</li><li>尝试使用HAN进行文本分类</li></ul><p><strong>参考：</strong></p><ol><li><a href="https://mp.weixin.qq.com/s/I-yeHQopTFdNk67Ir_iWiA">https://mp.weixin.qq.com/s/I-yeHQopTFdNk67Ir_iWiA</a></li><li><a href="https://github.com/hecongqing/2018-daguan-competition">https://github.com/hecongqing/2018-daguan-competition</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Datawhale</tag>
      
      <tag>Classification</tag>
      
      <tag>word2vec</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】新闻文本分类之深度学习FastText</title>
    <link href="/2020/07/27/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0FastText/"/>
    <url>/2020/07/27/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0FastText/</url>
    
    <content type="html"><![CDATA[<p>当你写东西或讲话的时候，始终要想到使每个普通工人都懂得，都相信你的号召，都决心跟着你走。要想到你究竟为什么人写东西，向什么人讲话。——《反对党八股》<br><!--more---></p><p>在上一章节，我们使用传统机器学习算法来解决了文本分类问题，从本章开始我们将尝试使用深度学习方法。与传统机器学习不同，深度学习既提供特征提取功能，也可以完成分类的功能。</p><p>本次学习我们主要介绍FastText。</p><p>fastText是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：<br>1、fastText在保持高精度的情况下加快了训练速度和测试速度<br>2、fastText不需要预训练好的词向量，fastText会自己训练词向量<br>3、fastText两个重要的优化：层级 Softmax、N-gram</p><pre><code class="lang-python">import fasttextmodel = fasttext.train_supervised(&#39;train.csv&#39;, lr=1.0, wordNgrams=2, verbose=2, minCount=1, epoch=25, loss=&quot;hs&quot;)val_pred = [model.predict(x)[0][0].split(&#39;__&#39;)[-1] for x in df_train.iloc[-5000:][&#39;text&#39;]]print(f1_score(df_train[&#39;label&#39;].values[-5000:].astype(str), val_pred, average=&#39;macro&#39;))0.8256254253081777</code></pre>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Datawhale</tag>
      
      <tag>Classification</tag>
      
      <tag>FastText</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】新闻文本分类之机器学习文本分类</title>
    <link href="/2020/07/23/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    <url>/2020/07/23/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<p>今我睹子之难穷也，吾非至于子之门则殆矣。<br><!--more---></p><h1 id="文本表示方法实践"><a href="#文本表示方法实践" class="headerlink" title="文本表示方法实践"></a>文本表示方法实践</h1><p>自然语言总需要转换成数值等表示才能被线性模型等处理。下面利用task1&amp;2提到的编码方式进行实践。</p><h2 id="Label"><a href="#Label" class="headerlink" title="Label"></a>Label</h2><p>这种编码方式是把自然语言首先分词成基本语素单元，然后把不同的单元赋予唯一的整数编码。比如本次比赛提供的数据集就是该编码方式。每条数据都是整数序列，最大整数为7549，最小整数为0。</p><p>这样做的好处是相对节约内存，且实现简单；坏处是破坏了自然语言中部分语义，无法进一步进行诸如删除停用词、词根提取等操作。另外，编码的大小可能会让算法误以为词和词之间有大小关系。</p><p>因为原来的数据集就是此编码方法，故不再赘述。</p><h2 id="Bag-of-Words-CountVectorizer"><a href="#Bag-of-Words-CountVectorizer" class="headerlink" title="Bag of Words: CountVectorizer"></a>Bag of Words: CountVectorizer</h2><p>用于机器学习的文本表示有一种最简单的方法，也是最有效且最常用的方法，就是使用词袋（bag-of-words）表示。使用这种表示方式时，我们舍弃了输入文本中的大部分结构，如章节、段落、句子和格式，<strong>只计算语料库中每个单词在每个文本中的出现频次</strong>。舍弃结构并仅计算单词出现次数，这会让脑海中出现将文本表示为“袋”的画面。</p><pre><code class="lang-python">from sklearn.feature_extraction.text import CountVectorizerbagvect = CountVectorizer(max_df=.15)bagvect.fit(corpus)feature_names = bagvect.get_feature_names()print(&quot;Number of features: &#123;&#125;&quot;.format(len(feature_names)))print(&quot;First 20 features:\n&#123;&#125;&quot;.format(feature_names[:20]))Number of features: 4740First 20 features:[&#39;10&#39;, &#39;100&#39;, &#39;1000&#39;, &#39;1001&#39;, &#39;1002&#39;, &#39;1004&#39;, &#39;1005&#39;, &#39;1006&#39;, &#39;1007&#39;, &#39;1008&#39;, &#39;1009&#39;, &#39;101&#39;, &#39;1010&#39;, &#39;1012&#39;, &#39;1013&#39;, &#39;1014&#39;, &#39;102&#39;, &#39;1020&#39;, &#39;1022&#39;, &#39;1023&#39;]bag_of_words = bagvect.transform(corpus)print(&quot;bag_of_words: &#123;&#125;&quot;.format(repr(bag_of_words)))print(bag_of_words[0].toarray())bag_of_words: &lt;10000x4740 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;    with 813365 stored elements in Compressed Sparse Row format&gt;[[0 0 0 ... 0 0 0]]</code></pre><p>词袋表示保存在一个 SciPy 稀疏矩阵中，这种数据格式只保存非零元素</p><p>矩阵的形状为 10000x4740，每行对应于两个数据点之一，每个特征对应于词表中的一个单词。当然，我们在这里选了10000个样本，如果把所有数据集都给转化成词袋向量，那么矩阵形状将会是 200000×6859。</p><h2 id="Hash编码实践"><a href="#Hash编码实践" class="headerlink" title="Hash编码实践"></a>Hash编码实践</h2><p><a href="https://chenk.tech/posts/eb79fc5f.html">https://chenk.tech/posts/eb79fc5f.html</a></p><pre><code class="lang-python">from sklearn.feature_extraction.text import HashingVectorizerhvec = HashingVectorizer(n_features=10000)hvec.fit(corpus)h_words = hvec.transform(corpus)print(&quot;h_words: &#123;&#125;&quot;.format(repr(h_words)))h_words: &lt;10000x10000 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;    with 2739957 stored elements in Compressed Sparse Row format&gt;</code></pre><p>我选取了10000个样本，将其映射到10000个特征的Hash向量中。</p><pre><code class="lang-python">print(h_words[0]) (0, 0)    -0.009886463280261834  (0, 6)    -0.04943231640130917  (0, 9)    0.03954585312104734  (0, 26)    -0.04943231640130917  (0, 42)    0.01977292656052367  (0, 70)    0.01977292656052367  (0, 74)    -0.12852402264340385  (0, 94)    0.009886463280261834  (0, 99)    0.01977292656052367  (0, 109)    -0.009886463280261834  :    :  (0, 9642)    -0.009886463280261834  (0, 9650)    0.01977292656052367  (0, 9653)    0.009886463280261834  (0, 9659)    -0.04943231640130917  (0, 9715)    -0.009886463280261834  (0, 9721)    0.009886463280261834  (0, 9729)    -0.009886463280261834  (0, 9741)    0.009886463280261834  (0, 9765)    -0.01977292656052367  (0, 9781)    0.01977292656052367  (0, 9821)    -0.009886463280261834  (0, 9862)    0.009886463280261834  (0, 9887)    -0.009886463280261834  (0, 9932)    0.009886463280261834</code></pre><p>输出的含义，前面的元组代表了该特征在词袋中的位置，后面的数值代表了对应的Hash值。</p><h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><p>TF-IDF（Term Frequency-inverse Document Frequency）是一种针对关键词的统计分析方法，用于评估一个词对一个文件集或者一个语料库的重要程度。</p><pre><code class="lang-python">from sklearn.feature_extraction.text import TfidfVectorizertvec = TfidfVectorizer()tvec.fit(corpus)feature_names = tvec.get_feature_names()print(&quot;Number of features: &#123;&#125;&quot;.format(len(feature_names)))print(&quot;First 20 features:\n&#123;&#125;&quot;.format(feature_names[:20]))Number of features: 5333First 20 features:[&#39;10&#39;, &#39;100&#39;, &#39;1000&#39;, &#39;1001&#39;, &#39;1002&#39;, &#39;1004&#39;, &#39;1005&#39;, &#39;1006&#39;, &#39;1007&#39;, &#39;1008&#39;, &#39;1009&#39;, &#39;101&#39;, &#39;1010&#39;, &#39;1012&#39;, &#39;1013&#39;, &#39;1014&#39;, &#39;1018&#39;, &#39;102&#39;, &#39;1020&#39;, &#39;1022&#39;]t_words = tvec.transform(corpus)print(&quot;t_words: &#123;&#125;&quot;.format(repr(t_words)))print(t_words[0].toarray())t_words: &lt;10000x5333 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;    with 2797304 stored elements in Compressed Sparse Row format&gt;[[0. 0. 0. ... 0. 0. 0.]]# 找到数据集中每个特征的最大值max_value = t_words.max(axis=0).toarray().ravel()sorted_by_tfidf = max_value.argsort()# 获取特征名称feature_names = np.array(tvec.get_feature_names())print(&quot;Features with lowest tfidf:\n&#123;&#125;&quot;.format(feature_names[sorted_by_tfidf[:20]]))print(&quot;Features with highest tfidf: \n&#123;&#125;&quot;.format(feature_names[sorted_by_tfidf[-20:]]))sorted_by_idf = np.argsort(tvec.idf_)print(&quot;Features with lowest idf:\n&#123;&#125;&quot;.format(feature_names[sorted_by_idf[:100]]))Features with lowest tfidf:[&#39;6844&#39; &#39;6806&#39; &#39;7201&#39; &#39;5609&#39; &#39;5585&#39; &#39;5485&#39; &#39;3453&#39; &#39;7390&#39; &#39;2322&#39; &#39;2083&#39; &#39;1222&#39; &#39;2360&#39; &#39;319&#39; &#39;2520&#39; &#39;6268&#39; &#39;3105&#39; &#39;6049&#39; &#39;4888&#39; &#39;2390&#39; &#39;2849&#39;]Features with highest tfidf: [&#39;3198&#39; &#39;2346&#39; &#39;5480&#39; &#39;4375&#39; &#39;6296&#39; &#39;1710&#39; &#39;682&#39; &#39;354&#39; &#39;4381&#39; &#39;482&#39; &#39;5990&#39; &#39;2798&#39; &#39;5907&#39; &#39;3992&#39; &#39;418&#39; &#39;513&#39; &#39;4759&#39; &#39;6250&#39; &#39;6220&#39; &#39;1633&#39;]Features with lowest idf:[&#39;3750&#39; &#39;900&#39; &#39;648&#39; &#39;6122&#39; &#39;7399&#39; &#39;2465&#39; &#39;4811&#39; &#39;4464&#39; &#39;1699&#39; &#39;299&#39; &#39;2400&#39; &#39;3659&#39; &#39;3370&#39; &#39;2109&#39; &#39;4939&#39; &#39;669&#39; &#39;5598&#39; &#39;5445&#39; &#39;4853&#39; &#39;5948&#39; &#39;2376&#39; &#39;7495&#39; &#39;4893&#39; &#39;5410&#39; &#39;340&#39; &#39;619&#39; &#39;4659&#39; &#39;1460&#39; &#39;6065&#39; &#39;1903&#39; &#39;5560&#39; &#39;6017&#39; &#39;2252&#39; &#39;4516&#39; &#39;1519&#39; &#39;2073&#39; &#39;5998&#39; &#39;5491&#39; &#39;2662&#39; &#39;5977&#39; &#39;6093&#39; &#39;1324&#39; &#39;5780&#39; &#39;3915&#39; &#39;3800&#39; &#39;5393&#39; &#39;2210&#39; &#39;5915&#39; &#39;3223&#39; &#39;4490&#39; &#39;2490&#39; &#39;1375&#39; &#39;803&#39; &#39;1635&#39; &#39;7539&#39; &#39;4411&#39; &#39;4128&#39; &#39;7543&#39; &#39;5602&#39; &#39;1866&#39; &#39;5176&#39; &#39;2799&#39; &#39;4646&#39; &#39;3700&#39; &#39;5858&#39; &#39;307&#39; &#39;913&#39; &#39;25&#39; &#39;6045&#39; &#39;1702&#39; &#39;4822&#39; &#39;3099&#39; &#39;5330&#39; &#39;1920&#39; &#39;1567&#39; &#39;2614&#39; &#39;4190&#39; &#39;1080&#39; &#39;5510&#39; &#39;4149&#39; &#39;3166&#39; &#39;3530&#39; &#39;192&#39; &#39;5659&#39; &#39;3618&#39; &#39;4525&#39; &#39;3686&#39; &#39;6038&#39; &#39;1767&#39; &#39;5589&#39; &#39;5736&#39; &#39;6831&#39; &#39;7377&#39; &#39;4969&#39; &#39;1394&#39; &#39;6104&#39; &#39;7010&#39; &#39;6407&#39; &#39;5430&#39; &#39;23&#39;]</code></pre><h2 id="多个单词的词袋：N-gram《》"><a href="#多个单词的词袋：N-gram《》" class="headerlink" title="多个单词的词袋：N-gram《》"></a>多个单词的词袋：N-gram《》</h2><p>使用词袋表示的主要缺点之一是完全舍弃了单词顺序。因此，“it’s bad, not good at all”（电影很差，一点也不好）和“it’s good, not bad at all”（电影很好，还不错）这两个字符串的词袋表示完全相同，尽管它们的含义相反。将“not”（不）放在单词前面，这只是上下文很重要的一个例子（可能是一个极端的例子）。幸运的是，使用词袋表示时有一种获取上下文的方法，就是不仅考虑单一词例的计数，而且还考虑相邻的两个或三个词例的计数。两个词例被称为二元分词（bigram），三个词例被称为三元分词（trigram），更一般的词例序列被称为 n 元分词（n-gram）。我们可以通过改变 CountVectorizer 或 TfidfVectorizer 的 ngram_range 参数来改变作为特征的词例范围。ngram_range 参数是一个元组，包含要考虑的词例序列的最小长度和最大长度。</p><p>在大多数情况下，添加二元分词会有所帮助。添加更长的序列（一直到五元分词）也可能有所帮助，但这会导致特征数量的大大增加，也可能会导致过拟合，因为其中包含许多非常具体的特征。原则上来说，二元分词的数量是一元分词数量的平方，三元分词的数量是一元分词数量的三次方，从而导致非常大的特征空间。在实践中，更高的 n 元分词在数据中的出现次数实际上更少，原因在于（英语）语言的结构，不过这个数字仍然很大。</p><pre><code class="lang-python">from sklearn.feature_extraction.text import TfidfVectorizertvec = TfidfVectorizer(ngram_range=(1,3), min_df=5)tvec.fit(corpus)# 找到数据集中每个特征的最大值max_value = t_words.max(axis=0).toarray().ravel()sorted_by_tfidf = max_value.argsort()# 获取特征名称feature_names = np.array(tvec.get_feature_names())print(&quot;Features with lowest tfidf:\n&#123;&#125;&quot;.format(feature_names[sorted_by_tfidf[:20]]))print(&quot;Features with highest tfidf: \n&#123;&#125;&quot;.format(feature_names[sorted_by_tfidf[-20:]]))sorted_by_idf = np.argsort(tvec.idf_)print(&quot;Features with lowest idf:\n&#123;&#125;&quot;.format(feature_names[sorted_by_idf[:100]]))Features with lowest tfidf:[&#39;1008 5612&#39; &#39;100 5560&#39; &#39;1018 4089 5491&#39; &#39;101 648 900&#39; &#39;1006 3750 826&#39; &#39;1018 1066 3231&#39; &#39;101 5560 3568&#39; &#39;100 5589&#39; &#39;1018 2119 281&#39; &#39;101 5560 3659&#39; &#39;1018 1066 3166&#39; &#39;100 5598 1465&#39; &#39;1000 5011&#39; &#39;101 2662 4939&#39; &#39;100 5602&#39; &#39;101 873 648&#39; &#39;1006 2265 648&#39; &#39;1008 5640&#39; &#39;1008 5689&#39; &#39;101 856 531&#39;]Features with highest tfidf: [&#39;101 2087&#39; &#39;1018 1066 281&#39; &#39;101 648 3440&#39; &#39;1006 5640 3641&#39; &#39;101 760 4233&#39; &#39;1006 6017&#39; &#39;1018 1066 6983&#39; &#39;1014 3750 3659&#39; &#39;100 5430 2147&#39; &#39;100 5510 2471&#39; &#39;1018 1141&#39; &#39;1006 1866 5977&#39; &#39;1018 2119 3560&#39; &#39;1018 2662 3068&#39; &#39;101 1844 4486&#39; &#39;101 2304 3659&#39; &#39;1006 3750 5330&#39; &#39;101 5589&#39; &#39;1008 900 3618&#39; &#39;100 6122 2489&#39;]Features with lowest idf:[&#39;3750&#39; &#39;900&#39; &#39;648&#39; &#39;2465&#39; &#39;6122&#39; &#39;7399&#39; &#39;4811&#39; &#39;4464&#39; &#39;1699&#39; &#39;3659&#39; &#39;2400&#39; &#39;299&#39; &#39;3370&#39; &#39;2109&#39; &#39;4939&#39; &#39;5598&#39; &#39;669&#39; &#39;5445&#39; &#39;4853&#39; &#39;2376&#39; &#39;5948&#39; &#39;7495&#39; &#39;4893&#39; &#39;5410&#39; &#39;340&#39; &#39;619&#39; &#39;4659&#39; &#39;1460&#39; &#39;6065&#39; &#39;4516&#39; &#39;1903&#39; &#39;5560&#39; &#39;6017&#39; &#39;2252&#39; &#39;2073&#39; &#39;1519&#39; &#39;5491&#39; &#39;5998&#39; &#39;2662&#39; &#39;5977&#39; &#39;1324&#39; &#39;5780&#39; &#39;6093&#39; &#39;3915&#39; &#39;5393&#39; &#39;2210&#39; &#39;3800&#39; &#39;3223&#39; &#39;5915&#39; &#39;4490&#39; &#39;2490&#39; &#39;803&#39; &#39;1635&#39; &#39;4128&#39; &#39;1375&#39; &#39;7539&#39; &#39;4411&#39; &#39;7543&#39; &#39;5602&#39; &#39;2799&#39; &#39;1866&#39; &#39;5176&#39; &#39;5858&#39; &#39;4646&#39; &#39;3700&#39; &#39;307&#39; &#39;6045&#39; &#39;1702&#39; &#39;25&#39; &#39;913&#39; &#39;5330&#39; &#39;4822&#39; &#39;2614&#39; &#39;3099&#39; &#39;1920&#39; &#39;1567&#39; &#39;4190&#39; &#39;4149&#39; &#39;5510&#39; &#39;1080&#39; &#39;3166&#39; &#39;3659 3370&#39; &#39;3530&#39; &#39;192&#39; &#39;3618&#39; &#39;4525&#39; &#39;5659&#39; &#39;3686&#39; &#39;6038&#39; &#39;1767&#39; &#39;5736&#39; &#39;7377&#39; &#39;5589&#39; &#39;6831&#39; &#39;3370 3370&#39; &#39;1394&#39; &#39;4969&#39; &#39;5430&#39; &#39;7010&#39; &#39;6104&#39;]</code></pre><h1 id="无监督探索"><a href="#无监督探索" class="headerlink" title="无监督探索"></a>无监督探索</h1><h2 id="PCA可视化"><a href="#PCA可视化" class="headerlink" title="PCA可视化"></a>PCA可视化</h2><pre><code class="lang-python">from sklearn.decomposition import PCApca = PCA(n_components=2)pca_vec = pca.fit_transform(t_words.toarray())pca_vec.shape, pca.explained_variance_ratio_((10000, 2), array([0.03817303, 0.02684457]))</code></pre><p>我们成功将tfidf转化后的词向量压缩成2维向量，这样就能够在二维平面可视化了。</p><p>后面的<code>explained_variance_ratio_</code>代表着经过PCA算法压缩后，保留的信息量。这个数值还是偏低，因此这种PCA压缩方法仅适用于实验。</p><pre><code class="lang-python">plt.figure(figsize=(12,10))plt.scatter(pca_vec[:,0], pca_vec[:,1], c=labels)</code></pre><p><img src="/2020/07/23/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/1.png" alt></p><h2 id="主题建模"><a href="#主题建模" class="headerlink" title="主题建模"></a>主题建模</h2><p>常用于文本数据的一种特殊技术是主题建模（topic modeling），这是描述将每个文档分配给一个或多个主题的任务（通常是无监督的）的概括性术语。这方面一个很好的例子是新闻数据，它们可以被分为“政治”“体育”“金融”等主题。如果为每个文档分配一个主题，那么这是一个文档聚类任务。我们学到的每个成分对应于一个主题，文档表示中的成分系数告诉我们这个文档与该主题的相关性强弱。通常来说，人们在谈论主题建模时，他们指的是一种叫作隐含狄利克雷分布（Latent Dirichlet Allocation，LDA）的特定分解方法。</p><p>我们将 LDA 应用于新闻数据集，来看一下它在实践中的效果。对于无监督的文本文档模型，通常最好删除非常常见的单词，否则它们可能会支配分析过程。我们将删除至少在15% 的文档中出现过的单词，并在删除前 15% 之后，将词袋模型限定为最常见的 10 000 个单词：</p><pre><code class="lang-python">vect = CountVectorizer(max_features=10000, max_df=.15)X = vect.fit_transform(corpus)from sklearn.decomposition import LatentDirichletAllocationlda = LatentDirichletAllocation(n_components=14, learning_method=&quot;batch&quot;, max_iter=25, random_state=0)# 我们在一个步骤中构建模型并变换数据# 计算变换需要花点时间，二者同时进行可以节省时间document_topics = lda.fit_transform(bag_of_words)# 对于每个主题（components_的一行），将特征排序（升序）# 用[:, ::-1]将行反转，使排序变为降序sorting = np.argsort(lda.components_, axis=1)[:, ::-1]# 从向量器中获取特征名称feature_names = np.array(bagvect.get_feature_names())plt.figure()plt.bar(x=range(14), height=document_topics[0])plt.xticks(list(range(14)))</code></pre><p><img src="/2020/07/23/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/2.png" alt></p><p>由LDA确定的主题词如下：</p><pre><code class="lang-python">topic 0       topic 1       topic 2       topic 3       topic 4       --------      --------      --------      --------      --------      6654          1970          7349          3464          4412          4173          2716          7354          7436          7363          1219          4553          1684          5562          6689          6861          7042          5744          3289          4986          5006          5822          6569          5105          2506          7400          5099          1999          5810          3056          3508          3654          1351          3134          6220          6223          3021          56            3648          5117          6227          4967          4036          6308          6319          7257          3396          4223          1706          2695          topic 5       topic 6       topic 7       topic 8       topic 9       --------      --------      --------      --------      --------      4967          1334          1934          4114          7328          7528          5166          1146          3198          5547          3644          6143          532           517           4768          1899          2695          4802          812           3231          6678          1616          419           3090          5492          5744          7532          4089          4163          4080          6047          368           3725          5305          4120          1252          2918          2851          177           2331          5814          2968          6227          7251          3019          1170          3032          6639          2835          6613          topic 10      topic 11      topic 12      topic 13      --------      --------      --------      --------      3523          4902          5178          5122          3342          1258          6014          6920          6722          343           5920          5519          6352          4089          4603          7154          3186          5226          3648          4381          5179          810           4042          4760          4369          6284          1724          4412          3501          3477          4450          4595          2334          7127          657           3377          2722          7077          5803          7006</code></pre><h2 id="t-SNE可视化"><a href="#t-SNE可视化" class="headerlink" title="t-SNE可视化"></a>t-SNE可视化</h2><p>t-SNE是当前最流行的数据可视化方法。将TF-IDF转化后的向量可视化如下：</p><pre><code class="lang-python">from sklearn.manifold import TSNEwords_emb = TSNE(n_components=2).fit_transform(t_words)plt.figure(figsize=(12,10))plt.scatter(words_emb[:,0], words_emb[:,1], c=labels)</code></pre><p><img src="/2020/07/23/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/3.png" alt></p><p>将HashingVectorizer转化后的向量经过t-SNE算法可视化结果分享如下：</p><pre><code class="lang-python">hash_words_emb = TSNE(n_components=2).fit_transform(h_words)plt.figure(figsize=(12,10))plt.scatter(hash_words_emb[:,0], hash_words_emb[:,1], c=labels)</code></pre><p><img src="/2020/07/23/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/4.png" alt></p><h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><p>首先导入相关库</p><pre><code class="lang-python">from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizerfrom sklearn.pipeline import make_pipelinefrom sklearn.linear_model import LogisticRegression, RidgeClassifierfrom sklearn.model_selection import GridSearchCV, train_test_split, cross_val_scorefrom sklearn.multiclass import OneVsRestClassifierfrom sklearn.svm import SVCfrom sklearn.metrics import f1_score</code></pre><p><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model</a></p><h2 id="RidgeClassifier-CountVectorizer"><a href="#RidgeClassifier-CountVectorizer" class="headerlink" title="RidgeClassifier+CountVectorizer"></a>RidgeClassifier+CountVectorizer</h2><p>首先我们使用教程中的范例</p><pre><code class="lang-python">X, y = df_train[&#39;text&#39;][:10000], df_train[&#39;label&#39;][:10000]vectorizer = CountVectorizer(max_features=3000)X = vectorizer.fit_transform(X)clf = RidgeClassifier()clf.fit(X[:5000], y[:5000]) # 使用前5000个样本进行训练val_pred = clf.predict(X[5000:]) # 使用后5000个样本进行预测print(f1_score(y[5000:], val_pred, average=&#39;macro&#39;))0.6322204326986258</code></pre><p>我们把10000个样本中，前5000个用于训练，后5000个用于测试，最终结果以F1指标展示，结果为0.63，不太令人满意。</p><h2 id="LogisticRegression-TFIDF"><a href="#LogisticRegression-TFIDF" class="headerlink" title="LogisticRegression+TFIDF"></a>LogisticRegression+TFIDF</h2><p>最常见的线性分类算法是 Logistic 回归。虽然 LogisticRegression 的名字中含有回归（regression），但它是一种分类算法，并不是回归算法，不应与 LinearRegression 混淆。<br>我们可以将 LogisticRegression 和 LinearSVC 模型应用到经过tfidf处理的新闻文本数据集上。</p><p>我们使用了sklearn中的划分数据集的方法<code>train_test_split</code>，将数据集划分成训练集和测试集两部分。但是这样一来，数据集中的测试集部分将不能被训练，未免有点可惜。</p><p>我们在训练时，采用了pipeline方式，Pipeline 类可以将多个处理步骤合并（glue）为单个 scikit-learn 估计器。Pipeline 类本身具有 fit、predict 和 score 方法，其行为与 scikit-learn 中的其 他模型相同。Pipeline 类最常见的用例是将预处理步骤（比如数据缩放）与一个监督模型 （比如分类器）链接在一起。</p><pre><code class="lang-python">%%timeX_train, X_test, y_train, y_test = train_test_split(df_train[&#39;text&#39;][:10000], df_train[&#39;label&#39;][:10000], random_state=0)pipe_logis = make_pipeline(TfidfVectorizer(min_df=5, ngram_range=(1,3)), LogisticRegression())param_grid = &#123;&#39;logisticregression__C&#39;: [0.001, 0.01, 0.1, 1, 10]&#125;grid = GridSearchCV(pipe_logis, param_grid, cv=5)grid.fit(X_train, y_train)print(&quot;Best params:\n&#123;&#125;\n&quot;.format(grid.best_params_))print(&quot;Best cross-validation score: &#123;:.2f&#125;&quot;.format(grid.best_score_))print(&quot;Test-set score: &#123;:.2f&#125;&quot;.format(grid.score(X_test, y_test)))Best params:&#123;&#39;logisticregression__C&#39;: 10&#125;Best cross-validation score: 0.91Test-set score: 0.92</code></pre><h2 id="SVC-tfidf"><a href="#SVC-tfidf" class="headerlink" title="SVC+tfidf"></a>SVC+tfidf</h2><p>这次我们使用非线性模型中大名鼎鼎的SVM模型，并采用交叉验证的方法划分数据集，不浪费任何一部分数据。</p><pre><code class="lang-python">X_train, y_train = df_train[&#39;text&#39;][:10000], df_train[&#39;label&#39;][:10000]pipe_svc = make_pipeline(TfidfVectorizer(min_df=5), SVC()) # decision_function_shape=&#39;ovr&#39;scores = cross_val_score(pipe_svc, X_train, y_train, cv=5, n_jobs=-1)print(scores.mean())0.8924</code></pre><h1 id="模型评价"><a href="#模型评价" class="headerlink" title="模型评价"></a>模型评价</h1><p>TR，FN，precision，recall等的进一步解释，请参考以下链接：<br><a href="https://www.zhihu.com/question/30643044">https://www.zhihu.com/question/30643044</a></p><h1 id="进一步优化"><a href="#进一步优化" class="headerlink" title="进一步优化"></a>进一步优化</h1><p>使用机器学习模型+巧妙的特征工程，我们可以达到90%以上的精度，这在14分类问题中已经很惊人了。然而我们的工作并没有结束，还有许许多多的问题等着我们去探索。比如</p><ul><li>删除停用词、罕见词、其他常见词和不能反映特征的词</li><li>类别不平衡问题</li></ul><p>周志华《机器学习》中介绍到，分类学习方法都有一个共同的基本假设，即不同类别的训练样例数目相当。如果不同类别的训练样例数目稍有差别，对学习结果的影响通常也不大，但若样本类别数目差别很大，属于极端不均衡，则会对学习过程（模型训练）造成困扰。这些学习算法的设计背后隐含的优化目标是数据集上的分类准确度，而这会导致学习算法在不平衡数据上更偏向于含更多样本的多数类。多数不平衡学习（imbalance learning）算法就是为了解决这种“对多数类的偏好”而提出的。如果正负类样本类别不平衡比例超过4:1，那么其分类器会大大地因为数据不平衡性而无法满足分类要求</p><p>关于如何解决类别不平衡的问题，可以参考以下链接：<br><a href="https://zhuanlan.zhihu.com/p/84322912">https://zhuanlan.zhihu.com/p/84322912</a><br><a href="https://zhuanlan.zhihu.com/p/36381828">https://zhuanlan.zhihu.com/p/36381828</a></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>Datawhale</tag>
      
      <tag>Classification</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】零基础入门NLP之新闻文本分类之数据读取与分析</title>
    <link href="/2020/07/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    <url>/2020/07/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>不要对我有任何期待哦！<br><!--more---></p><h1 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h1><p>Colab</p><pre><code class="lang-python">import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitimport seaborn as snsimport scipyfrom collections import Counter</code></pre><h1 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h1><pre><code class="lang-python">df_train = pd.read_csv(train_path, sep=&#39;\t&#39;)df_test = pd.read_csv(test_path, sep=&#39;\t&#39;)</code></pre><h1 id="数据探索"><a href="#数据探索" class="headerlink" title="数据探索"></a>数据探索</h1><h2 id="简单查看数据"><a href="#简单查看数据" class="headerlink" title="简单查看数据"></a>简单查看数据</h2><pre><code class="lang-python">df_train.head(), len(df_train)(   label                                               text 0      2  2967 6758 339 2021 1854 3731 4109 3792 4149 15... 1     11  4464 486 6352 5619 2465 4802 1452 3137 5778 54... 2      3  7346 4068 5074 3747 5681 6093 1777 2226 7354 6... 3      2  7159 948 4866 2109 5520 2490 211 3956 5520 549... 4      3  3646 3055 3055 2490 4659 6065 3370 5814 2465 5..., 200000)</code></pre><p>发现text域的数据是字符串。我们想要得到整数序列。可以用字符串分割<code>split()</code>。</p><pre><code class="lang-python">print(len(df_train[&#39;text&#39;][0]), type(df_train[&#39;text&#39;][0]))df_train.head()</code></pre><p><img src="/2020/07/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/1.png" alt></p><h2 id="长度分布"><a href="#长度分布" class="headerlink" title="长度分布"></a>长度分布</h2><h3 id="describe"><a href="#describe" class="headerlink" title="describe"></a>describe</h3><p>这里与教程中的方法有所不同。vectorize是numpy中很方便的函数，作用和pandas中<code>apply</code>差不多。用法：</p><p><code>np.vectorize(function)(array)</code></p><p>输入待处理的array，以及逐元素处理函数function，返回经过处理后的ndarray。原来的array则不受影响。</p><p>当前我使用的函数<code>split_df</code>负责将一行数据按空格切分成整数列表，然后计算该列表的长度。</p><pre><code class="lang-python">def split_df(df_row):    return len(str(df_row).split())len_dist = np.vectorize(split_df)(df_train[&#39;text&#39;])len_test_dist = np.vectorize(split_df)(df_test[&#39;text&#39;])</code></pre><p>使用describe函数查看训练集和测试集中的数据长度分布</p><pre><code class="lang-python">print(pd.Series(len_dist).describe())print(pd.Series(len_test_dist).describe())count    200000.000000mean        907.207110std         996.029036min           2.00000025%         374.00000050%         676.00000075%        1131.000000max       57921.000000dtype: float64count    50000.000000mean       909.844960std       1032.313375min         14.00000025%        370.00000050%        676.00000075%       1133.000000max      41861.000000dtype: float64</code></pre><p>通过数据描述可以看到</p><p>训练集共200,000条新闻，每条新闻平均907个字符，最短的句子长度为2，最长的句子长度为57921，其中75%以下的数据长度在1131以下。</p><p>测试集共50,000条新闻，每条新闻平均909个字符，最短句子长度为14，最长句子41861,75%以下的数据长度在1133以下。</p><p>训练集和测试集就长度来说似乎是同一分布。</p><h3 id="直方图"><a href="#直方图" class="headerlink" title="直方图"></a>直方图</h3><p>绘制直方图查看训练集和测试集中的数据长度分布</p><pre><code class="lang-python">fig, ax = plt.subplots(1,1,figsize=(12,6))ax = plt.hist(x=len_dist, bins=100)ax = plt.hist(x=len_test_dist, bins=100)plt.xlim([0, max(max(len_dist), max(len_test_dist))])plt.xlabel(&quot;length of sample&quot;)plt.ylabel(&quot;number of sample&quot;)plt.legend([&#39;train_len&#39;,&#39;test_len&#39;])plt.show()</code></pre><p><img src="/2020/07/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/2.png" alt></p><p>使用seaborn绘制更好的图</p><p>seaborn计算的纵坐标是频率，而不是出现次数。由于训练集和测试集的数据量不一样，因此用频率更加科学、更能看出是否符合同一分布。</p><pre><code class="lang-python">plt.figure(figsize=(15,5))ax = sns.distplot(len_dist, bins=100)ax = sns.distplot(len_test_dist, bins=100)plt.xlim([0, max(max(len_dist), max(len_test_dist))])plt.xlabel(&quot;length of sample&quot;)plt.ylabel(&quot;prob of sample&quot;)plt.legend([&#39;train_len&#39;,&#39;test_len&#39;])</code></pre><p><img src="/2020/07/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/3.png" alt></p><p>通过直方图，我们能直观感受到训练集和测试集的长度分布都属于右偏分布。按理说分析到这份儿上就该停了。</p><h3 id="同分布验证"><a href="#同分布验证" class="headerlink" title="同分布验证"></a>同分布验证</h3><pre><code class="lang-python">import scipyscipy.stats.ks_2samp(len_dist, len_test_dist)Ks_2sampResult(statistic=0.004049999999999998, pvalue=0.5279614323123156)</code></pre><p>P值为0.52，比指定的显著水平（假设为5%）大，我们认为二者同分布。</p><h3 id="截断位置"><a href="#截断位置" class="headerlink" title="截断位置"></a>截断位置</h3><p>在输入模型进行训练之前，我们要把所有的数据长度统一化，数据肯定要截断。但是在什么位置截断合适呢？</p><p>考虑到数据长度分布是长尾分布，log一下看看是不是正态分布，如果是正态分布，使用3sigma法则作为截断的参考。如果不是，则就只能瞎猜了</p><p>测量拟合分布的均值和方差sigma原则</p><p>$1\sigma$原则：数值分布在$(\mu-\sigma,\mu+\sigma)$中的概率为0.6526；</p><p>$2\sigma$原则：数值分布在$(\mu-2\sigma,\mu+2\sigma)$中的概率为0.9544；</p><p>$3\sigma$原则：数值分布在$(\mu-3\sigma,\mu+3\sigma)$中的概率为0.9974；</p><p>由于“小概率事件”和假设检验的基本思想 “小概率事件”通常指发生的概率小于5%的事件，认为在一次试验中该事件是几乎不可能发生的。由此可见X落在$(\mu-3\sigma,\mu+3\sigma)$以外的概率小于千分之三，在实际问题中常认为相应的事件是不会发生的，基本上可以把区间$(\mu-3\sigma,\mu+3\sigma)$看作是随机变量X实际可能的取值区间，这称之为正态分布的“$3\sigma$”原则。</p><pre><code class="lang-python">log_len_dist = np.log(1+len_dist)log_len_test_dist = np.log(1+len_test_dist)plt.figure(figsize=(15,5))ax = sns.distplot(log_len_dist)ax = sns.distplot(log_len_test_dist)plt.xlabel(&quot;log length of sample&quot;)plt.ylabel(&quot;prob of log&quot;)plt.legend([&#39;train_len&#39;,&#39;test_len&#39;])</code></pre><p><img src="/2020/07/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/4.png" alt></p><p>从log图上也能看出二者（很像）同分布。</p><p>下面我想验证一下我的猜想：该分布为正态分布，且训练集和测试集为同分布。</p><p>先验证训练集分布为正态分布：</p><pre><code class="lang-python">_, lognormal_ks_pvalue = scipy.stats.kstest(rvs=log_len_dist, cdf=&#39;norm&#39;)print(&#39;P value is &#39;, lognormal_ks_pvalue)P value is  0.0</code></pre><p>？0？？？拟合优度检验，p值为0，意思就是说这不是一个正态分布。<br>关于分布检验，参考<a href="https://blog.csdn.net/QimaoRyan/article/details/72861387">这篇文章</a></p><p>之前我们把数据log了一下，但是这里有更科学的变换方式。log只是box-cox变换的特殊形式。我们使用box-cox变换再次做一下验证，是否为正态分布：</p><pre><code class="lang-python">trans_data, lam = scipy.stats.boxcox(len_dist+1)scipy.stats.normaltest(trans_data)NormaltestResult(statistic=1347.793358118494, pvalue=2.1398873511704724e-293)</code></pre><p>e后面跟了那么多负数，我佛了。这说明我们的假设不成立。</p><p>但总归是要猜一个截断值的。看log图上8.5的位置比较靠谱。np.exp(8.5)=4914约等于5000，因此我初步决定把截断长度定为5000。</p><h2 id="类别信息"><a href="#类别信息" class="headerlink" title="类别信息"></a>类别信息</h2><h3 id="简单查看类别信息表"><a href="#简单查看类别信息表" class="headerlink" title="简单查看类别信息表"></a>简单查看类别信息表</h3><p>先改造一下df_train，多加几个字段，分别是</p><ul><li>text-split，将text字段分词</li><li>len，每条新闻长度</li><li>first_char，新闻第一个字符</li><li>last_char，新闻最后一个字符</li><li>most_freq，新闻最常出现的字符</li></ul><pre><code class="lang-python">df_train[&#39;text_split&#39;] = df_train[&#39;text&#39;].apply(lambda x:x.split())df_train[&#39;len&#39;] = df_train[&#39;text&#39;].apply(lambda x:len(x.split()))df_train[&#39;first_char&#39;] = df_train[&#39;text_split&#39;].apply(lambda x:x[0])df_train[&#39;last_char&#39;] = df_train[&#39;text_split&#39;].apply(lambda x:x[-1])df_train[&#39;most_freq&#39;] = df_train[&#39;text_split&#39;].apply(lambda x:np.argmax(np.bincount(x)))df_train.head()</code></pre><p><img src="/2020/07/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/2020-07-25-12-22-01.png" alt></p><p>构建一个类别信息表。</p><ul><li>count，该类别新闻个数</li><li>len_mean，该类别新闻平均长度</li><li>len_std，该类别新闻长度标准差</li><li>len_min，该类别新闻长度最小值</li><li>len_max，该类别新闻长度最大值</li><li>freq_fc，该类别新闻最常出现的第一个字符</li><li>freq_lc，该类别新闻最常出现的最后一个字符</li><li>freq_freq，该类别新闻最常出现的字符</li></ul><pre><code class="lang-python">df_train_info = pd.DataFrame(columns=[&#39;count&#39;,&#39;len_mean&#39;,&#39;len_std&#39;,&#39;len_min&#39;,&#39;len_max&#39;,&#39;freq_fc&#39;,&#39;freq_lc&#39;,&#39;freq_freq&#39;])for name, group in df_train.groupby(&#39;label&#39;):    count = len(group) # 该类别新闻数    len_mean = np.mean(group[&#39;len&#39;]) # 该类别长度平均值    len_std = np.std(group[&#39;len&#39;]) # 长度标准差    len_min = np.min(group[&#39;len&#39;]) # 最短的新闻长度    len_max = np.max(group[&#39;len&#39;]) # 最长的新闻长度    freq_fc = np.argmax(np.bincount(group[&#39;first_char&#39;])) # 最频繁出现的首词    freq_lc = np.argmax(np.bincount(group[&#39;last_char&#39;])) # 最频繁出现的末词    freq_freq = np.argmax(np.bincount(group[&#39;most_freq&#39;])) # 该类别最频繁出现的词    df_train_info.loc[name] = [count,len_mean,len_std,len_min,len_max,freq_fc,freq_lc,freq_freq]df_train_info</code></pre><p><img src="/2020/07/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/2020-07-25-12-30-24.png" alt></p><h3 id="类别分布"><a href="#类别分布" class="headerlink" title="类别分布"></a>类别分布</h3><p>之前的讨论是从数据集总体验证同分布的，我们还需要验证训练集的类别足够均匀。</p><p>在数据集中标签的对应的关系如下</p><pre><code class="lang-python">label_2_index_dict = &#123;&#39;科技&#39;: 0, &#39;股票&#39;: 1, &#39;体育&#39;: 2, &#39;娱乐&#39;: 3, &#39;时政&#39;: 4, &#39;社会&#39;: 5, &#39;教育&#39;: 6, &#39;财经&#39;: 7, &#39;家居&#39;: 8, &#39;游戏&#39;: 9, &#39;房产&#39;: 10, &#39;时尚&#39;: 11, &#39;彩票&#39;: 12, &#39;星座&#39;: 13&#125;index_2_label_dict = &#123;v:k for k,v in label_2_index_dict.items()&#125;plt.figure()plt.bar(x=range(14), height=np.bincount(df_train[&#39;label&#39;]))plt.xlabel(&quot;label&quot;)plt.ylabel(&quot;number of sample&quot;)plt.xticks(range(14), list(index_2_label_dict.values()), fontproperties=zhfont, rotation=60)plt.show()</code></pre><p><img src="/2020/07/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/6.png" alt></p><p>从统计结果可以看出</p><p>赛题的数据集类别分布存在较为不均匀的情况。在训练集中科技类新闻最多，其次是股票类新闻，最少的新闻是星座新闻。</p><p>科技类新闻最多，星座类新闻最少。这个国家的人大部分是唯物主义者哈，神秘学受众比较少（啊这，我在分析什么？）。</p><p>由于类别不均衡，会严重影响模型的精度。但是我们也是有办法应对的。</p><h3 id="类别长度"><a href="#类别长度" class="headerlink" title="类别长度"></a>类别长度</h3><pre><code class="lang-python">df_train[&#39;len&#39;] = df_train[&#39;text&#39;].apply(lambda x: len(x.split()))plt.figure()ax = sns.catplot(x=&#39;label&#39;, y=&#39;len&#39;, data=df_train, kind=&#39;strip&#39;)plt.xticks(range(14), list(index_2_label_dict.values()), fontproperties=zhfont, rotation=60)</code></pre><p><img src="/2020/07/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/7.png" alt></p><p>在散点图中，股票类新闻的长度都飘到天上去了，可以看出股票分析类文章真的很容易写得又臭又长啊（发现：不同类别的文章长度不同，可以把长度作为一个Feature，以供机器学习模型训练）！</p><h2 id="字符分布"><a href="#字符分布" class="headerlink" title="字符分布"></a>字符分布</h2><p>训练集中总共包括6869个字，最大数字为7549，最小数字为0，其中编号3750的字出现的次数最多，编号3133的字出现的次数最少，仅出现一次。</p><pre><code class="lang-python"># 内存警告！！！没有8G内存不要运行该代码all_lines = &#39; &#39;.join(list(df_train[&#39;text&#39;]))word_count = Counter(all_lines.split(&quot; &quot;))word_count = sorted(word_count.items(), key=lambda d:d[1], reverse=True)print(len(word_count))# 6869print(word_count[0])# (&#39;3750&#39;, 7482224)print(word_count[-1])# (&#39;3133&#39;, 1)</code></pre><p>下面代码统计了不同字符在多少个句子中出现过，其中字符3750、字符900和字符648在20w新闻的覆盖率接近99%，很有可能是标点符号。</p><pre><code class="lang-python">%%timedf_train[&#39;text_unique&#39;] = df_train[&#39;text&#39;].apply(lambda x: &#39; &#39;.join(list(set(x.split(&#39; &#39;)))))all_lines = &#39; &#39;.join(list(df_train[&#39;text_unique&#39;]))word_count = Counter(all_lines.split(&quot; &quot;))word_count = sorted(word_count.items(), key=lambda d:int(d[1]), reverse=True)# 打印整个训练集中覆盖率前5的词for i in range(5):    print(&quot;&#123;&#125; occurs &#123;&#125; times, &#123;&#125;%&quot;.format(word_count[i][0], word_count[i][1], (word_count[i][1]/200000)*100))</code></pre><p>3750 occurs 197997 times, 98.9985%<br>900 occurs 197653 times, 98.8265%<br>648 occurs 191975 times, 95.9875%<br>2465 occurs 177310 times, 88.655%<br>6122 occurs 176543 times, 88.2715%</p><h2 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h2><p><strong>假设字符3750，字符900和字符648是句子的标点符号，请分析赛题每篇新闻平均由多少个句子构成？</strong></p><p>如果这是英文文章，那么3750应该是空格吧？如果3750是逗号怎么办？先要判断哪个是句号。</p><p>思路：该新闻的句子数为该个数。每条新闻最后的字符往往是句号，先看看每条新闻最后一个字符是什么：</p><pre><code class="lang-python">last_char = np.vectorize(lambda x:int(x.split()[-1]))(df_train[&#39;text&#39;])last_char_count = Counter(last_char)last_char_count = sorted(last_char_count.items(), key=lambda d:d[1], reverse=True)# 打印出现次数最多的前十个for i in range(10):    print(&quot;&#123;&#125;在新闻末尾出现了&#123;&#125;次&quot;.format(last_char_count[i][0], last_char_count[i][1]))900在新闻末尾出现了85040次2662在新闻末尾出现了39273次885在新闻末尾出现了14473次1635在新闻末尾出现了7379次2465在新闻末尾出现了7076次57在新闻末尾出现了3284次3231在新闻末尾出现了2758次1633在新闻末尾出现了2706次3568在新闻末尾出现了1504次2265在新闻末尾出现了1389次</code></pre><p>因此我们有理由认为900是句号。至于3750应该是逗号吧？猜的，理由是3750不太容易在新闻末尾出现。</p><p>但是除了句号之外，感叹号和问号照样能划分句子，我们试着将2662当作感叹号，将885当作问号。什么理由？猜的。</p><p>下面开始计算每篇新闻所含标点符号（900、2662、885）的个数，</p><pre><code class="lang-python">def sum_of_sep(row):    counter = Counter(row.split())    return counter.get(&#39;900&#39;, 0)+counter.get(&#39;2662&#39;, 0)+counter.get(&#39;885&#39;, 0)sum_sep = np.vectorize(sum_of_sep)(df_train[&#39;text&#39;])print(&quot;平均每条新闻的句子个数约为：&quot;, np.round(np.mean(sum_sep)))pd.Series(sum_sep).describe()平均每条新闻的句子个数约为： 19.0count    200000.000000mean         19.070155std          21.463798min           0.00000025%           7.00000050%          14.00000075%          24.000000max        1392.000000dtype: float64</code></pre><p>平均长度为19，其实这是把那些股票文章也算上了，拉高了平均值。75%的新闻长度都在24个句子以下。</p><p>给df_train_info新加一列sent_num，计算分词后的句子个数；sent_len为句子长度。</p><pre><code class="lang-python">list_num_sentence = []for name, group in df_train.groupby(&#39;label&#39;):    sum_sep_label = np.vectorize(sum_of_sep)(group[&#39;text&#39;])    num_sentence = np.mean(sum_sep_label)    list_num_sentence.append(num_sentence)df_train_info[&#39;sent_num&#39;] = list_num_sentencedf_train_info[&#39;sent_len&#39;] = df_train_info[&#39;len_mean&#39;] / df_train_info[&#39;sent_num&#39;]df_train_info</code></pre><p>不同类别的新闻，其句子长度和个数也是不同的。</p><p>之前我们分析，股票类文章往往很长，而社会（label=5）和教育（label=6）类文章的句子最多。家居（label=8）和时尚（label=11）类新闻的句子最少。游戏类（label=9）句子是最长的，社会（label=5）句子是最短的（发现：句子和个数长度也可以作为特征）。</p><p><img src="/2020/07/22/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/2020-07-25-13-00-28.png" alt></p><p><strong>每类新闻中出现次数前10</strong></p><p>在每类新闻中出现频率最高的词汇，就是df_train_info表中的freq_freq列。可以看到，清一色的3750，这个字符我们在后期处理时可以拿掉。</p><pre><code class="lang-python">word_count_dict = &#123;&#125;for name, df in df_train.groupby(&#39;label&#39;):    # print(name, type(df))    all_text = &#39; &#39;.join(list(df[&#39;text&#39;].apply(lambda x: &#39; &#39;.join(list(x.split(&#39; &#39;))))))    word_count_single_class = Counter(all_text.split(&quot; &quot;))    word_count_single_class = sorted(word_count_single_class.items(), key=lambda d:int(d[1]), reverse = True)    word_count_dict[name] = word_count_single_classfor label in range(14):    print(index_2_label_dict[label], [x for x,_ in word_count_dict[label][:10]])科技 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;3370&#39;, &#39;4464&#39;, &#39;2465&#39;, &#39;6122&#39;, &#39;3659&#39;, &#39;7399&#39;, &#39;4939&#39;]股票 [&#39;3750&#39;, &#39;648&#39;, &#39;3370&#39;, &#39;900&#39;, &#39;4464&#39;, &#39;3659&#39;, &#39;5036&#39;, &#39;6250&#39;, &#39;1633&#39;, &#39;6065&#39;]体育 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;7399&#39;, &#39;6122&#39;, &#39;4939&#39;, &#39;4704&#39;, &#39;1667&#39;, &#39;5598&#39;, &#39;669&#39;]娱乐 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;6122&#39;, &#39;4939&#39;, &#39;4893&#39;, &#39;7399&#39;, &#39;669&#39;, &#39;803&#39;, &#39;1635&#39;]时政 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;4411&#39;, &#39;7399&#39;, &#39;4893&#39;, &#39;6122&#39;, &#39;4464&#39;, &#39;2400&#39;, &#39;4853&#39;]社会 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;6122&#39;, &#39;5598&#39;, &#39;4893&#39;, &#39;7399&#39;, &#39;4939&#39;, &#39;3370&#39;, &#39;669&#39;]教育 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;6248&#39;, &#39;2555&#39;, &#39;5620&#39;, &#39;2465&#39;, &#39;6122&#39;, &#39;5560&#39;, &#39;3370&#39;]财经 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;3370&#39;, &#39;5296&#39;, &#39;4464&#39;, &#39;6835&#39;, &#39;3659&#39;, &#39;6122&#39;, &#39;7399&#39;]家居 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;6122&#39;, &#39;4939&#39;, &#39;913&#39;, &#39;5560&#39;, &#39;7399&#39;, &#39;3961&#39;, &#39;4811&#39;]游戏 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;7328&#39;, &#39;6122&#39;, &#39;7399&#39;, &#39;5547&#39;, &#39;4939&#39;, &#39;3370&#39;, &#39;2465&#39;]房产 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;3370&#39;, &#39;2465&#39;, &#39;5560&#39;, &#39;3686&#39;, &#39;4464&#39;, &#39;3523&#39;, &#39;6122&#39;]时尚 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;4939&#39;, &#39;6122&#39;, &#39;5560&#39;, &#39;669&#39;, &#39;4811&#39;, &#39;7539&#39;, &#39;4893&#39;]彩票 [&#39;3750&#39;, &#39;4464&#39;, &#39;3370&#39;, &#39;648&#39;, &#39;2465&#39;, &#39;900&#39;, &#39;3659&#39;, &#39;6065&#39;, &#39;1667&#39;, &#39;2614&#39;]星座 [&#39;3750&#39;, &#39;648&#39;, &#39;900&#39;, &#39;4939&#39;, &#39;669&#39;, &#39;6122&#39;, &#39;4893&#39;, &#39;3864&#39;, &#39;4811&#39;, &#39;1465&#39;]</code></pre><h1 id="分析结果"><a href="#分析结果" class="headerlink" title="分析结果"></a>分析结果</h1><p>数据分析肯定要有结论，没有结论的数据分析是不完整的。</p><ol><li><p>训练集共200,000条新闻，每条新闻平均907个字符，最短的句子长度为2，最长的句子长度为57921，其中75%以下的数据长度在1131以下。测试集共50,000条新闻，每条新闻平均909个字符，最短句子长度为14，最长句子41861,75%以下的数据长度在1133以下。</p></li><li><p>训练集和测试集就长度来说似乎是同一分布，但是不属于正态分布。</p></li><li><p>把截断长度定为5000？</p></li><li><p>赛题的数据集类别分布存在较为不均匀的情况。在训练集中科技类新闻最多，其次是股票类新闻，最少的新闻是星座新闻。需要用采样方法解决。文章最长的是股票类新闻。不同类别的文章长度不同，可以把长度和句子个数作为一个Feature，以供机器学习模型训练。</p></li><li><p>训练集中总共包括6869个字，最大数字为7549，最小数字为0，其中编号3750的字出现的次数最多，编号3133的字出现的次数最少，仅出现一次，其中字符3750、字符900和字符648在20w新闻的覆盖率接近99%，很有可能是标点符号。</p></li><li><p>900很有可能是句号，2662和885则很有可能为感叹号和问号，3750出现频率很高但是基本不在新闻最后出现，因此初步判断为逗号。按照这种划分，训练集中每条新闻平均句子个数约为19。</p></li><li><p>在训练集中，不同类别新闻出现词汇有特色。但是需要把共有的常用词停用。自然想到利用TF-IDF编码方式。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Feature Engineering</tag>
      
      <tag>Data Science</tag>
      
      <tag>Datawhale</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【竞赛打卡】零基础入门NLP之新闻文本分类之赛题理解</title>
    <link href="/2020/07/21/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/"/>
    <url>/2020/07/21/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p>这项任务不好做呀，稍不小心就占用了task2的内容。<br><!--more---></p><h1 id="零基础入门NLP之新闻文本分类之赛题理解"><a href="#零基础入门NLP之新闻文本分类之赛题理解" class="headerlink" title="零基础入门NLP之新闻文本分类之赛题理解"></a>零基础入门NLP之新闻文本分类之赛题理解</h1><h2 id="一、现在公开的情报"><a href="#一、现在公开的情报" class="headerlink" title="一、现在公开的情报"></a>一、现在公开的情报</h2><h3 id="1-比赛内容"><a href="#1-比赛内容" class="headerlink" title="1. 比赛内容"></a>1. 比赛内容</h3><p>本次比赛的任务为文本的分类任务。虽然简单，但是想要取得高分还是不容易。</p><p>待分类文本为新闻文本。新闻文本根据来源，分为财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐，共14类。</p><h3 id="2-数据内容"><a href="#2-数据内容" class="headerlink" title="2. 数据内容"></a>2. 数据内容</h3><p>训练集和测试集在<a href="https://tianchi.aliyun.com/competition/entrance/531810/introduction">官网</a>下载。</p><p>其中训练集是带正确标签的，测试集不带标签，是真正的题目。我们的任务是训练模型，正确分类测试集中每一条新闻的标签。</p><p>训练集由20万条新闻构成，测试集五万条数据。</p><p>每条新闻都被编码为整数序列。每个单词对应一个整数。</p><p>数据集中标签的对应的关系如下：</p><p><code>&#123;&#39;科技&#39;: 0, &#39;股票&#39;: 1, &#39;体育&#39;: 2, &#39;娱乐&#39;: 3, &#39;时政&#39;: 4, &#39;社会&#39;: 5, &#39;教育&#39;: 6, &#39;财经&#39;: 7, &#39;家居&#39;: 8, &#39;游戏&#39;: 9, &#39;房产&#39;: 10, &#39;时尚&#39;: 11, &#39;彩票&#39;: 12, &#39;星座&#39;: 13&#125;</code></p><h3 id="3-赛制"><a href="#3-赛制" class="headerlink" title="3. 赛制"></a>3. 赛制</h3><ul><li>第一阶段（7月15日-9月7日），每天两次提交自己答案的机会，系统根据成绩自动排名，排行榜每小时更新。该排行与最终成绩无关。</li><li>第二阶段（9月7日～9月8日）清空排行榜，7日11：00放出新测试数据。同样每天只能提交两次，每小时更新榜单，9月8日晚上20点的排行即为最终成绩。</li><li>排行前13名选手在9月11日12:00前提交代码，获得奖励。</li></ul><h3 id="4-结果提交"><a href="#4-结果提交" class="headerlink" title="4. 结果提交"></a>4. 结果提交</h3><p>将测试集的label保存成csv格式，上传到<a href="https://tianchi.aliyun.com/competition/entrance/531810/submission/">这里</a>。</p><p>注意第一行是标题label，从第二行开始写入标签。</p><h3 id="5-评分标准"><a href="#5-评分标准" class="headerlink" title="5. 评分标准"></a>5. 评分标准</h3><p>F1评价指标</p><h3 id="6-其他"><a href="#6-其他" class="headerlink" title="6. 其他"></a>6. 其他</h3><p>可以充分发挥自己的特长来完成各种特征工程，不限制使用任何外部数据和模型。</p><h2 id="二、赛题理解"><a href="#二、赛题理解" class="headerlink" title="二、赛题理解"></a>二、赛题理解</h2><h3 id="1-数据编码"><a href="#1-数据编码" class="headerlink" title="1. 数据编码"></a>1. 数据编码</h3><p>赛题使用的数据为新闻，但是数据已经编码成了整数序列。分词我们不必操心了，但是这种编码方式我们有必要熟悉一下。</p><p>文本数据的编码方式通常有：</p><h4 id="1-1-根据单词表编码"><a href="#1-1-根据单词表编码" class="headerlink" title="1.1 根据单词表编码"></a>1.1 根据单词表编码</h4><p>本题就是根据单词表编码的，每个单词被编码为在单词表中的位置。比如整数40就是单词表中第40个单词。</p><p>本题中最大的数据就是7549，因此推测单词表大小为7550。剩下的特征工程留给下次打卡，要不没得写了XD</p><p>当然你可以基于单词表编码的方法，使用大名鼎鼎的<strong>One-Hot编码</strong>方法，把每个单词对应的整数映射成一个7550维的向量$\mathbf{x}$，该向量的第$i$维$\mathbf{x}_i=1$，其他维度为0。</p><p>One-Hot编码方法的坏处显而易见，那就是数据太过稀疏。好处则是，实践证明，深度学习模型是可以从这种稀疏表示的特征中高效地学习到知识的。</p><h4 id="1-2-词袋模型"><a href="#1-2-词袋模型" class="headerlink" title="1.2 词袋模型"></a>1.2 词袋模型</h4><p>文本数据通常被表示为由字符组成的字符串。我们需要先处理数据，然后才能对其应用机器学习算法。</p><p>在文本分析的语境中，数据集通常被称为语料库（corpus），每个由单个文本表示的数据点被称为文档（document）。</p><p>最简单的处理方法，是<strong>只计算语料库中每个单词在每个文本中的出现频次</strong>。这种文本处理模型称之为<strong>词袋模型</strong>。</p><p>不考虑词语出现的顺序，每个出现过的词汇单独作为一列特征，这些不重复的特征词汇集合为词表。</p><p>每一个文本都可以在很长的词表上统计出一个很多列的特征向量。如果每个文本都出现的词汇，一般被标记为<strong>停用词</strong>不计入特征向量。</p><p>为了搞清楚词袋模型，也就是<code>CountVectorizer</code>到底做了什么，我们执行以下代码：</p><pre><code class="lang-python">bards_words =[&quot;The fool doth think he is wise,&quot;,    &quot;but the wise man knows himself to be a fool&quot;]</code></pre><p>我们导入 CountVectorizer 并将其实例化，然后对 bards_words 进行拟合，如下所示：</p><pre><code class="lang-python">from sklearn.feature_extraction.text import CountVectorizervect = CountVectorizer()vect.fit(bards_words)</code></pre><p>拟合 CountVectorizer 包括训练数据的分词与词表的构建，我们可以通过 vocabulary_ 属性来访问词表：</p><pre><code class="lang-python">print(&quot;Vocabulary size: &#123;&#125;&quot;.format(len(vect.vocabulary_)))print(&quot;Vocabulary content:\n &#123;&#125;&quot;.format(vect.vocabulary_))#------------------#Vocabulary size: 13Vocabulary content:&#123;&#39;the&#39;: 9, &#39;himself&#39;: 5, &#39;wise&#39;: 12, &#39;he&#39;: 4, &#39;doth&#39;: 2, &#39;to&#39;: 11, &#39;knows&#39;: 7,&#39;man&#39;: 8, &#39;fool&#39;: 3, &#39;is&#39;: 6, &#39;be&#39;: 0,  &#39;think&#39;: 10, &#39;but&#39;: 1&#125;</code></pre><p>词表共包含 13 个词，从 “be” 到 “wise”。<br>我们可以调用 transform 方法来创建训练数据的词袋表示：</p><pre><code class="lang-python">bag_of_words = vect.transform(bards_words)print(&quot;bag_of_words: &#123;&#125;&quot;.format(repr(bag_of_words)))#--------------------#bag_of_words: &lt;2x13 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;with 16 stored elements in Compressed Sparse Row format&gt;</code></pre><p>词袋表示保存在一个 SciPy 稀疏矩阵中，这种数据格式只保存非零元素。这个矩阵的形状为 2×13，每行对应于两个数据点之一，每个特征对应于词表中的一个单词。要想查看稀疏矩阵的实际内容，可以使用 toarray 方法将其转换为“密集的”NumPy 数组（保存所有 0 元素）：</p><pre><code class="lang-python">print(&quot;Dense representation of bag_of_words:\n&#123;&#125;&quot;.format(    bag_of_words.toarray()))#---------------------#Dense representation of bag_of_words:[[0 0 1 1 1 0 1 0 0 1 1 0 1][1 1 0 1 0 1 0 1 1 1 0 1 1]]</code></pre><p>删除没有信息量的单词，除了使用<code>min_df</code>参数设定词例至少需要在多少个文档中出现过之外，还可以通过添加停用词的方法。</p><h4 id="1-3-用tf-idf编码数据"><a href="#1-3-用tf-idf编码数据" class="headerlink" title="1.3 用tf-idf编码数据"></a>1.3 用tf-idf编码数据</h4><p>词频 - 逆向文档频率（term frequency–inverse document frequency，tf-idf）方法，对在某个特定文档中经常出现的术语给予很高的权重，但对在语料库的许多文档中都经常出现的术语给予的权重却不高。</p><p>scikit-learn 在两个类中实现了 tf-idf 方法：TfidfTransformer 和 TfidfVectorizer，前者接受 CountVectorizer 生成的稀疏矩阵并将其变换，后者接受文本数据并完成词袋特征提取与 tf-idf 变换。</p><p>单词w在文档d中的tf-idf分数为：</p><script type="math/tex; mode=display">\operatorname{tfidf}(w, d)=\operatorname{tf} \log \left(\frac{N+1}{N_{w}+1}\right)+1</script><p>式中，tf为词频，Term Frequency, 表示一个词在一个文档中的出现频率。该频率最后要除以该文档的长度，用以归一化。</p><p>式中，$N$为总文档数，$N_w$为带有单词$w$的文档数。由于分子比分母大，所以该 $\log$ 值必不可能小于零。</p><pre><code class="lang-python">from sklearn.feature_extraction.text import TfidfVectorizercorpus=[&quot;I come to China to travel&quot;,&quot;This is a car polupar in China&quot;,&quot;I love tea and Apple &quot;,&quot;The work is to write some papers in science&quot;]tfidf = TfidfVectorizer()vector = tfidf.fit_transform(corpus)print(vector)#---------------#(0, 16)    0.4424621378947393(0, 3)    0.348842231691988(0, 15)    0.697684463383976(0, 4)    0.4424621378947393(1, 5)    0.3574550433419527(1, 9)    0.45338639737285463(1, 2)    0.45338639737285463(1, 6)    0.3574550433419527(1, 14)    0.45338639737285463(1, 3)    0.3574550433419527(2, 1)    0.5(2, 0)    0.5(2, 12)    0.5(2, 7)    0.5(3, 10)    0.3565798233381452(3, 8)    0.3565798233381452(3, 11)    0.3565798233381452(3, 18)    0.3565798233381452(3, 17)    0.3565798233381452(3, 13)    0.3565798233381452(3, 5)    0.2811316284405006(3, 6)    0.2811316284405006(3, 15)    0.2811316284405006</code></pre><p>返回值什么意思呢？(0, 16)代表第0个文档，第一个单词在单词表（词袋）中的位置是第16个，该单词的tf-idf值为0.44246213；第二个单词在词袋中第3个位置……</p><p>显然这是个经过压缩的系数矩阵，每一行的元组表明该元素在稀疏矩阵中的位置，其值为右边的tf-idf值，代表一个单词。可以通过<code>.toarray()</code>方法令其恢复到系数矩阵状态。</p><pre><code class="lang-python">print(vector.toarray().shape)print(len(vector.toarray()))print(type(vector.toarray()))print(vector.toarray())#-----------------------------#(4, 19)4&lt;class &#39;numpy.ndarray&#39;&gt;[[0. 0. 0. 0.34884223 0.44246214 0.  0. 0. 0. 0. 0. 0.  0. 0. 0. 0.69768446 0.44246214 0.  0. ] [0. 0. 0.4533864  0.35745504 0. 0.35745504  0.35745504 0. 0. 0.4533864  0. 0.  0. 0. 0.4533864  0. 0. 0.  0. ] [0.5 0.5 0. 0. 0. 0.  0. 0.5 0. 0. 0. 0.  0.5 0. 0. 0. 0. 0.  0. ] [0. 0. 0. 0. 0. 0.28113163  0.28113163 0. 0.35657982 0. 0.35657982 0.35657982  0. 0.35657982 0. 0.28113163 0. 0.35657982  0.35657982]]</code></pre><h4 id="1-4-Hash编码"><a href="#1-4-Hash编码" class="headerlink" title="1.4 Hash编码"></a>1.4 Hash编码</h4><p>无论采用什么编码，只要令每个特征能够独一无二地表示即可。可采用Hash思想。</p><p>对于类别数量很多的分类变量，利用哈希函数将一个数据点转换成一个向量。相比较One-Hot模型，哈希编码维度下降了很多。</p><p>若采用哈希函数</p><pre><code>h(the) mod 5 = 0h(quick) mod 5 = 1h(brown) mod 5 = 1h(fox) mod 5 = 3</code></pre><p>则对于某句话：<br><code>the quick brown fox</code><br>来说，其使用哈希特转换的向量就是：<br><code>(1,2,0,1,0)</code><br>对比one-hot编码向量（在单词表里就这四个单词的情况下）：<br><code>(0001,0010,0100,1000)</code></p><p>在实践中，哈希编码通过调用sklearn的HashingVectorizer实现。</p><p>关于数据的编码及其他特征工程，请看<a href="https://superlova.github.io/2020/07/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/#%E4%B8%80%E3%80%81%E7%B1%BB%E5%88%AB%E7%89%B9%E5%BE%81">这里</a>。</p><h3 id="2-评分标准"><a href="#2-评分标准" class="headerlink" title="2. 评分标准"></a>2. 评分标准</h3><p>分类模型的评分标准非常丰富，通用评价指标有精度和错误率</p><script type="math/tex; mode=display">\operatorname{accuracy}=\frac{T P+T N}{N}, \text { Error Rate }=\frac{F P+F N}{N}</script><p>其中N是样本总数，TP、FP、TN、FN的含义如下表</p><p><img src="/2020/07/21/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/table.png" alt></p><p>除此之外，准确率、召回率、F1值也是常用的评价指标。</p><p><img src="/2020/07/21/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/metrics.png" alt></p><p>P-R曲线即召回率R为横轴、精确率P为纵轴画的曲线。分类器的P-R曲线下面积越大，表明分类性能越好。</p><p>ROC曲线分析的是二元分类模型，也就是输出结果只有两种类别的模型。ROC以伪阳性率（FPR）为 X 轴，以真阳性率（TPR）为 Y 轴绘制曲线。AUC（Area Under Curve）被定义为ROC曲线下与坐标轴围成的面积，完美分类器的AUC=1。</p><p><img src="/2020/07/21/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/metrics2.png" alt></p><h3 id="3-实验环境"><a href="#3-实验环境" class="headerlink" title="3. 实验环境"></a>3. 实验环境</h3><p>你还不知道Colab吗？我不允许有人不知道这么好的东西！Colab是一款在线Python编程工具。使用Colab，让你再也不用下载和安装Anaconda，再也不用纠结显卡驱动！有了它就可以白嫖Google的GPU服务器啦！(<a href="https://colab.research.google.com/">https://colab.research.google.com/</a>)</p><p>Colab深度学习乞丐炼丹师的最爱！但是想说爱你不容易，各种掉线、内存不足，心酸……有钱还是买带显卡的服务器吧！</p><p>关于如何使用Colab，可以参考知乎的<a href="https://zhuanlan.zhihu.com/p/35063343">这篇文章</a>。</p><p>（其实使用Colab最大的障碍是，你得有个稳定的VPN……）</p><h3 id="4-解题思路"><a href="#4-解题思路" class="headerlink" title="4. 解题思路"></a>4. 解题思路</h3><p>文本分类问题嘛，相比大家都用LSTM分类过IMDb影评，相当于Hello World之于程序员了。用LSTM分类IMDb影评的笔记我都写好了：<a href="https://superlova.github.io/2020/06/03/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8LSTM%E8%AE%AD%E7%BB%83imdb%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/">这里</a>。<a href="https://www.tensorflow.org/tutorials/text/text_classification_rnn?hl=zh-cn">Tensorflow的官方教程</a>也有用LSTM分类IMDb影评的Notebook。</p><p>因此使用LSTM的方法可以作为Baseline。</p><h3 id="5-小试牛刀"><a href="#5-小试牛刀" class="headerlink" title="5. 小试牛刀"></a>5. 小试牛刀</h3><p><img src="/2020/07/21/%E3%80%90%E7%AB%9E%E8%B5%9B%E6%89%93%E5%8D%A1%E3%80%91%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E4%B9%8B%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/score.png" alt></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Feature Engineering</tag>
      
      <tag>Data Science</tag>
      
      <tag>Datawhale</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】特征工程</title>
    <link href="/2020/07/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    <url>/2020/07/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<p>本文翻译自HJ van Veen的Feature Engineering一文，总结了数据竞赛中常用的特征工程方法。<br><!--more---></p><h1 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h1><p>本文翻译自HJ van Veen的Feature Engineering一文。由于原文以PPT方式呈现，信息高度压缩，因此在我整理本文过程中，添加了自己的理解。如有错误，敬请指正！</p><p><img src="/2020/07/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/title.png" alt></p><p><strong>全文概览：</strong></p><p><img src="/2020/07/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/特征工程.png" alt></p><p>机器学习的特征工程是将原始的输入数据转换成特征，以便于更好的表示潜在的问题、提高预测模型准确性的过程。</p><p>特征工程是数据科学中最具创造性的部分，决定了机器学习模型的性能上限。</p><p>即便是号称端到端的神经网络也需要特征工程，比如cv需要获得HOG，SIFT，whitening, perturbation, image pyramids, rotation, z-scaling, log-scaling, frame-grams, external semantic data等信息。</p><p>机器学习的输入特征包括几种：</p><ul><li>类别特征：如ID、性别等，值的大小无意义。</li><li>数值特征：包括整型、浮点型等，值的大小有意义。</li><li>时间特征：如月份、年份、季度、日期、小时等。</li><li>空间特征：经纬度等，可以转换成邮编，城市等。</li><li>文本特征：文档，自然语言，程序语句等。</li></ul><h2 id="一、类别特征"><a href="#一、类别特征" class="headerlink" title="一、类别特征"></a>一、类别特征</h2><p>分类特征的特点是几乎总是需要处理后才能输入算法。</p><p>类别特征难以估算缺失数据，类别太多（数据维度高）会导致数据稀疏，因此类别特征是特征工程的重点。</p><h3 id="One-Hot编码"><a href="#One-Hot编码" class="headerlink" title="One-Hot编码"></a>One-Hot编码</h3><p>将每个特征编码成长度为K的向量，每个向量只有一个维度值为1，其他值为0。这样做的好处是简单易实现，且数据样本的长度也归一化了。</p><p>比如分类特征{黄，绿，蓝}可以编码成{001,010,100}；{男，女}=&gt;{01,10}；{1,2,3,4,5,6,7,8,9}=&gt;{000000001,…,100000000}</p><p><strong>通常丢弃第一列以避免线性相关。</strong></p><p>比如Pandas中，<code>get_dummies</code>函数有一个参数为<code>drop_first</code>，如果为True就丢弃One-Hot编码后数据的第一列，因为丢弃的一列可以通过其他剩余的k-1列计算得到，这一列也就变成了重复数据。</p><p>缺陷：容易造成数据稀疏，且现有的One-Hot编码实现方式，较难应对缺失数据和没出现过的变量。</p><p>对于类别不多的分类变量，可以采用独热编码。但其实自然语言处理任务中也广泛使用了One-Hot编码。要知道自然语言处理任务的词典大小动辄上万，那么一个单词的One-Hot向量长度也上万，一篇文档转化成One-Hot矩阵，这个数据量就非常可观了。</p><h3 id="Hash编码"><a href="#Hash编码" class="headerlink" title="Hash编码"></a>Hash编码</h3><p>无论采用什么编码，只要令每个特征能够独一无二地表示即可。可采用Hash思想。</p><p>对于类别数量很多的分类变量，利用哈希函数将一个数据点转换成一个向量。相比较One-Hot模型，哈希编码维度下降了很多。</p><p>若采用哈希函数</p><pre><code>h(the) mod 5 = 0h(quick) mod 5 = 1h(brown) mod 5 = 1h(fox) mod 5 = 3</code></pre><p>则对于某句话：<br><code>the quick brown fox</code><br>来说，其使用哈希特转换的向量就是：<br><code>(1,2,0,1,0)</code><br>对比one-hot编码向量（在单词表里就这四个单词的情况下）：<br><code>(0001,0010,0100,1000)</code></p><p>哈希表有如下特性：</p><ul><li>相同的输入可能有相同的输出（一般情况下比例不高）</li><li>不同的输出一定对应不同的输入</li><li>正向计算很简单，反向计算很困难</li><li>根据输入查找输出效率很高</li></ul><p>本部分参考自 <a href="https://www.datalearner.com/blog/1051537932880901">https://www.datalearner.com/blog/1051537932880901</a></p><h3 id="标签编码"><a href="#标签编码" class="headerlink" title="标签编码"></a>标签编码</h3><p>给予每个类别变量一个独一无二的数字ID。</p><p>假如有三种颜色特征：红、黄、蓝，那么你可能想令红=1，黄=2，蓝=3，这就是标签编码。</p><p>这种编码对于树算法等非线性算法比较有用，好处是不会升维，坏处是会让机器学习算法误以为颜色存在数值大小关系。</p><h3 id="计数编码"><a href="#计数编码" class="headerlink" title="计数编码"></a>计数编码</h3><p>将每个类别的编码定义为其在数据集中出现的次数。比如在数据集中’红’出现了300次，那么’红’的编码就是300。</p><p><code>df.groupby([&#39;category&#39;])[&#39;target&#39;].transform(sum)</code><br>这种做法很常见，也很简单。缺点是对异常值敏感，且不同类别出现次数相同时可能引入冲突。</p><h3 id="计数排序编码"><a href="#计数排序编码" class="headerlink" title="计数排序编码"></a>计数排序编码</h3><p>根据类别变量在训练集中出现的次数<strong>排序</strong></p><p>对异常值不敏感，不会引入冲突。在实际比赛中效果可能出乎意料的好。</p><h3 id="Target编码"><a href="#Target编码" class="headerlink" title="Target编码"></a>Target编码</h3><p>将类别变量编码为分类为正的比例，类似于该类别对正类的贡献指数。</p><p>比如职业类别特征{‘manager’,’engineer’,’scientist’}在数据集中，但凡scientist都使得target=1，那么scientist就编码成1.00。</p><p><img src="/2020/07/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/2020-07-20-11-20-25.png" alt></p><h3 id="类别嵌入"><a href="#类别嵌入" class="headerlink" title="类别嵌入"></a>类别嵌入</h3><p>由神经网络得到每个类别的嵌入表达，将特征投影到更高维度的空间。在进行文档分类等，原本具有语义相似性的单词映射之后的向量之间的距离也比较小，进而可以帮助我们进一步进行机器学习的应用，这一点比独热模型好很多。</p><p>参考 <a href="https://arxiv.org/abs/1604.06737">https://arxiv.org/abs/1604.06737</a></p><p><img src="/2020/07/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/2020-07-20-11-21-54.png" alt></p><h3 id="把Nan也当作一个类别"><a href="#把Nan也当作一个类别" class="headerlink" title="把Nan也当作一个类别"></a>把Nan也当作一个类别</h3><p>只在Nan确定有意义时使用。</p><h3 id="多元编码"><a href="#多元编码" class="headerlink" title="多元编码"></a>多元编码</h3><p>将两个二元分类变量组合，形成四元向量表示</p><p><img src="/2020/07/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/2020-07-19-14-09-28.png" alt></p><h3 id="扩张编码"><a href="#扩张编码" class="headerlink" title="扩张编码"></a>扩张编码</h3><p>将一个Feature拆分成多个Feature</p><h3 id="合并编码"><a href="#合并编码" class="headerlink" title="合并编码"></a>合并编码</h3><p>将多个Feature合并成一个Feature</p><h2 id="二、数值特征"><a href="#二、数值特征" class="headerlink" title="二、数值特征"></a>二、数值特征</h2><h3 id="舍入"><a href="#舍入" class="headerlink" title="舍入"></a>舍入</h3><p>过高的精度有时是噪音。</p><p>舍入后的数据可以被当作是类别变量。</p><p>舍入前可以log一下，log转换可以将范围很大的值缩小在一定范围内，这对某些异常值的处理也很有效。</p><p><img src="http://www.datalearner.com/resources/blog_images/6a4db741-d9bb-4391-8d24-7214ecd07b3b.jpg" alt></p><h3 id="按数值大小分箱（Binning）"><a href="#按数值大小分箱（Binning）" class="headerlink" title="按数值大小分箱（Binning）"></a>按数值大小分箱（Binning）</h3><p>数据分箱是一种将多个或多或少连续值分组为较少数量的“箱(bin)”的方法。通过分箱，数值特征转换成类别特征了。</p><p>例如一天24小时可以分成早晨[5,8)，上午[8,11)，中午[11,14)，下午[14,19)，夜晚[10,22)，深夜[19,24)和[24,5)。因为比如中午11点和12点其实没有很大区别，可以使用分箱技巧处理之后可以减少这些“误差”。</p><h3 id="特征缩放（Scaling）"><a href="#特征缩放（Scaling）" class="headerlink" title="特征缩放（Scaling）"></a>特征缩放（Scaling）</h3><p>也称为数据标准化。可以将很大范围的数据限定在指定范围内。由于原始数据的值范围变化很大，在一些机器学习算法中，如果没有标准化，目标函数将无法正常工作。</p><p>特征缩放方法主要有：</p><ul><li>转为标准正态</li><li>最大最小缩放</li><li>对数缩放</li><li>开方缩放 root scale</li></ul><h3 id="估算缺失数据（Imputation）"><a href="#估算缺失数据（Imputation）" class="headerlink" title="估算缺失数据（Imputation）"></a>估算缺失数据（Imputation）</h3><p>由于各种原因，许多真实世界的数据集包含缺失的值，通常编码为空白、NAN或其他占位符。简单删除该条数据项就太可惜了，一个更好的策略是估算缺失的值， 即从数据的已知部分推断它们。</p><p>缺失的值可以用提供的常量值来计算，或使用缺失值所在的每一列的统计数据(平均值不如中位数鲁棒)。处理缺失数据的方法还有很多。</p><p>sklearn的SimpleImputer类就是用来估算缺失值的。<br><a href="https://www.studyai.cn/modules/impute.html">https://www.studyai.cn/modules/impute.html</a></p><h3 id="特征交叉-feature-interactions"><a href="#特征交叉-feature-interactions" class="headerlink" title="特征交叉(feature interactions)"></a>特征交叉(feature interactions)</h3><p>在回归模型中加入交互项是一种非常常见的处理方式。它可以极大的拓展回归模型对变量之间的依赖的解释。</p><p>举个例子：<a href="https://www.datalearner.com/blog/1051508158689792">来源</a></p><p>不同特征之间可能相互影响，比如阳光和土壤质量同时决定树木生长高度，我们可以建模：</p><script type="math/tex; mode=display">\text{Tree}=a\times \text{Sun}+b\times \text{Soil}+c</script><p>但是阳光本身也可能影响土壤质量，而我们建立的线性模型事实上是把土壤和阳光当作独立变量的。想要学习线性相关性，我们可以增加一个特征:</p><script type="math/tex; mode=display">\text{Tree}=a\cdot\text{Sun}+b\cdot\text{Soil}+c\cdot(\text{Sun}\cdot\text{Soil})+d</script><p>特征交叉的方法有很多，比如不同特征间进行加减乘除、指数操作等。</p><h3 id="非线性特征在线性模型中的应用"><a href="#非线性特征在线性模型中的应用" class="headerlink" title="非线性特征在线性模型中的应用"></a>非线性特征在线性模型中的应用</h3><ul><li><p>通过多项式核函数(polynomial kernel)和径向基核函数(RBF kernel)将线性不可分的数据映射到高维空间去</p></li><li><p>Leafcoding（随机森林嵌入）（acebook的gbdt+lr这种思路）</p></li><li><p>遗传算法（典型代表gplearn）</p></li><li><p>局部线性嵌入 Locally Linear Embedding，频谱嵌入 Spectral Embedding，t-SNE （降维提取重要特征）</p></li></ul><h3 id="行统计"><a href="#行统计" class="headerlink" title="行统计"></a>行统计</h3><p>统计一行数据中Nan个数、0的个数、负数个数、最大值、最小值、中位数、峰度偏度等</p><h2 id="三、时间特征"><a href="#三、时间特征" class="headerlink" title="三、时间特征"></a>三、时间特征</h2><p>时间变量非常容易出错，需要更好的局部验证方案（如回测）</p><h3 id="将时间映射成环（周期性变量）"><a href="#将时间映射成环（周期性变量）" class="headerlink" title="将时间映射成环（周期性变量）"></a>将时间映射成环（周期性变量）</h3><p>Turn single features, like day_of_week, into two coordinates on a circle</p><p>Ensures that distance between max and min is the same as min and min +1.</p><p>含义是指周六和周天，与周天和周一的距离是一样的。</p><p>Use for day_of_week, day_of_month, hour_of_day, etc.</p><p>就是将大时间项，比如2019年11月11日分解成小时间项的意思。</p><h3 id="趋势线"><a href="#趋势线" class="headerlink" title="趋势线"></a>趋势线</h3><p>数据的变化趋势本身也是信息。因此不要使用诸如total_spend这种总结式变量，要有一些中间变量，诸如spend_in_last_week，spend_in_last_month。展现数据趋势，利于模型获取信息。</p><h3 id="事件编码"><a href="#事件编码" class="headerlink" title="事件编码"></a>事件编码</h3><p>将某日期与重大节日之间的距离也作为特征，比如法定假日、重大体育赛事、周末、每月的第一个星期六等。</p><h2 id="四、空间特征"><a href="#四、空间特征" class="headerlink" title="四、空间特征"></a>四、空间特征</h2><h3 id="将地点视作分类特征"><a href="#将地点视作分类特征" class="headerlink" title="将地点视作分类特征"></a>将地点视作分类特征</h3><ul><li>克里金法 Kriging，空间插值方法</li><li>K-means 聚类</li><li>原始经纬度</li><li>将城市转换为经纬度</li><li>在街道名称中添加邮政编码</li></ul><h3 id="将某地点与关键地点之间的距离也作为特征"><a href="#将某地点与关键地点之间的距离也作为特征" class="headerlink" title="将某地点与关键地点之间的距离也作为特征"></a>将某地点与关键地点之间的距离也作为特征</h3><p>诸如大城市、超市等，对你的任务有重要影响的地区。</p><h3 id="位置事件数据可以指示可疑行为"><a href="#位置事件数据可以指示可疑行为" class="headerlink" title="位置事件数据可以指示可疑行为"></a>位置事件数据可以指示可疑行为</h3><p>比如同时出现在不同城市的两笔交易、在与住所或送货地址不同的城镇中消费、从不在同一个位置消费。</p><h2 id="五、数据探索"><a href="#五、数据探索" class="headerlink" title="五、数据探索"></a>五、数据探索</h2><p>数据探索的目的是提前发现数据的潜在问题，诸如异常值、噪音；然后探索数据的特征工程方法、清洗方法，为数据预处理做准备。</p><p>一开始尝试简单统计量：min、max。</p><p>Incorporate the target so find correlation between signal.</p><p>我的理解是，探索该特征与该数据的label的相关性。</p><h3 id="迭代和Debugging"><a href="#迭代和Debugging" class="headerlink" title="迭代和Debugging"></a>迭代和Debugging</h3><p>特征工程是一个迭代的过程，确保你的Pipeline能够快速迭代。</p><p>Use sub-linear debugging: Output intermediate information on the process, do spurious logging</p><p>使用sub-linear debugging：输出有关过程的中间信息，进行伪记录。</p><p>使用一些帮助快速实验的工具。</p><p>一鸟在手胜过双鸟在林，想法太多不容易成功。</p><h3 id="Label工程"><a href="#Label工程" class="headerlink" title="Label工程"></a>Label工程</h3><p>可以把数据集的标签label给变换一下，当成数据的特征（有点泄漏答案的意思）。</p><ul><li>log变换：$y\rightarrow\log{(y+1)}$、$\exp{y_{pred}}-1$</li><li>平方变换</li><li>Box-Cox变换</li><li>创建一个评分，用来把二元分类target变成回归问题</li><li>训练回归模型，用于预测测试集中不可获取的特征</li></ul><h2 id="六、文本特征"><a href="#六、文本特征" class="headerlink" title="六、文本特征"></a>六、文本特征</h2><p>与类别特征类似，特征工程手段更为丰富，举例：</p><ul><li>Lowercasing,</li><li>Removing non-alphanumeric,</li><li>Repairing,</li><li>Encoding punctuation marks,</li><li>Tokenizing,</li><li>Token-grams,</li><li>skipgrams,</li><li>char-grams,</li><li>Removing stopwords,</li><li>Removing rare words</li><li>and very common words,</li><li>Spelling Correction,</li><li>Chopping,</li><li>Stemming,</li><li>Lemmatization,</li><li>Document features,</li><li>Entitity Insertion &amp; Extraction</li><li>Simplification,</li><li>Word2Vec and GloVe / Doc2Vec,</li><li>String Similarity,</li><li>Reading level,</li><li>Nearest Neighbors,</li><li>TF-IDF,</li><li>BayesSVM, Vectorization, LDA, LSA.</li></ul><h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><p>从大写字母转换成小写，从unicode转换成ascii，移除非字母字符，修复源文本中的格式问题等。</p><h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><ul><li>将句子分成单词token序列，英文可能好做一点，中文之类的语言就需要特定的工具了，比如jieba分词。</li><li>将标点符号也硬编码为token，因为标点可能也代表有用的信息。</li><li>词袋模型（bag-of-word model）</li><li>N元分词：“I like the Beatles” -&gt; [“I like”, “like the”, “the Beatles”]</li><li>Skip-grams：“I like Beatles” -&gt; [“I the”, “like Beatles”]</li><li>Char-grams：“Beatles” -&gt; [“Bea”, “eat”, “atl”, “tle”, “les”]</li><li>Affixes：Same as char-grams, but only the postfixes and prefixe</li></ul><h3 id="删除词"><a href="#删除词" class="headerlink" title="删除词"></a>删除词</h3><ul><li>删除停用词</li><li>删除罕见词</li><li>删除其他常见词，不能反映特征的词</li></ul><h3 id="还原词形-root"><a href="#还原词形-root" class="headerlink" title="还原词形 root"></a>还原词形 root</h3><ul><li>拼写检查</li><li>Chop，只取每个token的前8个字符</li><li>Stem，将token缩减为词根形式</li><li>Lemmatize，词形还原</li></ul><h3 id="更多特征"><a href="#更多特征" class="headerlink" title="更多特征"></a>更多特征</h3><ul><li>文档特征，诸如统计空格、tab、换行、字符、token出现次数等</li><li>添加一些通用的描述，“Microsoft releases Windows” -&gt; “Microsoft (company) releases Windows (application)”</li><li>语法分析树 Parse Tree，NLTK工具包有实现</li><li>Reading level: Compute the reading level of a document.（国外的阅读分级制度）</li></ul><h3 id="相似度"><a href="#相似度" class="headerlink" title="相似度"></a>相似度</h3><ul><li>token相似度：计算两段文本中相同token数</li><li>压缩距离 Compression distance：一段句子是否能被压缩成另外一段</li><li>Levenshitein、Hamming、Jaccard距离，用来衡量两个string的距离，计算将该文本转为另一个文本所需的最小操作次数</li><li>word2vec、glove：计算两向量的余弦距离</li></ul><h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><ul><li>Term Frequency: 能够降低长文本造成的bias</li><li>Inverse Document Frequency: 降低常用词造成的Bias</li><li>TF-IDF: 辨别在document中最重要的token，删除不重要的token；或者用在数据预处理上，可以使维度下降</li></ul><h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h3><ul><li>PCA: 将文本压缩到50~100维的向量</li><li>SVD: 同上</li><li>LDA: 与TF-IDF配套</li><li>LSA: 创建主题向量</li></ul><h3 id="外部模型"><a href="#外部模型" class="headerlink" title="外部模型"></a>外部模型</h3><ul><li>感情分析器：获得text的情感倾向</li><li>主题模型：使用一个dataset创建主题向量，用于另一个任务</li></ul><h2 id="七、泄露的特征-黄金特征"><a href="#七、泄露的特征-黄金特征" class="headerlink" title="七、泄露的特征 / 黄金特征"></a>七、泄露的特征 / 黄金特征</h2><p>特征工程能帮助发现泄露的特征，这些特征可能对你帮助很大。</p><ul><li>比如“逆向”工程：<ul><li>用彩虹表破解MD5哈希。<a href="https://www.cnblogs.com/by-3ks/articles/4137562.html">彩虹表（Rainbow Table）</a>是一种破解哈希算法的技术，可以破解MD5、HASH等多种密码。</li><li>利用TF-IDF获得术语频率。</li><li>编码样本数据集的顺序。</li><li>编码文件创建日期。</li></ul></li><li>比如规则挖掘：<ul><li>查找简单的规则（并对它们进行编码）以帮助模型决策。</li></ul></li></ul><h2 id="八、案例研究"><a href="#八、案例研究" class="headerlink" title="八、案例研究"></a>八、案例研究</h2><p>Quora重复问题数据集，约440000个问题，将其分成重复问题或非重复问题。作者将自己解决Quora重复问题的过程分享如下：</p><ul><li>First attempt: 词袋模型+逻辑回归</li><li>Second attempt: token之间进行数据的多项式交互</li><li>Third attempt: 使用NLTK的SnowballStemmer进行词干提取</li><li>Fourth attempt: 使用2-grams分词</li><li>Fifth attempt: 添加以下手工构造的特征：<ul><li>归一化问答对的长度</li><li>归一化问答对的compression距离</li><li>计算问答对的词向量之间的余弦距离</li><li>Chargram co-occurence between question pairs.</li><li>计算word出现次数：which，what，where</li></ul></li><li>还能想到更多的改进方法吗？<ul><li>外部或预训练模型？</li><li>Search engine models?</li><li>Logic based models?</li></ul></li></ul><h2 id="九、其他资源参考"><a href="#九、其他资源参考" class="headerlink" title="九、其他资源参考"></a>九、其他资源参考</h2><ul><li><strong>Kaggle forums &amp; kernels:</strong> Far0n, KazAnova, Fchollet, Abhishek, Gilberto Titericz, Leustagos, Owen Zhang, Gert Jacobusse …</li><li><strong>Introduction:</strong> <a href="http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/">http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/</a></li><li><strong>Books:</strong><ul><li>Mastering Feature Engineering (Alice Zheng),</li><li>Feature Extraction (Isabelle Guyon et al.)</li></ul></li><li><strong>Blogs:</strong><ul><li><a href="https://smerity.com/articles/2016/architectures_are_the_new_feature_engineering.html">https://smerity.com/articles/2016/architectures_are_the_new_feature_engineering.html</a></li><li><a href="http://hunch.net/~jl/projects/hash_reps/">http://hunch.net/~jl/projects/hash_reps/</a></li><li><a href="https://blogs.technet.microsoft.com/machinelearning/2014/09/24/online-learning-and-sub-linear-debugging/">https://blogs.technet.microsoft.com/machinelearning/2014/09/24/online-learning-and-sub-linear-debugging/</a></li><li><a href="http://blog.kaggle.com/2015/12/03/dato-winners-interview-1st-place-mad-professors/">http://blog.kaggle.com/2015/12/03/dato-winners-interview-1st-place-mad-professors/</a></li><li><a href="http://blog.kaggle.com/2016/08/24/avito-duplicate-ads-detection-winners-interview-1st-place-team-devil-team-stanislav-dmitrii/">http://blog.kaggle.com/2016/08/24/avito-duplicate-ads-detection-winners-interview-1st-place-team-devil-team-stanislav-dmitrii/</a></li><li><a href="http://www.slideshare.net/DataRobot/featurizing-log-data-before-xgboost">http://www.slideshare.net/DataRobot/featurizing-log-data-before-xgboost</a></li></ul></li><li><strong>Data:</strong> <a href="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs">https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs</a></li><li><strong>Software:</strong> <a href="https://github.com/trevorstephens/gplearn">https://github.com/trevorstephens/gplearn</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
      <category>translation</category>
      
      <category>转载</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>Feature Engineering</tag>
      
      <tag>Data Science</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读笔记】CAGFuzz: Coverage-Guided Adversarial Generative Fuzzing Testing of Deep Learning Systems</title>
    <link href="/2020/06/28/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91CAGFuzz-Coverage-Guided-Adversarial-Generative-Fuzzing-Testing-of-Deep-Learning-Systems/"/>
    <url>/2020/06/28/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91CAGFuzz-Coverage-Guided-Adversarial-Generative-Fuzzing-Testing-of-Deep-Learning-Systems/</url>
    
    <content type="html"><![CDATA[<p>提出了CAGFuzz，这是一种覆盖率指导的灰盒对抗性生成模糊测试方法，可以为目标DNN生成对抗性示例，以发现其潜在缺陷。<br><!--more---></p><p>DeepXplore，使用多个DNN来发现并生成位于这些DNN决策边界之间的对抗性示例。</p><p>DeepHunter，使用变形突变策略来生成新的测试示例。</p><p>DeepGauge，提出了深度神经网络的新覆盖标准。</p><ol><li>为了提高泛化能力，仅从数据的角度添加较小的扰动就非常重要。</li><li>现有研究完全忽略了包含高级语义信息（例如图像对象类别和场景语义）的深度特征约束。使用L0和L∞来限制对抗性示例的像素级变化，这样的约束只能代表对抗实例和原始实例之间的视觉一致性，而不能保证对抗实例和原始实例的高级语义信息之间的一致性。</li></ol><p>具体地，</p><ol><li>双重GAN构造对抗样本生成器AEG<br>CAGFuzz的目标是最大化神经元覆盖范围，并在对目标DNN产生较小扰动的情况下尽可能多地生成对抗性测试示例。同时，生成的示例适用于不同种类的DNN。</li></ol><p>CycleGAN [19]的目标是将图像A转换为具有不同样式的图像B。</p><p>基于CycleGAN，我们的目标是将图像B转换回图像A，以获得与原始图像A类似的图像A’。</p><p>因此，我们将两个具有相反功能的CycleGAN的生成器组合为对抗示例生成器。</p><p>基于几个特定的​​数据集对AEG进行训练，并且不需要依赖任何特定的DNN模型。</p><ol><li>采用网络信息确保两样本语义一致<br>我们提取原始示例和对抗示例的深度特征，并通过相似性度量使其尽可能相似。</li></ol><p>我们使用VGG-19网络[20]提取原始示例和对抗示例的深度语义信息，并使用余弦相似度测量方法来确保对抗示例的深度语义信息与原始示例一致 越多越好。</p><ol><li>结果<br>AGFuzz可以有效地改善目标DNN模型的神经元覆盖范围。证明了由CAGFuzz生成的对抗示例可以发现目标DNN模型中的隐藏缺陷。通过AEG训练的DNN模型的准确性和鲁棒性已得到显着提高。</li></ol><h1 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h1><h2 id="覆盖率引导的灰盒模糊测试"><a href="#覆盖率引导的灰盒模糊测试" class="headerlink" title="覆盖率引导的灰盒模糊测试"></a>覆盖率引导的灰盒模糊测试</h2><p>最新的CGF方法主要包括三个部分：突变，反馈指导和模糊策略</p><h2 id="CycleGAN"><a href="#CycleGAN" class="headerlink" title="CycleGAN"></a>CycleGAN</h2><p>生成对抗性示例的想法是增加人们无法与原始示例区分开的干扰。这与GAN [27]生成示例的想法非常相似。</p><p>GAN的生成器G和鉴别器D根据噪声数据交替生成与原始示例非常相似但不完全相同的对抗示例。</p><p>考虑到不同目标DL系统（例如某些带有标签数据的DL系统和其他DL系统的数据集）的差异，我们选择CycleGAN [19]作为对抗性示例生成器的训练模型，因为CycleGAN不需要数据匹配集和标签信息。</p><p>CycleGAN的目标是学习X和Y之间的映射关系G和F。有两个对抗判别器$D_x$和$D_y$，$D_x$分辨图片$x$和转换后的图片$F(x)$。$D_y$相似。</p><p>和其他GAN一样，用损失函数优化映射函数。这里用最小二乘损失。</p><p>本示例的目的是将真实图片和梵高风格的绘画相互转化。<br><img src="/2020/06/28/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91CAGFuzz-Coverage-Guided-Adversarial-Generative-Fuzzing-Testing-of-Deep-Learning-Systems/2020-06-28-00-36-54.png" alt></p><h2 id="VGG-19"><a href="#VGG-19" class="headerlink" title="VGG-19"></a>VGG-19</h2><p>VGG-19网络可以从图像中提取高级语义信息[29]，[30]，可用于识别图像之间的相似性。</p><p>本文将最后一个完整连接层的输出作为特征向量进行融合，以比较对抗性示例与原始示例之间的相似性，并作为过滤生成的对抗性示例的阈值。</p><h2 id="神经网络覆盖率"><a href="#神经网络覆盖率" class="headerlink" title="神经网络覆盖率"></a>神经网络覆盖率</h2><p><img src="/2020/06/28/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91CAGFuzz-Coverage-Guided-Adversarial-Generative-Fuzzing-Testing-of-Deep-Learning-Systems/2020-06-28-00-39-59.png" alt></p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><img src="/2020/06/28/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91CAGFuzz-Coverage-Guided-Adversarial-Generative-Fuzzing-Testing-of-Deep-Learning-Systems/2020-06-28-00-41-11.png" alt></p><p>第一步是数据收集和训练对抗性示例生成器</p><p>将数据集分为两个子集，并作为CycleGAN的输入来训练AEG。</p><p>在根据存储时间等设置了优先级之后，将这些示例放入处理池中，作为模糊测试的原始示例集。</p><p>第二步是对抗示例生成。</p><p>每次从处理池中选择优先的原始示例，并将其用作AEG的输入以生成对抗示例。</p><p>VGG-19处理对抗性示例和原始示例，以提取示例的特征矩阵。</p><p>计算对抗性示例与原始示例之间的特征矩阵的余弦相似度，以确保对抗性示例的深层语义与原始示例一致。</p><p>第三步是使用神经元覆盖率来指导生成过程。</p><p>第二步中生成的对抗示例将输入到被测DNN中以进行覆盖率分析。</p><p>如果发生新的覆盖，则对抗性示例将作为数据集的一部分放入处理池中。</p><p>新的覆盖范围意味着对抗示例的神经元覆盖范围高于原始示例的神经元覆盖范围。</p><p><img src="/2020/06/28/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91CAGFuzz-Coverage-Guided-Adversarial-Generative-Fuzzing-Testing-of-Deep-Learning-Systems/2020-06-28-00-44-35.png" alt></p><p>CAGFuzz的输入包括目标数据集D，给定深度神经网络DNN，最大迭代数N，每个原始示例生成的对抗性示例N1以及top-k的参数K。</p><p>输出是生成的测试示例，可改善目标DNN的覆盖范围。</p><p>在整个模糊测试过程之前，我们需要处理数据集。</p><p>一方面，它被分为两个相等的数据字段（第1行），以训练对抗性示例生成器AEG（第2行）。</p><p>另一方面，所有示例都经过预处理（第3行），并存储在处理池中（第4行）。</p><p>在每个迭代过程中（第5行），根据时间优先级（第6、7行）从处理池中选择原始示例父对象。</p><p>然后，每个原始示例父对象都会生成多次（第8行）。</p><p>对于每一代，对抗性示例生成器AEG用于变异原始示例性父级，以生成对抗性示例数据（第9行）。</p><p>分别提取原始示例父对象和对抗示例数据的深度特征，并计算它们之间的余弦相似度（第10-11行）。</p><p>最后，将原始样本产生的所有对抗性示例按照相似性从高到低排序，并选择其中的前k个作为目标示例（第13行）。</p><p>排名前k位的对抗性示例是具有覆盖范围的反馈（第15行）。</p><p>如果对抗性示例增加了目标DNN的覆盖范围，它们将被存储在处理池中并设置时间优先级（第16-19行）。</p><h2 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h2><p>首先对数据集进行排序，将排好序的数据集储存在队列中。然后将队列分成两部分，训练cycleGAN使用。</p><h2 id="训练对抗样本生成器"><a href="#训练对抗样本生成器" class="headerlink" title="训练对抗样本生成器"></a>训练对抗样本生成器</h2><p>如何把握突变程度degree是关键。本文提出了一个新的对抗样本生成方法，能够保证深层次semantic信息的不变性，且扰动对人类是不可察觉的。</p><p>这个新的方法就是引用了cyclegan，依赖adversarial loss函数添加对抗扰动，且通过cyclic consistency loss控制扰动在人类不可察觉的范围内。</p><p>CycleGAN是怎么做的呢？首先将数据集均匀分成两部分X、Y，模型试图学习两个映射，分别是从X到Y的映射P，以及从Y到X的映射Q。对应地，有两个对抗判别器$D_x$和$D_y$，分别负责判断当前的$x$是来自X的样本还是来自Q生成的样本，以及负责判断当前的$y$是来自Y的样本还是来自P生成的样本。</p><p>对于映射函数P和对应的对抗样本判别器$D_y$，对抗损失函数为：</p><script type="math/tex; mode=display">\begin{aligned}\min _{P} \max _{D} Y V\left(P, D_{Y}, X, Y\right)=E_{y \sim P_{\text {data}}(y)}\left[\log D_{Y}(y)\right] &+\\E_{x \sim P_{\text {data}}(x)}\left[\log \left(1-D_{Y}(P(x))\right)\right] &]\end{aligned}</script><p>最小化映射函数P，最大化对抗判别器$D_y$。</p><p>P映射的目的是生成对抗样本$y’=P(x)$，让$y’$看起来像是从Y中来的一样。这个过程可以理解成，把大量的来自于Y的特性给添加到来自于X的$x$中。同时训练一个辨别器$D_y$，辨别真正的$y$与P生成出来的$y’$。</p>]]></content>
    
    
    <categories>
      
      <category>paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Fuzzing</tag>
      
      <tag>Robustness</tag>
      
      <tag>Deep Neural Networks</tag>
      
      <tag>Coverage</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读笔记】Towards Evaluating the Robustness of Neural Networks</title>
    <link href="/2020/06/27/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Towards-Evaluating-the-Robustness-of-Neural-Networks/"/>
    <url>/2020/06/27/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Towards-Evaluating-the-Robustness-of-Neural-Networks/</url>
    
    <content type="html"><![CDATA[<p>2017年的攻击文章，目标是当时防御性较高的<strong>防御性蒸馏</strong>模型，属于图像分类模型。结果是成功令其鲁棒性下降。文章最后的结论是，做防御的人应该用较先进的攻击手段测试自己的模型，同时应该注意防御对抗样本的迁移特性，即在其他弱模型上生成样本后在目标模型上测试的行为。<br><!--more---></p><blockquote><p>警告！本文为简读，大量省略了原文内容，只留下了我关注的细节。</p></blockquote><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>防御性模型蒸馏（Defensive distillation）是近年来抵御对抗样本攻击的最有效方法之一。防御性模型蒸馏应用于前馈神经网络，只需一次重训练，就能将攻击成功率从95%降至0.5%。</p><p>目前有两种衡量神经网络鲁棒性的方法，一是试图证明一个下界，二是攻击模型，试图展示一个上界。第一种方法显然更加难以实践，但是如果攻击算法不够好，第二种方法得到的结果也没有说服力。</p><p>本文构建一系列攻击手段，可以用于构建鲁棒性上界。</p><p>本文还建议使用高置信度（high-confidence）的对抗性示例来评估模型的鲁棒性。</p><p>对抗样本有转移性，可以使用不安全的模型找出对抗样本，这些对抗样本在采取蒸馏防御的模型上同样有效。</p><p>换句话说，有效的防御手段必须破坏转移性</p><p>总的来说，本文的贡献有以下几点：</p><ol><li>引入了攻击手段，分别以$L<em>0$，$L_2$，$L</em>{infty}$作为距离指标。</li><li>调研了优化目标对生成对抗样本的影响。</li></ol><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>目前生成对抗样本的关键问题之一是添加distortion的程度。不同领域的distance metric肯定是不同的。</p><p>本文默认的攻击方法是白盒攻击，因为我们可以通过迁移的方法，将白盒生成的对抗样本迁移到目标样本的黑盒测试中。</p><p>神经网络的robustness定义为生成对抗样本的容易程度。</p><p>本文攻击的对象是以模型蒸馏为防御方法的模型。这种方法能够防御住迄今为止的大部分攻击手段。</p><p>本文攻击的模型皆为图像识别领域的m分类器，输出层为softmax层。这也就意味着输出向量维度为m，$y_i$即为样本分类为i的概率。</p><p>对抗攻击分为带目标的和不带目标的。本文将带目标攻击分成三种情况：</p><p>一般情况：在不是正确标签的标签中随机选择目标类别。</p><p>最佳情况：对所有不正确的类别进行攻击，并报告最难攻击的目标类别。</p><p>在所有评估中，我们执行所有三种类型的攻击：最佳情况，平均情况和最坏情况。</p><p>请注意，如果分类器在80％的时间中仅是准确的，则最佳案例攻击将需要在20％的案例中将其更改为0。</p><p>无论是L0,L2,Loo，没有距离度量是人类感知相似性的完美度量，并且我们没有准确判断哪个距离度量是最佳的。</p><p><strong>防御性蒸馏</strong>：以标准方式在训练数据上训练具有相同架构的网络。当我们在训练该网络时计算softmax时，将其替换为更平滑的softmax版本（将对数除以某个常数T）。</p><p>训练结束后，输入训练集得到模型推断的标签，收集这些标签并代替真实标签，再训练第二个网络。</p><p>这样做的理由是，提出蒸馏防御的人认为，对抗样本是模型过拟合的体现，那么避免过拟合可能会消除高维空间中的盲点。</p><p>但事实上蒸馏不会增加网络健壮性。</p><h1 id="攻击方法"><a href="#攻击方法" class="headerlink" title="攻击方法"></a>攻击方法</h1><h2 id="L-BFGS"><a href="#L-BFGS" class="headerlink" title="L-BFGS"></a>L-BFGS</h2><p>找对抗样本的过程建模为优化过程。<br><img src="/2020/06/27/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Towards-Evaluating-the-Robustness-of-Neural-Networks/2020-06-27-23-41-04.png" alt><br>loss函数可以取交叉熵。上式求解的是c，使用的距离是$L_2$</p><h2 id="FGSM"><a href="#FGSM" class="headerlink" title="FGSM"></a>FGSM</h2><p>用的是$L_{\infty}$</p><h2 id="JSMA"><a href="#JSMA" class="headerlink" title="JSMA"></a>JSMA</h2><p>Jacobian-based Saliency Map Attack</p><p>JSMA使用的不是softmax的输出，而是softmax的输入，换句话说就是前一层的输出。$L_0$</p><h2 id="DeepFool"><a href="#DeepFool" class="headerlink" title="DeepFool"></a>DeepFool</h2><p>$L_2$</p><h1 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h1><p><img src="/2020/06/27/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Towards-Evaluating-the-Robustness-of-Neural-Networks/2020-06-27-23-54-11.png" alt></p><p><img src="/2020/06/27/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Towards-Evaluating-the-Robustness-of-Neural-Networks/2020-06-27-23-54-31.png" alt></p>]]></content>
    
    
    <categories>
      
      <category>paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Robustness</tag>
      
      <tag>Deep Neural Networks</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读笔记】Fuzzing: A Survey</title>
    <link href="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/"/>
    <url>/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/</url>
    
    <content type="html"><![CDATA[<p>2018年CyberSecurity收录的一篇关于软件测试中模糊测试的综述。作者来自清华大学。文章名越短越霸气。<br><!--more---></p><blockquote><p>警告！本文在写作过程中大量使用了谷歌翻译。</p></blockquote><h1 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h1><p>模糊测试几乎不需要了解目标，并且可以轻松扩展到大型应用程序，因此已成为最受欢迎的漏洞发现解决方案</p><p>模糊的随机性和盲目性导致发现错误的效率低下</p><p>反馈驱动的模糊模式（feedback-driven fuzzing mode）和遗传算法（genetic algorithms）的结合提供了更灵活和可自定义的模糊框架，并使模糊过程更加智能和高效。</p><h1 id="二、背景"><a href="#二、背景" class="headerlink" title="二、背景"></a>二、背景</h1><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/2020-06-26-22-37-23.png" alt></p><h2 id="1-静态分析"><a href="#1-静态分析" class="headerlink" title="1.静态分析"></a>1.静态分析</h2><p>静态分析（Static Analysis）是对在没有实际执行程序的情况下执行的程序的分析。</p><p>通常对源代码执行静态分析，有时还对目标代码执行静态分析。通过分析词法，语法，语义特征以及数据流分析，模型检查，静态分析，可以检测到隐藏的错误。</p><p>静态分析的优点是检测速度快。</p><p>由于缺乏易于使用的漏洞检测模型，因此静态分析工具容易产生大量误报。因此，确定静态分析的结果仍然是一项艰巨的工作。</p><h2 id="2-动态分析"><a href="#2-动态分析" class="headerlink" title="2.动态分析"></a>2.动态分析</h2><p>动态分析（Dynamic Analysis）与静态分析相比，分析人员需要在实际系统或仿真器中执行目标程序。</p><p>通过监视运行状态并分析运行时知识，动态分析工具可以精确地检测程序错误。</p><p>动态分析的优点是精度高，缺点是速度慢，效率低，对测试人员的技术水平要求高，可扩展性差，并且难以进行大规模测试。</p><h2 id="3-符号执行"><a href="#3-符号执行" class="headerlink" title="3.符号执行"></a>3.符号执行</h2><p>符号执行（Symbolic Execution）是另一种发现漏洞的技术。</p><p>通过符号化程序输入，符号执行为每个执行路径维护了一组约束（constraint）。</p><p>执行之后，约束求解器（constraint solvers）将用于求解约束并确定导致执行的输入。</p><p>从技术上讲，符号执行可以覆盖程序中的任何执行路径，并且在小型程序的测试中已显示出良好的效果，但也存在许多限制。</p><p>首先，路径爆炸问题。随着程序规模的增长，执行状态会爆炸，这超出了约束求解器的求解能力。</p><p>第二，环境的相互作用。在符号执行中，当目标程序执行与符号执行环境之外的组件交互时，例如系统调用，处理信号等，可能会出现一致性问题。</p><p>因此符号执行仍然很难扩展到大型应用程序。</p><h2 id="4-模糊测试"><a href="#4-模糊测试" class="headerlink" title="4.模糊测试"></a>4.模糊测试</h2><p>模糊测试（Fuzzing）是目前最流行的漏洞发现技术。</p><p>从概念上讲，模糊测试从为目标应用程序生成大量正常和异常输入开始，并尝试通过将生成的输入提供给目标应用程序并监视执行状态来检测异常。</p><p>与其他技术相比，模糊测试易于部署并且具有良好的可扩展性和适用性，并且可以在有或没有源代码的情况下执行。</p><p>此外，由于模糊测试是在实际执行中执行的，因此它具有很高的准确性。</p><p>而且，模糊测试几乎不需要了解目标应用程序，并且可以轻松扩展到大型应用程序。</p><p>尽管模糊测试存在许多缺点，例如效率低和代码覆盖率低，但是，缺点却胜过缺点，模糊处理已成为当前最有效，最高效的最新漏洞发现技术。</p><h1 id="三、模糊测试介绍"><a href="#三、模糊测试介绍" class="headerlink" title="三、模糊测试介绍"></a>三、模糊测试介绍</h1><h2 id="1-模糊测试的工作过程"><a href="#1-模糊测试的工作过程" class="headerlink" title="1.模糊测试的工作过程"></a>1.模糊测试的工作过程</h2><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/2020-06-26-22-38-53.png" alt></p><p>模糊测试包括四个主要阶段，即测试用例生成阶段，测试用例运行阶段，程序执行状态监视和异常分析。</p><p>模糊测试从生成一堆程序输入（即测试用例）开始。生成的测试用例的质量直接影响测试效果。</p><p>一方面，输入应尽可能满足测试程序对输入格式的要求。</p><p>另一方面，应充分破坏输入，以便对这些输入进行处理很可能会使程序失败。</p><p>根据目标程序，输入可以是具有不同文件格式的文件，网络通信数据，具有指定特征的可执行二进制文件等。</p><p>如何生成足够多的测试用例是Fuzzer面临的主要挑战。</p><p>在上一阶段生成测试用例后，将它们馈送到目标程序。</p><p>模糊测试器自动开始和完成目标程序的过程，并驱动目标程序的测试用例处理过程。</p><p>在执行之前，分析人员可以配置目标程序的启动和完成方式，并预定义参数和环境变量。</p><p>通常，模糊处理过程在预定义的超时、程序执行挂起或崩溃时停止。</p><p>模糊器在目标程序执行期间监视执行状态，以防异常和崩溃。</p><p>常用的异常监视方法包括监视特定的系统信号，崩溃和其他违规（violations）。</p><p>当捕获到违规时，模糊器将存储相应的测试用例，以供以后重播和分析。</p><p>在分析阶段，分析人员尝试确定捕获的违规的位置和根本原因。</p><p>自动崩溃分析是另一个重要的研究领域。</p><h2 id="2-模糊测试器的种类"><a href="#2-模糊测试器的种类" class="headerlink" title="2.模糊测试器的种类"></a>2.模糊测试器的种类</h2><h3 id="A-基于生成和基于突变"><a href="#A-基于生成和基于突变" class="headerlink" title="A.基于生成和基于突变"></a>A.基于生成和基于突变</h3><p>模糊器可以分为基于生成（generation based）和基于突变（mutation based）两类。</p><p>对于基于生成的模糊器，需要有关程序输入的知识。</p><p>对于文件格式模糊测试（file format fuzzing），通常会提供一个预定义文件格式的配置文件。测试用例根据配置文件生成。</p><p>有了给定的文件格式知识（file format knowledge），基于生成的模糊器生成的测试用例就可以更轻松地通过程序的验证，并且更有可能测试目标程序的更深层代码。</p><p>但是，如果没有友好的文档，分析文件格式将是一项艰巨的工作。</p><p>因此，基于变异的模糊器更容易启动并且更适用，并且被最新的模糊器广泛使用。</p><p>对于基于突变的模糊器，需要一组有效的初始输入（a set of valid initial inputs）。</p><p>测试用例是通过对初始输入和测试过程中产生的测试用例进行变异而生成的。</p><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/2020-06-26-22-43-30.png" alt></p><h3 id="B-黑盒白盒灰盒"><a href="#B-黑盒白盒灰盒" class="headerlink" title="B.黑盒白盒灰盒"></a>B.黑盒白盒灰盒</h3><p>关于对程序源代码的依赖性（the dependence on program source<br>code）和程序分析的程度（the degree of program analysis），模糊器可以分为白盒，灰盒和黑盒（white box, gray box and black box）。</p><p>白盒模糊测试器可以访问程序的源代码，因此可以通过对源代码进行分析以及测试用例如何影响程序运行状态来收集更多信息。</p><p>黑盒模糊器会在不了解目标程序内部的情况下进行模糊测试。</p><p>灰箱模糊器也不使用源代码，但可以通过程序分析获得目标程序的内部信息。</p><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/2020-06-26-22-44-18.png" alt></p><h3 id="C-定向模糊和基于覆盖的模糊"><a href="#C-定向模糊和基于覆盖的模糊" class="headerlink" title="C.定向模糊和基于覆盖的模糊"></a>C.定向模糊和基于覆盖的模糊</h3><p>根据探索程序的策略（the strategies of exploring the programs），模糊器可以分为定向模糊（directed fuzzing）和基于覆盖的模糊（coverage-based fuzzing）。</p><p>定向模糊器旨在生成覆盖目标代码和程序目标路径的测试用例（cover target code and target paths of programs），而基于覆盖率的模糊器旨在生成覆盖尽可能多的程序代码的测试用例（cover as much code of programs as possible）。</p><p>定向模糊器期望对程序进行更快的测试，而基于覆盖率的模糊器期望进行更彻底的测试并检测到尽可能多的错误。</p><p>对于定向模糊器和基于覆盖的模糊器，如何提取执行路径的信息是一个关键问题。</p><h3 id="D-哑模糊和智能模糊"><a href="#D-哑模糊和智能模糊" class="headerlink" title="D.哑模糊和智能模糊"></a>D.哑模糊和智能模糊</h3><p>根据对程序执行状态的监视和测试用例的生成之间是否存在反馈（whether there is a feedback between the monitoring of program execution state and testcase generation），模糊器可以分为哑类（dumb fuzz）和智能类（smart fuzz）。</p><p>智能模糊器会根据收集的信息（测试用例如何影响程序行为）来调整测试用例的生成。</p><p>对于基于变异的模糊测试器，反馈信息可用于确定应该对测试用例的哪一部分进行变异以及对它们进行变异的方式。</p><p>哑模糊测试器（Dumb fuzzers）具有更好的测试速度（speed），而智能模糊测试器可以生成更好的测试用例并获得更高的效率（efficiency）。</p><h2 id="3-模糊测试面临的挑战"><a href="#3-模糊测试面临的挑战" class="headerlink" title="3.模糊测试面临的挑战"></a>3.模糊测试面临的挑战</h2><h3 id="A-如何改变种子输入的挑战"><a href="#A-如何改变种子输入的挑战" class="headerlink" title="A.如何改变种子输入的挑战"></a>A.如何改变种子输入的挑战</h3><p>基于变异的模糊测试工具在进行变异时需要回答两个问题：（1）变异的位置，以及（2）变异的方式。</p><p>只有几个关键位置的突变会影响执行的控制流程。因此，如何在测试用例中定位这些关键位置非常重要。</p><p>此外，模糊器改变关键位置的方式是另一个关键问题，即如何确定可以将测试定向到程序有趣路径的值。</p><p>简而言之，测试用例的盲目突变会严重浪费测试资源，更好的变异策略可以显着提高模糊测试的效率。</p><h3 id="B-低代码覆盖率的挑战"><a href="#B-低代码覆盖率的挑战" class="headerlink" title="B.低代码覆盖率的挑战"></a>B.低代码覆盖率的挑战</h3><p>较高的代码覆盖率表示对程序执行状态的覆盖率更高，并且测试更加彻底。发现错误的可能性更高。</p><p>但是，大多数测试用例仅涵盖相同的少数路径。</p><p>因此，仅通过大量生成测试用例并投入测试资源来实现高覆盖率是不明智的选择。</p><p>基于覆盖率的模糊器试图借助程序分析技术（例如程序检测，program instrumentation）来解决问题。</p><h3 id="C-通过验证的挑战"><a href="#C-通过验证的挑战" class="headerlink" title="C.通过验证的挑战"></a>C.通过验证的挑战</h3><p>程序通常在解析和处理之前验证（Validation）输入。</p><p>验证可以保护程序，节省计算资源，并保护程序免受无效输入和恶意构造输入造成的损坏。</p><p>黑盒和灰盒模糊器生成的测试用例很难通过盲目生成策略（blind generation strategy）的验证，从而导致效率很低。</p><h1 id="四、覆盖引导的模糊测试"><a href="#四、覆盖引导的模糊测试" class="headerlink" title="四、覆盖引导的模糊测试"></a>四、覆盖引导的模糊测试</h1><p>为了实现深入而彻底的程序模糊测试，模糊测试人员应尝试<strong>遍历尽可能多的程序运行状态</strong>。</p><p>但是，由于程序行为的不确定性，因此<strong>没有用于程序状态的简单度量</strong>。</p><p>测量代码覆盖率成为一种近似的替代解决方案。代码覆盖率的增加代表了新的程序状态。此外，代码覆盖率很容易测量。</p><p>但是，代码覆盖率是一种近似的度量，因为在实践中，恒定的代码覆盖率并不表示恒定的程序状态数。使用此指标可能会导致某些信息丢失。</p><h2 id="1-代码覆盖率计数"><a href="#1-代码覆盖率计数" class="headerlink" title="1.代码覆盖率计数"></a>1.代码覆盖率计数</h2><p>在程序分析中，程序由基本块（basic blocks）组成。基本块是具有单个入口和出口点的代码段，基本块中的指令将顺序执行，并且只会执行一次。</p><p>在代码覆盖率测量中，最新技术将基本块作为最佳粒度（granularity）。原因包括：（1）基本块是程序执行的最小连贯单位；（2）测量功能或指令会导致信息丢失或冗余；（3）基本块可以由第一条指令的地址标识；以及基本块信息可通过代码检测轻松提取。</p><p>当前，有两种基于基本块的基本测量选择：简单地计算已执行的基本块和计算基本块的跃迁（transitions）。</p><p>在后一种方法中，程序被解释为图，顶点用于表示基本块，边用于表示基本块之间的跃迁。</p><p>后一种方法记录边缘，而前一种方法记录顶点。</p><p>但是实验表明仅对已执行的基本块进行计数将导致严重的信息丢失。</p><p>如图所示，如果首先执行程序路径（BB1，BB2，BB3，BB4），然后执行时遇到路径（BB1，BB2，BB4），则新边（BB2，BB4）信息为丢失。</p><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/2020-06-27-00-11-48.png" alt></p><h2 id="2-基于覆盖的模糊测试的工作过程"><a href="#2-基于覆盖的模糊测试的工作过程" class="headerlink" title="2.基于覆盖的模糊测试的工作过程"></a>2.基于覆盖的模糊测试的工作过程</h2><p>算法1显示了基于覆盖的模糊器的一般工作过程。</p><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/alg1.png" alt></p><p>测试从初始给定的种子输入开始。</p><p>在主模糊循环中，模糊器反复选择一个有趣的种子来进行后续的突变和测试用例的生成。然后在模糊器的监视下驱动目标程序以执行生成的测试用例。</p><p>收集触发崩溃的测试用例，并将其他有趣的用例添加到种子库中。</p><p>对于基于覆盖的模糊测试，达到新控制流<strong>边缘的测试用例</strong>被认为很有趣。</p><p>主模糊循环在预先配置的超时或中止信号时停止。</p><p>在模糊测试过程中，模糊器通过各种方法跟踪执行情况。</p><p>基本上，模糊器出于两个目的跟踪执行，即<strong>代码覆盖率</strong>和<strong>安全性违规</strong>（security violations）。</p><p>代码覆盖率信息用于进行彻底的程序状态探索，而安全违规跟踪则用于更好地发现错误。</p><p>下图显示了AFL（American Fuzzy Lop，一个非常有代表性的基于覆盖的模糊器）的工作过程。</p><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/2020-06-27-00-18-50.png" alt></p><p>在执行覆盖范围收集之前，模糊器将对目标应用程序进行检测。</p><p>在主模糊循环中，提供初始种子输入后，（1）模糊器根据种子选择策略从种子库中选择喜欢的种子，比如AFL则选择最快和最小的种子。（2）根据变异策略对种子文件进行变异，并生成一堆测试用例。</p><p>AFL当前采用一些随机修改和测试用例拼接方法，包括长度和步长变化的顺序位翻转，小整数的顺序加法和减法以及已知有趣的整数（如0、1，INT_MAX等）的顺序插入。（3）测试用例已执行，并且执行情况正在跟踪中。</p><p>收集覆盖率信息以确定有趣的测试用例，即达到新控制流边缘的测试用例。</p><p>有趣的测试用例将添加到种子池中，以进行下一轮运行。</p><h2 id="3-关键问题"><a href="#3-关键问题" class="headerlink" title="3.关键问题"></a>3.关键问题</h2><p>前面的介绍表明，要运行一个高效的、基于覆盖的模糊，需要解决很多问题。最近一些研究如下表。</p><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/2020-06-27-00-22-32.png" alt></p><h3 id="A-如何获得初始输入？"><a href="#A-如何获得初始输入？" class="headerlink" title="A.如何获得初始输入？"></a>A.如何获得初始输入？</h3><p>大多数最先进的基于覆盖的模糊器都采用了基于突变的测试用例生成策略，这在很大程度上取决于初始种子输入的质量。</p><p>良好的初始种子输入可以显著提高模糊的效率和效果。</p><p>具体来说，</p><p>(1)提供格式良好的种子输入可以节省构建一个种子输入所消耗的大量cpu时间；</p><p>(2)良好的初始输入可以满足复杂文件格式的要求，而复杂文件格式在突变阶段是很难猜测的；</p><p>(3)基于格式良好的种子输入的突变更容易产生测试用例，可以达到更深层次和难以达到的路径；</p><p>(4)良好的种子输入可以在多次测试中重复使用。</p><p>常用的收集种子输入的方法包括使用标准基准、从互联网上抓取和使用现有的POC（Proof of Concept）样本。</p><p>考虑到目标应用输入的多样性，从互联网上抓取是最直观的方法。</p><p>过量的种子输入会导致第一次干运行的时间浪费，从而带来另一个问题，即如何提炼初始语料。</p><p>AFL提供了一个工具，它可以提取一个最小的输入集，以达到相同的代码覆盖率。</p><h3 id="B-如何生成测试用例？"><a href="#B-如何生成测试用例？" class="headerlink" title="B.如何生成测试用例？"></a>B.如何生成测试用例？</h3><p>testcases的质量是影响模糊测试效率和效果的重要因素。</p><p>首先，好的testcases可以在较短的时间内探索更多的程序执行状态，覆盖更多的代码。</p><p>此外，好的测试用例可以针对潜在的脆弱位置，带来更快的程序bug发现。</p><p>因此，如何基于种子输入生成好的测试用例是一个重要的问题。</p><p>种子输入的突变涉及两个关键问题：在哪里突变和用什么值进行突变。</p><p>随着机器学习技术的发展和广泛应用，一些研究尝试使用机器学习技术来辅助生成testcases。</p><p>微软研究院的Godefroid等（2017）利用基于神经网络的统计机器学习技术来自动生成testcases。</p><p>具体来说，他们首先通过机器学习技术从一堆有效输入中学习输入格式，然后利用学习到的知识指导测试用例的生成。</p><p>他们介绍了微软Edge浏览器中PDF解析器的模糊过程。</p><p>虽然实验并没有给出一个令人鼓舞的结果，但仍是一个不错的尝试。</p><p>微软的Rajpal等人（2017）使用神经网络从过去的模糊探索中学习，并预测输入文件中哪些字节要突变。</p><p>Nichols等人（2017）使用生成对抗网络（GAN）模型来帮助用新颖的种子文件重新初始化系统。</p><p>实验表明，GAN比LSTM更快、更有效，并且有助于发现更多的代码路径。</p><h3 id="C-如何从种子池中选择种子？"><a href="#C-如何从种子池中选择种子？" class="headerlink" title="C.如何从种子池中选择种子？"></a>C.如何从种子池中选择种子？</h3><p>模糊器在主模糊循环中新一轮测试开始时，反复从种子池中选择种子进行突变。</p><p>以往的工作已经证明，良好的种子选择策略可以显著提高模糊效率，并帮助更快地找到更多的Bugs。</p><p>通过良好的种子选择策略，模糊器可以</p><ul><li>（1）优先选择更有帮助的种子，包括覆盖更多的代码，更容易触发漏洞；</li><li>（2）减少重复执行路径的浪费，节省计算资源；</li><li>（3）优化选择覆盖更深、更易受攻击的代码的种子，帮助更快地识别隐藏的漏洞。</li></ul><p>Böhme等人（2017）提出了AFLFast，一个基于覆盖的灰盒模糊器。在模糊过程中，AFLFast会测量执行路径的频率，优先处理被模糊次数较少的种子，并为行使低频路径的种子分配更多的能量。</p><p>Rawat等(2017)综合了静态和动态分析来识别难以到达的深层路径，并对到达深层路径的种子进行优先级排序。</p><p>AFLGo将一些易受攻击的代码定义为目标位置，并优化选择离目标位置较近的测试case。</p><p>已知漏洞的特征也可用于种子选择策略。然而，收集耗费资源的信息带来了沉重的开销，降低了模糊的效率。</p><h3 id="D-如何高效地测试应用程序？"><a href="#D-如何高效地测试应用程序？" class="headerlink" title="D.如何高效地测试应用程序？"></a>D.如何高效地测试应用程序？</h3><p>目标应用程序由主模糊循环中的模糊器反复启动和完成。</p><p>对于用户区应用程序的模糊处理，程序的创建和完成将消耗大量的cpu时间。频繁创建和完成该程序将严重降低模糊测试的效率。</p><p>AFL使用forkserver方法，该方法创建一个已加载程序的完全相同的克隆，并在每次运行时重复使用该克隆。</p><h1 id="五、模糊集成技术"><a href="#五、模糊集成技术" class="headerlink" title="五、模糊集成技术"></a>五、模糊集成技术</h1><p>使用随机突变方法的模糊模糊测试策略会导致大量无效测试用例，并且模糊测试效率较低。</p><p>当前，最先进的模糊器通常采用智能模糊策略。</p><p>智能模糊器通过程序分析技术来收集程序控制流和数据流信息，并因此利用收集到的信息来改进测试用例的生成。</p><p>由智能模糊器生成的测试用例具有更好的针对性，更有可能满足程序对数据结构和逻辑判断的要求。</p><p>下图描绘了智能模糊的示意图。</p><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/2020-06-27-00-36-55.png" alt></p><p>表5中总结了模糊测试中集成的主要技术。</p><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/2020-06-27-00-37-51.png" alt></p><h2 id="1-测试用例生成"><a href="#1-测试用例生成" class="headerlink" title="1.测试用例生成"></a>1.测试用例生成</h2><p>如前所述，模糊测试中的测试用例是基于生成的方法或基于变异的方法生成的。</p><p>如何生成满足复杂数据结构要求并更有可能触发难以到达的路径的测试用例是一个关键挑战。</p><p>在基于生成的模糊测试中，生成器根据输入数据格式的知识生成测试用例。</p><p>Work（Godefroid et al.2017）使用机器学习技术（特别是递归神经网络）来学习输入文件的语法，并因此使用所学的语法来生成满足格式要求的测试用例。</p><p>更多的最先进的模糊器采用基于变异的模糊策略。通过在突变过程中修改部分种子输入来生成测试用例。</p><p>在盲目的突变模糊化过程中，变异者使用随机值或几个特殊值随机修改种子字节，这被证明是效率很低的。因此，如何确定要修改的位置以及修改中使用的值是另一个关键挑战。</p><h2 id="2-程序执行"><a href="#2-程序执行" class="headerlink" title="2.程序执行"></a>2.程序执行</h2><p>执行阶段涉及的两个关键问题是<strong>如何指导模糊测试过程</strong>以及<strong>如何探索新路径</strong>。</p><p>仪表（Instrumentation）技术用于记录路径执行情况并计算基于覆盖率的模糊测试中的覆盖率信息。</p><p>测试执行过程中的另一个问题是探索新路径。符号执行技术在路径探索中具有天然的优势。</p><p>通过求解约束集，符号执行技术可以计算出满足特定条件要求的值。</p><p>反馈驱动的模糊测试提供了一种有效的引导测试方法，传统和新技术都可以发挥传感器的作用，以在测试执行过程中获取各种信息，并准确地指导模糊测试。</p><h1 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a>六、总结</h1><ul><li>反馈驱动的模糊模式（feedback-driven fuzzing mode）和遗传算法（genetic algorithms）的结合</li><li>测试用例生成阶段，测试用例运行阶段，程序执行状态监视和异常分析</li><li>根据探索程序的策略（the strategies of exploring the programs），模糊器可以分为定向模糊（directed fuzzing）和基于覆盖的模糊（coverage-based fuzzing）。</li><li>定向模糊器期望对程序进行更快的测试，而基于覆盖率的模糊器期望进行更彻底的测试并检测到尽可能多的错误。</li><li>对于定向模糊器和基于覆盖的模糊器，如何提取执行路径的信息是一个关键问题。</li><li>根据对程序执行状态的监视和测试用例的生成之间是否存在反馈（whether there is a feedback between the monitoring of program execution state and testcase generation），模糊器可以分为哑类（dumb fuzz）和智能类（smart fuzz）。</li><li>智能模糊器会根据收集的信息（测试用例如何影响程序行为）来调整测试用例的生成。</li><li>对于基于变异的模糊测试器，反馈信息可用于确定应该对测试用例的哪一部分进行变异以及对它们进行变异的方式。</li><li>基于变异的模糊测试工具在进行变异时需要回答两个问题：（1）变异的位置，以及（2）变异的方式。</li><li>覆盖率的定义。定义覆盖率时要防止信息丢失。代码覆盖率的增加代表了新的程序状态。此外，代码覆盖率很容易测量。边缘的测试用例被认为很有趣</li><li>通过验证的挑战。验证可以保护程序，节省计算资源，并保护程序免受无效输入和恶意构造输入造成的损坏。</li><li>一些研究尝试使用机器学习技术来辅助生成testcases。GAN比LSTM更快、更有效，并且有助于发现更多的代码路径。</li><li>如何从种子池中选择种子？</li><li>如何高效地测试应用程序？</li></ul>]]></content>
    
    
    <categories>
      
      <category>paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Fuzzing</tag>
      
      <tag>Software Testing</tag>
      
      <tag>Survey</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】LSTM网络结构简介与对应的keras实现</title>
    <link href="/2020/06/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91LSTM%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AF%B9%E5%BA%94%E7%9A%84keras%E5%AE%9E%E7%8E%B0/"/>
    <url>/2020/06/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91LSTM%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AF%B9%E5%BA%94%E7%9A%84keras%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<p>从理论和代码两个层面介绍了LSTM网络。<br><!--more---></p><h2 id="一、理论来一波"><a href="#一、理论来一波" class="headerlink" title="一、理论来一波"></a>一、理论来一波</h2><p>循环神经网络（Recurrent Neural Network，RNN）是一类有短期记忆能力的神经网络。在循环神经网络中，神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构。</p><p><img src="/2020/06/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91LSTM%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AF%B9%E5%BA%94%E7%9A%84keras%E5%AE%9E%E7%8E%B0/2020-06-20-11-51-21.png" alt></p><p>长短期记忆网络（Long Short-Term Memory Network，LSTM）[Gers et al.,2000; Hochreiter et al., 1997] 是循环神经网络的一个变体，可以有效地解决简单循环神经网络的梯度爆炸或消失问题。</p><p><img src="/2020/06/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91LSTM%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AF%B9%E5%BA%94%E7%9A%84keras%E5%AE%9E%E7%8E%B0/2020-06-20-11-51-13.png" alt></p><p>LSTM 网络引入一个新的内部状态（internal state） $\boldsymbol{c}<em>{t}\in \mathbb{R}^{\boldsymbol{D}}$ 专门进行线性的循环信息传递，同时（非线性地）输出信息给隐藏层的外部状态 $\boldsymbol{h}</em>{t}\in \mathbb{R}^{\boldsymbol{D}}$ 。这两个状态通过下式计算：</p><script type="math/tex; mode=display">\begin{array}{l}\boldsymbol{c}_{t}=\boldsymbol{f}_{t} \odot \boldsymbol{c}_{t-1}+\boldsymbol{i}_{t} \odot \tilde{\boldsymbol{c}}_{t} \\\boldsymbol{h}_{t}=\boldsymbol{o}_{t} \odot \tanh \left(\boldsymbol{c}_{t}\right)\end{array}</script><p>其中，$\odot$为向量逐元素乘积（代表左右两边向量维度相同）；$\boldsymbol{c}_{t-1}$为上一时刻的记忆单元；$\tilde{\boldsymbol{c}}\in \mathbb{R}^{\boldsymbol{D}}$是通过非线性函数得到的候选状态：</p><script type="math/tex; mode=display">\tilde{\boldsymbol{c}}_{t}=\tanh \left(\boldsymbol{W}_{c} \boldsymbol{x}_{t}+\boldsymbol{U}_{c} \boldsymbol{h}_{t-1}+\boldsymbol{b}_{c}\right)</script><p>在每个时刻 $t$，LSTM 网络的内部状态 $\boldsymbol{c}_{t}$ 记录了到当前时刻为止的历史信息。</p><p>LSTM内部多了三个gate，分别是forget、input、output。$\boldsymbol{f}<em>{t}\in [0,1]^{\boldsymbol{D}}$、$\boldsymbol{i}</em>{t}\in [0,1]^{\boldsymbol{D}}$、$\boldsymbol{o}_{t}\in [0,1]^{\boldsymbol{D}}$。这三个门与输入、隐状态和输出的维度应该相同，都是维度为输入序列维度n的向量（其实应该为n+1）$=D$。</p><p>与此同时，三个门的值依赖于$t$时刻的输入$x<em>t$、$t-1$时刻的隐变量$h</em>{t-1}$以及不同的权重矩阵($W_i$/$W_f$/$W_o$/$U_i$/$U_f$/$U_o$)。</p><p>门控机制（Gating Mechanism）是用来控制信息传递的路径的手段。</p><ul><li>遗忘门 $\boldsymbol{f}<em>{t}$ 控制上一个时刻的内部状态$\boldsymbol{c}</em>{t-1}$ 需要遗忘多少信息。</li><li>输入门 $\boldsymbol{i}<em>{t}$ 控制当前时刻的候选状态 ̃$\tilde{\boldsymbol{c}}</em>{t}$ 有多少信息需要保存。</li><li>输出门 $\boldsymbol{o}<em>{t}$ 控制当前时刻的内部状态 $\boldsymbol{c}</em>{t}$ 有多少信息需要输出给外部状态 $\boldsymbol{h}_{t}$。</li></ul><p>举个例子，当$\boldsymbol{f}<em>{t}=\mathbf{0}, \boldsymbol{i}</em>{t}=\mathbf{1}$时，记忆单元将历史信息清空，并将候选状态向量$\tilde{\boldsymbol{c}}<em>{t}$写入。但此时记忆单元 $\boldsymbol{c}</em>{t}$ 依然和上一时刻的历史信息相关。当$\boldsymbol{f}<em>{t}=\mathbf{1}, \boldsymbol{i}</em>{t}=\mathbf{0}$时，记忆单元将复制上一时刻的内容，不写入新的信息。</p><p>LSTM 网络中的“门”是一种“软”门，取值在 (0, 1) 之间，表示以一定的比例允许信息通过．三个门的计算方式为：</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{i}_{t} &=\sigma\left(\boldsymbol{W}_{i} \boldsymbol{x}_{t}+\boldsymbol{U}_{i} \boldsymbol{h}_{t-1}+\boldsymbol{b}_{i}\right) \\\boldsymbol{f}_{t} &=\sigma\left(\boldsymbol{W}_{f} \boldsymbol{x}_{t}+\boldsymbol{U}_{f} \boldsymbol{h}_{t-1}+\boldsymbol{b}_{f}\right) \\\boldsymbol{o}_{t} &=\sigma\left(\boldsymbol{W}_{o} \boldsymbol{x}_{t}+\boldsymbol{U}_{o} \boldsymbol{h}_{t-1}+\boldsymbol{b}_{o}\right)\end{aligned}</script><p>其中$\sigma(\cdot)$ 为 Logistic 函数，其输出区间为 (0, 1)；$\boldsymbol{x}_{t}$为当前时刻的输入。</p><h2 id="二、还是得看代码"><a href="#二、还是得看代码" class="headerlink" title="二、还是得看代码"></a>二、还是得看代码</h2><p>下面是我定义的一个专用于IMDb影评情感分析的二分类模型，包装在一个函数中。输入训练集、测试集及其标签，设定好参数就可以运行、训练。可以选择是否保存模型到本地。最后函数返回训练好的模型。</p><p>这个二分类模型中，输入是长度为80的整数列表（maxlen=80），代表着80个不同的单词构成的一句话。</p><p>如果有影评不够80个词，就在影评前面加足够的0，直到这条影评达到80个词为止。如果影评单词量大于80个，便截取前面的80个词。</p><p>每个整数都代表一个单词表中的单词。当然单词表的大小是固定的（num_words=10000个单词），如果出现不在单词表中的单词，固定将其编码成2，表示UNKNOWN（这条设置不在下面的代码中，属于数据预处理）。</p><p>第一层是Embedding层，负责将一句话中的每个单词映射成固定维度的词向量；</p><p>注意，每个单词（在这里是每个整数）都会变成固定维度（embedding_dim=128）的向量，因此每条影评从Embedding层输出后，都会变成80*128的矩阵。</p><p>第二层是LSTM层。如果你看了理论部分的叙述，就知道LSTM层中无论是隐状态$\boldsymbol{c}$、$\boldsymbol{h}$还是三个门$\boldsymbol{f}$、$\boldsymbol{i}$、$\boldsymbol{o}$，他们的维度都是$\boldsymbol{D}$。这个$\boldsymbol{D}$的大小就需要我们用参数<code>lstm_dim=32</code>来定义。这个参数越大，代表LSTM层的参数越多、泛化能力越强，也更难训练、更容易过拟合。</p><p>第三层是单个神经元的sigmoid层，在这里就直接转换成概率并分类了。</p><pre><code class="lang-python">def train_lstm(x_train, y_train, x_test, y_test,                num_words=10000,                maxlen=80,                embedding_dim=128,                lstm_dim=32,                batch_size=32,                epochs=10):    # 接收一个含有 100 个整数的序列，每个整数在 1 到 20000 之间    inputs = Input(shape=(maxlen,), dtype=&#39;int32&#39;, name=&#39;main_input&#39;)    # Embedding 层将输入序列编码为一个稠密向量的序列，    # 每个向量维度为 512。    x = Embedding(input_dim=num_words,                   input_length=maxlen,                   output_dim=embedding_dim,                   name=&#39;embedding&#39;)(inputs)    # LSTM 层把向量序列转换成单个向量，    # 它包含整个序列的上下文信息    lstm_output = LSTM(lstm_dim, name=&#39;lstm&#39;)(x)    # 插入辅助损失，    #使得即使在模型主损失很高的情况下，LSTM 层和 Embedding 层都能被平稳地训练    outputs = Dense(1, activation=&#39;sigmoid&#39;, name=&#39;output&#39;)(lstm_output)    model = Model(inputs=inputs, outputs=outputs)    model.compile(optimizer=&#39;adam&#39;,            loss=&#39;binary_crossentropy&#39;,            metrics=[&#39;accuracy&#39;])    model.fit(x_train, y_train,        batch_size=batch_size,        epochs=epochs,        validation_data=(x_test, y_test,))    score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)    print(&#39;Test score:&#39;, score)    print(&#39;Test accuracy:&#39;, acc)    # model.save(&quot;lstm_imdb.h5&quot;)    return model</code></pre><h2 id="三、LSTM返回所有时间步的hidden-state向量"><a href="#三、LSTM返回所有时间步的hidden-state向量" class="headerlink" title="三、LSTM返回所有时间步的hidden state向量"></a>三、LSTM返回所有时间步的hidden state向量</h2><p>数据经过LSTM层，输出的是最后一个时间步得到的output向量（即$\boldsymbol{h}_{finally}$），维度为$\boldsymbol{D}$。</p><p>其实LSTM能够在每个时间步都输出output（即$\boldsymbol{h}_{t}$），只不过我们把这些没到时间的半成品output选择性忽略了。</p><p>如果你想要堆叠LSTM层，也就是LSTM层下面还有LSTM，或者你<strong>需要所有时间步的</strong>$\boldsymbol{h}_{t}$，那么你可以在训练的时候把<code>return_sequences=True</code>写进LSTM参数之中。</p><p><img src="/2020/06/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91LSTM%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AF%B9%E5%BA%94%E7%9A%84keras%E5%AE%9E%E7%8E%B0/堆叠rnn.png" alt></p><p>下面让我们来比较一下<code>return_sequences</code>参数开启之后输出值的变化。</p><h3 id="return-sequences-False"><a href="#return-sequences-False" class="headerlink" title="return_sequences=False"></a>return_sequences=False</h3><p>首先固定随机数种子。</p><pre><code class="lang-python">np.random.seed(0)tf.random.set_seed(0)</code></pre><p>然后构建输入Input向量和LSTM层，此时LSTM层使用默认参数<code>return_sequences=False</code>。</p><pre><code class="lang-python">input1 = Input(shape=(3,1)) # 输入是三维向量lstm1 = LSTM(1)(input1) # 内部hidden和cell的维度为1model = Model(inputs=input1, outputs=lstm1)</code></pre><p>构造一批输入，包括6个句子，每个句子三个单词，然后输入LSTM，查看LSTM层的输出。</p><pre><code class="lang-python">data = np.array([[0.1, 0.2, 0.3],                    [0.3, 0.2, 0.1],                    [0.2, 0.6, 0.3],                    [0.8, 0.2, 0.3],                    [0.3, 0.5, 0.1],                    [0.2, 0.6, 0.2]])print(model.predict(data))</code></pre><p>此时输出为：</p><pre><code class="lang-python">[[0.00844267] [0.00617958] [0.01279002] [0.01231858] [0.009055  ] [0.01108878]]Process finished with exit code 0</code></pre><h3 id="return-sequences-True"><a href="#return-sequences-True" class="headerlink" title="return_sequences=True"></a>return_sequences=True</h3><p>然后打开<code>return_sequences</code>的开关</p><pre><code class="lang-python">lstm1 = LSTM(1, return_sequences=True)(input1)</code></pre><p>此时的输出为：</p><pre><code>[[[0.00190693]  [0.00490441]  [0.00844267]] #  [[0.0055262 ]  [0.00704476]  [0.00617958]] # [[0.00374958]  [0.01259477]  [0.01279002]] # [[0.01337298]  [0.01142679]  [0.01231858]] # [[0.0055262 ]  [0.01206062]  [0.009055  ]] # [[0.00374958]  [0.01259477]  [0.01108878]]] #Process finished with exit code 0</code></pre><p>此为输出所有时间步的hidden state。鉴于一共6个测试输入，每个输入有3个feature，所以时间步也就三步。LSTM的输出结果从6个hidden state变成了6*3个hidden state。</p><h3 id="return-state-True"><a href="#return-state-True" class="headerlink" title="return_state=True"></a>return_state=True</h3><p>我们再来看另一个参数，这个参数能够控制LSTM输出cell state。</p><pre><code class="lang-python">lstm1 = LSTM(1, return_state=True)(input1)</code></pre><pre><code>[array([[0.00844267],       [0.00617958],       [0.01279002],       [0.01231858],       [0.009055  ],       [0.01108878]], dtype=float32), array([[0.00844267],       [0.00617958],       [0.01279002],       [0.01231858],       [0.009055  ],       [0.01108878]], dtype=float32), array([[0.01655067],       [0.01227413],       [0.02506882],       [0.02414548],       [0.01798305],       [0.02187706]], dtype=float32)]Process finished with exit code 0</code></pre><p>开启<code>return_state=True</code>之后，LSTM返回3个array，第一个array和第二个array一样，都是hidden state，和默认返回的一样。第三个array就是最后一个时间步的cell state。</p><h3 id="return-state-True-return-sequences-True"><a href="#return-state-True-return-sequences-True" class="headerlink" title="return_state=True, return_sequences=True"></a>return_state=True, return_sequences=True</h3><p>如果两个开关都打开，则结果变成</p><pre><code class="lang-python">lstm1 = LSTM(1, return_state=True, return_sequences=True)(input1)</code></pre><pre><code>[array([[[0.00190693],        [0.00490441],        [0.00844267]],       [[0.0055262 ],        [0.00704476],        [0.00617958]],       [[0.00374958],        [0.01259477],        [0.01279002]],       [[0.01337298],        [0.01142679],        [0.01231858]],       [[0.0055262 ],        [0.01206062],        [0.009055  ]],       [[0.00374958],        [0.01259477],        [0.01108878]]], dtype=float32), array([[0.00844267],       [0.00617958],       [0.01279002],       [0.01231858],       [0.009055  ],       [0.01108878]], dtype=float32), array([[0.01655067],       [0.01227413],       [0.02506882],       [0.02414548],       [0.01798305],       [0.02187706]], dtype=float32)]Process finished with exit code 0</code></pre><p>还是返回三个array，第一个是所有时间步的hidden state，这是开启<code>return_sequences=True</code>的效果；第二个则是原本LSTM的输出hidden state；第三个是开启<code>return_state=True</code>的效果，返回最后一个时间步的cell state</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LSTM</tag>
      
      <tag>RNN</tag>
      
      <tag>Keras</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读笔记】Deep Text Classification Can be Fooled</title>
    <link href="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/"/>
    <url>/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/</url>
    
    <content type="html"><![CDATA[<p>国内人民大学的一篇论文，被IJCAI-2018接收。主要研究文本领域的对抗样本生成，被测模型是文本分类领域的模型。<br><!--more---></p><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>当前的对抗样本生成领域集中在图像的扰动和生成，文本领域少有涉及。</p><p><strong>文本对抗样本应当满足</strong>：</p><ul><li>使模型分类错误</li><li>与原样本相比，扰动难以察觉</li><li>实用性，即文本含义不发生改变</li></ul><p><strong>本文采用的生成思路</strong>：</p><ul><li>三种扰动策略：插入、修改和删除</li><li>自然语言水印</li><li>白盒+黑盒</li></ul><blockquote><p>自然语言水印（Natural Language Watermarking）是通过修改文本元素（如线条，文字或字符）的外观，或更改文本格式或字体（例如，通过-文字中的单词和字母间距）来嵌入信息的技术。</p></blockquote><h2 id="2-目标模型和数据集"><a href="#2-目标模型和数据集" class="headerlink" title="2. 目标模型和数据集"></a>2. 目标模型和数据集</h2><p><strong>字符级模型</strong>：</p><ul><li>以字母为单位，编码长度为字母表大小（26+空白符个数）</li><li>结构：6conv + 3fc</li><li>数据集：DBPedia，包含14个类别，56000训练、70000测试</li></ul><p><strong>单词级模型</strong>：</p><ul><li>以单词为单位，编码长度为单词表大小（数万到数十万）</li><li>结构：embedding + conv + maxpooling + fc(带dropout) + softmax</li><li>数据集：影评MR、产品评价CR、产品意见MPQA，都是二分类</li></ul><h2 id="3-白盒攻击"><a href="#3-白盒攻击" class="headerlink" title="3. 白盒攻击"></a>3. 白盒攻击</h2><p>所谓白盒攻击，就是在已知被测模型内部信息的情况下开展的攻击。由于已知信息较多，所以攻击起来也比较容易。</p><p>白盒攻击生成对抗样本的思想在图像领域用的比较多。比如利用网络参数和损失函数进行指导攻击过程的FGSM算法。</p><p>本文采用的白盒攻击手段，也是利用模型内部的参数和网络训练时的损失函数作为引导，但是不会直接生成对抗样本，而是<strong>首先识别对分类有重大贡献的文本项</strong>。</p><h3 id="HTP：Hot-Training-Phrases"><a href="#HTP：Hot-Training-Phrases" class="headerlink" title="HTP：Hot Training Phrases"></a>HTP：Hot Training Phrases</h3><p>以字符级模型为例，识别过程如下：</p><ol><li>输入训练样本x，计算cost gradients ∇x C(M, x, label)；</li><li>每个样本x选取令gradient最大的前50个字符定义为<strong>hot character</strong>；</li><li>包含3个及以上hot characters的单词定义为<strong>hot word</strong>；</li><li>相邻的hot words定义为<strong>hot phrase</strong>；</li><li>不同类别有不同的hot phrase，代表着该类对分类贡献最大的词组；</li><li>每个类别中最常出现的hot phrase定义为<strong>Hot Training Phrases (HTPs)</strong>。</li></ol><p>下图是DBPedia的Building类别文本数据集中的HTP词汇排名前10。<br><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/2020-06-23-20-11-30.png" alt></p><h3 id="HSP：Hot-Sample-Phrases"><a href="#HSP：Hot-Sample-Phrases" class="headerlink" title="HSP：Hot Sample Phrases"></a>HSP：Hot Sample Phrases</h3><p>给定样本x，识别x中的hot phrase作为操作位置，该位置的单词或词组就定义为<strong>Hot Sample Phrases (HSPs)</strong>。</p><h3 id="Attacking-Character-level-DNN"><a href="#Attacking-Character-level-DNN" class="headerlink" title="Attacking Character-level DNN"></a>Attacking Character-level DNN</h3><h4 id="Insertion"><a href="#Insertion" class="headerlink" title="Insertion"></a><strong>Insertion</strong></h4><p>通过在样本的HSP位置插入以下内容实现变异操作：</p><ul><li>HTP；</li><li>可有可无的事实（文本水印算法生成）；</li><li>不伤害文本主语义的伪造事实（文本水印算法生成）。</li></ul><p>下面是三个通过插入红色文本造成标签改变的例子。<br><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/2020-06-23-20-11-43.png" alt="Figure2"><br><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/2020-06-23-20-11-56.png" alt="Figure3"><br><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/Figure4.png" alt="Figure4"></p><h4 id="Modification"><a href="#Modification" class="headerlink" title="Modification"></a><strong>Modification</strong></h4><p>对HSP稍加操作，比如typo-based watermarking technique（基于错别字的水印技术）：</p><ul><li>(1)替换以常见的错误拼写（需要有错别字语料库）</li><li>(2)替换以外观相似的字符</li></ul><p>下图是替换操作后生成对抗样本的一个例子。<br><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/Figure5.png" alt></p><p>下图是将film替换成flim后模型内部损失函数梯度的改变。<br><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/Figure6.png" alt></p><h4 id="Removal"><a href="#Removal" class="headerlink" title="Removal"></a><strong>Removal</strong></h4><p>删除HSP可降低模型对样本的confidence。只能删除HSPs中起辅助作用的词，要不然会改变本来的含义。</p><p>下面是通过删除来导致置信程度下降的例子。<br><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/Figure7.png" alt></p><h4 id="Combination"><a href="#Combination" class="headerlink" title="Combination"></a><strong>Combination</strong></h4><p>组合上述三种手法。</p><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/Figure8.png" alt></p><h3 id="Attacking-Word-level-DNN"><a href="#Attacking-Word-level-DNN" class="headerlink" title="Attacking Word-level DNN"></a>Attacking Word-level DNN</h3><p>单词级模型也是同理，不仅如此，甚至省了hot-character这一步。下面是几个例子。</p><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/Figure9.png" alt></p><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/Figure10.png" alt></p><h2 id="4-黑盒攻击"><a href="#4-黑盒攻击" class="headerlink" title="4. 黑盒攻击"></a>4. 黑盒攻击</h2><p>黑盒攻击显然不能通过比较Cost Gradient的方式确定HTP和HSP了。但是我们可以采用其他方法确定HTP和HSP。</p><p>具体地，我们通过生成一些测试样本来探测目标模型，判断哪些是Hot Phrases。</p><p>生成方法：</p><ul><li>用若干空格逐个代替单词（空格个数与单词长度相同）</li><li>将测试样本的分类结果与种子进行比较</li><li>偏差越大，相应单词对正确分类的重要性就越大</li><li>带来最大偏差的单词被标识为种子样本的HSP</li></ul><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/Figure11.png" alt></p><p>下面是黑盒攻击确定的HTP之后进行攻击的例子：</p><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/Figure12.png" alt></p><h2 id="5-Evaluation"><a href="#5-Evaluation" class="headerlink" title="5. Evaluation"></a>5. Evaluation</h2><h3 id="Q1-Can-our-method-perform-effective-source-target-misclassification-attack"><a href="#Q1-Can-our-method-perform-effective-source-target-misclassification-attack" class="headerlink" title="Q1: Can our method perform effective source/target misclassification attack?"></a>Q1: Can our method perform effective source/target misclassification attack?</h3><p>这个问题是问本方法能不能对模型实行定向的攻击，即“指哪打哪”，无论哪个类别的样本都能通过适当修改，突变成指定类别。</p><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/Fig13.png" alt></p><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/Fig14.png" alt></p><p>通过上表可知，source栏是源类别。target栏是目标类别，No栏是样本编号。本来都是类别source中定义的样本，经过右边三栏的突变方法，最终都以较高置信度被模型分类成了target栏中的类别。可以证明此方法确实能实现定向突变、定向攻击。</p><h3 id="Q2-Can-the-adversarial-samples-avoid-being-distinguished-by-human-observers-and-still-keep-the-utility"><a href="#Q2-Can-the-adversarial-samples-avoid-being-distinguished-by-human-observers-and-still-keep-the-utility" class="headerlink" title="Q2: Can the adversarial samples avoid being distinguished by human observers and still keep the utility?"></a>Q2: Can the adversarial samples avoid being distinguished by human observers and still keep the utility?</h3><p>这个问题是问对抗样本是不是能够避免被人类识别。毕竟文本突变还是很容易被人类识别的。</p><p>本论文是这么设计实验的：</p><ul><li>找23名学生，每个人都提供了20个文本样本，其中一半带有扰动，对每个样本进行手动分类</li><li>如果他们认为样品是人为修改的，则要求他们查明修改的位置</li><li>原样本准确率：94.2%</li><li>扰动样本准确率：94.8%</li><li>总共生成594个变化，有240个被受试者标记为已修改的位置，其中12个正确。准确率为12/240 = 5.0％，召回率为12/594 = 2.0％</li></ul><p>可以看到虽然样本数比较小，但是结果还是很显著的，在那23个同学都比较靠谱的前提下，该算法还是能够保证生成的文本与原文本差距不大的。</p><h3 id="Q3-Is-our-method-efficient-enough"><a href="#Q3-Is-our-method-efficient-enough" class="headerlink" title="Q3: Is our method efficient enough?"></a>Q3: Is our method efficient enough?</h3><p>算法效率其实不是很重要，毕竟在实践中，攻击者往往愿意花费更多时间来制作理想的对抗性样本。</p><p>白盒攻击（计算梯度、确定HTP），总共116小时，平均每类8.29小时；<br>黑盒攻击（生成样本、确定HTP），总共107小时，平均每类7.63小时。</p><h3 id="Q4-White-box-and-black-box-which-is-more-powerful"><a href="#Q4-White-box-and-black-box-which-is-more-powerful" class="headerlink" title="Q4: White-box and black-box, which is more powerful?"></a>Q4: White-box and black-box, which is more powerful?</h3><p>两种方式都有效并且彼此互补。</p><p>下图分别是黑盒和白盒生成的HTP比较，可以看到都是比较类似的。</p><p><img src="/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/Fig15.png" alt></p><h2 id="6-读后感"><a href="#6-读后感" class="headerlink" title="6. 读后感"></a>6. 读后感</h2><p>其实本篇文章的思想并不复杂，核心是确定一段文本的HTP和HSP。所谓HTP可以认为是，模型一看到这种词就相信这句话是该类别的了。那如果把类别1的句子x中的HSP给替换成类别2的HTP，的确可能让模型以为句子x是类别2的句子了。</p><p>所以延伸出来一个方向，那就是确定HSP和HTP的方法上。对于白盒攻击，还是查看内部的信息，然后计算梯度，这是一种比较传统的方法。对于黑盒攻击，则是遍历所有可能删除的单词，从结果上来看，比较这些删除单词的重要程度。</p><p>所以说有没有其他方法能够衡量单词的重要程度？这是一个值得研究的方向。</p>]]></content>
    
    
    <categories>
      
      <category>paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>testing</tag>
      
      <tag>DNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】机器学习中处理文本数据的常用方法</title>
    <link href="/2020/06/09/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%A4%84%E7%90%86%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <url>/2020/06/09/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%A4%84%E7%90%86%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>总结词袋模型、Tf-idf等文本特征提取方法<br><!--more---></p><h2 id="一、词袋模型"><a href="#一、词袋模型" class="headerlink" title="一、词袋模型"></a>一、词袋模型</h2><p>文本数据通常被表示为由字符组成的字符串。我们需要先处理数据，然后才能对其应用机器学习算法。</p><p>在文本分析的语境中，数据集通常被称为语料库（corpus），每个由单个文本表示的数据点被称为文档（document）。</p><p>最简单的处理方法，是<strong>只计算语料库中每个单词在每个文本中的出现频次</strong>。这种文本处理模型称之为<strong>词袋模型</strong>。</p><p>不考虑词语出现的顺序，每个出现过的词汇单独作为一列特征，这些不重复的特征词汇集合为词表。</p><p>每一个文本都可以在很长的词表上统计出一个很多列的特征向量。如果每个文本都出现的词汇，一般被标记为<strong>停用词</strong>不计入特征向量。</p><p>为了搞清楚词袋模型，也就是<code>CountVectorizer</code>到底做了什么，我们执行以下代码：</p><pre><code class="lang-python">bards_words =[&quot;The fool doth think he is wise,&quot;,    &quot;but the wise man knows himself to be a fool&quot;]</code></pre><p>我们导入 CountVectorizer 并将其实例化，然后对 bards_words 进行拟合，如下所示：</p><pre><code class="lang-python">from sklearn.feature_extraction.text import CountVectorizervect = CountVectorizer()vect.fit(bards_words)</code></pre><p>拟合 CountVectorizer 包括训练数据的分词与词表的构建，我们可以通过 vocabulary_ 属性来访问词表：</p><pre><code class="lang-python">print(&quot;Vocabulary size: &#123;&#125;&quot;.format(len(vect.vocabulary_)))print(&quot;Vocabulary content:\n &#123;&#125;&quot;.format(vect.vocabulary_))#------------------#Vocabulary size: 13Vocabulary content:&#123;&#39;the&#39;: 9, &#39;himself&#39;: 5, &#39;wise&#39;: 12, &#39;he&#39;: 4, &#39;doth&#39;: 2, &#39;to&#39;: 11, &#39;knows&#39;: 7,&#39;man&#39;: 8, &#39;fool&#39;: 3, &#39;is&#39;: 6, &#39;be&#39;: 0,  &#39;think&#39;: 10, &#39;but&#39;: 1&#125;</code></pre><p>词表共包含 13 个词，从 “be” 到 “wise”。<br>我们可以调用 transform 方法来创建训练数据的词袋表示：</p><pre><code class="lang-python">bag_of_words = vect.transform(bards_words)print(&quot;bag_of_words: &#123;&#125;&quot;.format(repr(bag_of_words)))#--------------------#bag_of_words: &lt;2x13 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;with 16 stored elements in Compressed Sparse Row format&gt;</code></pre><p>词袋表示保存在一个 SciPy 稀疏矩阵中，这种数据格式只保存非零元素。这个矩阵的形状为 2×13，每行对应于两个数据点之一，每个特征对应于词表中的一个单词。要想查看稀疏矩阵的实际内容，可以使用 toarray 方法将其转换为“密集的”NumPy 数组（保存所有 0 元素）：</p><pre><code class="lang-python">print(&quot;Dense representation of bag_of_words:\n&#123;&#125;&quot;.format(    bag_of_words.toarray()))#---------------------#Dense representation of bag_of_words:[[0 0 1 1 1 0 1 0 0 1 1 0 1][1 1 0 1 0 1 0 1 1 1 0 1 1]]</code></pre><p>删除没有信息量的单词，除了使用<code>min_df</code>参数设定词例至少需要在多少个文档中出现过之外，还可以通过添加停用词的方法。</p><h2 id="二、用tf-idf缩放数据"><a href="#二、用tf-idf缩放数据" class="headerlink" title="二、用tf-idf缩放数据"></a>二、用tf-idf缩放数据</h2><p>词频 - 逆向文档频率（term frequency–inverse document frequency，tf-idf）方法，对在某个特定文档中经常出现的术语给予很高的权重，但对在语料库的许多文档中都经常出现的术语给予的权重却不高。</p><p>scikit-learn 在两个类中实现了 tf-idf 方法：TfidfTransformer 和 TfidfVectorizer，前者接受 CountVectorizer 生成的稀疏矩阵并将其变换，后者接受文本数据并完成词袋特征提取与 tf-idf 变换。</p><p>单词w在文档d中的tf-idf分数为：</p><p><img src="/2020/06/09/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%A4%84%E7%90%86%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95/2020-06-09-23-20-54.png" alt></p><p>式中，tf为词频，Term Frequency, 表示一个词在一个文档中的出现频率。该频率最后要除以该文档的长度，用以归一化。</p><p>式中，$N$为总文档数，$N_w$为带有单词$w$的文档数。由于分子比分母大，所以该 $\log$ 值必不可能小于零。</p><pre><code class="lang-python">from sklearn.feature_extraction.text import TfidfVectorizercorpus=[&quot;I come to China to travel&quot;,&quot;This is a car polupar in China&quot;,&quot;I love tea and Apple &quot;,&quot;The work is to write some papers in science&quot;]tfidf = TfidfVectorizer()vector = tfidf.fit_transform(corpus)print(vector)#---------------#(0, 16)    0.4424621378947393(0, 3)    0.348842231691988(0, 15)    0.697684463383976(0, 4)    0.4424621378947393(1, 5)    0.3574550433419527(1, 9)    0.45338639737285463(1, 2)    0.45338639737285463(1, 6)    0.3574550433419527(1, 14)    0.45338639737285463(1, 3)    0.3574550433419527(2, 1)    0.5(2, 0)    0.5(2, 12)    0.5(2, 7)    0.5(3, 10)    0.3565798233381452(3, 8)    0.3565798233381452(3, 11)    0.3565798233381452(3, 18)    0.3565798233381452(3, 17)    0.3565798233381452(3, 13)    0.3565798233381452(3, 5)    0.2811316284405006(3, 6)    0.2811316284405006(3, 15)    0.2811316284405006</code></pre><p>返回值什么意思呢？(0, 16)代表第0个文档，第一个单词在单词表（词袋）中的位置是第16个，该单词的tf-idf值为0.44246213；第二个单词在词袋中第3个位置……</p><p>显然这是个经过压缩的系数矩阵，每一行的元组表明该元素在稀疏矩阵中的位置，其值为右边的tf-idf值，代表一个单词。可以通过<code>.toarray()</code>方法令其恢复到系数矩阵状态。</p><pre><code class="lang-python">print(vector.toarray().shape)print(len(vector.toarray()))print(type(vector.toarray()))print(vector.toarray())#-----------------------------#(4, 19)4&lt;class &#39;numpy.ndarray&#39;&gt;[[0. 0. 0. 0.34884223 0.44246214 0.  0. 0. 0. 0. 0. 0.  0. 0. 0. 0.69768446 0.44246214 0.  0. ] [0. 0. 0.4533864  0.35745504 0. 0.35745504  0.35745504 0. 0. 0.4533864  0. 0.  0. 0. 0.4533864  0. 0. 0.  0. ] [0.5 0.5 0. 0. 0. 0.  0. 0.5 0. 0. 0. 0.  0.5 0. 0. 0. 0. 0.  0. ] [0. 0. 0. 0. 0. 0.28113163  0.28113163 0. 0.35657982 0. 0.35657982 0.35657982  0. 0.35657982 0. 0.28113163 0. 0.35657982  0.35657982]]</code></pre><h2 id="三、多元词袋"><a href="#三、多元词袋" class="headerlink" title="三、多元词袋"></a>三、多元词袋</h2><p>词袋模型将一段文档拆分成单词后，忽略了单词的上下文可能对文档的含义造成影响。</p><h2 id="四、英语的词干提取与词形还原"><a href="#四、英语的词干提取与词形还原" class="headerlink" title="四、英语的词干提取与词形还原"></a>四、英语的词干提取与词形还原</h2><h2 id="五、中文的分词"><a href="#五、中文的分词" class="headerlink" title="五、中文的分词"></a>五、中文的分词</h2><h2 id="六、主题建模与文档聚类"><a href="#六、主题建模与文档聚类" class="headerlink" title="六、主题建模与文档聚类"></a>六、主题建模与文档聚类</h2>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>Machine Learning</tag>
      
      <tag>IMDb</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【经验分享】IMDb数据集的预处理</title>
    <link href="/2020/06/09/%E3%80%90%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E3%80%91IMDb%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <url>/2020/06/09/%E3%80%90%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E3%80%91IMDb%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>IMDb从官网下载与从keras直接调用的处理方法是不同的。<br><!--more---></p><h2 id="一、IMDb数据集的处理方法"><a href="#一、IMDb数据集的处理方法" class="headerlink" title="一、IMDb数据集的处理方法"></a>一、IMDb数据集的处理方法</h2><h3 id="1-官网下载法"><a href="#1-官网下载法" class="headerlink" title="1. 官网下载法"></a>1. 官网下载法</h3><pre><code class="lang-python">import pandas as pdimport numpy as npfrom sklearn.datasets import load_files</code></pre><pre><code class="lang-shell">!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz!tar -zxvf aclImdb_v1.tar.gz!ls</code></pre><p>对于 v1.0 版数据，其训练集大小是 75 000，而不是 25 000，因为其中还包含 50 000 个用于无监督学习的无标签文档。</p><p>在进行后续操作之前，建议先将这 50 000 个无标签文档从训练集中剔除。</p><pre><code class="lang-bash">!mkdir aclImdb/train_unlabel!mv aclImdb/train/unsupBow.feat aclImdb/train_unlabel!mv aclImdb/train/urls_unsup.txt aclImdb/train_unlabel!mv aclImdb/train/unsup aclImdb/train_unlabel</code></pre><pre><code class="lang-python">reviews_train = load_files(&quot;aclImdb/train/&quot;)text_train, y_train = reviews_train.data, reviews_train.targetreviews_test = load_files(&quot;aclImdb/test/&quot;)text_test, y_test = reviews_test.data, reviews_test.target# 删掉HTML换行符text_train = [doc.replace(b&quot;&lt;br /&gt;&quot;, b&quot; &quot;) for doc in text_train]text_test = [doc.replace(b&quot;&lt;br /&gt;&quot;, b&quot; &quot;) for doc in text_test]print(&quot;type of text_train: &#123;&#125;&quot;.format(type(text_train))) # 查看训练集类型：listprint(&quot;length of text_train: &#123;&#125;&quot;.format(len(text_train))) # 查看训练集大小print(&quot;text_train[1]:\n&#123;&#125;&quot;.format(text_train[1])) # 查看第二段文本print(&quot;Samples per class (training): &#123;&#125;&quot;.format(np.bincount(y_train))) # 查看数据集是否均等#------------------------------------------------#type of text_train: &lt;class &#39;list&#39;&gt;length of text_train: 25000text_train[1]:b&#39;Words can\&#39;t describe how bad this movie is. I can\&#39;t explain it by writing only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do that. There are so many clich\xc3\xa9s, mistakes (and all other negative things you can imagine) here that will just make you cry. To start with the technical first, there are a LOT of mistakes regarding the airplane. I won\&#39;t list them here, but just mention the coloring of the plane. They didn\&#39;t even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original Boeing livery. Very bad. The plot is stupid and has been done many times before, only much, much better. There are so many ridiculous moments here that i lost count of it really early. Also, I was on the bad guys\&#39; side all the time in the movie, because the good guys were so stupid. &quot;Executive Decision&quot; should without a doubt be you\&#39;re choice over this one, even the &quot;Turbulence&quot;-movies are better. In fact, every other movie in the world is better than this one.&#39;Samples per class (training): [12500 12500]</code></pre><p>采用词袋模型整理数据</p><pre><code class="lang-python">vect = CountVectorizer().fit(text_train)X_train = vect.transform(text_train)print(&quot;X_train:\n&#123;&#125;&quot;.format(repr(X_train)))#---------------------------#X_train:&lt;25000x74849 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;with 3431196 stored elements in Compressed Sparse Row format&gt;</code></pre><p>X_train 是训练数据的词袋表示，其形状为 25 000×74 849，这表示词表中包含 74 849 个元素。数据被保存为 SciPy 稀疏矩阵。</p><p>访问词表的另一种方法是使用向量器（vectorizer）的 get_feature_name 方法，它将返回一个列表，每个元素对应于一个特征：</p><p>feature_names = vect.get_feature_names()<br>print(“Number of features: {}”.format(len(feature_names)))<br>print(“First 20 features:\n{}”.format(feature_names[:20]))<br>print(“Features 20010 to 20030:\n{}”.format(feature_names[20010:20030]))<br>print(“Every 2000th feature:\n{}”.format(feature_names[::2000]))</p><p>Number of features: 74849<br>First 20 features:<br>[‘00’, ‘000’, ‘0000000000001’, ‘00001’, ‘00015’, ‘000s’, ‘001’, ‘003830’,<br>‘006’, ‘007’, ‘0079’, ‘0080’, ‘0083’, ‘0093638’, ‘00am’, ‘00pm’, ‘00s’,’01’, ‘01pm’, ‘02’]<br>Features 20010 to 20030:<br>[‘dratted’, ‘draub’, ‘draught’, ‘draughts’, ‘draughtswoman’, ‘draw’, ‘drawback’,<br>‘drawbacks’, ‘drawer’, ‘drawers’, ‘drawing’, ‘drawings’, ‘drawl’,<br>‘drawled’, ‘drawling’, ‘drawn’, ‘draws’, ‘draza’, ‘dre’, ‘drea’]<br>Every 2000th feature:<br>[‘00’, ‘aesir’, ‘aquarian’, ‘barking’, ‘blustering’, ‘beête’, ‘chicanery’,<br>‘condensing’, ‘cunning’, ‘detox’, ‘draper’, ‘enshrined’, ‘favorit’, ‘freezer’,<br>‘goldman’, ‘hasan’, ‘huitieme’, ‘intelligible’, ‘kantrowitz’, ‘lawful’,<br>‘maars’, ‘megalunged’, ‘mostey’, ‘norrland’, ‘padilla’, ‘pincher’,<br>‘promisingly’, ‘receptionist’, ‘rivals’, ‘schnaas’, ‘shunning’, ‘sparse’,<br>‘subset’, ‘temptations’, ‘treatises’, ‘unproven’, ‘walkman’, ‘xylophonist’]</p><p>词表的前 10 个元素都是数字。所有这些数字都出现在评论中的某处，因此被提取为单词。</p><h3 id="2-使用keras自带的IMDb数据集"><a href="#2-使用keras自带的IMDb数据集" class="headerlink" title="2. 使用keras自带的IMDb数据集"></a>2. 使用keras自带的IMDb数据集</h3><pre><code class="lang-python">from tensorflow.keras.datasets import imdb(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000) # 仅保留训练数据中前10000个最经常出现的单词，低频单词被舍弃print(&#39;len of X_train: &#123;&#125;&#39;.format(len(X_train)))print(&#39;shape of X_train: &#123;&#125;&#39;.format(X_train.shape))print(&#39;first of X_train: &#123;&#125;&#39;.format(X_train[0]))print(&#39;training sample per class: &#123;&#125;&#39;.format(np.bincount(y_train)))#-------------------#len of X_train: 25000shape of X_train: (25000,)first of X_train: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]training sample per class: [12500 12500]</code></pre><p>可以看到keras已经把IMDb数据集给提前整理过了。此处每条数据都是一个向量，每个数值代表一个单词。数值的大小代表了该单词在单词表中的位置。显然，每条数据向量的长度不一定相同。</p><p>为了方便处理，我们可以规定每条文档的长度为maxlen</p><pre><code class="lang-python">from tensorflow.keras.preprocessing import sequenceprint(&#39;Pad sequences (samples x time)&#39;)x_train = sequence.pad_sequences(x_train, maxlen=maxlen)x_test = sequence.pad_sequences(x_test, maxlen=maxlen)print(&#39;x_train shape:&#39;, x_train.shape)print(&#39;x_test shape:&#39;, x_test.shape)#-------------------#Pad sequences (samples x time)x_train shape: (25000, 80)x_test shape: (25000, 80)</code></pre><p>训练集中一共25000条文档，其中12500个正类，12500个负类。每个文档都是由80个数字组成的向量。测试集亦然。</p>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>preprocessing</tag>
      
      <tag>IMDb</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【经验分享】停电了怎么办？Python获取Windows电源连接信息</title>
    <link href="/2020/06/08/%E3%80%90%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E3%80%91%E5%81%9C%E7%94%B5%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9FPython%E8%8E%B7%E5%8F%96Windows%E7%94%B5%E6%BA%90%E8%BF%9E%E6%8E%A5%E4%BF%A1%E6%81%AF/"/>
    <url>/2020/06/08/%E3%80%90%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E3%80%91%E5%81%9C%E7%94%B5%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9FPython%E8%8E%B7%E5%8F%96Windows%E7%94%B5%E6%BA%90%E8%BF%9E%E6%8E%A5%E4%BF%A1%E6%81%AF/</url>
    
    <content type="html"><![CDATA[<p>一旦停电，就令笔记本电脑发出响声、发送信息……。看似简单的功能，该如何利用Python实现呢？<br><!--more---></p><p>采用笔记本电脑办公的好处是不必害怕突然停电。然而笔记本电脑不可能使用电池工作太久，此时必须尽快通知管理人员，恢复供电。</p><p>看似简单的功能，只需在Windows中注册一个HANDLE，负责接收电源适配器更改这一事件即可。但是本人没有Windows编程和系统编程的经验，只对Python熟悉。如何实现这一功能？</p><p>废话不多说，下面是代码。</p><pre><code class="lang-python">import win32conimport win32apiimport win32guiimport timefrom ctypes import POINTER, windll, Structure, cast, CFUNCTYPE, c_int, c_uint, c_void_p, c_boolfrom comtypes import GUIDfrom ctypes.wintypes import HANDLE, DWORDPBT_POWERSETTINGCHANGE = 0x8013GUID_CONSOLE_DISPLAY_STATE = &#39;&#123;6FE69556-704A-47A0-8F24-C28D936FDA47&#125;&#39;GUID_ACDC_POWER_SOURCE = &#39;&#123;5D3E9A59-E9D5-4B00-A6BD-FF34FF516548&#125;&#39;GUID_BATTERY_PERCENTAGE_REMAINING = &#39;&#123;A7AD8041-B45A-4CAE-87A3-EECBB468A9E1&#125;&#39;GUID_MONITOR_POWER_ON = &#39;&#123;02731015-4510-4526-99E6-E5A17EBD1AEA&#125;&#39;GUID_SYSTEM_AWAYMODE = &#39;&#123;98A7F580-01F7-48AA-9C0F-44352C29E5C0&#125;&#39;class POWERBROADCAST_SETTING(Structure):    _fields_ = [(&quot;PowerSetting&quot;, GUID),                (&quot;DataLength&quot;, DWORD),                (&quot;Data&quot;, DWORD)]def wndproc(hwnd, msg, wparam, lparam):    if msg == win32con.WM_POWERBROADCAST:        if wparam == win32con.PBT_APMPOWERSTATUSCHANGE:            print(&#39;Power status has changed&#39;)        if wparam == win32con.PBT_APMRESUMEAUTOMATIC:            print(&#39;System resume&#39;)        if wparam == win32con.PBT_APMRESUMESUSPEND:            print(&#39;System resume by user input&#39;)        if wparam == win32con.PBT_APMSUSPEND:            print(&#39;System suspend&#39;)        if wparam == PBT_POWERSETTINGCHANGE:            print(&#39;Power setting changed...&#39;)            settings = cast(lparam, POINTER(POWERBROADCAST_SETTING)).contents            power_setting = str(settings.PowerSetting)            data_length = settings.DataLength            data = settings.Data            if power_setting == GUID_CONSOLE_DISPLAY_STATE:                if data == 0: print(&#39;Display off&#39;)                if data == 1: print(&#39;Display on&#39;)                if data == 2: print(&#39;Display dimmed&#39;)            elif power_setting == GUID_ACDC_POWER_SOURCE:                if data == 0: print(&#39;AC power&#39;)                if data == 1:                    print(&#39;Battery power&#39;)                    #################################################                    playsound(&#39;alert.mp3&#39;) # 此处自定义你的操作                    #################################################                if data == 2: print(&#39;Short term power&#39;)            elif power_setting == GUID_BATTERY_PERCENTAGE_REMAINING:                print(&#39;battery remaining: %s&#39; % data)            elif power_setting == GUID_MONITOR_POWER_ON:                if data == 0: print(&#39;Monitor off&#39;)                if data == 1: print(&#39;Monitor on&#39;)            elif power_setting == GUID_SYSTEM_AWAYMODE:                if data == 0: print(&#39;Exiting away mode&#39;)                if data == 1: print(&#39;Entering away mode&#39;)            else:                print(&#39;unknown GUID&#39;)        return True    return Falseif __name__ == &quot;__main__&quot;:    print(&quot;*** STARTING ***&quot;)    hinst = win32api.GetModuleHandle(None)    wndclass = win32gui.WNDCLASS()    wndclass.hInstance = hinst    wndclass.lpszClassName = &quot;testWindowClass&quot;    CMPFUNC = CFUNCTYPE(c_bool, c_int, c_uint, c_uint, c_void_p)    wndproc_pointer = CMPFUNC(wndproc)    wndclass.lpfnWndProc = &#123;win32con.WM_POWERBROADCAST : wndproc_pointer&#125;    try:        myWindowClass = win32gui.RegisterClass(wndclass)        hwnd = win32gui.CreateWindowEx(win32con.WS_EX_LEFT,                                     myWindowClass,                                     &quot;testMsgWindow&quot;,                                     0,                                     0,                                     0,                                     win32con.CW_USEDEFAULT,                                     win32con.CW_USEDEFAULT,                                     0,                                     0,                                     hinst,                                     None)    except Exception as e:        print(&quot;Exception: %s&quot; % str(e))    if hwnd is None:        print(&quot;hwnd is none!&quot;)    else:        print(&quot;hwnd: %s&quot; % hwnd)    guids_info = &#123;                    &#39;GUID_MONITOR_POWER_ON&#39; : GUID_MONITOR_POWER_ON,                    &#39;GUID_SYSTEM_AWAYMODE&#39; : GUID_SYSTEM_AWAYMODE,                    &#39;GUID_CONSOLE_DISPLAY_STATE&#39; : GUID_CONSOLE_DISPLAY_STATE,                    &#39;GUID_ACDC_POWER_SOURCE&#39; : GUID_ACDC_POWER_SOURCE,                    &#39;GUID_BATTERY_PERCENTAGE_REMAINING&#39; : GUID_BATTERY_PERCENTAGE_REMAINING                 &#125;    for name, guid_info in guids_info.items():        result = windll.user32.RegisterPowerSettingNotification(HANDLE(hwnd), GUID(guid_info), DWORD(0))        print(&#39;registering&#39;, name)        print(&#39;result:&#39;, hex(result))        print(&#39;lastError:&#39;, win32api.GetLastError())        print()    print(&#39;\nEntering loop&#39;)    while True:        win32gui.PumpWaitingMessages()        time.sleep(1)</code></pre><p>COM: The Component Object Model 组件对象模型，是微软的一套软件组件的二进制接口标准。COM使得跨编程语言的进程间通信、动态对象创建成为可能。</p><p>COM实质上是一种语言无关的对象实现方式，这使其可以在创建环境不同的场合、甚至跨计算机的分布环境下被复用。COM允许复用这些对象，而不必知道对象内部是如何实现，因为组件实现者必须提供良好定义的接口从而屏蔽实现细节。通过引用计数，组件对象自己负责动态创建与销毁，从而屏蔽了不同编程语言之间的内存分配语义差异。</p><p>对于某些应用程序来说，COM已经部分被.NET框架取代。.NET Framework是新一代的Microsoft Windows应用程序开发平台。</p><p>COM是基于组件对象方式概念来设计的，在基础中，至少要让每个组件都可以支持二个功能：</p><p>查询组件中有哪些接口<br>让组件做自我生命管理，此概念的实践即为引用计数（Reference Counting）</p><p>GUID 是一个 128 位整数（16 字节），COM将其用于计算机和网络的唯一标识符。全局唯一标识符（英语：Globally Unique Identifier，缩写：GUID；发音为/ˈɡuːɪd/或/ˈɡwɪd/）是一种由算法生成的唯一标识，通常表示成32个16进制数字（0－9，A－F）组成的字符串，如：{21EC2020-3AEA-1069-A2DD-08002B30309D}，它实质上是一个128位长的二进制整数。</p><p>Windows操作系统使用GUID来标识COM对象中的类和界面。一个脚本可以不需知道DLL的位置和名字直接通过GUID来激活其中的类或对象。</p><p>参考：<a href="https://stackoverflow.com/questions/48720924/python-3-detect-monitor-power-state-in-windows">https://stackoverflow.com/questions/48720924/python-3-detect-monitor-power-state-in-windows</a></p>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>Windows</tag>
      
      <tag>PowerOff</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【经验分享】TensorFlow模型训练和保存</title>
    <link href="/2020/06/03/%E3%80%90%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E3%80%91TensorFlow%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%92%8C%E4%BF%9D%E5%AD%98/"/>
    <url>/2020/06/03/%E3%80%90%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E3%80%91TensorFlow%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%92%8C%E4%BF%9D%E5%AD%98/</url>
    
    <content type="html"><![CDATA[<p>使用LSTM训练最简单的IMDB影评分类任务，总结文本分类任务常见流程。<br><!--more---></p><h2 id="1-模型训练和保存"><a href="#1-模型训练和保存" class="headerlink" title="1. 模型训练和保存"></a>1. 模型训练和保存</h2><h3 id="1-1-训练结束时保存"><a href="#1-1-训练结束时保存" class="headerlink" title="1.1 训练结束时保存"></a>1.1 训练结束时保存</h3><p>训练模型，使用fit函数。fit函数的参数如下。</p><pre><code class="lang-python">fit(    x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None,    validation_split=0.0, validation_data=None, shuffle=True, class_weight=None,    sample_weight=None, initial_epoch=0, steps_per_epoch=None,    validation_steps=None, validation_batch_size=None, validation_freq=1,    max_queue_size=10, workers=1, use_multiprocessing=False)</code></pre><p>x：训练数据<br>y：训练标签<br>batch_size：批次大小，默认为32<br>validation_data：在每个epoch结束之时计算loss等其他模型性能指标，不用做训练。<br>epoch：训练轮次<br>verbose：输出的详细程度，为1则输出进度条，表明每个epoch训练完成度；为0则什么也不输出，为2则很啰嗦地输出所有信息</p><p>最后保存模型用<code>model.save(&#39;xxx.h5&#39;)</code>，这里模型格式为HDF5，因此结尾为h5。</p><pre><code class="lang-python">model.fit(X_train, y_train, validation_data=(X_test, y_test), epoch=10, batch_size=64) scores = model.evaluate(X_test, y_test, verbose=0)print(&quot;Accuracy: %.2f%%&quot; % (scores[1]*100))model.save(&#39;models/sentiment-lstm.h5&#39;)</code></pre><h3 id="1-2-在训练期间保存模型（以-checkpoints-形式保存）"><a href="#1-2-在训练期间保存模型（以-checkpoints-形式保存）" class="headerlink" title="1.2 在训练期间保存模型（以 checkpoints 形式保存）"></a>1.2 在训练期间保存模型（以 checkpoints 形式保存）</h3><p>您可以使用训练好的模型而无需从头开始重新训练，或在您打断的地方开始训练，以防止训练过程没有保存。<code>tf.keras.callbacks.ModelCheckpoint</code> 允许在训练的过程中和结束时回调保存的模型。</p><pre><code class="lang-python">checkpoint_path = &quot;training_1/cp.ckpt&quot;checkpoint_dir = os.path.dirname(checkpoint_path)# 创建一个保存模型权重的回调函数cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,                                                 save_weights_only=True,                                                 verbose=1)# 使用新的回调函数训练模型model.fit(train_images,           train_labels,            epochs=10,          validation_data=(test_images,test_labels),          callbacks=[cp_callback])  # 通过回调训练# 这可能会生成与保存优化程序状态相关的警告。# 这些警告（以及整个笔记本中的类似警告）是防止过时使用，可以忽略。</code></pre><p>这将创建一个 TensorFlow checkpoint 文件集合，这些文件在每个 epoch 结束时更新</p><pre><code>cp.ckpt.data-00001-of-00002cp.ckpt.data-00000-of-00002  cp.ckpt.index</code></pre><p>默认的 tensorflow 格式仅保存最近的5个 checkpoint 。</p><h3 id="1-3-手动保存权重"><a href="#1-3-手动保存权重" class="headerlink" title="1.3 手动保存权重"></a>1.3 手动保存权重</h3><p>不必等待epoch结束，通过执行<code>save_weights</code>就可以生成ckpt文件。</p><pre><code class="lang-python"># 保存权重model.save_weights(&#39;./checkpoints/my_checkpoint&#39;)</code></pre><h2 id="2-模型加载"><a href="#2-模型加载" class="headerlink" title="2. 模型加载"></a>2. 模型加载</h2><h3 id="2-1-从h5文件中恢复"><a href="#2-1-从h5文件中恢复" class="headerlink" title="2.1 从h5文件中恢复"></a>2.1 从h5文件中恢复</h3><pre><code class="lang-python"># 重新创建完全相同的模型model=load_model(&#39;models/sentiment-lstm.h5&#39;)# 加载后重新编译模型，否则您将失去优化器的状态model.compile(loss=&#39;binary_crossentropy&#39;,optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) model.summary()</code></pre><p>加载模型的时候，损失函数等参数需要重新设置。</p><h3 id="2-2-从ckpt文件中断点续训"><a href="#2-2-从ckpt文件中断点续训" class="headerlink" title="2.2 从ckpt文件中断点续训"></a>2.2 从ckpt文件中断点续训</h3><p>仅恢复模型的权重时，必须具有与原始模型具有相同网络结构的模型。</p><pre><code class="lang-python"># 这个模型与ckpt保存的一样架构，只不过没经过fit训练model = create_model()# 加载权重model.load_weights(checkpoint_path)</code></pre><p>我们可以对回调函数增加一些新的设置，之前的回调函数每个epoch都覆盖掉之前的ckpt，现在我们想每过5个epoch保存一个新的断点：</p><pre><code class="lang-python"># 在文件名中包含 epoch (使用 `str.format`)checkpoint_path = &quot;training_2/cp-&#123;epoch:04d&#125;.ckpt&quot;checkpoint_dir = os.path.dirname(checkpoint_path)# 创建一个回调，每 5 个 epochs 保存模型的权重cp_callback = tf.keras.callbacks.ModelCheckpoint(    filepath=checkpoint_path,     verbose=1,     save_weights_only=True,    period=5)</code></pre><p>利用新的回调训练，并随后选择最新的断点文件：</p><pre><code class="lang-python"># 使用新的回调训练模型model.fit(train_images,               train_labels,              epochs=50,               callbacks=[cp_callback],              validation_data=(test_images,test_labels),              verbose=0)# 选择新的断点latest = tf.train.latest_checkpoint(checkpoint_dir)&gt;&gt;&gt; &#39;training_2/cp-0050.ckpt&#39;# 加载以前保存的权重model.load_weights(latest)</code></pre>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>TensorFlow</tag>
      
      <tag>SaveModel</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】使用LSTM训练imdb情感分类模型</title>
    <link href="/2020/06/03/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8LSTM%E8%AE%AD%E7%BB%83imdb%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/"/>
    <url>/2020/06/03/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8LSTM%E8%AE%AD%E7%BB%83imdb%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p>使用LSTM训练最简单的IMDB影评分类任务，总结文本分类任务常见流程。<br><!--more---></p><h2 id="1-查看数据集"><a href="#1-查看数据集" class="headerlink" title="1. 查看数据集"></a>1. 查看数据集</h2><h3 id="1-1-官网上的数据集压缩包"><a href="#1-1-官网上的数据集压缩包" class="headerlink" title="1.1 官网上的数据集压缩包"></a>1.1 官网上的数据集压缩包</h3><p>从IMDB官网上下载的数据集，是一个压缩包<code>aclImdb_v1.tar.gz</code>。解压后的目录如下：<br><img src="/2020/06/03/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E4%BD%BF%E7%94%A8LSTM%E8%AE%AD%E7%BB%83imdb%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/2020-06-03-18-12-58.png" alt></p><ul><li><code>test</code></li><li><code>train</code></li><li><code>imdb.vocab</code></li><li><code>imdbEr.txt</code></li><li><code>README</code></li></ul><p>其内部不仅有完整的影评文件，还包含该影评的链接等信息。</p><h3 id="1-2-keras自带的数据集"><a href="#1-2-keras自带的数据集" class="headerlink" title="1.2 keras自带的数据集"></a>1.2 keras自带的数据集</h3><p>keras里的IMDB影评数据集，内部结构分为两个部分：影评部分和情感标签部分，也就是数据集的X和y部分。</p><p>X部分的每条影评都被编码为一个整数列表。另外，每个单词的在单词表中的编码越小，代表在影评中出现频率越高。这使得我们能在取数据时指定只使用某一出现频率内范围的单词（其他单词由于出现频率太低，可以直接标记为未知）。</p><p>“0”在数据集中代表“未知”单词。</p><p>我们采用内置的<code>load_data</code>函数来取出数据。</p><pre><code class="lang-python">tf.keras.datasets.imdb.load_data(    path=&#39;imdb.npz&#39;, num_words=None, skip_top=0, maxlen=None, seed=113,    start_char=1, oov_char=2, index_from=3, **kwargs)</code></pre><p>num_words: 即设定取出现频率在前num_words的单词。如果不填，所有单词表中的单词都会标记。<br>skip_top: 设定前skip_top频率出现的单词不予标记。这可能是由于高频出现的单词信息量太低（如the、a等）。<br>maxlen: 设定最大影评长度，超过该长度的影评都会被截断。<br>x_train, x_test: 返回影评列表，长度为影评个数（25000个训练，25000个测试），每个影评是整数数组。<br>y_train, y_test: 返回整数数组，长度为影评个数，代表影评的情感倾向（0或1）。</p><pre><code class="lang-python">from tensorflow.keras.datasets import imdbmax_features = 50000 # 取前50000个最常见的单词，组建词典(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)</code></pre><p>需要注意的是这个<code>max_features</code>与数据集的数目没有关系，不要搞混了。</p><h2 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h2><pre><code class="lang-python">from tensorflow.keras.preprocessing import sequenceprint(&#39;Pad sequences (samples x time)&#39;)x_train = sequence.pad_sequences(x_train, maxlen=maxlen)x_test = sequence.pad_sequences(x_test, maxlen=maxlen)X_trainarray([[    0,     0,     0, ...,    19,   178,    32],       [    0,     0,     0, ...,    16,   145,    95],       [    0,     0,     0, ...,     7,   129,   113],       ...,       [    0,     0,     0, ...,     4,  3586, 22459],       [    0,     0,     0, ...,    12,     9,    23],       [    0,     0,     0, ...,   204,   131,     9]], dtype=int32)</code></pre><p>经过这个函数处理，每条影评被规整成了长度为500的整形元素列表，长度不够500个单词的影评，在最前面加0；长度不够的则在最后截断。</p><h2 id="3-模型构建"><a href="#3-模型构建" class="headerlink" title="3. 模型构建"></a>3. 模型构建</h2><p><strong>Embedding层</strong></p><p>在最开始我们加入了Embedding层，max_features是字典长度，也可以说是one-hot向量长度。<br>input_length=500为每个序列为500个单词构成。<br>input_shape=(max_features,)表明one-hot的维度，这两个都可以不填，直接通过fit的时候推断出来</p><p><strong>LSTM层</strong></p><p>LSTM层的参数是output_dim，这个参数可以自定义，因为它不受之前影响，只表明输出的维度。</p><p>同时也是是门结构（forget门、update门、output门）的维度。之所以理解成维度，是因为LSTM中隐藏单元个数这个概念不好理解。其实该参数名称为<code>units</code>，官方说法就是“隐藏单元个数”。</p><p>LSTM层的输入是形如（samples，timesteps，input_dim）的3D张量；输出是形如（samples，timesteps，output_dim）的3D张量，或者返回形如（samples，output_dim）的2D张量。二者区别在于，若LSTM层中参数<code>return_sequences=True</code>，就返回带时间步的张量。</p><p>若我们有很多LSTM层，我们可以把很多LSTM层串在一起，为了方便LSTM层与层之间的信息传递，可以设置<code>return_sequences=True</code>。但是最后一个LSTM层return_sequences通常为false，此时输出的就是每个样本的结果张量。</p><p>假如我们输入有25000个句子，每个句子都由500个单词组成，而每个单词用64维的词向量表示。那么样本数目samples=25000，时间步timesteps=500（可以简单地理解timesteps就是输入序列的长度input_length），前一层Embedding词向量输出维度input_dim=128。</p><p>也就是说通过LSTM，把词的维度由128转变成了100。</p><p>在LSTM层中还可以设置Dropout，这一点在之后会详细说明。</p><p><strong>全连接层</strong></p><p>汇总至一个神经元的全连接层，即sigmoid层，判断0或1即可。</p><pre><code class="lang-python">from tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Dense, Embedding, LSTMmodel = Sequential() model.add(Embedding(max_features, 128, input_length=500, input_shape=(max_features,))) model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))model.add(Dense(1, activation=&#39;sigmoid&#39;)) model.compile(loss=&#39;binary_crossentropy&#39;,optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) print(model.summary())</code></pre><h2 id="4-模型训练和保存"><a href="#4-模型训练和保存" class="headerlink" title="4. 模型训练和保存"></a>4. 模型训练和保存</h2><pre><code class="lang-python">model.fit(X_train, y_train, validation_data=(X_test, y_test), epoch=10, batch_size=64) scores = model.evaluate(X_test, y_test, verbose=0)print(&quot;Accuracy: %.2f%%&quot; % (scores[1]*100))model.save(&#39;models/sentiment-lstm.h5&#39;)</code></pre>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>LSTM</tag>
      
      <tag>IMDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读笔记】Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey</title>
    <link href="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Adversarial-Attacks-on-Deep-Learning-Models-in-Natural-Language-Processing-A-Survey/"/>
    <url>/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Adversarial-Attacks-on-Deep-Learning-Models-in-Natural-Language-Processing-A-Survey/</url>
    
    <content type="html"><![CDATA[<p>本文总结了常见的nlp领域的对抗样本生成及攻击方法。<br><!--more---></p><p>翻译腔预警！长文预警！</p><h1 id="一、Introduction"><a href="#一、Introduction" class="headerlink" title="一、Introduction"></a>一、Introduction</h1><p>为代表的神经网络在最近几年取得了较大的成功。然而先前的研究表明，DNN等深度学习模型容易受到精心构建的对抗样本的攻击，这些攻击算法在原有样本基础上产生一些不可察觉的扰动，欺骗DNN给出错误的预测。</p><p>此类对抗样本攻击手段广泛应用于图像识别，图像分类任务等，但是此类攻击手段在自然语言处理nlp任务中表现不佳，究其原因是图像与文本之间的内在差异，使得图像识别任务中的攻击方法无法直接使用于nlp领域。</p><p>目前针对深度学习系统的对抗样本研究有以下三个角度：第一，通过给数据集添加无法被人类察觉的扰动来欺骗模型；第二，故意更改神经网络内部的输出值和权重；第三，检测深度神经网络的弱点，找到防御攻击的解决方案。</p><h2 id="为什么CV领域的攻击手段不能直接迁移到NLP领域？"><a href="#为什么CV领域的攻击手段不能直接迁移到NLP领域？" class="headerlink" title="为什么CV领域的攻击手段不能直接迁移到NLP领域？"></a>为什么CV领域的攻击手段不能直接迁移到NLP领域？</h2><p>由于图像和文本数据之间的内在差异，对图像的对抗攻击方法不能直接应用于后者。</p><p><strong>首先，图像数据（例如像素值）是数值特征，但是文本数据本质上是离散的特征。</strong></p><p>如果我们将文本数据采用文本向量化方法转化成数值向量，然后应用在图像攻击中常用的基于梯度的对抗攻击时，生成的对抗样本为无效字符或无意义的单词序列[157]。</p><p>如果我们采用词嵌入算法得到具有语义的词嵌入空间，但是如果采用基于梯度的对抗攻击，也会产生无法与词嵌入空间中的任何词匹配的向量[38]。</p><p><strong>其次，对文本的扰动很容易被察觉</strong></p><p>图像的扰动是人眼难以察觉的像素值的微小变化。人类可以正确地对扰动图像进行分类而模型不可以，这表明深度神经模型的鲁棒性较差。</p><p>但是对于文本的对抗性攻击，人类很容易察觉到很小的扰动。例如，替换字符或单词会生成无效的单词或语法错误的句子。</p><p>此外，它将大大改变句子的语义。在这种情况下，即使人类也无法提供“正确”的预测。</p><h2 id="其他类似的综述"><a href="#其他类似的综述" class="headerlink" title="其他类似的综述"></a>其他类似的综述</h2><ul><li>[9]针对不同类别的<strong>机器学习系统</strong>的攻击和防御进行了全面的综述</li><li>[35]从安全角度总结了对抗性攻击的防御措施。介绍对象不仅限于机器学习算法，而且包括其他安全相关应用的对抗样本防御措施，以及如何有效地<strong>评估攻击和防御手段</strong>，建立了根据动机、限制、性能的<strong>分类方法</strong>。</li><li>[13]全面的总结了从2008年到2018年的十年中，<strong>计算机视觉</strong>和<strong>网络安全</strong>方面的对抗性攻击研究的发展，提供了有关攻击和防御<strong>效果的详细分析</strong>。</li><li>[79]从数据驱动的角度研究对抗样本攻击和防御问题，<strong>根据学习阶段（即训练阶段和测试阶段）分析</strong>了攻击和防御手段。</li><li>[154]回顾了当前<strong>针对各种深度神经网络的攻击</strong>手段和研究成果</li><li>[2]对<strong>计算机视觉</strong>任务中使用的<strong>深度学习模型</strong>的对抗性攻击和<strong>防御手段</strong>进行了全面综述</li></ul><p>本文针对文本深度学习模型的攻击和防御进行全面的综述，涵盖了来自各个方面的信息。论文来源包括ACL，COLING，NAACL，EMNLP，ICLR，AAAI，IJCAI在内的顶级会议。论文指标：论文质量，方法新颖度，引文数量。</p><p>本文贡献：</p><ol><li>全面调研了迄今为止的文本领域的深度神经网络对抗样本生成算法，并使用不同的分类方案；</li><li>讨论了一些未解决的新颖的topic</li></ol><h1 id="二、Overview"><a href="#二、Overview" class="headerlink" title="二、Overview"></a>二、Overview</h1><h2 id="深度学习模型对抗性攻击的一般分类法"><a href="#深度学习模型对抗性攻击的一般分类法" class="headerlink" title="深度学习模型对抗性攻击的一般分类法"></a>深度学习模型对抗性攻击的一般分类法</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ul><li>深度神经网络（Deep Neural Network, DNN）：略</li><li>扰动（Perturbation）：这里的扰动指自然或人为添加到测试集中的噪声，旨在使深度学习模型出错</li><li>对抗样本（Adversarial Examples）：对深度学习模型添加最坏扰动（worst-case perturbation）而生成的样本。在分类任务下，理想的DNN仍能将对抗样本分类为正确的类别，而目标DNN（victim）则以较高的置信程度误判样本的类别。</li></ul><p>假设$x$为原样本，$x’$为对抗样本，$\eta$为最坏情况的扰动，$f(x)$为深度学习模型，$y$为$x$的正确类别，$y’$为$x$的错误类别，则对抗样本形式化定义为：</p><script type="math/tex; mode=display">\begin{array}{c}\mathbf{x}^{\prime}=\mathbf{x}+\eta, f(\mathbf{x})=\mathbf{y}, \mathbf{x} \in \mathbf{X}, \\f\left(\mathbf{x}^{\prime}\right) \neq \mathbf{y} \\\text { or } f\left(\mathbf{x}^{\prime}\right)=\mathbf{y}^{\prime}, \mathbf{y}^{\prime} \neq \mathbf{y}\end{array}</script><ul><li>对抗攻击（或者说，对抗样本生成）的目标是$f(x’)\neq y$或者$f(x’)=y’$</li></ul><h3 id="分类标准-Threat-Model"><a href="#分类标准-Threat-Model" class="headerlink" title="分类标准 Threat Model"></a>分类标准 Threat Model</h3><p><span id="Threat_Model" style.display="none"></span></p><ul><li>根据模型信息利用程度，分为黑盒攻击和白盒攻击<ul><li>不必访问DNN的体系结构、参数、损失函数、激活函数和训练数据的攻击手段称为黑盒攻击，黑盒攻击通过测试集或检查DNN输出来生成对抗样本。</li><li>对应地，白盒攻击基于上述的DNN的结构细节信息。黑盒攻击需要的信息量更少，因此是一种更有吸引力的攻击手段；但是白盒攻击往往更有效。</li></ul></li><li>根据是否能够令模型误判为一个特定的错误结果，分为目标攻击和无目标攻击。显然指定目标的攻击手段更加严格。对于二分类任务，目标攻击即为非目标攻击。</li><li>根据攻击算法生成的扰动的粒度，可以分类攻击手段，比如针对图像像素的攻击，针对字符、单词、句嵌入的攻击等</li><li>根据产生对抗样本的动机，可以把对抗样本生成算法分成攻击方法和防御方法。攻击算法旨在检查DNN的鲁棒性，而防御算法则进一步利用生成的对抗样本增强DNN的鲁棒性。</li></ul><h3 id="衡量标准-Measurements"><a href="#衡量标准-Measurements" class="headerlink" title="衡量标准 Measurements"></a>衡量标准 Measurements</h3><ul><li><strong>扰动限制（Perturbation Constraint）</strong></li></ul><p>扰动$\eta$不应更改输入的真实类别标签。以分类任务为例，理想的DNN分类器将能够正确辨别对抗样本。$\eta$也不能太小，以免最终对目标DNN没有影响。</p><p>理想情况下，有效$\eta$是在受限范围内影响最大的噪声。</p><p>[132]添加图像领域的扰动限制$(\mathbf{x}+\eta) \in[0,1]^{n}$以确保对抗样本具有与原始数据相同的像素值范围</p><p>[40]用最大范数约束扰动：$|\eta|_{\infty} \leq \epsilon$。最大范数的含义是求向量中的最大分量。因此该限制保证任何特定像素的变化不会超过某个量$\epsilon$。</p><p>在计算机视觉领域，max-norm、$L_2$、$L_0$都可以用于控制扰动大小，而文本领域则有所不同，</p><ul><li><strong>攻击效果度量</strong></li></ul><p>攻击算法可以使得模型性能大幅下降，则衡量攻击算法可以直接使用模型度量指标。比如分类任务的精度、F1和AUC，文本生成任务的困惑度、BLEU评价指标等。</p><h2 id="深度学习技术在自然语言处理中的应用"><a href="#深度学习技术在自然语言处理中的应用" class="headerlink" title="深度学习技术在自然语言处理中的应用"></a>深度学习技术在自然语言处理中的应用</h2><p><span id="Models" style.display="none"> <span><br>前馈神经网络（FNN），卷积神经网络（CNN），循环神经网络（RNN）是NLP任务中最常使用的神经网络。</span></span></p><p>Seq2Seq[131]和Attention机制[8]是最新的NLP技术，强化学习（Reinforcement learning）和生成模型（generative models）也取得了不错的效果[152]。[100，152]讨论了NLP领域的神经网络应用状况。</p><h3 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h3><p>前馈神经网络（Feed Forward Networks）一般指完全由全连接层构成的网络。这种网络分成多个层，每一层有多个神经元，一层中的每个神经元都与下一层中的每个神经元相连接。这种网络无法记录元素顺序，不能处理文本序列。</p><h3 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><p>卷积神经网络（Convolutional Neural Networks）一般由卷积层、池化层和最后的全连接层构成。著名的卷积网络架构包括典型的LeNet、AlexNet、Vgg、GoogLeNet、ResNet、DenseNet等。卷积神经网络在计算机视觉领域取得了成功。</p><p>卷积层使用卷积运算来提取有意义的输入模式。另外卷积层也对输入顺序敏感，因此可以用来处理文本数据。[59]采用CNN对句子分类，[156]提出将CNN用于字符级别的文本分类方法。[12、29、30、34、76]则对上述CNN应用做了对抗样本评估。</p><h3 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><p>循环神经网络（Recurrent Neural Networks）通过在计算图中引入循环，能够编码时间顺序，在处理顺序数据方面具有令人印象深刻的性能。</p><p>递归神经网络（Recursive Neural Networks）将循环神经网络拓扑从顺序结构扩展到树结构；双向RNN则对输入采用从前向后和从后向前两个方面进行建模；长短期记忆（Long Short-Term Memory, LSTM）网络通过控制信息的三个门（即输入门，忘记门和输出门）的组合，实现长期记忆；GRU是LSTM的简化版本，它仅包含两个门，因此在计算成本方面更加高效。</p><p>各种LSTM变体：[21、50、112、133、141、146]。<br>对这些RNN模型的对抗攻击：[34、53、54、91、103、112、118、130、157]。</p><h3 id="序列到序列学习"><a href="#序列到序列学习" class="headerlink" title="序列到序列学习"></a>序列到序列学习</h3><p>序列到序列学习（Seq2Seq）[131]是深度学习的重要突破之一。Seq2Seq模型由两个RNN组成：一个处理输入并将其压缩为矢量表示的编码器（Encoder），以及一个预测输出的解码器（Decoder）。</p><p>隐变量分层循环编码器/解码器（VHRED）模型[122]是最近流行的Seq2Seq模型，该模型利用子序列之间的复杂依赖性生成序列。</p><p>[24]是采用Seq2Seq模型的第一个神经机器翻译（NMT）模型之一。OpenNMT [63]是最近提出的Seq2Seq NMT模型，已成为NMT的基准工作之一。</p><p>随着它们的广泛应用，攻击工作也应运而生[22，30，98，127]。</p><h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><p>注意力机制（Attention）[8]是深度学习的另一个突破。最初开发它是为了克服编码Seq2Seq模型中所需的长序列的困难[27]。注意力机制使解码器可以回顾源序列的隐藏状态，将这些隐藏状态按照注意力加权平均，作为解码器的附加输入。</p><p>NLP中的自注意力[136]用来查看序列中的周围单词，以获得更多上下文相关的单词表示形式[152]，而不是查看原始注意力模型中的输入序列。</p><p>BiDAF [121]是一种用于机器理解的双向注意力流机制，并在提出时取得了出色的性能。</p><p>[54，127]通过对抗样本评估了该模型的鲁棒性，是率先使用对抗样本攻击文本DNN的几部作品。其他基于注意力的DNN [25，107]最近也受到了对抗攻击[29，91]。</p><h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p>强化学习（Reinforcement Learning）通过在智能体执行离散操作后给予不同的奖励来训练智能体。在NLP中，强化学习框架通常由一个智能体（Agent，以DNN实现），一个策略（Policy，指导行动）和一个奖励（Reward）组成。智能体根据策略选择一个动作（例如，预测序列中的下一个单词），然后相应地更新其内部状态，直到到达计算奖励的序列末尾为止。</p><p>到目前为止，在NLP中攻击强化学习模型的工作有限，比如[98]。</p><h3 id="深度生成模型"><a href="#深度生成模型" class="headerlink" title="深度生成模型"></a>深度生成模型</h3><p>深度生成模型（Deep Generative Models）能够生成与隐空间（Latent Space）中的真实数据（Ground Truth）非常相似的真实数据实例，比如用于生成文本。但是深度生成模型不容易训练和评估，这阻碍了它们在许多实际应用中的广泛使用[152]。</p><p>近年来，提出了两个强大的深度生成模型，即生成对抗网络（GAN）[39]和变分自动编码器（VAE）[62]。</p><p>GAN [39]由两个对抗网络组成：生成器和鉴别器。鉴别器将鉴别真实样本和生成的样本，而生成器将生成旨在欺骗鉴别器的真实样本。GAN使用最小-最大损失函数来同时训练两个神经网络。</p><p>VAE由编码器（Encoder）和生成器（Generator）网络组成。编码器将输入编码到隐空间中，生成器从隐空间中生成样本。</p><p>尽管它们已被用于生成文本，但到目前为止，尚无任何作品使用对抗样本来检验其健壮性。</p><h1 id="三、FROM-IMAGE-TO-TEXT"><a href="#三、FROM-IMAGE-TO-TEXT" class="headerlink" title="三、FROM IMAGE TO TEXT"></a>三、FROM IMAGE TO TEXT</h1><h2 id="计算机视觉领域的对抗攻击技术"><a href="#计算机视觉领域的对抗攻击技术" class="headerlink" title="计算机视觉领域的对抗攻击技术"></a>计算机视觉领域的对抗攻击技术</h2><p>有关计算机视觉中的攻击工作的全面概述，请参阅参考文献[2]。另外，[17、40、95、104、105、132、157]是计算机视觉中颇有代表性的对抗攻击算法。</p><h3 id="L-BFGS"><a href="#L-BFGS" class="headerlink" title="L-BFGS"></a>L-BFGS</h3><p>Szegedy等人率先提出了对抗样本的概念[132]。他将生成对抗样本的过程建模成最优化问题：</p><script type="math/tex; mode=display">\eta=\arg \min _{\eta} \lambda\|\eta\|_{2}^{2}+J\left(\mathbf{x}+\eta, y^{\prime}\right) \quad \text { s.t. } \quad(\mathbf{x}+\eta) \in[0,1]^{n}</script><p>其中$\eta$是扰动，$\lambda$是超参数，$\mathbf{x}$是输入样例，$y$和$y’$分别是正确标签和错误标签，$J(x,y)$是DNN的损失函数。他们采用Box-constrained Limited memory Broyden-Fletcher-Goldfarb-Shanno算法优化求解，因此得名L-BFGS算法。整个优化过程可能迭代多次。</p><h3 id="Fast-Gradient-Sign-Method-FGSM"><a href="#Fast-Gradient-Sign-Method-FGSM" class="headerlink" title="Fast Gradient Sign Method (FGSM)"></a>Fast Gradient Sign Method (FGSM)</h3><p><span style.display="none" id="FGSM"></span><br>L-BFGS计算扰动的方法的计算代价过高，因此Goodfellow[40]提出了一个简化版本。与L-BFGS算法的先固定$y’$、确定最有效的$\eta$的方法不同，FGSM算法先固定$|\eta|_{\infty}$，然后最小化损失函数$J$。然后用一阶泰勒级数逼近，并得到$\eta$的闭式解[143]：</p><script type="math/tex; mode=display">\begin{array}{c}\eta=\arg \min _{\eta} J(\mathbf{x}+\eta, y) \text { s.t. }\|\eta\|_{\infty} \leq \epsilon \\\eta=\arg \min _{\eta} J(\mathbf{x}, y)+\eta^{\mathrm{T}} \nabla_{\mathbf{x}} J(\mathbf{x}, y) \text { s.t. }\|\eta\|_{\infty} \leq \epsilon \\\eta=\epsilon \cdot \operatorname{sign}\left(\nabla_{\mathbf{x}} J(\mathbf{x}, \mathbf{y})\right)\end{array}</script><p>其中$\epsilon$是攻击者设置的参数，用于控制扰动的大小。$sign(x)$是一个符号函数，当$x&gt; 0$时返回1，当$x &lt;0$时返回-1，否则返回0。$\nabla_{x}J(x,y)$表示损失函数相对于输入的梯度，可以通过反向传播计算出来。</p><h3 id="Jacobian-Saliency-Map-Adversary-JSMA"><a href="#Jacobian-Saliency-Map-Adversary-JSMA" class="headerlink" title="Jacobian Saliency Map Adversary (JSMA)"></a>Jacobian Saliency Map Adversary (JSMA)</h3><p><span style.display="none" id="JSMA"></span><br>JSMA使用其雅可比矩阵（Jacobian Matrix）评估神经模型对每个输入特征对输出的灵敏度。雅可比矩阵形成对抗显着性图（adversarial saliency maps），给每个输入特征对目标攻击的贡献进行排名，然后在对抗显着性图中选择一个特征做扰动。</p><p>给定输入$\mathbf{x}$，对应的雅可比矩阵为：</p><script type="math/tex; mode=display">\operatorname{Jacb}_{F}[i, j]=\frac{\partial F_{i}}{\partial \mathbf{x}_{j}}</script><p>其中$\mathbf{x}_i$是输入的第i个特征（component），$F_j$是输出的第j个特征。F的分量代表着logit值，$J_F[i,j]$度量了$F_j$对$\mathbf{x}_i$的敏感程度（sensitivity）。</p><h3 id="C-amp-W-Attack"><a href="#C-amp-W-Attack" class="headerlink" title="C&amp;W Attack"></a>C&amp;W Attack</h3><p><span style.display="none" id="C_W"></span><br>Carlini and Wagner [17]旨在评估防御性蒸馏策略[49]，以缓解对抗性攻击。C&amp;W算法的优化目标：</p><script type="math/tex; mode=display">\eta=\arg \min _{\eta}\|\eta\|_{p}+\lambda J\left(\mathbf{x}+\eta, y^{\prime}\right) \quad \text { s.t. } \quad(\mathbf{x}+\eta) \in[0,1]^{n}</script><p>C&amp;W算法使用$L_p$范数来限制扰动，其中$p=0,2,\infty$，并提出了7种不同的损失函数$J$。</p><h3 id="DeepFool"><a href="#DeepFool" class="headerlink" title="DeepFool"></a>DeepFool</h3><p>DeepFool [95]是一个迭代的L2正则化算法。</p><p>作者首先假设神经网络是线性的，因此可以使用超平面将不同类分开。</p><p>作者基于此假设找到了最佳解决方案，并构建了对抗样本。</p><p>为了解决神经网络的非线性问题，他们重复了这一过程，直到找到一个真实的对抗示例。</p><h3 id="Substitute-Attack"><a href="#Substitute-Attack" class="headerlink" title="Substitute Attack"></a>Substitute Attack</h3><p>上述代表性作品都是白盒方法，需要对神经模型的参数和结构有全面的了解。实际上，由于对模型的访问受限，攻击者并非总是能够以白盒方式制造对抗样本。</p><p>Papernot等人解决了该限制，[104]引入了黑盒攻击策略。</p><p>他们<strong>训练了一个替代模型</strong>，以通过查询目标模型获得的标签来近似目标模型的决策边界。然后，他们对该替代品进行了白盒攻击，并据此生成了对抗样本。</p><p>具体来说，他们在生成替代DNN的对抗示例时<strong>采用了FSGM和JSMA</strong>。</p><h3 id="GAN-like-Attack"><a href="#GAN-like-Attack" class="headerlink" title="GAN-like Attack"></a>GAN-like Attack</h3><p>另一种黑盒攻击方法利用了生成对抗网络（GAN）。</p><p>Zhao等人[157]首先在训练集X上训练了生成模型WGAN，WGAN可以生成与X遵循相同分布的数据。然后分别训练了一个逆变器（inverter），以通过最大程度地减少重构误差（reconstruction error），将数据样本x映射到隐密集空间（latent dense space）中。</p><p>他们不是在扰动$x$，而是先从$x$在隐空间中对应的变量$z$的邻近样本中搜索$z^<em>$，然后将$z^</em>$映射回$x^<em>$，并检查$x^</em>$是否会改变预测。</p><p>他们介绍了两种搜索算法：迭代随机搜索和混合收缩搜索（iterative stochastic search and hybrid shrinking search）。</p><p>前者使用扩大策略逐渐扩大搜索空间，而后者则使用缩小策略，从大范围开始并递归地缩小搜索范围的上限。</p><h2 id="图像攻击算法和文本攻击算法的对比"><a href="#图像攻击算法和文本攻击算法的对比" class="headerlink" title="图像攻击算法和文本攻击算法的对比"></a>图像攻击算法和文本攻击算法的对比</h2><h3 id="离散输入与连续输入"><a href="#离散输入与连续输入" class="headerlink" title="离散输入与连续输入"></a>离散输入与连续输入</h3><p>图像输入是连续的，通常该方法使用$L_p$范数来测量原始数据点与被扰动数据点之间的距离。</p><p>但是，文本数据是离散的。针对文本扰动的变量或距离测量方法难以构建。</p><p>也可以将文本数据映射到连续数据，然后采用计算机视觉的攻击方法。</p><h3 id="可感知与不可感知"><a href="#可感知与不可感知" class="headerlink" title="可感知与不可感知"></a>可感知与不可感知</h3><p>人们通常不容易察觉到图像像素的微小变化，因此，对抗样本不会改变人类的判断力，而只会使DNN模型蒙蔽。</p><p>但是，人类很容易察觉到文本上的小变化，例如字符或单词的变化，从而导致攻击失败。</p><p>可以在输入文本DNN模型之前通过拼写检查和语法检查来识别或纠正更改。</p><h3 id="语义与无语义"><a href="#语义与无语义" class="headerlink" title="语义与无语义"></a>语义与无语义</h3><p>就图像而言，微小的变化通常不会改变图像的语义（Semantic），因为它们是微不足道且不可感知的。</p><p>但是，对文本的干扰会轻易改变单词和句子的语义，因此很容易被检测到并严重影响模型的输出。</p><p>更改输入的语义违反了对抗攻击的目的，即在欺骗目标DNN时保持正确的标签不变。</p><h2 id="文本向量化方法"><a href="#文本向量化方法" class="headerlink" title="文本向量化方法"></a>文本向量化方法</h2><p>DNN模型需要向量作为输入，对于图像任务，通常的方法是使用像素值将向量/矩阵作为DNN输入。</p><p>但是对于文本模型，需要特殊操作才能将文本转换为矢量。</p><p>方法主要有三种：基于单词计数的编码，one-hot编码和密集编码（又称为特征嵌入），后两种主要用于文本应用程序的DNN模型中。</p><h3 id="基于单词计数的编码方法"><a href="#基于单词计数的编码方法" class="headerlink" title="基于单词计数的编码方法"></a>基于单词计数的编码方法</h3><p>词袋模型（Bag-of-Word, BOW），TF-IDF编码。</p><h3 id="One-Hot编码"><a href="#One-Hot编码" class="headerlink" title="One-Hot编码"></a>One-Hot编码</h3><p>略</p><h3 id="密集编码"><a href="#密集编码" class="headerlink" title="密集编码"></a>密集编码</h3><p>Word2Vec [90]使用连续词袋（CBOW）和 Skip-gram 模型来生成单词的密集表示，即单词嵌入。词嵌入的基本假设是出现在相似上下文中的单词具有相似含义。词嵌入在某种程度上减轻了向量化文本数据的离散性和数据稀疏性问题[36]。</p><p>除词嵌入外，doc2vec和para2vec [69]等词嵌入的扩展能直接将句子/段落编码为密集向量。</p><h2 id="文本扰动测量"><a href="#文本扰动测量" class="headerlink" title="文本扰动测量"></a>文本扰动测量</h2><p>文本扰动的度量与图像扰动完全不同。通常，扰动的大小是通过原始数据$x$与它的对抗样本$x’$之间的距离测量的。</p><p>但是在文本中，距离测量还需要考虑语法正确性，句法正确性和语义保留性 (grammar correctness, syntax correctness and semantic-preservance)。</p><h3 id="基于范数的度量方法"><a href="#基于范数的度量方法" class="headerlink" title="基于范数的度量方法"></a>基于范数的度量方法</h3><p>直接使用范数来度量词向量、词嵌入向量之间的距离。</p><h3 id="语法和句法相关的指标"><a href="#语法和句法相关的指标" class="headerlink" title="语法和句法相关的指标"></a>语法和句法相关的指标</h3><p><strong>语法检查</strong>和<strong>句法检查</strong>器可以用于度量该指标，用来保证对抗样本也符合正确的语法和句法。</p><p><strong>困惑度</strong>（Perplexity）通常用于衡量语言模型的质量。在一篇综述文献[91]中，作者使用困惑来确保生成的对抗样本（句子）有效。</p><p><strong>转义</strong>（Paraphrase）将一段文本转成另外一个语言，本身可被视作生成对抗样本。转义的有效性取决于生成模型的有效性。</p><h3 id="语义相似度度量"><a href="#语义相似度度量" class="headerlink" title="语义相似度度量"></a>语义相似度度量</h3><p>一般采用向量间距离度量指标来作为词向量的语义相似度度量指标。给定两个n维的词向量$\mathbf{p}=(p_1,p_2,\dots,p_n)$和$\mathbf{q}=(q_1,q_2,\dots,q_n)$，有如下定义：</p><ul><li>欧氏距离：  <script type="math/tex; mode=display">d(\mathbf{p}, \mathbf{q})=\sqrt{\left(p_{1}-q_{1}\right)^{2}+\left(p_{2}-q_{2}\right)^{2}+\cdots+\left(p_{n}-q_{n}\right)^{2}}</script>-余弦相似度<script type="math/tex; mode=display">\cos (\mathrm{p}, \mathrm{q})=\frac{\sum_{i=1}^{n} p_{i} \times q_{i}}{\sqrt{\sum_{i=1}^{n}\left(p_{i}\right)^{2}} \times \sqrt{\sum_{i=1}^{n}\left(q_{i}\right)^{2}}}</script></li></ul><h3 id="编辑距离"><a href="#编辑距离" class="headerlink" title="编辑距离"></a>编辑距离</h3><p>编辑距离是一类衡量字符串转化成另一字符串时，需要进行的最小操作数。不同的编辑距离，区别在于不同的字符串操作。</p><ul><li>Levenshtein Distance 使用插入，移除和替换操作</li><li>Word Mover’s Distance (WMD) [68] 计算的是两词嵌入向量互相转换的编辑距离，它测量一个文档的单词在词嵌入空间中变成另一文档的单词所需进行的最少操作[38]。通过计算下面这个最优化问题来计算距离：</li></ul><script type="math/tex; mode=display">\begin{array}{c}\min \sum_{i, j=1}^{n} \mathbf{T}_{i j} \mid \mathbf{e}_{\mathbf{i}}-\mathbf{e}_{\mathbf{j}} \|_{2} \\\text {s.t.}, \sum_{j=1}^{n} \mathbf{T}_{i j}=d_{i}, \forall i \in\{i, \ldots, n\}, \sum_{i=1}^{n} \mathbf{T}_{i j}=d_{i}^{\prime}, \forall j \in\{i, \ldots, n\}\end{array}</script><p>其中$\mathbf{e}<em>i$和$\mathbf{e}_j$分别是单词$i$和$j$的词嵌入向量；$n$是单词总数；$\mathbf{d}$和$\mathbf{d}’$分别是两文档经过归一化的词袋向量；$\mathrm{T} \in \mathcal{R}^{n \times n}$是流动矩阵（Flow Matrix，所有可能的源和目的地对之间的流量），$\mathrm{T}</em>{i j} \leq 0$代表了$\mathbf{d}$空间的单词$i$转变成$\mathbf{d}’$空间的单词$j$时，要改变的单词数目。</p><h3 id="杰卡德相似度（Jaccard-similarity-coefficient）"><a href="#杰卡德相似度（Jaccard-similarity-coefficient）" class="headerlink" title="杰卡德相似度（Jaccard similarity coefficient）"></a>杰卡德相似度（Jaccard similarity coefficient）</h3><p>杰卡德相似度计算的是有限样本集合间元素的相似程度，计算方法为：</p><script type="math/tex; mode=display">J(A, B)=\frac{|A \cap B|}{|A \cup B|}</script><p>$A$、$B$是两段文档或者两句话，$|A\cap B|$是同时出现在两文档中的单词数目，$|A\cup B|$是两文档的单词总数，重复出现的单词不予计数。</p><h1 id="四、ATTACKING-NEURAL-MODELS-IN-NLP-THE-SOTA"><a href="#四、ATTACKING-NEURAL-MODELS-IN-NLP-THE-SOTA" class="headerlink" title="四、ATTACKING NEURAL MODELS IN NLP: THE SOTA"></a>四、ATTACKING NEURAL MODELS IN NLP: THE SOTA</h1><h2 id="文本领域的深度学习系统攻击方法之分类"><a href="#文本领域的深度学习系统攻击方法之分类" class="headerlink" title="文本领域的深度学习系统攻击方法之分类"></a>文本领域的深度学习系统攻击方法之分类</h2><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Adversarial-Attacks-on-Deep-Learning-Models-in-Natural-Language-Processing-A-Survey/Categories_of_adv_attack_methods_on_textual_DL_models.png" alt></p><p>如<a href="#Threat_Model">分类标准 Threat Model</a>所述，可以把攻击方法按一下分类标准分类：</p><ul><li>模型内部信息</li><li>根据被攻击模型的应用场景</li><li>根据是否带目标</li><li>根据扰动粒度</li><li>根据<a href="#Models">被攻击模型类型</a></li></ul><h2 id="白盒攻击"><a href="#白盒攻击" class="headerlink" title="白盒攻击"></a>白盒攻击</h2><p>攻击需要访问模型的完整信息，包括体系结构，参数，损失函数，激活函数，输入和输出数据。白盒攻击通常会针对特定模型尽可能严酷的攻击，包括添加扰动。白盒攻击通常非常有效果。</p><h3 id="基于FGSM的攻击"><a href="#基于FGSM的攻击" class="headerlink" title="基于FGSM的攻击"></a>基于FGSM的攻击</h3><p>FGSM算法我们<a href="#FGSM">已有讨论</a>。很多文本模型攻击方法都是受到FGSM启发.</p><p>TextFool [76]采用了FGSM的概念，通过使用反向传播计算损失梯度$\nabla_x J$，根据损失梯度的大小估计文本项的贡献，标识出对文本分类任务有重大贡献的项，这些项被称为Hot Character。包含足够Hot Character的短语被称为HTP。针对HTP，采用三种类型的攻击：插入（另外类别的HTP），修改（该样本自带的HTP字符或视觉上相近的字符）和删除（该样本自带的HTP字符）。最后在CNN文本分类器上评估了这三种策略及其组合[156]。该工作的缺陷在于，这些方法是手动进行的。</p><blockquote><p>关于本文的讨论在<a href="https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Text-Classification-Can-be-Fooled/">这里</a>。</p></blockquote><p>[117]中的工作采用了与TextFool相同的思想，但是它提供了一个remove-addition-replacement策略，该策略首先尝试<strong>删除</strong>对文本分类任务贡献最大（使用损失梯度来衡量）的副词（$w_i$）。如果此步骤输出句子语法不正确，则取消删除，该方法将在$w_i$之前<strong>插入</strong>单词$p_j$。从候选库中选择$p_j$，其中候选词（candidate words）是同义词、错别字（typos）和类型特定的关键字（genre specific keywords）（通过词频（term frequency）确定）。如果插入算法尝试过的所有$p_j$都不能令输出的成本梯度达到最高，则放弃插入，该方法将$w_i$<strong>替换</strong>为$p_j$。</p><p>由于该方法按单词的贡献等级对单词进行排序，并根据顺序制作对抗性样本，因此这是一种贪婪的方法，始终会获得最少的操作，直到输出变化为止。为了避免被人类感知，作者限制了替换/添加的单词，以不影响原始单词的语法和词性（part-of-Speech, POS）。</p><p>虽然恶意软件检测不是典型的文本应用程序，我们仍可以将攻击文本DNN的方法应用于攻击恶意软件检测DNN。在恶意软件检测中，可移植可执行文件（portable executable, PE）用二元向量${x_1,\dots,x_m}$b表示，其中$x_i\in{0,1}$表示PE是否存在，$m$是PE个数。</p><p>[3]的作者研究了生成二进制编码的对抗样本的方法。为了不改变对抗样本的功能，他们使用了四种限制方法（bounding methods）来生成扰动。前两种方法为FSGM$^k$ [67]，即FGSM的多步改进版，通过引入确定性舍入（dFGSM$^k$）和随机舍入（rFGSM$^k$）来限制二进制域中的扰动，类似于图像领域的$L_{\infty}$球约束[40]。第三种方法是多步位梯度上升法（multi-step Bit Gradient Ascent, BGA$^k$），如果损失函数的第$j$偏导数大于或等于损失梯度的$L_2$范数除以$\sqrt{m}$，则设置对应第$j$个二进制位为1。第四种方法是多步位坐标上升法（multi-step Bit Coordinate Ascent, BCA$^k$），通过考虑损失函数的最大对应部分导数的特征，每一步更新一个位（太难翻译了）。该工作还提出了一个对抗学习框架，旨在增强恶意软件检测模型的鲁棒性。</p><p>[114]攻击恶意软件检测DNN，通过扰动二进制序列的嵌入表示，并将扰动后的词嵌入序列重建为二进制。具体地，他们在原始二进制序列后附加了一个统一的随机字节序列（Payload），再将新的二进制附加到原二进制程序嵌入表示中，并且执行FGSM时只更新Payload部分。反复执行扰动，直到检测器输出错误的预测为止。由于仅对Payload而不是对整个输入执行扰动，因此此方法将保留恶意软件的功能。最后，他们通过将对抗样本的嵌入表示在有效嵌入空间中找与其最接近的邻居，将其重构为有效的二进制文件。</p><p>AdvGen[23]是一种基于梯度的攻击神经机器翻译（NMT）模型的方法。首先，它会考虑损失函数的梯度以及一个单词与其替换单词（即对抗性单词）之间的距离，从而生成对抗样本。用语言模型来识别给定单词的最可能替换单词，因为语言模型可使对抗样本保留更多的语义。然后AdvGen将生成的对抗样本合并到NMT模型的解码器中以防御攻击。</p><p>上述基于FGSM的算法都利用了损失函数的梯度，但大部分使用梯度本身而不是像FGSM那样使用梯度的正负号或量级。也有许多研究直接采用FGSM进行对抗训练，即在训练模型时将其用作正则化工具。</p><h3 id="基于JSMA的攻击"><a href="#基于JSMA的攻击" class="headerlink" title="基于JSMA的攻击"></a>基于JSMA的攻击</h3><p>JSMA我们<a href="#JSMA">已有讨论</a>。[103]使用正向导数（forward derivative）作为JSMA矩阵来找到最有利于生成对抗样本的方向。网络的雅可比行列式是通过利用计算图展开来计算的[96]。作者针对两种RNN模型构建了对抗样本，被攻击的RNN模型分别应用于分类任务和序列生成。</p><p>对于分类RNN，输出分量$j$对应的雅可比矩阵的第$j$列，即$Jac_F[:,j]$。对于每个单词$i$，扰动的方向定义为：</p><script type="math/tex; mode=display">\begin{array}{l}\operatorname{sign}\left(\operatorname{Jac} b_{F}\left(x^{\prime}\right)\left[i, g\left(x^{\prime}\right)\right]\right), \\g\left(x^{\prime}\right)=\arg \max _{0,1}\left(p_{j}\right)\end{array}</script><p>$p_j$是输出向量中目标类的概率，这在JSMA中是logit而不是概率。作者还将扰动后的样本投影到嵌入空间中，获得最近的词嵌入向量</p><p>对于序列生成RNN，计算完雅可比矩阵后，将输入时间步$i$更改为雅可比矩阵中的较高值，将输出时间步$j$更改为雅可比矩阵中的较低值。</p><p>[43,44]是首项对恶意软件检测DNN进行攻击的工作。使用二进制指示器特征向量来表示应用程序，通过采用JSMA在输入特征向量上制作对抗样本。为了确保由扰动引起的修改不会对应用程序造成太大影响，从而使恶意软件应用程序的功能保持完整，作者使用$L_1$范数将功能总数限制为20，更改数目限制为20。</p><p>此外，作者提供了三种防御攻击的方法，即特征缩减（feature reduction），精简（distillation）和对抗训练（adversarial training）。他们发现对抗训练是最有效的防御方法。</p><h3 id="基于C-amp-W的攻击方法"><a href="#基于C-amp-W的攻击方法" class="headerlink" title="基于C&amp;W的攻击方法"></a>基于C&amp;W的攻击方法</h3><p>C&amp;W我们<a href="#C_W">已有讨论</a>。</p><p>[130]中的工作采用C＆W方法攻击病历预测模型，该模型用于检测每位患者的病历中的易感事件和测量结果，为临床使用提供指导。该模型使用LSTM。</p><p>给定病人的病历数据为$X^{i} \in \mathbf{R}^{d \times t_{i}}$，其中$d$是每条数据的特征个数，$t_i$是医疗检查的时间指数。对抗样本的生成公式如下：</p><script type="math/tex; mode=display">\min _{\hat{X}} \max \left\{-\epsilon,\left[logit\left(\mathbf{x}^{\prime}\right)\right]_{y}-[logit(\mathbf{x})]_{y^{\prime}}\right\}+\lambda\left\|\mathbf{x}^{\prime}-\mathbf{x}\right\|_{1}</script><p>其中$logit(\cdot)$表示网络输出的logit，$\lambda$是控制正则项大小的超参数，$y$和$y’$分别是原始类别和突变目标类别。作者还从扰动幅度和攻击的结构两个角度评估了生成的对抗样本。最后使用对抗样本来计算EHR的敏感性分数（susceptibility score）以及不同测量值的累积敏感性分数（cumulative susceptibility score）。</p><p>Seq2Sick [22]使用两种带目标攻击方法攻击Seq2seq模型：非重叠攻击（non-overlapping attack）和关键字攻击（keywords attack）。</p><p>非重叠攻击是指生成与原始输出完全不同的对抗序列。作者提出了一种类似于铰链（hinge-like）的损失函数，可以在神经网络的logit层上进行优化：</p><script type="math/tex; mode=display">\sum_{i=1}^{|K|} \min _{t \in[M]}\left\{m_{t}\left(\max \left\{-\epsilon, \max _{y \neq k_{i}}\left\{z_{t}^{(y)}\right\}-z_{t}^{\left(k_{i}\right)}\right\}\right)\right\}</script><p>其中$z_t$是输入对抗样本后的模型logit层输出。</p><p>关键字攻击是指攻击模型，使得预期关键字出现在输出中。作者还是将优化放在了logit层上，并试图确保目标关键字的logit在所有单词中最大。此外，他们定义了掩码函数m以解决关键字冲突问题。添加掩码后的损失函数为：</p><script type="math/tex; mode=display">L_{\text {keywords}}=\sum_{i=1}^{|K|} \min _{t \in[M]}\left\{m_{t}\left(\max \left\{-\epsilon, \max _{y \neq k_{i}}\left\{z_{t}^{(y)}\right\}-z_{t}^{\left(k_{i}\right)}\right\}\right)\right\}</script><p>其中$k_i$表示输出单词中的第$i$个单词。为了确保生成的词嵌入向量是有效的，作者还引入了两种正则化项：group lasso 正则化来增强group稀疏性；group 梯度正则化使对抗样本位于有效的嵌入空间范围内。</p><h3 id="基于方向（导数）的攻击"><a href="#基于方向（导数）的攻击" class="headerlink" title="基于方向（导数）的攻击"></a>基于方向（导数）的攻击</h3><p>HotFlip [30]执行原子翻转操作以生成对抗样本。HotFlip没有利用损失函数的梯度，而是使用方向导数。具体而言，HotFlip将字符级操作（即交换，插入和删除）表示为输入空间中的向量，并通过针对这些操作向量的方向导数来估计损失的变化。具体来说，给定输入的One-Hot表示，第i个单词的第j个字符的变化（比如从a变到b）可以由以下向量表示：</p><script type="math/tex; mode=display">\vec{v}_{i j b}=\left(0, . . ;\left(0, . .(0, . .-1,0, . ., 1,0)_{j}, . .0\right)_{i} ; 0, . .\right),</script><p>其中最内层向量$j$的内部，-1的位置在字母表中为a，1的位置在字母表中为b。用这个向量来模拟操作。那么可以通过沿操作矢量的方向导数最大化损失变化的一阶近似值来找到最佳的字符交换：</p><script type="math/tex; mode=display">\max \nabla_{x} J(x, y)^{T} \cdot \vec{v}_{i j b}=\max _{i j v} \frac{\partial J^{(b)}}{\partial x_{i j}}-\frac{\partial J^{(a)}}{\partial x_{i j}}</script><p>其中$J(x,y)$是模型的损失函数，输入为$x$，真实输出为$y$。这个方法不仅可以表示字符替换，还可以表示插入和删除字符。在第i个字的第j个位置插入也可以当作一个字符翻转，然后再进行更多的翻转，因为字符是向右移位的，直到字的末尾。字符删除则是多次字符翻转，因为字符是向左移位的。利用集束搜索（Beam Search），HotFlip可以有效地找到多次翻转的最佳方向。</p><p>[29]扩展了HotFlip的功能，使其可以进行目标攻击。除了HotFlip中提供的交换，插入和删除功能外，作者还提出了受控攻击（controlled attack）：使输出序列中特定单词消失的方法，以及目标攻击（targeted attack）：以预计的单词替换输出序列中的特定单词。为了实现这些攻击，他们使损失函数$J (x,y_t )$最大化，并使$J (x,y^{′}_t )$最小化，其中$t$是受控攻击的目标词，$t′$是替换$t$的词。</p><p>此外，他们还提出了三种类型的攻击，提供多种文本修改方法。在one-hot攻击中，他们对文本中的所有词进行了最优操作。在Greedy攻击中，他们除了从整个文本中挑选出最佳操作外，还进行了另一次向前和向后的传递。在波束搜索攻击中，他们用波束搜索代替了greedy中的搜索方法。在本作提出的所有攻击中，作者都设置了最大修改次数的阈值，例如，允许20%的字符被修改。</p><h3 id="基于注意力的攻击"><a href="#基于注意力的攻击" class="headerlink" title="基于注意力的攻击"></a>基于注意力的攻击</h3><p><span id="ref14" style.display="none"></span><br>[14]的作者为了比较CNN与RNN的鲁棒性，提出了两种白盒攻击。他们利用模型的内部注意力分布来寻找关键句子，模型赋予该句子较大的权重，从而得出正确答案。然后，他们将获得注意力最高的词与已知词汇中随机选择的词进行交换。他们还进行了另一种白盒攻击，将获得最高注意力的整句去掉。虽然他们关注的是基于注意力的模型，但他们的攻击并没有研究注意力机制本身，而是仅仅利用了注意力部分的输出（即注意力得分）。</p><h3 id="基于重编程的攻击"><a href="#基于重编程的攻击" class="headerlink" title="基于重编程的攻击"></a>基于重编程的攻击</h3><p><span id="Reprogramming" style.display="none"></span><br>[97]采用对抗重编程（adversarial reprogramming, AP）来攻击序列神经分类器。训练一个对抗重编程函数$g<em>{\theta}$，在不修改DNN参数的情况下，将被攻击的DNN重新用于执行另一个任务（如问题分类到名称分类）。AP采用迁移学习的思想，但保持参数不变。在白盒攻击中，作者应用Gumbel-Softmax来训练可以在离散数据上工作的$g</em>{\theta}$。作者在各种文本分类任务上评估了他们的方法，并证实了他们方法的有效性。</p><h3 id="混合方法"><a href="#混合方法" class="headerlink" title="混合方法"></a>混合方法</h3><p>[38]针对CNN模型，应用FGSM和DeepFool对输入文本的词嵌入进行扰动。作者通过使用词移动距离（Word Mover’s Distance，WMD）作为距离测量，将对抗样本舍入（Rounded）到最近的有意义的词向量。在情感分析和文本分类数据集上的评估表明，WMD是控制扰动的合格度量。</p><h3 id="白盒攻击总结"><a href="#白盒攻击总结" class="headerlink" title="白盒攻击总结"></a><strong>白盒攻击总结</strong></h3><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Adversarial-Attacks-on-Deep-Learning-Models-in-Natural-Language-Processing-A-Survey/white_box_attack_methods_table.png" alt></p><h2 id="黑盒攻击"><a href="#黑盒攻击" class="headerlink" title="黑盒攻击"></a>黑盒攻击</h2><p>黑盒攻击不能获得神经网络内部信息，但是可以获得输入和输出。这种攻击方法经常依靠启发式算法生成对抗样本。鉴于现实生活中不能获得模型信息，因此黑盒攻击往往更加实用。本文将文本领域的黑盒攻击分成五类。</p><h3 id="连词对抗器"><a href="#连词对抗器" class="headerlink" title="连词对抗器"></a>连词对抗器</h3><p>[54]是第一个攻击阅读理解系统的研究。作者提出了连词对抗器（concatenation adversaries），即在段落末尾附加一些无意义的句子。这些分散网络精力的句子不会改变段落的语义和Ground Truth，但会欺骗神经模型。分心句子要么是精心生成的信息句子，要么是从常用词池随机取20个以并任意词序构建的句子。</p><p>这两种扰动都是通过迭代查询神经网络获得的，直到输出发生变化。</p><p>图2说明了参考文献[54]中的一个例子，加入干扰句（蓝色）后，答案从正确的（绿色）变为不正确的（红色）。</p><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Adversarial-Attacks-on-Deep-Learning-Models-in-Natural-Language-Processing-A-Survey/fig_2.png" alt></p><p>[142]的作者通过改变分心句子放置的位置，以及扩大生成分心句子的假答案集，改进了这项工作，呈现了新的对抗样本，有助于训练更健壮的神经模型。</p><p>[14]利用分心句子来评估其阅读理解模型的鲁棒性。他们从常用词池中使用随机的10个词，结合所有问题词和所有错误答案候选的词来生成分心的句子。作者还通过同义词替换最常出现的词，进行简单的词级黑盒攻击。作者还提供了两种<a href="#ref14">白盒策略</a>。</p><p>图3说明了连词攻击的一般工作流程。</p><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Adversarial-Attacks-on-Deep-Learning-Models-in-Natural-Language-Processing-A-Survey/fig_3.png" alt></p><p>正确的输出(即MRC任务中的答案)通常被利用来生成扭曲的输出，稍后这些输出将被用来构建分散注意力的内容。将分心内容附加到原始段落中，形成被攻击的DNN的对抗性输入。分心的内容不会分散人类和理想DNN的注意力，但会使易受攻击的DNN产生错误的输出。</p><h3 id="编辑对抗器"><a href="#编辑对抗器" class="headerlink" title="编辑对抗器"></a>编辑对抗器</h3><p>[12]中的工作对神经机器翻译应用的输入数据进行了两种方式的扰动。<strong>合成</strong>(Synthetic)，即进行字符顺序的改变，如交换、中间随机（即随机改变除首尾字符以外的字符顺序）、完全随机（即随机改变所有字符的顺序）和键盘类型（keyboard type）。他们还收集了自然生成的拼写错误（typos）和错别字（misspellings）作为对抗单元（adversaries）。</p><p>[98]的作者还攻击了用于对话生成的神经模型。他们在对话语境中应用了各种扰动，即Random Swap（随机转置相邻的标记）和Stopword Dropout（随机删除停顿词）、Paraphrasing（用其转述替换单词）、语法错误（如将动词改成错误的时态），以及添加否定策略（否定源输入的动词根）和反义词策略（将动词、形容词或副词改成其反义词）。</p><p>DeepWordBug[34]使用字符转换来生成对抗样本。首先确定了重要的“标记（token）”，即给模型输出造成大量影响的那些单词或字符，影响程度依赖一个评价函数，输入模型的输出向量，输出评分。然后修改这些token，包括替换、删除、添加和交换四种策略。作者在各种NLP任务上评估了他们的方法，例如，文本分类、情感分析和垃圾邮件检测。</p><p>[73]中的工作完善了[34]中的评分函数。此外这项工作还有采用了JSMA的白盒攻击版本。通过使用四种文本相似性测量方法进行扰动限制：文本的编辑距离；Jaccard相似性系数；词向量的欧氏距离；词嵌入的余弦相似性。他们的方法只在情感分析任务上进行了评估。图4是参考文献[73]中的一个编辑对抗的例子，在这个例子中，只有少数的编辑操作会误导分类器给出错误的预测。</p><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Adversarial-Attacks-on-Deep-Learning-Models-in-Natural-Language-Processing-A-Survey/fig_4.png" alt></p><p>[110]攻击文本分类模型提供了一种概率方法来选择要替换的词。首先收集现有语料库中该词的所有同义词，然后通过测量其对分类概率的影响，从同义词中选择一个拟替代词（proposed substitute words）。导致模型分类概率发生最显著变化的词将被选中。作者还结合词的显著性来确定替换顺序。词显著性（word saliency）是指如果将一个词设置为未知，输出分类概率的变化程度。</p><p>[91]的作者提出了一种在自然语言推理（NLI）中自动生成违反一组给定一阶逻辑约束的对抗样本的方法。他们提出了一个不一致性损失（inconsistency loss）来衡量一组句子导致模型违反规则的程度。对抗样本生成的过程是寻找规则中变量与句子之间的映射，使不一致性损失最大化的过程。这些句子是由低困惑度（由语言模型定义）的句子组成。为了生成低困惑性的对抗句样本，他们使用了三种编辑扰动。(i)改变其中一个输入句中的一个词；(ii)从其中一个输入句中删除一个解析子树（parse subtree）；(iii)将语料库中一个句子的一个解析子树插入到另一个句子的解析树（parse tree）中。</p><p>[5]中的工作采用遗传算法(GA)生成对抗样本，能够使得原文中最小化单词替换的数量同时可以让被攻击模型的结果发生改变。作者采用了GA中的交叉和突变操作来产生扰动。他们测量了单词替换的有效性，以衡量对被攻击DNNs的影响。他们的攻击主要集中在情感分析和文本蕴含（textual entailment）DNNs上。</p><p>[19]提出了一种对可分化神经计算机（Differentiable Neural Computer, DNC）进行对抗性攻击的框架工作。DNC是一种以DNN为中央控制器的计算机，运行在外部内存模块上，执行数据处理操作。他们使用两种自动化且可扩展的策略，生成语法正确的对抗样本，供问答领域使用。他们还使用了蜕变变换（metamorphic transformation）。第一个策略：Pick-n-Plug，由一个Pick操作符和Plug操作符组成，pick操作符从源任务域中选择一句作为对抗性插入，plug操作符将这些句子注入到另一个目标任务中，没有改变句子的Ground Truth。另一个策略是Pick-Permute-Plug，在从源任务中抽取句子后增加一个permute操作符，扩展了Pick-n-Plug生成对抗样本的能力。Permute操作的具体做法是将特定对抗样本中的单词进行其同义词替换，以产生更大范围的攻击。</p><p>[155]提出了一种基于编辑的方法MHA（Metropolis-Hastings Attack），旨在提供流畅有效的对抗性攻击。MHA基于语言模型和MetropolisHastings（M-H）采样。作者使用M-H采样来生成替换旧词的词（用于替换操作）和随机词（用于插入操作）。语言模型用于保证替换/插入/删除操作后的句子的流畅性。作者提出了黑盒和白盒两种版本。两者唯一的区别是在选择最可能操作的词时，对预选功能的定义。图5展示了编辑对抗样本的一般工作流程。通过替换、删除、插入和交换等编辑策略对句子、单词或字符进行扰动。</p><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Adversarial-Attacks-on-Deep-Learning-Models-in-Natural-Language-Processing-A-Survey/fig_5.png" alt></p><h3 id="基于释义的对抗器"><a href="#基于释义的对抗器" class="headerlink" title="基于释义的对抗器"></a>基于释义的对抗器</h3><p>SCPNs[53]通过将给定句子和目标句法形式输入到encoder-decoder架构中，产生具有期望句法的释义。该方法首先对原句进行编码，然后将反译产生的释义和目标句法树输入到解码器中，其输出是原句的目标释义。创新点在于解析模板的选择和处理。作者从SCPNs中分别训练了一个解析生成器（parse generator），并在PARANMT-50M数据集中选取了20个最常见的模板。</p><p>在使用选定的解析模板生成解析后，他们通过检查n-gram重叠（overlap ）和基于解析的相似性（paraphrase-based similarity），进一步修剪了非可感句子（non-sensible sentences）。被攻击的分类器可以正确地预测原始句子的标签，但在其释义上却失败了，这被认为是对抗性的例子。</p><p>SCPNs在情感分析和文本包涵（textual entailment）DNNs上进行了评估，并显示出对被攻击模型的显著影响。虽然这种方法使用目标策略来生成对抗样本，但它没有指定目标输出。因此，我们将其归为非目标攻击。</p><p>[127]中的工作使用了创建语义等价的对抗样本（semantically equivalent adversaries, SEA）的释义生成技术的思想。作者生成了一个输入句子$x$的释义，并从$f$中得到预测，直到改变原始预测。同时，他们考虑了与$x′$的语义等同，即如果$x$与$x′$语义等同，则为1，否则为0，如下式所示。</p><script type="math/tex; mode=display">\operatorname{SEA}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\mathbf{1}\left[\operatorname{Sem} E q\left(x, x^{\prime}\right) \wedge f(\mathbf{x}) \neq f\left(\mathbf{x}^{\prime}\right)\right]</script><p>之后，本工作提出了一种基于语义等价规则的方法，将这些生成的对抗样本泛化为语义等价规则，以理解和修复影响最大的bug。</p><p>图6描述了基于释义的对抗样本算法生成原则，我们把源文本的释义当作对抗样本。生成释义时添加扰动。</p><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Adversarial-Attacks-on-Deep-Learning-Models-in-Natural-Language-Processing-A-Survey/fig_6.png" alt></p><h3 id="基于GAN的对抗样本生成器"><a href="#基于GAN的对抗样本生成器" class="headerlink" title="基于GAN的对抗样本生成器"></a>基于GAN的对抗样本生成器</h3><p>采用GAN的目的是为了使对抗样本更加自然。</p><p>[157]提出的生成对抗样本的模型由两个关键组件组成：一个GAN，生成虚假的数据样本；一个转换器，将输入$x$映射到其潜伏表示$z′$。这两个组件通过最小化原始输入和对抗样本之间的重构误差来对原始输入进行训练。通过识别$z′$附近的扰动样本$\hat{z}$在潜密空间中进行扰动。作者提出了两种搜索方法，即迭代随机搜索（iterative stochastic search）和混合收缩搜索（hybrid shrinking search）来识别合适的$\hat{z}$。该工作既适用于图像数据，也适用于文本数据，因为它从本质上消除了文本数据的离散属性所带来的问题。作者对他们的方法在三个应用上进行了评估，即：文本包含、机器翻译和图像分类。然而，每次求解使模型出错的$\hat{z}$都需要重新查询攻击模型并运行算法，相当耗时。</p><h3 id="基于替换的方法"><a href="#基于替换的方法" class="headerlink" title="基于替换的方法"></a>基于替换的方法</h3><p>[52]中的工作提出了一个攻击RNN模型的黑盒框架，用于恶意软件检测。该框架包括两个模型：生成RNN和替换RNN。生成式RNN基于seq2seq模型[131]，旨在从恶意软件的API序列中生成对抗性的API序列。具体来说，生成一小段API序列，并将该序列插入到输入序列之后。替代RNN是一种具有注意力机制的双向RNN，它要模仿被攻击RNN的行为。因此，生成对抗样本不会查询原来的被攻击RNN，而是查询它的替代RNN。替换的RNN会在恶意软件和良性程序序列数据集上进行训练，以生成RNN的Gumbel-Softmax为输出。在这里，由于生成性RNN的原始输出是离散的，所以使用Gumbel-softmax来实现两个RNN模型的联合训练。具体来说，它可以使梯度从生成性RNN反推到替代性RNN。该方法对API进行攻击，API表示为一个One-Hot向量，即给定$M$个API，第$i$个API的向量是一个$M$维的二进制向量，即第$i$维为1，其他维度为0。</p><h3 id="重编程"><a href="#重编程" class="headerlink" title="重编程"></a>重编程</h3><p><a href="#Reprogramming">如前所述</a>，[97]提供了黑盒和白盒两种攻击版本。在黑盒攻击中，作者将序列生成建模为强化学习任务，对抗重训练函数$g<em>{\theta}$是强化学习中的policy网络。作者采用基于强化学习的优化算法训练$g</em>{\theta}$。</p><h3 id="黑盒攻击总结"><a href="#黑盒攻击总结" class="headerlink" title="黑盒攻击总结"></a>黑盒攻击总结</h3><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Adversarial-Attacks-on-Deep-Learning-Models-in-Natural-Language-Processing-A-Survey/black_box_attack_methods_table_part_1.png" alt><br><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Adversarial-Attacks-on-Deep-Learning-Models-in-Natural-Language-Processing-A-Survey/black_box_attack_methods_table_part_2.png" alt></p><h2 id="多模态任务攻击"><a href="#多模态任务攻击" class="headerlink" title="多模态任务攻击"></a>多模态任务攻击</h2><p>有些模型执行跨模态的任务，比如一些神经模型包含一个内部组件，执行图像到文本或语音到文本的转换。虽然这些攻击并不是针对纯文本数据的，但为了全面回顾，我们简单介绍一下有代表性的攻击。</p><h3 id="图像到文本模型的攻击方法"><a href="#图像到文本模型的攻击方法" class="headerlink" title="图像到文本模型的攻击方法"></a>图像到文本模型的攻击方法</h3><p>图像到文本模型是一类根据图像的语义内容为图像生成文本描述的技术。</p><h4 id="攻击光学字符识别模型"><a href="#攻击光学字符识别模型" class="headerlink" title="攻击光学字符识别模型"></a>攻击光学字符识别模型</h4><p>光学字符识别（Optical Character Recognition, OCR）是典型的多模态学习任务，它将图像作为输入，并输出识别的文本。[129]提出了对OCR的白盒攻击和后续的NLP应用。首先使用原始文本来渲染一个干净的图像（转换DNNs），然后在WordNet中找到满足编辑距离阈值的反义词。只有那些有效（valid）且保持语义不一致的反义词才会被保留。之后，该方法在干净的图像中找到包含上述单词的行，可以用它们所选择的反义词来替换。然后，该方法将目标词转化为目标序列。给定输入/目标图像和序列，作者将对抗样本生成问题建模为一个优化问题。</p><script type="math/tex; mode=display">\begin{array}{c}\min _{\omega} c \cdot J_{C T C} f\left(\mathbf{x}^{\prime}, t^{\prime}\right)+\left\|\mathbf{x}-\mathbf{x}^{\prime}\right\|_{2}^{2} \\\mathbf{x}^{\prime}=(\alpha \cdot \tanh (\omega)+\beta) / 2 \\\alpha=\left(\mathbf{x}_{\max }-\mathbf{x}_{\min }\right) / 2, \beta=\left(\mathbf{x}_{\max }+\mathbf{x}_{\min }\right) / 2 \\J_{C T C}(f(\mathbf{x}, t))=-\log p(t \mid \mathbf{x})\end{array}</script><p>其中$f(\mathbf{x})$代表神经网络模型，$J<em>{CTC}(\cdot)$代表CTC（Connectionist Temporal Classification）损失函数，$\mathbf{x}$是损失图像，$t$是真实（Ground Truth）序列，$x^{‘}$是对抗样本，$t^{‘}$是目标序列，$\omega,\alpha,\beta$是生成对抗样本时为了满足box-constraint条件$\mathbf{x}^{\prime} \in\left[\mathbf{x}</em>{\min }, \mathbf{x}_{\max }\right]^{p}$的参数。$p$是确保生成有效对抗样本$x^{‘}$所需的像素个数。在对抗样本生成完毕后，以对抗样本替换图像中相应行。</p><p>作者从三个方面对该方法进行了评估：单字识别、整篇文档识别以及基于识别文本的NLP应用（具体为情感分析和文档分类）。他们还发现，所提出的方法存在着可转移性（transferability）低和物理可实现性（physical realizability）低等局限性。</p><h4 id="攻击场景文本识别模型"><a href="#攻击场景文本识别模型" class="headerlink" title="攻击场景文本识别模型"></a>攻击场景文本识别模型</h4><p>场景文本识别（Scene Text Recognition, STR）是一种图像到文字的应用，整个图像直接被映射成字串。相比之下，OCR中的识别是一个流水线过程：首先将单词分割成字符，然后对单个字符进行识别。AdaptiveAttack[153]评估了对场景文本识别进行对抗性攻击的可能性。作者提出了两种攻击方式，即基本攻击和自适应攻击。基本攻击与参考文献[129]中的工作类似，它也将对抗式实例生成表述为一个优化问题。</p><script type="math/tex; mode=display">\begin{array}{l}\min _{\omega} J_{C T C} f\left(\mathbf{x}^{\prime}, t^{\prime}\right)+\lambda \mathcal{D}\left(\mathbf{x}, \mathbf{x}^{\prime}\right), \\\mathbf{x}^{\prime}=\tanh (\omega)\end{array}</script><p>其中$\mathcal{D}(\cdot)$是欧氏距离。</p><p>与[129]的不同之处在于$x′$的定义，以及$x$、$x′$之间的距离测量($L_2$法则 vs. 欧氏距离)，以及参数$\lambda$，它平衡了作为对抗样本和接近原始图像的重要性。由于寻找合适的$\lambda$相当耗时，作者提出了另一种方法来自适应地寻找$\lambda$。他们将这种方法命名为Adaptive Attack，他们将顺序分类任务的似然（likelihood）定义为遵循高斯分布，并推导出顺序对抗样本的自适应优化为：</p><script type="math/tex; mode=display">\min \frac{\left\|\mathrm{x}-\mathrm{x}^{\prime}\right\|_{2}^{2}}{\lambda_{1}^{2}}+\frac{J_{C T C} f\left(\mathrm{x}^{\prime}, t^{\prime}\right)}{\lambda_{2}^{2}}+\log \lambda_{1}^{2}+T \log \lambda_{2}^{2}+\frac{1}{\lambda_{2}^{2}}</script><p>其中$\lambda_1,\lambda_2$是用来平衡扰动与CTC损失函数的两个参数。作者在令输出中的插入文本、删除文本和替换文本的任务上评估了他们提出的方法。结果表明，自适应攻击比基本攻击快得多。</p><h4 id="攻击图像字幕生成模型"><a href="#攻击图像字幕生成模型" class="headerlink" title="攻击图像字幕生成模型"></a>攻击图像字幕生成模型</h4><p>图像字幕（Image Captioning）是另一个多模态学习任务，它将图像作为输入，并生成一个描述其视觉内容的文本字幕。Show-and-Fool[20]生成对抗样本来攻击基于CNN-RNN的图像字幕模型。被攻击的CNN-RNN模型采用CNN作为编码器进行图像特征提取，RNN作为解码器进行字幕生成。Show-and-Fool有两种攻击策略：预期目标字幕(targeted caption)（即生成的字幕与预期目标字幕相匹配）和预期目标关键词（targeted keywords）（即生成的字幕包含目标关键词）。形式化定义如下：</p><script type="math/tex; mode=display">\begin{array}{c}\min _{\omega} c \cdot J\left(\mathbf{x}^{\prime}\right)+\left\|\mathbf{x}^{\prime}-\mathbf{x}\right\|_{2}^{2} \\\mathbf{x}^{\prime}=\mathbf{x}+\eta \\x=\tanh (y), \quad \mathbf{x}^{\prime}=\tanh (\omega+y)\end{array}</script><p>其中$c&gt;0$是预先定义的正则化常数，$\eta$是扰动，$\omega,y$是控制$\mathbf{x}^{\prime} \in[-1,1]^{n}$的参数。两种策略的区别在于损失函数$J(\cdot)$的定义。</p><p>targeted caption策略的targeted caption定义为$S=\left(S<em>{1}, S</em>{2}, \ldots S<em>{t}, \ldots S</em>{N}\right)$，其中$S_t$为单词表中第$t$个单词的index，$N$是caption的长度。损失函数定义为：</p><script type="math/tex; mode=display">J_{S, l o g i t}\left(\mathbf{x}^{\prime}\right)=\sum_{t=2}^{N-1} \max \left\{-\epsilon, \max _{k \neq S_{t}}\left\{z_{t}^{(k)}\right\}-z_{t}^{\left(S_{t}\right)}\right\}</script><p>其中$S_t$是目标单词，$z_t^{(S_t)}$是目标单词的logit。这个方法实质上是最小化了$S_t$的logit与除$S_t$外的最大logit的距离。</p><p>targeted keywords策略。给定targeted keywords $\mathcal{K}:=K<em>{1}, \ldots, K</em>{M}$，损失函数定义为：</p><script type="math/tex; mode=display">J_{K, \text {logit}}\left(\mathrm{x}^{\prime}\right)=\sum_{j=1}^{M} \min _{t \in[N]}\left\{\max \left\{-\epsilon, \max _{k \neq K_{j}}\left\{z_{t}^{(k)}\right\}-z_{t}^{\left(K_{j}\right)}\right\}\right\}</script><p>作者对Show-and-Tell[137]进行了大量的实验，并改变了攻击损失中的参数。他们发现，Show-and-Fool不仅在攻击基于CNN-RNN的图像字幕模型Showand-Tell上有效，而且在另一个模型Show-Attend-and-Tell[147]上也具有很强的转移能力（transferable ）。</p><h4 id="攻击视觉问题回答模型"><a href="#攻击视觉问题回答模型" class="headerlink" title="攻击视觉问题回答模型"></a>攻击视觉问题回答模型</h4><p>视觉问题回答（Visual Question Answering, VQA）是指，给定一张图像和一个关于图像的自然语言问题，VQA要用自然语言提供一个准确的答案。[148]中的工作提出了一种迭代优化方法来攻击两种VQA模型。提出的目标函数可以最大化目标答案的概率，并在该距离低于阈值时，对与原始图像距离较小的对抗样本的偏好进行减权。</p><p>具体来说，该目标包含三个部分。第一部分类似于STR任务中的公式</p><script type="math/tex; mode=display">\begin{array}{l}\min _{\omega} J f\left(\mathbf{x}^{\prime}, t^{\prime}\right)+\lambda \mathcal{D}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\end{array}</script><p>，将损失函数替换为VQA模型的损失，并使用$\left|\mathbf{x}-\mathbf{x}^{\prime}\right|_{2} / \sqrt{N}$作为$x′$和$x$之间的距离。</p><p>第二部分使softmax输出与预测值的差值最大化，当其与目标答案不同时，就会使其差值最大化。</p><p>第三部分确保$x′$和$x$之间的距离在一个阈值之下。</p><p>通过检查是否比之前的攻击获得更好的成功率，以及模型预测目标答案的置信度得分来评估攻击。根据评估结果，作者得出结论：注意力（attention）、边界盒定位（bounding box localization）和组成式内部结构（compositional internal structures）容易受到对抗性攻击。这项工作还攻击了一个图像字幕神经模型。</p><h4 id="攻击视觉语义嵌入模型"><a href="#攻击视觉语义嵌入模型" class="headerlink" title="攻击视觉语义嵌入模型"></a>攻击视觉语义嵌入模型</h4><p>视觉语义嵌入（Visual-semantic Embeddings, VSE）目的是将自然语言和底层视觉世界连接起来。在VSE中，图像和描述性文本（标题）的嵌入空间都被联合优化和对齐。</p><p>[125]通过在测试集中生成对抗样本，对最新的VSE模型进行攻击，并评估了VSE模型的鲁棒性。通过引入三种方法对文本部分进行攻击。(i)利用WordNet中的超义词/同义词（hypernymy/hyponymy）关系替换图像标题中的名词；(ii)将数字改为不同的数字，并在必要时将相应的名词单数化或复数化；(iii)检测关系，将不可互换的名词短语打乱或替换介词。</p><p>这种方法可以认为是一种黑盒攻击。</p><h3 id="语音转文字模型攻击方法"><a href="#语音转文字模型攻击方法" class="headerlink" title="语音转文字模型攻击方法"></a>语音转文字模型攻击方法</h3><p>语音转文字（Speech-to-text）也称为语音识别，任务是自动识别口语并将其翻译成文本。[18]攻击了基于LSTM的最先进的语音转文字神经网络Deep Speech。给定一个自然波形，作者构造了一个音频扰动，这个扰动几乎听不到，但可以通过添加到原始波形中进行识别。该扰动的构建采用了C&amp;W方法的思想(参考<a href="#C_W">这里</a>)，即通过最大的像素变化量来测量图像失真。在此基础上，他们通过计算音频的相对响度来测量音频失真，并提出使用CTC损失来完成优化任务。然后他们用Adam优化器解决了这个任务[61]。</p><h3 id="多模态攻击技术总结"><a href="#多模态攻击技术总结" class="headerlink" title="多模态攻击技术总结"></a>多模态攻击技术总结</h3><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Adversarial-Attacks-on-Deep-Learning-Models-in-Natural-Language-Processing-A-Survey/cross_modal_attack_methods_table.png" alt></p><h2 id="按应用划分的基准数据集"><a href="#按应用划分的基准数据集" class="headerlink" title="按应用划分的基准数据集"></a>按应用划分的基准数据集</h2><p>近年来，神经网络在不同的NLP领域获得了成功，包括文本分类、阅读理解、机器翻译、文本摘要、问题回答、对话生成等等。在本节中，我们将从NLP应用的角度回顾目前关于神经网络生成对抗样本的作品。表4根据其应用领域总结了我们在本文中回顾的作品。</p><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Adversarial-Attacks-on-Deep-Learning-Models-in-Natural-Language-Processing-A-Survey/datasets_table.png" alt></p><p>我们还在表中列出了这些作品中使用的基准数据集作为辅助信息。如果想了解更多，我们推荐到我们收集的链接/参考文献中进一步获得数据集的详细描述。请注意，帮助生成对抗样本的辅助数据集并不包括在内。我们只介绍用于评估被攻击神经网络的数据集。</p><h3 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h3><p>大多数综述类文章都集中于神经网络分类任务的攻击算法上。情感分析任务试图将文本按照情感倾向分成几类（积极、消极、中立），性别识别、语法错误检测和恶意软件检测可以被框定为二元分类问题。关系提取可以表述为单分类或多分类问题。预测医疗状态是一个多类问题，类是由医学专家定义的。这些作品通常使用多个数据集来评估其攻击策略，以显示其方法的通用性和鲁棒性。</p><p>[76]使用DBpedia本体数据集[71]将文档样本分类为14个高级类。[38]使用IMDB电影评论[83]进行情感分析，使用NLTK包提供的Reuters-2和Reuters-5新闻数据集进行分类。[103]使用一个未指定的电影评论数据集进行情感分析，并使用IMDB电影评论[83]和NLTK包10提供的Reuters-2和Reuters-5新闻线数据集进行分类。[117]也使用IMDB电影评论数据集进行情感分析，还对和Twitter数据集进行了性别分类。[34]对Enron垃圾邮件数据集[89]进行垃圾邮件检测，并采用[156]中的6个大型数据集，即AG’s新闻、搜狗新闻[138]、DBPedia本体数据集、Yahoo！Answers进行文本分类和Yelp评论、亚马逊评论[88]进行情感分析。[30]也使用AG’s新闻进行文本分类，使用斯坦福情感树库（Stanford Sentiment Treebank，SST）数据集[128]进行情感分析。[118]对三个任务进行评估：情感分析（IMDB电影评论，Elec[55]，Rotten Tomatoes[102]），文本分类（DBpedia Ontology数据集和RCV1[72]）和语法错误检测（FCE-public[150]）。[130]用真实世界的电子健康记录数据生成了神经医疗状态预测系统的对抗样本。</p><p>许多作品攻击恶意软件检测模型。[43，44]对神经恶意软件检测系统进行攻击，使用DREBIN数据集，其中包含正常和恶意android应用程序[7]。[114]收集正常 windows应用文件，并使用微软恶意软件分类挑战数据集[113]作为恶意部分。[52]从某网站抓取180个程序及相应的行为报告进行恶意软件分析。在抓取的程序中，有70%是恶意软件。[97]以文本分类神经模型为目标，使用四个数据集来评估其攻击方法，分别是姓氏分类数据集、问题分类实验数据[75]、阿拉伯语推文情感分类数据集[1]和IMDB电影评论数据集。[145]将关系提取建模为一个分类问题，目标是预测给定文本提及的实体对之间存在的关系。他们使用了两个关系数据集，NYT数据集[111]和UW数据集[78]。[10]的目标是提高神经网络对联合实体和关系提取的功效。与参考文献[145]中的方法不同，作者将关系提取任务建模为一个多标签头选择（multi-label head selection）问题。他们的工作中使用了四个数据集，ACE04数据集[28]、CoNLL04 EC任务[115]、荷兰房地产分类(DREC)数据集[11]和不良药物事件(ADE)[45]。</p><h3 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h3><p>机器翻译工作在两个数据集上，其中一个使用源语言，另一个是目标语言。</p><p>[12]使用为IWSLT 2016[87]准备的TED talks并行语料来测试NMT系统。它还收集了法语、德语和捷克语料用于生成自然噪声，以建立一个查找表，其中包含可能的词汇替换。这些替换词随后被用于生成对抗样本。[29]也使用了同样的TED talks语料，并使用了德语对英语、捷克语对英语、法语对英语。</p><h3 id="机器理解"><a href="#机器理解" class="headerlink" title="机器理解"></a>机器理解</h3><p>机器理解数据集通常向机器提供上下文文档或段落。基于对上下文的理解，机器理解模型可以回答一个问题。</p><p>Jia和Liang是最早考虑文本对抗的人之一，他们将神经机器理解模型作为目标[54]。他们使用斯坦福答题数据集（SQuAD）来评估他们的攻击对神经机器理解模型的影响。SQuAD是一个被广泛认可的机器理解基准数据集。[142]沿用了前人的工作，也在SQuAD数据集上进行了研究。虽然参考文献[14]的重点是开发一个健壮的机器理解模型，而不是攻击MC模型，但作者使用对抗样本来评估他们提出的系统。他们使用MovieQA多选题回答数据集[134]进行评估。参考文献[19]针对可分化神经计算机(DNC)的攻击，DNC是一种具有DNN的新型计算机。它利用bAbI任务对逻辑问题回答的攻击进行了评估。</p><h3 id="文本摘要"><a href="#文本摘要" class="headerlink" title="文本摘要"></a>文本摘要</h3><p>文本摘要的目标是用简洁的表达方式来概括给定文档或段落的核心意思。参考文献[22]评估了他们对包括文本摘要在内的多种应用的攻击，它使用DUC2003,18 DUC2004,19和Gigaword20来评估对抗样本的有效性。</p><h3 id="文本蕴涵"><a href="#文本蕴涵" class="headerlink" title="文本蕴涵"></a>文本蕴涵</h3><p>文本蕴涵（Textual entailment，TE）的基本任务是判断两个文本片段有指向关系。当认为一个文本片段真实时，可以推断出另一个文本片断的真实性。也就是指，一个文本片段蕴涵了另一个文本片段的知识。可以分别称蕴涵的文本(entailing texts)为文本(text)，被蕴涵的文本(entailed texts)为假设(hypothesis)。文本蕴涵关系不是纯粹的逻辑推理，它的条件更为宽松，可以这样定义：如果一个人读了$t$能够推论$h$<strong>非常可能</strong>是真实的，那么$t$蕴涵 $h$ $(t\Rightarrow h)$。</p><p>[56]在两个entailment数据集上评估了各种模型，Stanford Natural Lauguage Inference（SNLI）[15]和SciTail[58]。[91]也使用了SNLI数据集，还使用了MultiNLI[144]数据集。</p><h3 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h3><p>词性标注（Part-of-Speech tagging 或 POS tagging）是指对于句子中的每个词都指派一个合适的词性，也就是要确定每个词是名词、动词、形容词或其他词性的过程，又称词类标注或者简称标注。词性标注是自然语言处理中的一项基础任务，在语音识别、信息检索及自然语言处理的许多领域都发挥着重要的作用。</p><p>[151]采用了参考文献[93]中的方法，通过引入对抗性训练来构建一个更健壮的神经网络，但它将该策略（稍作修改）应用于POS标记。通过对正常和对抗样本的混合训练，作者发现对抗样本不仅有助于提高标记精度，而且有助于下游的依赖性解析任务，在不同的序列标记任务中普遍有效。</p><p>他们在评估中使用的数据集包括：《华尔街日报》（WSJ）部分的Penn Treebank（PTB）[85]和Universal Dependencies（UD）v1.2[99]的树库。</p><h3 id="对话生成"><a href="#对话生成" class="headerlink" title="对话生成"></a>对话生成</h3><p>对话生成（Dialogue Generation）是Siri和Alexa等现实世界虚拟助手的基本组件，它是为用户给出的帖子自动生成响应的文本生成任务。</p><p>[98]是最早攻击对话生成模型的作品之一。它使用Ubuntu对话语料库[82]和动态知识图谱网络与协作通信代理(CoCoA)数据集[47]来评估其两种攻击策略。</p><h3 id="跨模态应用"><a href="#跨模态应用" class="headerlink" title="跨模态应用"></a>跨模态应用</h3><p>[129]使用希拉里-克林顿的电子邮件以图像的形式对OCR系统进行了对抗性实例的评估。它还使用Rotten Tomatoes和IMDB评论数据集对NLP应用进行了攻击。[153]中的工作攻击了为场景文本识别设计的神经网络。作者在三个裁剪字图像识别的标准基准上进行了实验，即街景文本数据集（SVT）[139]ICDAR 2013数据集（IC13）[57]和IIIT 5K字数据集（IIIT5K）[92]。[20]对图像字幕神经模型进行攻击。使用的数据集是微软COCO（MSCOCO）数据集[77]。[148]致力于攻击图像字幕和视觉问题回答的神经模型的问题。对于第一个任务，它使用Visual Genome数据集[64]。对于第二个任务，它使用参考文献[6]中收集和处理的VQA数据集。[125]致力于视觉-语义嵌入（Visual-Semantic Embedding）应用，其中使用了MSCOCO数据集。[18]针对语音识别问题。使用的数据集是Mozilla Common Voice数据集</p><h3 id="多应用"><a href="#多应用" class="headerlink" title="多应用"></a>多应用</h3><p>一些作品将他们的攻击方法改编成不同的应用，即评估他们的方法在不同应用中的可转移性。</p><p>[22]对序列到序列模型进行了攻击。具体来说，它评估了对两个应用的攻击：文本摘要和机器翻译。对于文本摘要，如前所述，它使用了三个数据集DUC2003、DUC2004和Gigaword。对于机器翻译，它采样了一个子集形式WMT’16多模态翻译数据集。参考文献[53]提出了生成句法对抗性的译文，并对情感分析和文本包含应用的攻击进行了评估。它使用SST进行情感分析，使用SICK[86]进行文本缩略。参考文献[157]是一种在神经模型上生成对抗样本的通用方法。研究的应用包括图像分类(MINIST数字图像数据集)、文本缩略(SNLI)和机器翻译。参考文献[93]评估了对五个数据集的攻击，涵盖了情感分析（IMDB电影评论、Elec产品评论、Rotten Tomatoes电影评论）和文本分类（DBpedia本体、RCV1新闻文章）。参考文献[127]的目标是情感分析和视觉问题回答。对于情感分析，它使用Rotten Tomato电影评论和IMDB电影评论数据集。对于视觉问题回答，它在Zhu等人[158]提供的数据集上进行测试。</p><h1 id="五、DEFENSE"><a href="#五、DEFENSE" class="headerlink" title="五、DEFENSE"></a>五、DEFENSE</h1><p>为神经网络生成对抗样本的一个重要目的是利用这些对抗样本来增强模型的鲁棒性[40]。在文本DNN中，有两种常见的方式来实现这一目标：对抗性训练和知识蒸馏。对抗性训练在模型训练过程中加入对抗样本。知识蒸馏则是对神经网络模型进行操作，训练一个新的模型。</p><p>在本节中，我们将介绍一些属于这两个方向的代表性研究。关于机器学习和深度学习模型及应用的更全面的防御策略，请参考参考文献[2，13]。</p><h2 id="对抗性训练"><a href="#对抗性训练" class="headerlink" title="对抗性训练"></a>对抗性训练</h2><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>数据增强将原始数据扩充以对抗样本，试图在训练阶段令模型接收更多样本的训练。在被攻击的DNN上用对抗样本进行额外训练，用来对抗黑盒攻击。</p><p>[54]的作者试图通过在包含对抗样本的增强数据集上进行训练来增强阅读理解模型。实验表明，数据增强对使用相同对抗样本的攻击是有效的、稳健的。然而，他们的工作也证明了这种增强策略在面对其他类型的对抗样本的攻击时仍然是脆弱的。[142]也有类似的想法来增强训练数据集，但选择了信息量更大的对抗样本。[56]中的工作是用对抗样本来训练text entailment系统，使系统更加健壮。作者提出了三种方法来生成更多具有多样化特征的数据。(1)基于知识的，用几个给定的知识库中提供的超词/同义词（hypernym/hyponym）替换单词;(2)手工制作，在现有的entailment中增加否定词;(3)基于神经模型的，利用seq2seq模型，通过强制执行损失函数来衡量原始假说和预测假说之间的交叉熵来生成entailment实例。在训练过程中，他们采用了生成式对抗网络的思想，训练一个判别器和一个生成器，并在判别器的优化步骤中加入对抗实例。[12]中的工作探索了另一种数据增强的方式。它将平均字符嵌入作为一个词的表示，并将其纳入到输入中。这种方法对字符加扰如swap、mid和Rand等本质上不敏感，因此可以抵御工作中提出的这些加扰攻击造成的噪声。但是，这种防御对其他不扰乱字符命令的攻击无效。</p><p>表5给出了参考文献[54]中对抗性训练的有效性的一个例子。</p><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Adversarial-Attacks-on-Deep-Learning-Models-in-Natural-Language-Processing-A-Survey/adversarial_training_example_table.png" alt></p><p>作者指出，在对对抗样本进行训练时，需要精心设计对抗样本，以改进模型。从表中两种攻击方法的结果可以看出这一点。</p><h3 id="模型正则化"><a href="#模型正则化" class="headerlink" title="模型正则化"></a>模型正则化</h3><p>模型正则化（Model regularization）强制令对抗样本作为正则项，在训练网络时进行联合优化。</p><script type="math/tex; mode=display">\min \left(J(f(x), y)+\lambda J\left(f\left(x^{\prime}\right), y\right)\right)</script><p>其中$\lambda$是超参数。</p><p>受[40]启发，[93]以线性近似的方式构建对抗式训练，</p><script type="math/tex; mode=display">\begin{array}{c}-\log p\left(\left.y|x+-\epsilon g /||g|\right|_{2}, ; \theta\right) \\g=\partial_{x} \log p(y \mid x ; \hat{\theta})\end{array}</script><p>其中$|g|_{2}$是$L_2$范数正则项，$\theta$是神经网络的参数，$\hat{\theta}$是$\theta$的一份静态拷贝。</p><p>与[40]不同的是，作者以词嵌入的方式进行了对抗生成和训练。此外，他们还扩展了之前关于攻击图像深度神经模型的工作[94]，其中局部分布平滑度（local distribution smoothness, LDS）被定义为两个分布（原始数据和对抗数据）的KL散度的负值。LDS衡量了模型对本地和 “虚拟 “对抗方向的扰动的鲁棒性。对抗样本被计算为模型分布对KL散度最敏感的方向。他们还将这种攻击策略应用在词嵌入上，并通过添加对抗例作为正则器进行对抗性训练。</p><p>[118]中的工作沿用了参考文献[93]的思想，并在LSTM上扩展了对抗性训练。作者沿用了FGSM，将对抗性训练作为正则器加入其中。但为了提升对抗样本的可解释性，即对抗例的词嵌入应该是词汇中的有效词嵌入，他们引入了一个方向向量，将扰动嵌入与有效词嵌入关联起来。</p><p>[145]简单地采用了参考文献[93]中利用的正则器，但将扰动应用于预先训练好的词嵌入，并且应用于不同的任务：关系提取。其他采用参考文献[93]的类似工作还有参考文献[10，118，145，151]。</p><h3 id="鲁棒优化"><a href="#鲁棒优化" class="headerlink" title="鲁棒优化"></a>鲁棒优化</h3><p>鲁棒优化（Robust Optimization）是提升深度学习系统鲁棒性的方法。</p><p>Madry等[84]将DNN模型学习投向min-max(鞍点)公式的鲁棒优化，它是由一个内侧非凹最大化问题(攻击)和一个外侧非凸最小化问题(防御)组成。根据Danskin定理，内侧最大化器处的梯度与min-max问题的下降方向相对应，因此优化仍可应用反向传播进行。该方法通过普遍训练和学习，成功地证明了DNN对对抗性图像的鲁棒性。</p><p>参考文献[3]采用了该思想，并应用在处理离散数据的恶意软件检测DNN上。其学习目标拟定为</p><script type="math/tex; mode=display">\theta^{*}=\arg \min _{\theta} \mathbb{E}_{(x, y) \sim D}\left[\max _{x^{\prime} \in S(x)} L\left(\theta, x^{\prime}, y\right)\right]</script><p>其中，$S(x)$是一组二元指示向量，用于保存恶意软件$x$的功能，$L$为原始分类模型的损失函数，$y$为真实标签，$\theta$为可学习参数，$D$表示数据样本$x$的分布。值得注意的是，本文所提出的鲁棒优化方法是一个通用的框架，在这个框架下，其他的对抗性训练策略都有自然的解释。</p><h2 id="模型蒸馏"><a href="#模型蒸馏" class="headerlink" title="模型蒸馏"></a>模型蒸馏</h2><p>Papernot等人[106]提出了模型蒸馏（distillation）作为另一种可能的对抗性例子的防御方法。其原理是利用原DNN的softmax输出（如classfication DNN中的类概率）来训练第二个DNN，该DNN与原DNN的结构相同。同时通过引入温度参数T来修改原DNN的softmax输出。</p><script type="math/tex; mode=display">q_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{k} \exp \left(z_{k} / T\right)}</script><p>其中$z_i$是softmax层的输入，$T$控制着知识压缩的程度，$T=1$时上式退化成普通softmax函数，$T$很大时，$q_i$接近均匀分布；$T$很小时，函数会倾向于输出极值。</p><p>[44]对离散数据上的DNN采用蒸馏防御，并应用高温T，因为高温softmax被证明可以降低模型对小扰动的敏感性[106]。作者用原始数据集的增强和原始DNN的softmax输出训练了第二个DNN。从评价中，他们发现对抗式训练比使用蒸馏法更有效（啊这？）。</p><h1 id="六、DISCUSSIONS-AND-OPEN-ISSUES"><a href="#六、DISCUSSIONS-AND-OPEN-ISSUES" class="headerlink" title="六、DISCUSSIONS AND OPEN ISSUES"></a>六、DISCUSSIONS AND OPEN ISSUES</h1><p>与在DNN上生成图像对抗性例子相比，生成文本对抗性例子的历史相对较短，因为在离散数据上进行扰动，同时保留有效的句法、语法和语义更具挑战性。</p><p>我们在本节中讨论了一些问题，并对未来的发展方向提出了建议。</p><h2 id="可感知性"><a href="#可感知性" class="headerlink" title="可感知性"></a>可感知性</h2><p>可感知性（Perceivability）</p><p>图像像素的扰动通常很难被感知，因此不影响人类的判断，但却能骗过深度神经网络。然而，对文本的扰动是显而易见的，无论扰动是翻转字符还是改变单词。无效词和语法错误很容易被人类识别，也很容易被语法检查软件检测出来，因此这种扰动很难攻击真正的NLP系统。</p><p>然而，许多研究工作都会产生这种类型的对抗性例子。只有在利用对抗性例子来对被攻击的DNN模型进行健壮化的情况下，这种方法才是可以接受的。</p><p>从语义保护的角度来看，改变一个句子中的一个词有时会极大地改变其语义，并且很容易被人类发现。对于NLP的应用，如阅读理解、情感分析等，为了不改变应该的输出，需要精心设计对抗性例子。否则，正确的输出和扰动的输出都会改变，违反了生成对抗性例子的目的。</p><p>只有少数作品考虑了这个约束。因此，对于实际的攻击，我们需要提出一些方法，使扰动不仅无法察觉，而且还能保留正确的语法和语义。</p><h2 id="可迁移性"><a href="#可迁移性" class="headerlink" title="可迁移性"></a>可迁移性</h2><p>可迁移性（Transferability）是对抗样本的普遍性质。它反映了攻击算法的泛化能力（通用性）。可迁移性的含义为，算法为一个模型生成的对抗样本也能影响另外一个模型（即跨模型泛化）或影响另外一个数据集（即跨数据泛化）的能力。由于深度神经网络的细节对攻击方法影响不大，因此这一特性在黑盒攻击中更常被利用。也有研究证明[81]无目标攻击生成的对抗样本比目标攻击生成的样本更具可迁移性。</p><p>可迁移性可以从三个层次来度量：(i)相同架构，不同数据；(ii)不同架构，相同应用；(iii)不同架构，不同数据[154]。虽然目前关于文本攻击的工作涵盖了这三个层次，但与原模型相比，转移攻击另一个模型表现的算法性能仍然大幅下降，即泛化能力差。</p><h2 id="自动化程度"><a href="#自动化程度" class="headerlink" title="自动化程度"></a>自动化程度</h2><p>在白盒攻击中，利用DNN的损失函数可以自动识别文本中受影响最大的点（如字符、单词），然后对这些点进行攻击，自动修改相应的文本。在黑盒攻击中，有些攻击，如 “替换”操作会训练一个替换的DNN，并对替换的DNN应用白盒攻击策略。以上操作都可以自动实现。</p><p>然而，大多数其他作品都是以人工的方式制作对抗性例子。例如[54]将人工选择的无意义段落进行连接，以愚弄阅读理解系统，发现被攻击的DNN的漏洞。很多研究工作都效仿[54]，不以实际攻击为目的，更多的是研究目标网络的健壮性。<strong>这些手工工作既耗时又不实用</strong>。</p><h2 id="新架构"><a href="#新架构" class="headerlink" title="新架构"></a>新架构</h2><p>虽然大多数常见的文本DNN已经从对抗性攻击的角度得到了关注，但许多DNN至今没有受到攻击，例如生成式神经模型：生成式对抗网络（GANs）和变异自动编码器（VAEs）。在NLP中，它们被用来生成文本。深度生成模型需要更复杂的技能进行模型训练。这将解释这些技术到目前为止主要被对抗性攻击所忽视。未来的工作可以考虑为这些生成式DNN生成对抗性的例子。</p><p>另一个例子是可分化神经计算机（DNC）。到目前为止，只有一项工作对DNC进行了攻击[19]。</p><p>注意力机制以某种方式成为大多数顺序模型中的标准组件。但一直没有研究该机制本身的工作。相反，作品要么是攻击包含注意力的整体系统，要么是利用注意力分数来识别扰动的词[14]。</p><h2 id="迭代生成-vs-一次生成"><a href="#迭代生成-vs-一次生成" class="headerlink" title="迭代生成 vs. 一次生成"></a>迭代生成 vs. 一次生成</h2><p>迭代攻击根据被攻击的DNN模型输出的梯度反复搜索和更新扰动。因此，它表现出较高的质量和有效性。也就是说，扰动可以足够小，难以防御。</p><p>然而，这些方法通常需要很长的时间来寻找合适的扰动，为实时攻击带来了障碍。因此，有人提出一次性攻击来解决这个问题。FGSM[40]就是一次性攻击的一个例子。直观地讲，一次性攻击比迭代攻击快得多，但效果较差，而且容易被防御[153]。当在实际应用上设计攻击方法时，攻击者需要仔细考虑攻击的效率和效果之间的权衡。</p><h1 id="七、CONCLUSION"><a href="#七、CONCLUSION" class="headerlink" title="七、CONCLUSION"></a>七、CONCLUSION</h1><p>本文首次在深度神经网络上生成文本对抗性实例的方向上进行了全面的调查。</p><p>我们回顾了最近的研究工作，并制定分类方案来整理现有的文献。</p><p>此外，我们还从不同方面对其进行了总结和分析。</p><p>我们试图为研究人员提供一个很好的参考，以深入了解该研究课题中的挑战、方法和问题，并对未来的发展方向有所启示。</p><p>我们希望在了解对抗性攻击的基础上，提出更多稳健的深度神经模型。</p>]]></content>
    
    
    <categories>
      
      <category>paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Robustness</tag>
      
      <tag>Deep Neural Networks</tag>
      
      <tag>Adversarial Attacks</tag>
      
      <tag>survey</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读笔记】 Fuzz Testing based Data Augmentation to Improve Robustness of Deep Neural Networks</title>
    <link href="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/"/>
    <url>/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/</url>
    
    <content type="html"><![CDATA[<p>通过模糊测试的方法进行数据增强，数据增强新思路。新加坡国立大学作品，被ICSE’2020接收。<br><!--more---></p><h1 id="内容概要"><a href="#内容概要" class="headerlink" title="内容概要"></a>内容概要</h1><ul><li>增强DNN的训练数据，从而增强其鲁棒性</li><li>将DNN数据扩充问题视为优化问题</li><li>使用遗传搜索来生成最合适的输入数据变体，用于训练DNN</li><li>学习识别跳过增强来加速训练</li><li>improve the robust accuracy of the DNN</li><li>reduce the average DNN training time by 25%, while still improving robust accuracy</li></ul><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>过拟合产生的问题：</p><ol><li>泛化性降低，测试集效果差；</li><li>鲁棒性不足，不能抵御样本微小扰动</li></ol><p>鲁棒性有两种：</p><ol><li>对抗鲁棒性，应对的是人为构造的扰动；</li><li>自然扰动鲁棒性，应对自然条件的变化。</li></ol><p>本文关注的是后一种鲁棒，采用数据增强手段模拟不同自然条件下的样本变化</p><h2 id="数据增强：传统软件"><a href="#数据增强：传统软件" class="headerlink" title="数据增强：传统软件"></a>数据增强：传统软件</h2><p>improves the generalization of generated programs by augmenting existing test suites</p><p>测试用例生成技术：随机测试，基于搜索的进化测试，符号执行，灰箱模糊测试等</p><p>AFL：涵盖更多的程序路径使我们能够找到代表性的测试并涵盖更多的程序功能</p><h2 id="数据增强：提升模型鲁棒性——研究现状"><a href="#数据增强：提升模型鲁棒性——研究现状" class="headerlink" title="数据增强：提升模型鲁棒性——研究现状"></a>数据增强：提升模型鲁棒性——研究现状</h2><p>基于梯度下降的鲁棒优化：</p><ul><li>尝试根据损失函数生成最差的变体，并将其添加到训练集中。</li><li>在自然环境变化（对于视觉应用）中突出显示的空间变换的输入空间是高度非凸的</li></ul><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-10-37.png" alt></p><p>几乎所有用于提高鲁棒性的技术都会通过分析训练后的模型并随后生成对抗性示例在新数据上重新训练模型</p><h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><ul><li>将数据扩充形式化为优化问题，利用模糊测试（遗传算法）求解</li><li>提出选择性扩充策略，仅选择部分数据点进行扩充</li></ul><h1 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h1><h2 id="模糊测试"><a href="#模糊测试" class="headerlink" title="模糊测试"></a>模糊测试</h2><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-11-29.png" alt></p><h2 id="Robustness-of-DNN"><a href="#Robustness-of-DNN" class="headerlink" title="Robustness of DNN"></a>Robustness of DNN</h2><p>DNN很容易遭受微小扰动的影响，产生完全不同的决策。DNN抵御输入扰动的能力被认为是鲁棒性。</p><p>由于这项工作聚焦于自然生成的而不是人工合成的扰动，因此像GAN等技术不被考虑在内。</p><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p>数据增强是一种扩充数据集、避免样本少导致过拟合的有效方法。增强效果受增强方法影响很大。一般认为数据增强后训练的模型鲁棒性能得到一定程度的提升。</p><h1 id="SENSEI工具"><a href="#SENSEI工具" class="headerlink" title="SENSEI工具"></a>SENSEI工具</h1><h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><p>生成增强样本的过程，是多目标优化过程。</p><p>一方面求扰动δ，使得损失函数L最大；另一方面求模型参数θ，使损失函数L最小；求得此时的δ和θ。本质是一个二元优化找鞍点的问题。</p><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-20-58.png" alt></p><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-21-07.png" alt></p><h2 id="优化思路"><a href="#优化思路" class="headerlink" title="优化思路"></a>优化思路</h2><p>基于模糊测试的方法（如引导搜索）能更有效地找到要训练的数据点的最佳Mutator，从而提高鲁棒性</p><p>并非训练数据集中的所有数据点都很难学习。一些数据点代表训练集中的理想示例，而另一些则令人困惑；</p><p>对所有这些点进行相同的处理可能会浪费宝贵的训练时间。因此仅在具有挑战性的数据点上花费扩充工作。</p><h2 id="整体算法"><a href="#整体算法" class="headerlink" title="整体算法"></a>整体算法</h2><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-21-37.png" alt></p><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-21-43.png" alt></p><p>Optimal Augmentation Module<br>选择最难的样本（L13~16）<br>Selective Augmentation Module<br>跳过突变不大的样本（L10~12,17）<br>数据增强过程是在训练时即时发生的</p><h2 id="遗传算法部分"><a href="#遗传算法部分" class="headerlink" title="遗传算法部分"></a>遗传算法部分</h2><p>将染色体表示为一组操作，该操作将应用于给定输入以获得真实的变化</p><p>将x旋转1度，然后将其平移一个像素，模拟相机在现实生活中的角度和移动，来得出图像（x）的真实变化（x’）</p><p>种群是代表当前解决方案子集的一组染色体</p><p>种群由两个遗传算子组成：突变和杂交</p><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-22-47.png" alt></p><p>通过合并两个随机选择的现有染色体来完成交叉以创建新染色体</p><ul><li>c1 = {旋转：1，平移：2，剪切：-0.15}和</li><li>c2 = {旋转：-1，平移：-3，剪切：0.1}</li><li>C = {旋转：1, 平移：-3，剪切：0.1}</li></ul><p>杂交算子在1和染色体长度（l）之间生成随机数r，取c1的1到r部分，与c2的r+1到l部分拼合成新的染色体</p><p>突变算子通过随机改变染色体中的单个操作（改变参数）来执行突变。</p><p>始终将生成的转换向量（染色体）应用于原始图像（而不是应用于已转换的数据），以防止生成的数据不真实</p><p>生成新种群后，将对其进行评估，并且仅将最佳集合作为当前种群传递给下一代（原算法L17）</p><p>适应度函数的设计在GA中起着重要作用，以测量给定解决方案的质量<br>根据DNN的经验损失定义适应度函数</p><script type="math/tex; mode=display">f_{\text {loss}}\left(x^{\prime}\right)=L\left(\theta, x^{\prime}, y\right)</script><p>在DNN的增强训练中应使用遭受DNN损失更大的变体，以使DNN更加健壮</p><h2 id="选择性增强"><a href="#选择性增强" class="headerlink" title="选择性增强"></a>选择性增强</h2><p>Sensei-SA会跳过已由M鲁棒分类的数据点<br>基于分类的鲁棒性：模型正确地分类了x和所有未改变类别的Mutator（x′）<br>基于损失的鲁棒性：x的预测损失或未改变类别（x’）的任何预测损失不大于损失阈值</p><p>如果种子是鲁棒的，则Sensei-SA不会对其进行数据增强；除非在随后的训练中将种子错误地分类，或预测损失小于阈值</p><h2 id="图像扰动"><a href="#图像扰动" class="headerlink" title="图像扰动"></a>图像扰动</h2><p>仿射变换操作、像素操作<br>旋转（x，d）：在[-30，30]范围内将x旋转d度。<br>平移（x，d）：在图像大小的[-10％，10％]范围内水平或垂直将x按d像素平移。<br>剪切（x，d）：水平剪切x，剪切因子d在[-0.1，0.1]范围内。<br>缩放（x，d）：以[0.9,1.1]的缩放系数d放大/缩小x<br>亮度（x，d）：在[-32，32]范围内为x的每个像素统一加减一个值<br>对比度（x，d）：将x的每个像素的RGB值按[0.8，1.2]范围内的因子d缩放。</p><h2 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h2><p>鲁棒性精度（robust accuracy）是指测试集中DNN的预测不随任何小的现实扰动而改变的图像比例</p><script type="math/tex; mode=display">\text {robust accuracy}=\frac{\text {nRobustInstances}}{\text {nInstances}}</script><h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><h2 id="1-Sensei是否可以有效解决鞍点问题？"><a href="#1-Sensei是否可以有效解决鞍点问题？" class="headerlink" title="1.Sensei是否可以有效解决鞍点问题？"></a>1.Sensei是否可以有效解决鞍点问题？</h2><p>关键是检查Sensei是否确实有效比最先进的技术更有效地找到损耗最大的变体<br>W-10在每一步为每个图像随机生成十个扰动，并用模型表现最差的图像替换原始图像<br>结果表明，Sensei更有效地解决了内部最大化问题<br><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-26-16.png" alt></p><h2 id="2-Sensei是否比基于对抗样本再训练方法表现更好"><a href="#2-Sensei是否比基于对抗样本再训练方法表现更好" class="headerlink" title="2.Sensei是否比基于对抗样本再训练方法表现更好"></a>2.Sensei是否比基于对抗样本再训练方法表现更好</h2><p>对抗样本再训练方法：<br>i）使用原始训练数据训练模型；<br>ii）通过我们的转换（即使DNN蒙混的变体）生成对抗性示例；<br>iii）选择最佳对抗性示例，添加训练数据，然后重新训练5个模型</p><p><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-26-58.png" alt></p><h2 id="3-Sensei能否提高鲁棒性同时保持精度"><a href="#3-Sensei能否提高鲁棒性同时保持精度" class="headerlink" title="3.Sensei能否提高鲁棒性同时保持精度"></a>3.Sensei能否提高鲁棒性同时保持精度</h2><p>mixup 是一种数据增强方法。mixup和Sensei都总体上改善了泛化性能。实际上，在标准泛化方面，mixup比Sensei更好；但是，在改善现实世界中自然发生的Mutator的鲁棒性方面，mixup效果不佳。<br><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-27-21.png" alt></p><h2 id="4-选择性数据增强（-Sensei-SA-）的效果和效率"><a href="#4-选择性数据增强（-Sensei-SA-）的效果和效率" class="headerlink" title="4.选择性数据增强（ Sensei-SA ）的效果和效率"></a>4.选择性数据增强（ Sensei-SA ）的效果和效率</h2><p>与W-10相比，Sensei-SA减少了25％的训练时间，而鲁棒性则提高了3％。<br><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-27-38.png" alt></p><h2 id="5-对超参数的敏感程度"><a href="#5-对超参数的敏感程度" class="headerlink" title="5.对超参数的敏感程度"></a>5.对超参数的敏感程度</h2><p>突变体集合规模最好为10~15</p><p>神经元覆盖率在鲁棒性评估方面表现出与损失函数相似的性能</p><p>基于神经元覆盖的适应度函数比基于损失的适应度函数将训练时间增加了50％。 原因是神经元覆盖率的计算比训练损失要昂贵<br><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-27-55.png" alt><br><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-28-00.png" alt></p><p>在判断样本鲁棒性上，基于损失的选择均优于基于分类的选择。</p><p>基于损失的选择足以跳过足够数量的数据点，从而平均减少25％的训练时间。<br><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-28-12.png" alt></p><p>Loss threshold越大，训练时间越短，然而鲁棒准确性也下降</p><p>Cifar10数据集比其他数据集对loss threshold更为敏感<br><img src="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzz-Testing-based-Data-Augmentation-to-Improve-Robustness-of-Deep-Neural-Networks/2020-06-27-01-28-22.png" alt></p><h1 id="Threads-to-Validity"><a href="#Threads-to-Validity" class="headerlink" title="Threads to Validity"></a>Threads to Validity</h1><p>our results may not generalize to other datasets, or models, or for other applications</p><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><h2 id="测试充足性指标"><a href="#测试充足性指标" class="headerlink" title="测试充足性指标"></a>测试充足性指标</h2><ul><li>DeepXplore，Neuron Coverage</li><li>DeepGauge，k截面神经元覆盖率和神经元边界覆盖率</li><li>惊奇度 surprise adequacy</li><li>MODE，执行状态差分分析以识别模型的错误特征，然后在此基础上执行训练输入选择</li></ul><h2 id="测试用例生成"><a href="#测试用例生成" class="headerlink" title="测试用例生成"></a>测试用例生成</h2><ul><li>对抗性测试，有选择地修改几个像素，将对抗性实例的生成建模为优化问题，并使用一阶优化算法解决优化问题。生成机器学习也可以用来生成对抗性输入</li><li>但是对于自然产生的变化（旋转和平移等），由于其变换非凸，不利于一阶优化</li><li>TensorFuzz，不适合我们的数据增强驱动的鲁棒性训练目标</li><li>DeepTest和DeepRoad，通过metamorphic testing来生成暴露DNN bug的测试用例</li></ul><h2 id="测试合并策略，Test-incorporation-strategy"><a href="#测试合并策略，Test-incorporation-strategy" class="headerlink" title="测试合并策略，Test incorporation strategy"></a>测试合并策略，Test incorporation strategy</h2><ul><li>绝大多数DNN测试用例生成技术首先使用经过训练的DNN生成测试（或对抗实例），然后使用它们重新训练DNN，以提高其准确性或健壮性。</li><li>AutoAugment ，使用强化学习在搜索空间中找到最佳的扩增策略，从而使神经网络达到最高精度</li><li>Mixup，是最近提出的最先进的数据增强技术，但是在良性变异中，鲁棒性不佳。</li><li>Engstrom，除了没用遗传算法之外都一样</li></ul><h2 id="稳健性模型，Robust-models"><a href="#稳健性模型，Robust-models" class="headerlink" title="稳健性模型，Robust models"></a>稳健性模型，Robust models</h2><ul><li>基于正则化的白盒方法，通过修改DNN损失函数，并在标准经验损失中加入一个不变量来正则化，提高深度神经网络模型的鲁棒性</li></ul>]]></content>
    
    
    <categories>
      
      <category>paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Data Augmentation</tag>
      
      <tag>Robustness</tag>
      
      <tag>Deep Neural Networks</tag>
      
      <tag>Fuzz Testing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读笔记】EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks</title>
    <link href="/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91EDA-Easy-Data-Augmentation-Techniques-for-Boosting-Performance-on-Text-Classification-Tasks/"/>
    <url>/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91EDA-Easy-Data-Augmentation-Techniques-for-Boosting-Performance-on-Text-Classification-Tasks/</url>
    
    <content type="html"><![CDATA[<p>这篇论文介绍了一个文本领域的数据增强工具，提出了一些数据增强方法。<br><!--more---></p>]]></content>
    
    
    <categories>
      
      <category>paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Data Augmentation</tag>
      
      <tag>Text Classification</tag>
      
      <tag>Deep Neural Networks</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Datawhale——SVHN——Task05：模型集成</title>
    <link href="/2020/06/02/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task05%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90/"/>
    <url>/2020/06/02/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task05%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90/</url>
    
    <content type="html"><![CDATA[<p>三个臭皮匠，顶个诸葛亮。<br><!--more---></p><h2 id="1-集成学习"><a href="#1-集成学习" class="headerlink" title="1. 集成学习"></a>1. 集成学习</h2><p>集成学习的一般结构，是首先产生一组“个体学习器”，再用<strong>某种策略</strong>将其结合起来。类似于“三个臭皮匠，顶个诸葛亮”的思想。</p><p><img src="/2020/06/02/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task05%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90/2020-06-02-22-32-43.png" alt></p><p>如果自己的<strong>个体学习器</strong>性能不是很令人满意，使用集成学习将能够提升一定的性能，集成学习器一般都能够获得比个体学习器要好的效果。</p><p>集成学习效果要提升，要尽可能满足两个条件：</p><ol><li>个体学习器性能不太差；</li><li>个体学习器之间不能太相似，结构、所用数据差异越大越好。</li></ol><p>可以证明，如果个体学习器之间的决策误差不存在关联，决策相互独立，那么随着个体学习器数量的增多，集成学习器的错误率将指数下降。</p><p>根据个体学习器的生成方式，目前的集成学习方法分成两大类：</p><ol><li>个体学习器之间存在强依赖关系、必须串行生成的序列化方法，代表为Boosting；</li><li>学习器之间不存在强依赖关系、可以并行的方法，代表为Bagging和随机森林。</li></ol><p>集成学习只能在一定程度上提高精度，并需要耗费较大的训练时间。具体的集成学习方法需要与验证集划分方法结合。</p><h3 id="1-1-Boosting"><a href="#1-1-Boosting" class="headerlink" title="1.1 Boosting"></a>1.1 Boosting</h3><p>Boosting算法是一类能将弱学习器提升为强学习器的算法。基本思想是：先利用初始训练集训练一个基本学习器，再基于基本学习器的表现，对训练样本做出调整，改变样本分布，使得先前被分类错误的训练样本在随后受到更多关注。如此重复训练，直到基学习器的数目达到指定的数值。最终将这几个基学习器进行加权结合。</p><h3 id="1-2-Bagging"><a href="#1-2-Bagging" class="headerlink" title="1.2 Bagging"></a>1.2 Bagging</h3><p>欲得到泛化性能强的集成学习模型，个体学习器之间应当相互独立。但是完全独立是做不到的，即便模型架构完全不同、训练数据完全不一样，这些个体学习器也是为了解决同一个任务而训练的，训练数据之间肯定存在关系，从而导致模型的决策存在相关性。因此Bagging算法就是想要尽可能提升个体学习器之间的差异性。</p><p>一种可能的做法是对训练样本进行采样，产生若干个不同的子集，每个子集都训练一个个体学习器。但是这样学习得到的个体学习器都没能获得足够的训练样本，因此我们可以进行折中，采用互相存在交集的分割方法分割数据集，然后训练模型。</p><p>随机森林本质上是许多决策树的集合，其中每棵树都和其他树略有不同。随机森林背后的思想是，每棵树的预测可能都相对较好，但可能对部分数据过拟合。如果构造很多树，并且每棵树的预测都很好，但都以不同的方式过拟合，那么我们可以对这些树的结果取平均值来降低过拟合。既能减少过拟合又能保持树的预测能力，这可以在数学上严格证明。</p><h2 id="2-深度学习中的集成方法"><a href="#2-深度学习中的集成方法" class="headerlink" title="2. 深度学习中的集成方法"></a>2. 深度学习中的集成方法</h2><h3 id="2-1-Dropout"><a href="#2-1-Dropout" class="headerlink" title="2.1 Dropout"></a>2.1 Dropout</h3><p>每个训练批次中，在更新权重之前，随机让一部分的节点停止工作，增加模型训练时的精度提升难度。</p><p><img src="/2020/06/02/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task05%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90/2020-06-02-23-33-21.png" alt></p><p>需要注意的是，训练的时候加dropout，测试的时候以及实际使用时，是不需要dropout的。这就像平时训练的时候腿上绑上沙袋，战时就能够获得更卓越的效果。有效的缓解模型过拟合</p><p><img src="/2020/06/02/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task05%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90/2020-06-02-23-36-15.png" alt></p><p>直观来讲，Dropout法使得每个节点都无法单纯依赖其他节点而滥竽充数，因为随时随地自己的同伴就可能被dropout。这样训练时，每个节点都会学到更多的知识。从而提升整体学习器的性能。因此这也算是集成学习。</p><h3 id="2-2-测试集数据扩增"><a href="#2-2-测试集数据扩增" class="headerlink" title="2.2 测试集数据扩增"></a>2.2 测试集数据扩增</h3><p>测试集数据扩增（Test Time Augmentation，简称TTA）也是常用的集成学习技巧，数据扩增不仅可以在训练时候用，而且可以同样在预测时候进行数据扩增，对同一个样本预测三次，然后对三次结果进行平均。</p><h2 id="3-结果后处理"><a href="#3-结果后处理" class="headerlink" title="3. 结果后处理"></a>3. 结果后处理</h2><ul><li>统计图片中每个位置字符出现的频率，使用规则修正结果；</li><li>单独训练一个字符长度预测模型，用来预测图片中字符个数，并修正结果。</li></ul>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>datawhale</tag>
      
      <tag>Python</tag>
      
      <tag>Ensemble</tag>
      
      <tag>Boosting</tag>
      
      <tag>Bagging</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>numpy拼合数组方法大全</title>
    <link href="/2020/06/01/numpy%E6%8B%BC%E5%90%88%E6%95%B0%E7%BB%84/"/>
    <url>/2020/06/01/numpy%E6%8B%BC%E5%90%88%E6%95%B0%E7%BB%84/</url>
    
    <content type="html"><![CDATA[<p>numpy的一大特色就是其内部的矩阵向量运算。矩阵之间的拼接方法，你掌握多少？<br><!--more---></p><h2 id="1-append拼接"><a href="#1-append拼接" class="headerlink" title="1. append拼接"></a>1. append拼接</h2><p>我们都知道对于Python原生列表list来说，append是最方便的添加元素的方法，一句list.append(elem)就能在列表最后添加一个元素。</p><p>在numpy中，append也是一个直观且好用的方法，np.append(A,B)能够直接拼合两个ndarray数组。</p><p>首先我们新建两个三维数组，一个全为零，一个全为一。</p><pre><code class="lang-python">C = np.zeros((2,2,2))D = np.ones((2,2,2))print(&quot;C: &quot;, C, C.shape)print(&quot;D: &quot;, D, D.shape)C:  [[[0. 0.]  [0. 0.]] [[0. 0.]  [0. 0.]]] shape=(2, 2, 2)D:  [[[1. 1.]  [1. 1.]] [[1. 1.]  [1. 1.]]] shape=(2, 2, 2)</code></pre><p>然后我们采用不同的方法将其拼合在一起。</p><p>首先是append(C,D)这种直观的方法，可以看到C和D都被展开成了一维。</p><pre><code class="lang-python">np.append(C,D)array([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.])</code></pre><p>在很多时候我们希望数组拼接时能够保持原有的维度，按照行拼接/列拼接/其他维度拼接。此时只需要改动append的参数axis即可。</p><pre><code class="lang-python">np.append(C,D,axis=0)array([[[0., 0.],        [0., 0.]],       [[0., 0.],        [0., 0.]],       [[1., 1.],        [1., 1.]],       [[1., 1.],        [1., 1.]]])np.append(C,D,axis=1)array([[[0., 0.],        [0., 0.],        [1., 1.],        [1., 1.]],       [[0., 0.],        [0., 0.],        [1., 1.],        [1., 1.]]])np.append(C,D,axis=2)array([[[0., 0., 1., 1.],        [0., 0., 1., 1.]],       [[0., 0., 1., 1.],        [0., 0., 1., 1.]]])</code></pre><p>对于三维数组，axis=0为层，axis=1为行，axis=2为列。这不难理解，因为确定单位数组中元素位置的坐标就是(层，行，列)</p><h2 id="2-concatenate拼接"><a href="#2-concatenate拼接" class="headerlink" title="2. concatenate拼接"></a>2. concatenate拼接</h2><p>concatenate从字面意义上就让人明白这个函数专门负责数组拼接。不仅仅是两个，还可以负责多个数组一起拼接。理论上来说concatenate的速度和内存消耗都比append要小，但我并没有实际做实验验证。</p><pre><code class="lang-python">np.concatenate((C,D)) # default axis=0array([[[0., 0.],        [0., 0.]],       [[0., 0.],        [0., 0.]],       [[1., 1.],        [1., 1.]],       [[1., 1.],        [1., 1.]]])np.concatenate((C,D), axis=1) # =np.append(C,D,axis=1)array([[[0., 0.],        [0., 0.],        [1., 1.],        [1., 1.]],       [[0., 0.],        [0., 0.],        [1., 1.],        [1., 1.]]])</code></pre><h2 id="3-stack系列"><a href="#3-stack系列" class="headerlink" title="3. stack系列"></a>3. stack系列</h2><p>stack系列函数包括np.stack/hstack/vstack/dstack/column_stack/row_stack，顾名思义，hstack是按照横向拼接，vstack竖着拼接，dstack则是层叠数组。其实我最烦这种抽象描述了，因为二维数组和三维数组/高维数组的抽象描述根本不一致，还是axis好。不明白axis的同学可以看<a href="https://superlova.github.io/2020/05/19/numpy%E4%B8%ADaxis%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/">这篇文章</a>。</p><pre><code class="lang-python">np.hstack((C,D)) # = np.append(C,D,axis=1) = np.column_stack()array([[[0., 0.],        [0., 0.],        [1., 1.],        [1., 1.]],       [[0., 0.],        [0., 0.],        [1., 1.],        [1., 1.]]])np.vstack((C,D)) # =np.append(C,D,axis=0) = np.row_stack()array([[[0., 0.],        [0., 0.]],       [[0., 0.],        [0., 0.]],       [[1., 1.],        [1., 1.]],       [[1., 1.],        [1., 1.]]])np.dstack((C,D)) # =np.append(C,D,aixs=2)array([[[0., 0., 1., 1.],        [0., 0., 1., 1.]],       [[0., 0., 1., 1.],        [0., 0., 1., 1.]]])</code></pre><h2 id="4-np-r"><a href="#4-np-r" class="headerlink" title="4. np.r_[]"></a>4. np.r_[]</h2><p>神奇的numpy总能给出神奇的解法。np.r<em>是构建数组/拼合数组的最简便写法，但不一定是好理解的。这种写法和之前写的append没什么不同，但是更加简洁。你也可以使用np.r</em>做出更加复杂的功能。</p><p>一言以蔽之，np.r_[]表达式能够快速使得多个在中括号里面的array/array切片，按照axis=0拼接起来。</p><p>np.r_[]存在两种使用情况：</p><ol><li>如果中括号内部是由若干逗号(comma,)分隔的array，就将他们按照axis=0拼接起来。</li><li>如果中括号内部包括矩阵切片(slices)或者标量(scalars)，就将他们全部变成一维数组首尾相接。</li></ol><p><strong>注意：</strong></p><ul><li>中括号<code>[3:6:1]</code>内部代表的切片，其含义相当于<code>np.arange(3,6,1)</code>，即在<code>[3,6)</code>范围内，从3开始走一步取一个元素，也就是<code>[3,4,5]</code>。</li><li>中括号<code>[0:5:3j]</code>在最后加了字母<code>j</code>，相当于<code>np.linspace(0,5,3,endpoint=True)</code>，在<code>[0,5]</code>范围内，均匀地取三个元素。</li></ul><pre><code class="lang-python">np.r_[C,D] # =np.append(C,D,axis=0)array([[[0., 0.],        [0., 0.]],       [[0., 0.],        [0., 0.]],       [[1., 1.],        [1., 1.]],       [[1., 1.],        [1., 1.]]])np.r_[0:10:3, 0:5:4j]array([0.        , 3.        , 6.        , 9.        , 0.        ,       1.66666667, 3.33333333, 5.        ])</code></pre><p>在中括号内，如果最开始是一个<strong>特定的字符串</strong>，np.r_会试图根据字符串的含义，改变其输出格式。</p><ul><li><code>np.r_[&#39;r&#39;, index_expression]</code>和<code>np.r_[&#39;c&#39;, index_expression]</code>将输出从array类型转变成matrix类型。<code>np.r_[&#39;c&#39;, index_expression]</code>会把一维index_expression组装成N*1的列向量。</li></ul><pre><code class="lang-python">np.r_[&quot;r&quot;, 0:10:3, 0:5:4j]matrix([[0.        , 3.        , 6.        , 9.        , 0.        ,         1.66666667, 3.33333333, 5.        ]])np.r_[&quot;c&quot;, 0:10:3, 0:5:4j]matrix([[0.        ],        [3.        ],        [6.        ],        [9.        ],        [0.        ],        [1.66666667],        [3.33333333],        [5.        ]])</code></pre><ul><li><code>np.r_[&quot;n&quot;, index_expression]</code>前面字符串是整数，则拼接时将会按照axis=n进行拼接。</li></ul><pre><code class="lang-python">np.r_[&quot;-1&quot;,C,D]array([[[0., 0., 1., 1.],        [0., 0., 1., 1.]],       [[0., 0., 1., 1.],        [0., 0., 1., 1.]]])</code></pre><h2 id="5-np-c"><a href="#5-np-c" class="headerlink" title="5. np.c_"></a>5. np.c_</h2><p>在日常使用时，我们经常需要按照最后一个维度拼合两个数组，也就是np.r<em>[‘-1’,index_expression]。此时我们可以直接使用`np.c</em>[]`</p><pre><code class="lang-python">np.c_[C,D] # =np.append(C,D,axis=2)array([[[0., 0., 1., 1.],        [0., 0., 1., 1.]],       [[0., 0., 1., 1.],        [0., 0., 1., 1.]]])np.c_[0:10:3, 0:5:4j]array([[0.        , 0.        ],       [3.        , 1.66666667],       [6.        , 3.33333333],       [9.        , 5.        ]])</code></pre><p>关于numpy中的<code>np.c_</code>和<code>np.r_</code>相关知识，可以参考<a href="https://numpy.org/devdocs/reference/generated/numpy.r_.html#numpy.r_">官方文档</a>，里面有关于中括号前参数的详细解释。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>array</tag>
      
      <tag>numpy</tag>
      
      <tag>concatenate</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>移动硬盘文件或目录损坏且无法读取解决方法</title>
    <link href="/2020/05/31/%E7%A7%BB%E5%8A%A8%E7%A1%AC%E7%9B%98%E6%96%87%E4%BB%B6%E6%88%96%E7%9B%AE%E5%BD%95%E6%8D%9F%E5%9D%8F%E4%B8%94%E6%97%A0%E6%B3%95%E8%AF%BB%E5%8F%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"/>
    <url>/2020/05/31/%E7%A7%BB%E5%8A%A8%E7%A1%AC%E7%9B%98%E6%96%87%E4%BB%B6%E6%88%96%E7%9B%AE%E5%BD%95%E6%8D%9F%E5%9D%8F%E4%B8%94%E6%97%A0%E6%B3%95%E8%AF%BB%E5%8F%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h2><p>家里的移动硬盘寿命已有4年之久，里面存储了200多G的学习资料（字面意思）。今天我将其插在系统为win10的电脑上，却出现了以下情况：</p><ul><li>硬盘通电指示灯亮；</li><li>右下角托盘区域出现usb插入提示，并可以点击“安全删除硬件”；</li><li>在“我的电脑”界面，显示“本地磁盘 D:”，但是双击之后出现错误“文件或目录损坏且无法读取”。</li></ul><p>重启电脑、重新插拔、更换另一台win7系统的电脑，都是该状况。至此基本确定是移动硬盘本身的问题。</p><h2 id="硬盘参数："><a href="#硬盘参数：" class="headerlink" title="硬盘参数："></a>硬盘参数：</h2><ul><li>黑甲虫 640G 移动机械硬盘</li><li>磁盘格式为NTFS</li><li>使用4年有余</li><li>之前出现过数据丢失的状况，转移敏感数据之后，在该盘中只留有非敏感的学习资料，约280G</li></ul><p>查阅资料可知，我这种错误大概率是由于某次未断电插拔硬盘导致的文件目录错误。好消息是，这种错误可以通过一句简单的指令解决。</p><h2 id="解决方案："><a href="#解决方案：" class="headerlink" title="解决方案："></a>解决方案：</h2><ol><li>打开cmd</li><li>输入 chkdsk D: /f 请注意，我的移动硬盘盘符为D:</li></ol><p>参考：<a href="https://cloud.tencent.com/developer/article/1487000">https://cloud.tencent.com/developer/article/1487000</a></p><p>chkdsk 参数说明：</p><p>volume 指定驱动器(后面跟一个冒号)、装入点或卷名。<br>filename 仅用于 FAT/FAT32: 指定要检查是否有碎片的文件<br>/F 修复磁盘上的错误。<br>/V　 在 FAT/FAT32 上: 显示磁盘上每个文件的完整路径和名称。在 NTFS 上: 如果有清除消息，将其显示。<br>/R 查找不正确的扇区并恢复可读信息(隐含 /F)。<br>/L:size 仅用于 NTFS:? 将日志文件大小改成指定的 KB 数。如果没有指定大小，则显示当前的大小。<br>/X 如果必要，强制卷先卸下。卷的所有打开的句柄就会无效(隐含 /F)<br>/I 仅用于 NTFS: 对索引项进行强度较小的检查<br>/C 仅用于 NTFS: 跳过文件夹结构的循环检查。<br>/I 和 /C 命令行开关跳过卷的某些检查，减少运行 Chkdsk 所需的时间</p><h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>这种错误一般产生于外置移动硬盘上面，或者外置U盘等等。之所以产生这些问题，一般有以下几个原因：</p><ol><li>没有点击“安全删除硬件”直接拔USB接口导致系统没有完成读写操作。这会使得文件目录不完整，损坏文件目录系统。</li><li>劣质产品，或者劣质硬盘盒。硬盘盒内部的电源、电路供电不稳定，也会产生文件系统错误的状况。</li><li>停电了</li></ol><h2 id="恢复效果质量"><a href="#恢复效果质量" class="headerlink" title="恢复效果质量"></a>恢复效果质量</h2><p>如果是大移动硬盘并且是NTFS分区格式的，恢复质量十分理想，基本都能成功恢复文件和目录结构。</p><p>如果是FAT或FAT32格式，根据损坏程度不同，恢复质量效果比NTFS格式结构的分区稍差一些，所以日常使用建议使用NTFS格式分区，其数据安全性更高一些。</p><p>一般情况下，CHKDSK可以成功修复出错的分区。但仍有可能没有反应。此时建议不要拔出设备，重启电脑，再观察是否仍然错误。 如果故障依然存在，可以尝试用EasyRecovery、R-STUDIO等软件恢复分区数据。</p>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hardware</tag>
      
      <tag>移动硬盘</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>numpy中delete的使用方法</title>
    <link href="/2020/05/31/np-delete%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <url>/2020/05/31/np-delete%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>本文将介绍np.delete中的参数及使用方法<br><!--more---></p><h2 id="Python中列表元素删除"><a href="#Python中列表元素删除" class="headerlink" title="Python中列表元素删除"></a>Python中列表元素删除</h2><p>在列表中删除元素，我们可以：</p><pre><code class="lang-python">list_a = [1,2,3,4,5]list_a.pop(-1)print(list_a) # [1,2,3,4]del list_a[0]print(list_a) # [2,3,4]del list[1:]print(list_a) # [2]</code></pre><h2 id="在numpy的ndarray中删除元素"><a href="#在numpy的ndarray中删除元素" class="headerlink" title="在numpy的ndarray中删除元素"></a>在numpy的ndarray中删除元素</h2><p>numpy中的数组ndarray是定长数组，对ndarray的处理不像对python中列表的处理那么方便。想要删除ndarray中的元素，我们往往只能退而求其次，返回一个没有对应元素的副本。在numpy中我们一般使用delete函数。此外，numpy的delete是可以删除数组的整行和整列的。</p><p>简单介绍一下<code>np.delete</code>：</p><pre><code class="lang-python">numpy.delete(arr, obj, axis=None)</code></pre><ul><li>arr：输入数组</li><li>obj：切片，整数，表示哪个子数组要被移除</li><li>axis：删除子数组的轴</li><li>返回：一个新的子数组</li></ul><p>下面是使用举例：</p><pre><code class="lang-python">A = np.arange(15).reshape((3,5))print(A)[[ 0  1  2  3  4] [ 5  6  7  8  9] [10 11 12 13 14]]B = np.delete(A, 1) # 先把A给ravel成一维数组，再删除第1个元素。C = np.delete(A, 1, axis=0) # axis=0代表按行操作D = np.delete(A, 1, axis=1) # axis=1代表按列操作print(A) # 并没有改变，delete不会操作原数组。[[ 0  1  2  3  4] [ 5  6  7  8  9] [10 11 12 13 14]]print(B) # 先把A给ravel成一维数组，再删除第1个元素。[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14]print(C) # axis=0代表按行操作[[ 0  1  2  3  4] [10 11 12 13 14]]print(D) # axis=1代表按列操作[[ 0  2  3  4] [ 5  7  8  9] [10 12 13 14]]</code></pre><p>不了解axis的读者可以看我写的<a href="https://superlova.github.io/2020/05/19/numpy%E4%B8%ADaxis%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/">这篇文章</a>。</p><h2 id="在np-delete的index参数中应用切片操作"><a href="#在np-delete的index参数中应用切片操作" class="headerlink" title="在np.delete的index参数中应用切片操作"></a>在np.delete的index参数中应用切片操作</h2><p>index参数必须是个由整数元素组成的列表，内部存放着的整数代表着目标array的下标。</p><p>当我想实现删除从第5个到第100个之间的所有元素时，不能使用slice，这就比较尴尬了。</p><pre><code class="lang-python">In [5]: np.delete(x, [3:6])  File &quot;&lt;ipython-input-215-0a5bf5cc05ba&gt;&quot;, line 1    np.delete(x, [3:6])                   ^SyntaxError: invalid syntax</code></pre><p>我们没办法在函数参数部分让其接受slice。怎么解决呢？我们可以把参数从<code>[start:end]</code>换成<code>A[start:end]</code>吗？</p><pre><code class="lang-python">A = np.arange(10)*2print(A)[ 0  2  4  6  8 10 12 14 16 18]B = np.delete(A, A[1:4]) # 搞错了吧！预期结果：0 8 10 12 14 16 18print(B)[ 0  2  6 10 14 16 18]</code></pre><p>我们这段代码能够执行，但是不是我们想要的结果。什么原因呢？是因为np.delete的index参数接受的是下标数组，而A[1:4]=[2,4,6]，那么np.delete就忠实地执行了删除第2、4、6个元素的任务。但我们的本意只是想删除下标从1到4的元素而已。</p><pre><code class="lang-python">D = np.delete(A, [1,2,3])print(D) # [ 0  8 10 12 14 16 18]</code></pre><p>要想使用slice，可以采用下列方式：1. <code>slice</code>函数或者<code>range</code>函数；2. <code>np.s_</code></p><pre><code class="lang-python">C = np.delete(A, slice(1,4))print(C) # [ 0  8 10 12 14 16 18]E = np.delete(A, np.s_[1:4])print(E) # [ 0  8 10 12 14 16 18]</code></pre><p>其实<code>np.s_[1:4]</code>只不过是很方便产生slice(1,4)的一种方式而已。</p><h2 id="其他实用的方法"><a href="#其他实用的方法" class="headerlink" title="其他实用的方法"></a>其他实用的方法</h2><p>除此之外，我们还可以采用mask的方式选择原数组中的元素组成新数组</p><pre><code class="lang-python">mask = np.ones((len(A),), dtype=bool)mask[[1,2,3]] = Falseprint(A[mask]) # [ 0  8 10 12 14 16 18]</code></pre><p>或者干脆采用数组拼合的方式</p><pre><code class="lang-python">G = np.empty(len(A)-len(A[1:4]), dtype=int)G[0:1] = A[0:1]G[1:len(G)] = A[4:]print(G) # [ 0  8 10 12 14 16 18]</code></pre><p>后两种方法不像我们想象的那么没用，反而很常见，尤其是mask方法。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>numpy</tag>
      
      <tag>slice</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Datawhale——SVHN——Task04：模型训练与验证</title>
    <link href="/2020/05/30/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task04%EF%BC%9A%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%AA%8C%E8%AF%81/"/>
    <url>/2020/05/30/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task04%EF%BC%9A%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%AA%8C%E8%AF%81/</url>
    
    <content type="html"><![CDATA[<p>模型的训练和验证过程，基本到了最后咯。<br><!--more---></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>datawhale</tag>
      
      <tag>Python</tag>
      
      <tag>Validation</tag>
      
      <tag>Verification</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Datawhale——SVHN——Task03：字符识别</title>
    <link href="/2020/05/27/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task03%EF%BC%9A%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB/"/>
    <url>/2020/05/27/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task03%EF%BC%9A%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<p>介绍常见的字符识别模型<br><!--more---></p><h2 id="用-tf-data-加载图片"><a href="#用-tf-data-加载图片" class="headerlink" title="用 tf.data 加载图片"></a>用 tf.data 加载图片</h2><h2 id="用CNN建立字符识别模型"><a href="#用CNN建立字符识别模型" class="headerlink" title="用CNN建立字符识别模型"></a>用CNN建立字符识别模型</h2>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>datawhale</tag>
      
      <tag>Python</tag>
      
      <tag>Digit Recognition</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Datawhale——SVHN——Task02：数据扩增</title>
    <link href="/2020/05/23/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task02%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E/"/>
    <url>/2020/05/23/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task02%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E/</url>
    
    <content type="html"><![CDATA[<p>训练模型第一步、数据读取和扩增！<br><!--more---></p><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><p>图像领域的数据读取方法，使用Pillow或者OpenCV内置的函数即可。</p><h2 id="数据扩增"><a href="#数据扩增" class="headerlink" title="数据扩增"></a>数据扩增</h2><p>在读取图像时，还可以对原始图像添加扰动等，这就启发我们一件事：是不是对原数据增加一些扰动，就可以使其变成新的数据呢？</p><p>下面介绍利用该思想的数据扩增环节。</p><h3 id="1-数据扩增为什么会有用"><a href="#1-数据扩增为什么会有用" class="headerlink" title="1. 数据扩增为什么会有用"></a>1. 数据扩增为什么会有用</h3><p>数据扩增的最常见作用，是增加数据集，用以缓解样本量不足导致的模型过拟合现象，从而提升模型的泛化性能。</p><p>究其本质，还是扩展数据集的多样性。</p><p>试想一下，我们如果想要试图训练一个完美模型，必然要利用完美的架构+完美的训练集，这个完美的训练集必然要覆盖到样本空间的方方面面。</p><p>我们当然不可能真的搞到无限多的样本。所以为了尽可能趋近于这个目标，就要试图以有限的数据集覆盖无限的样本空间。</p><p>每个样本在样本空间中就是一个坐标点。通过添加扰动，就能生成许多个在该样本点附近的增强样本。</p><p>上一个利用样本空间中添加扰动、从而生成与原样本很相似的应用，叫做<strong>生成对抗样本</strong>。</p><p>数据增强和对抗样本生成之间的区别在于，数据增强要保证扰动之后样本不能和原样本有区别；然而对抗样本生成则保证<strong>必须</strong>与原样本有区别。</p><p>有一些学者也通过将对抗样本添加至模型重训练的方法，使模型的泛化性能得到了提高。这说明数据增强和对抗样本的生效原理是一样的，都是通过扩大样本覆盖的样本空间的程度，通俗来讲就是模型见多识广了，再碰到新的问题也不怕了。</p><p>可以参考这篇论文：<a href="https://arxiv.org/abs/2003.08773">Do CNNs Encode Data Augmentations?</a></p><h3 id="2-常见的数据扩增"><a href="#2-常见的数据扩增" class="headerlink" title="2. 常见的数据扩增"></a>2. 常见的数据扩增</h3><h4 id="2-1-图像数据扩增"><a href="#2-1-图像数据扩增" class="headerlink" title="2.1 图像数据扩增"></a>2.1 图像数据扩增</h4><ul><li><p>色彩抖动（Color Jittering）<br>调整图片的亮度、饱和度、对比度，针对图像的颜色进行的数据增强。<br>对比度受限自适应直方图均衡化算法（Clahe），锐化（Sharpen），凸点（Emboss）</p></li><li><p>主成分噪声（PCA  Jittering）<br>首先按照RGB三个颜色通道计算均值和标准差，对网络的输入数据进行规范化；再在整个训练集上计算协方差矩阵，进行特征分解，得到特征向量和特征值，最后做PCA Jittering；最后对RGB空间做PCA，然后对主成分做一个(0, 0.1)的高斯扰动。</p></li><li><p>弹性变换（Elastic Transform）</p></li></ul><p>算法一开始是由Patrice等人在2003年的ICDAR上发表的《Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis》提出的，最开始应用在mnist手写体数字识别数据集中。当前也有很多人把该方法应用到手写体汉字的识别问题中。</p><p>首先对于图像中的每个像素点，产生对应的随机数对$(\Delta x, \Delta y)$，大小介于-1~1之间，分别表示该像素点的x方向和y方向的移动距离；<br>然后生成一个以0为均值，以σ为标准差的高斯核$k_{nn}$，并用前面的随机数与之做卷积，并将结果作用于原图像。</p><p><img src="https://www.kaggleusercontent.com/kf/1181488/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..JOqe3f7Joxv4qWUFbLMQew.CgYzaNwIIFF7jVY1bX0NSs1-ti8NMrPcp61few_93xUuYBRF6ug3wh2awESp_gDWx1BvlZAI45TF3uJ5vNqVLNPS4sIjHMM1oX721fmldeIQAnh84dOKzvTaIK-J0HFhapl_dyBAcV7yOQY_GQYR2MJi3SrPLxvekt-t1hEL0pSdWIl3wbeghVLpgUBg1VmFUFvkti7XL0GltuXOPuQMylsbdbz4GpKX-4gIWyVr301jLLOH-woNbaJuPN0DI4346ok8sJIoG2k7ZdBLq9xRunhHHe4UvmWx2Dj0OK9g8vjKZGA2pRQcxd6_-vWj1KLYyFEWzBWK67rMSFLsQ3zi287T1NK8VJ9C0fLm6FdrQSA4v3BrgbHMwWijkcvG0MdMIdRdZxWbhmxYQ_eKLGntll-2k_1UMFID03h4qjFU-1p2HWv8VktUcpHhvUtu-N26j0vOuWXI484Ttwm2kBkiPZxD0jAfIUVIpSP6pVOTd-E6drxGbr_bqcycOJP2BSzOb5fMqCuP7f_c0B3fnVgVU7-SCb5ngcW6M4ayyy9SNfpobLc2pYj47aTlMmz0iXRrofaEcXiVzhlKz_r8EUUXAkPw5F_X5heFp2S_0hJpKpQJPsssty6FG_PwzN7b5BoSa2mxcNpqiX4ADy-J1dwXzgQesePXPj3rVyQAjxcQmQfXNKVKCo6ASCMBkJsnSQb3.npeRt-2aWHuMiEMkWLKuxA/__results___files/__results___5_1.png" alt></p><p>参考：<br><a href="https://www.kaggle.com/jiqiujia/elastic-transform-for-data-augmentation">https://www.kaggle.com/jiqiujia/elastic-transform-for-data-augmentation</a><br><a href="https://blog.csdn.net/lhanchao/article/details/54234490">https://blog.csdn.net/lhanchao/article/details/54234490</a></p><p>还有诸如透视变换（Perspective Transform）、分段仿射变换（Piecewise Affine transforms）、枕形畸变（Pincushion Distortion）等不同的图像变换操作。</p><p>根据Datawhale大佬分享，对于本次题目（SVHN街道彩色数字识别），最常见的、最有效的数据扩增方法是：</p><ul><li>随机改变大小（resize）</li><li>随机切割（randomcrop），即从原始图像中，随机的crop出一些图像。</li></ul><p>鉴于本次数据集中的图片大小不一，一般一开始我们都需要resize到指定大小。但也有的文章中提到了，先对图片resize会使得图片长宽比发生变化，造成失真。所以我们要具体问题具体分析。</p><p>我在博客中也有分享过一篇讲述图像数据增强的<a href>相关论文</a>，大家可以看一下。</p><h4 id="2-2-文本数据增强"><a href="#2-2-文本数据增强" class="headerlink" title="2.2 文本数据增强"></a>2.2 文本数据增强</h4><p>此部分参考我的<a href>文本数据增强</a>，而且由于本次赛题并不必进行文本数据增强，因此就不在这里赘述了。</p><h3 id="3-数据扩增实战——使用tensorflow"><a href="#3-数据扩增实战——使用tensorflow" class="headerlink" title="3.. 数据扩增实战——使用tensorflow"></a>3.. 数据扩增实战——使用tensorflow</h3><p>大家都用的Pytorch吗？不会只有我自己用tensorflow吧。我来给大家介绍一下tensorflow是怎么做数据扩增的。</p><p>参考：<a href="https://www.tensorflow.org/tutorials/images/data_augmentation">TensorFlow Core</a></p><h4 id="1-准备"><a href="#1-准备" class="headerlink" title="1. 准备"></a>1. 准备</h4><pre><code class="lang-python"># 首先安装一个tensorflow_docs的库!pip install git+https://github.com/tensorflow/docsimport urllib # 负责下载网上的图片import tensorflow as tffrom tensorflow.keras.datasets import mnistfrom tensorflow.keras import layersAUTOTUNE = tf.data.experimental.AUTOTUNEimport tensorflow_docs as tfdocsimport tensorflow_docs.plotsimport tensorflow_datasets as tfdsimport PIL.Image # 大名鼎鼎PILimport matplotlib.pyplot as pltimport matplotlib as mplmpl.rcParams[&#39;figure.figsize&#39;] = (12, 5)import numpy as np</code></pre><p>下载一张示例图片：</p><pre><code class="lang-python">image_path = tf.keras.utils.get_file(&quot;cat.jpg&quot;, &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg&quot;)PIL.Image.open(image_path)</code></pre><p><img src="/2020/05/23/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task02%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E/2020-05-23-22-29-50.png" alt></p><p>将该图片解析成tensor</p><pre><code class="lang-python">image_string=tf.io.read_file(image_path)image=tf.image.decode_jpeg(image_string,channels=3)</code></pre><p>定义一个函数，用于可视化图像。</p><pre><code class="lang-python">def visualize(original, augmented):  fig = plt.figure()  plt.subplot(1,2,1)  plt.title(&#39;Original image&#39;)  plt.imshow(original)  plt.subplot(1,2,2)  plt.title(&#39;Augmented image&#39;)  plt.imshow(augmented)</code></pre><h4 id="2-执行数据扩增"><a href="#2-执行数据扩增" class="headerlink" title="2. 执行数据扩增"></a>2. 执行数据扩增</h4><p><strong>翻转图像</strong></p><pre><code class="lang-python">flipped = tf.image.flip_left_right(image)visualize(image, flipped)</code></pre><p><img src="/2020/05/23/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task02%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E/2020-05-23-22-32-30.png" alt></p><p><strong>灰度处理</strong></p><pre><code class="lang-python">grayscaled = tf.image.rgb_to_grayscale(image)visualize(image, tf.squeeze(grayscaled))plt.colorbar()</code></pre><p><img src="/2020/05/23/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task02%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E/2020-05-23-22-33-51.png" alt></p><p><strong>改变图像饱和度</strong></p><pre><code class="lang-python">saturated = tf.image.adjust_saturation(image, 3)visualize(image, saturated)</code></pre><p><img src="/2020/05/23/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task02%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E/2020-05-23-22-35-05.png" alt></p><p><strong>改变图像亮度</strong></p><pre><code class="lang-python">bright = tf.image.adjust_brightness(image, 0.4)visualize(image, bright)</code></pre><p><img src="/2020/05/23/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task02%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E/2020-05-23-22-35-39.png" alt></p><p><strong>旋转图像</strong></p><pre><code class="lang-python">rotated = tf.image.rot90(image)visualize(image, rotated)</code></pre><p><img src="/2020/05/23/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task02%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E/2020-05-23-22-36-11.png" alt></p><p><strong>中心放大并裁剪图像</strong></p><pre><code class="lang-python">cropped = tf.image.central_crop(image, central_fraction=0.5)visualize(image,cropped)</code></pre><p><img src="/2020/05/23/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task02%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E/2020-05-23-22-36-51.png" alt></p><p>等等此类操作，不一而足。大家感兴趣的可以查阅tensorflow的<code>tf.image</code>文档。</p><h4 id="3-使用扩增数据集训练"><a href="#3-使用扩增数据集训练" class="headerlink" title="3. 使用扩增数据集训练"></a>3. 使用扩增数据集训练</h4><p>我们构造一个模型，该模型架构为纯全连接网络，数据集为MNIST手写数字识别数据集。我们可以直接在tensorflow_datasets这个库中使用这个数据集。</p><pre><code class="lang-python">dataset, info =  tfds.load(&#39;mnist&#39;, as_supervised=True, with_info=True)train_dataset, test_dataset = dataset[&#39;train&#39;], dataset[&#39;test&#39;]num_train_examples= info.splits[&#39;train&#39;].num_examples</code></pre><p>编写函数执行对原来数据集的扩增操作。</p><pre><code class="lang-python">def convert(image, label):  image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]  return image, labeldef augment(image,label):  image,label = convert(image, label)  image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]  image = tf.image.resize_with_crop_or_pad(image, 34, 34) # Add 6 pixels of padding  image = tf.image.random_crop(image, size=[28, 28, 1]) # Random crop back to 28x28  image = tf.image.random_brightness(image, max_delta=0.5) # Random brightness  return image,label</code></pre><pre><code class="lang-python">BATCH_SIZE = 64# Only use a subset of the data so it&#39;s easier to overfit, for this tutorialNUM_EXAMPLES = 2048</code></pre><p>创建扩增后的数据集</p><pre><code class="lang-python">augmented_train_batches = (    train_dataset    # Only train on a subset, so you can quickly see the effect.    .take(NUM_EXAMPLES)    .cache()    .shuffle(num_train_examples//4)    # The augmentation is added here.    .map(augment, num_parallel_calls=AUTOTUNE)    .batch(BATCH_SIZE)    .prefetch(AUTOTUNE))</code></pre><p>为了对照，我们创建没有扩增的数据集。</p><pre><code class="lang-python">non_augmented_train_batches = (    train_dataset    # Only train on a subset, so you can quickly see the effect.    .take(NUM_EXAMPLES)    .cache()    .shuffle(num_train_examples//4)    # No augmentation.    .map(convert, num_parallel_calls=AUTOTUNE)    .batch(BATCH_SIZE)    .prefetch(AUTOTUNE))</code></pre><p>设置验证集。验证集与数据增不增强无关，反正我们不使用验证机训练，只用于最后的打分。</p><pre><code class="lang-python">validation_batches = (    test_dataset    .map(convert, num_parallel_calls=AUTOTUNE)    .batch(2*BATCH_SIZE))</code></pre><p>建立模型。注意这个模型纯粹是为了体现数据扩增的效果而专门构建的，因为卷积网络CNN即便是不用数据扩增也能很好地解决MNIST手写数字识别问题，这样比较起来效果就不明显了。两层4096个神经元的全连接网络，激活函数为RELU。最后是一个softmax层分类。</p><pre><code class="lang-python">def make_model():  model = tf.keras.Sequential([      layers.Flatten(input_shape=(28, 28, 1)),      layers.Dense(4096, activation=&#39;relu&#39;),      layers.Dense(4096, activation=&#39;relu&#39;),      layers.Dense(10)  ])  model.compile(optimizer = &#39;adam&#39;,                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),                metrics=[&#39;accuracy&#39;])  return model</code></pre><p>先使用<strong>没有经过数据扩增</strong>的数据训练模型，并记录其精度变化和loss变化：</p><pre><code class="lang-python">model_without_aug = make_model()no_aug_history = model_without_aug.fit(non_augmented_train_batches, epochs=50, validation_data=validation_batches)</code></pre><p>再使用<strong>经过扩增的数据</strong>训练模型，并记录。</p><pre><code class="lang-python">model_with_aug = make_model()aug_history = model_with_aug.fit(augmented_train_batches, epochs=50, validation_data=validation_batches)</code></pre><p>最后绘制图标，看一下表现。</p><p>首先是精度随着训练轮次的变化曲线：</p><pre><code class="lang-python">plotter = tfdocs.plots.HistoryPlotter()plotter.plot(&#123;&quot;Augmented&quot;: aug_history, &quot;Non-Augmented&quot;: no_aug_history&#125;, metric = &quot;accuracy&quot;)plt.title(&quot;Accuracy&quot;)plt.ylim([0.75,1])</code></pre><p><img src="/2020/05/23/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task02%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E/2020-05-23-22-49-01.png" alt></p><p>从图中可以看出，橙色线（没有数据增强的模型）在训练的时候很容易过拟合，但是在验证集上的精度不及蓝色线（数据增强的模型）。</p><p>再来看loss变化。</p><pre><code class="lang-python">plotter = tfdocs.plots.HistoryPlotter()plotter.plot(&#123;&quot;Augmented&quot;: aug_history, &quot;Non-Augmented&quot;: no_aug_history&#125;, metric = &quot;loss&quot;)plt.title(&quot;Loss&quot;)plt.ylim([0,1])</code></pre><p><img src="/2020/05/23/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task02%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E/2020-05-23-22-51-30.png" alt></p><p>这里看的就更明显了，橙色线在训练时的loss很快就下降到趋近0，这说明模型已经很难从未经增强的数据中学到东西了，产生了严重的过拟合。</p><p>而蓝色线直到最后也在逐步地学习之中，我们可以得出结论，数据增强的确有助于避免过拟合、增强模型的泛化性能。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>datawhale</tag>
      
      <tag>Python</tag>
      
      <tag>Data Augmentation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何评价推荐系统以及其他智能系统</title>
    <link href="/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/"/>
    <url>/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<p>看到一篇介绍推荐系统的评价方法与指标的文章，刚好本课题组也有个同学在做推荐系统相关的课题，并且该文章的评价指标部分对我目前的课题有启发作用，因此转载。<br><!--more---></p><h1 id="如何评价推荐系统以及其他智能系统"><a href="#如何评价推荐系统以及其他智能系统" class="headerlink" title="如何评价推荐系统以及其他智能系统"></a>如何评价推荐系统以及其他智能系统</h1><h2 id="1-评价推荐系统"><a href="#1-评价推荐系统" class="headerlink" title="1. 评价推荐系统"></a>1. <a href="https://www.jianshu.com/p/9d7c228eee59">评价推荐系统</a></h2><p>评测一个推荐系统时，需要考虑用户、物品提供商、推荐系统提供网站的利益，一个好的推荐系统是能够令三方共赢的系统。比如弹窗广告就不是一个好的推荐系统。</p><h3 id="1-1-评测实验方法"><a href="#1-1-评测实验方法" class="headerlink" title="1.1 评测实验方法"></a>1.1 评测实验方法</h3><h4 id="1-1-1-离线实验（offline-experiment）"><a href="#1-1-1-离线实验（offline-experiment）" class="headerlink" title="1.1.1 离线实验（offline experiment）"></a>1.1.1 离线实验（offline experiment）</h4><p>离线实验的方法的步骤如下：</p><p>a）通过日志系统获得用户行为数据，并按照一定格式生成一个标准的数据集；<br>b）将数据集按照一定的规则分成训练集和测试集；<br>c）在训练集上训练用户兴趣模型，在测试集上进行预测；<br>d）通过事先定义的离线指标，评测算法在测试集上的预测结果。</p><p>从以上步骤看出，离线实验的都是在数据集上完成的。意味着，它不需要一个实际的系统作为支撑，只需要有一个从日志中提取的数据集即可。</p><p>离线实验的优点是：</p><ul><li>不需要有对实际系统的控制权；</li><li>不需要用户参与实践；</li><li>速度快，可以测试大量算法；</li></ul><p>缺点是：</p><ul><li>数据集的稀疏性限制了适用范围，例如一个数据集中没有包含某用户的历史行为，则无法评价对该用户的推荐结果；</li><li>评价结果的客观性，无法得到用户主观性的评价；</li><li>难以找到离线评价指标和在线真实反馈(如 点击率、转化率、点击深度、购买客单价、购买商 品类别等)之间的关联关系；</li></ul><h4 id="1-1-2-用户调查（user-study）"><a href="#1-1-2-用户调查（user-study）" class="headerlink" title="1.1.2 用户调查（user study）"></a>1.1.2 用户调查（user study）</h4><p>用户调查需要一些真实的用户，让他们在需要测试的推荐系统上完成一些任务。在他们完成任务时，需要观察和记录用户的行为，并让他们回答一些问题。</p><p>最后，我们通过分析他们的行为和答案，了解测试系统的性能。</p><p>用户调查的优点是：</p><ul><li>可以获得用户主观感受的指标，出错后容易弥补；</li></ul><p>缺点是：</p><ul><li>招募测试用户代价较大；</li><li>无法组织大规模的测试用户，统计意义不足；</li></ul><h4 id="1-1-3-在线实验（online-experiment）"><a href="#1-1-3-在线实验（online-experiment）" class="headerlink" title="1.1.3 在线实验（online experiment）"></a>1.1.3 在线实验（online experiment）</h4><p>在完成离线实验和用户调查之后，可以将系统上线做AB测试，将它和旧算法进行比较。</p><p>在线实验最常用的评测算法是【A/B测试】，它通过一定的规则将用户随机分成几组，对不同组的用户采用不同的算法，然后通过统计不同组的评测指标，比较不同算法的好坏。</p><p>它的核心思想是:</p><p>a) 多个方案并行测试;<br>b) 每个方案只有一个变量不同;<br>c) 以某种规则优胜劣汰。</p><p>其中第2点暗示了A/B 测试的应用范围：A/B测试必须是单变量。</p><p>对于推荐系统的评价中，唯一变量就是—推荐算法。</p><p>有个<a href="http://www.abtests.com">很棒的网站</a>，里面有很多通过实际AB测试提高网站用户满意度的例子。</p><p>AB测试的优点是：</p><ul><li>可以公平获得不同算法实际在线时的性能指标，包括商业上关注的指标；</li></ul><p>缺点是：</p><ul><li>周期较长，必须进行长期的实验才能得到可靠的结果；</li></ul><h4 id="1-1-4-总结"><a href="#1-1-4-总结" class="headerlink" title="1.1.4 总结"></a>1.1.4 总结</h4><p>一般来说，一个新的推荐算法最终上线，需要完成上述的3个实验。</p><ul><li>首先，通过<strong>离线实验</strong>证明它在很多离线指标上优于现有的算法；</li><li>其次，通过<strong>用户调查</strong>确定用户满意度不低于现有的算法；</li><li>最后，通过<strong>在线AB测试</strong>确定它在我们关心的指标上优于现有的算法；</li></ul><h3 id="1-2-评测指标"><a href="#1-2-评测指标" class="headerlink" title="1.2 评测指标"></a>1.2 评测指标</h3><p>评测指标用于评测推荐系统的性能，有些可以定量计算，有些只能定性描述。</p><h4 id="1）用户满意度"><a href="#1）用户满意度" class="headerlink" title="1）用户满意度"></a>1）用户满意度</h4><p>用户满意度是评测推荐系统的重要指标，无法离线计算，只能通过用户调查或者在线实验获得。</p><p>调查问卷，需要考虑到用户各方面的感受，用户才能针对问题给出准确的回答。</p><p>在线系统中，用户满意度通过统计用户行为得到。比如用户如果购买了推荐的商品，就表示他们在一定程度上满意，可以用购买率度量用户满意度。</p><p>一般情况，我们可以用用户点击率、停留时间、转化率等指标度量用户的满意度。</p><h4 id="2）预测准确度"><a href="#2）预测准确度" class="headerlink" title="2）预测准确度"></a>2）预测准确度</h4><p>预测准确度，度量的是推荐系统预测用户行为的能力。 是推荐系统最重要的离线评测指标。</p><p>大部分的关于推荐系统评测指标的研究，都是针对预测准确度的。因为该指标可以通过离线实验计算，方便了学术界的研究人员。</p><p>由于离线的推荐算法有不同的研究方向，准确度指标也不同，根据研究方向，可分为：预测评分准确度和TopN推荐。</p><h5 id="a）预测评分准确度"><a href="#a）预测评分准确度" class="headerlink" title="a）预测评分准确度"></a>a）预测评分准确度</h5><p>预测评分的准确度，衡量的是算法预测的评分与用户的实际评分的贴近程度。</p><p>这针对于一些需要用户给物品评分的网站。</p><p>预测评分的准确度指标，一般通过以下指标计算：</p><p><strong>平均绝对误差（MAE）</strong></p><p><img src="/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/2020-05-20-21-51-32.png" alt></p><p>MAE因其计算简单、通俗易懂得到了广泛的应用。但MAE指标也有一定的局限性，因为对MAE指标贡献比较大的往往是那种很难预测准确的低分商品。</p><p>所以即便推荐系统A的MAE值低于系统B，很可能只是由于系统A更擅长预测这部分低分商品的评分，即系统A比系统B能更好的区分用户非常讨厌和一般讨厌的商品，显然这样区分的意义不大。</p><p><strong>均方根误差（RMSE）</strong></p><p><img src="/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/2020-05-20-21-51-51.png" alt></p><p>Netflix认为RMSE加大了对预测不准的用户物品评分的惩罚（平方项的惩罚），因而对系统的评测更加苛刻。</p><p>研究表明，如果评分系统是基于整数建立的（即用户给的评分都是整数），那么对预测结果取整数会降低MAE的误差。</p><h5 id="b）TopN推荐"><a href="#b）TopN推荐" class="headerlink" title="b）TopN推荐"></a>b）TopN推荐</h5><p>网站提供推荐服务时，一般是给用户一个个性化的推荐列表，这种推荐叫做TopN推荐。</p><p>TopN推荐的预测准确率，一般通过2个指标度量：</p><p><strong>准确率（precision）</strong></p><p><img src="/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/2020-05-20-21-52-10.png" alt></p><p><strong>召回率（recall）</strong></p><p><img src="/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/2020-05-20-21-52-18.png" alt></p><p>R(u)是根据用户在训练集上的行为给用户做出的推荐列表，T(u)是用户在测试集上的行为列表。</p><p>TopN推荐更符合实际的应用需求，比如预测用户是否会看一部电影，比预测用户看了电影之后会给它什么评分更重要。</p><p><strong>ROC曲线、AUC曲线、F值</strong></p><p>分类任务一般都有这三兄弟。</p><p>除此之外，还有Hit Rate (HR)等。</p><h4 id="3）覆盖率"><a href="#3）覆盖率" class="headerlink" title="3）覆盖率"></a>3）覆盖率</h4><p><img src="/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/2020-05-20-21-52-29.png" alt></p><p>覆盖率（coverage）是描述一个推荐系统对物品长尾的发掘能力。</p><p>最简单的定义是，推荐系统推荐出来的物品占总物品的比例。</p><p>假设系统的用户集合为U，推荐系统给每个用户推荐一个长度为N的物品列表R(u)，覆盖率公式为：</p><p>覆盖率是内容提供者关心的指标，覆盖率为100%的推荐系统可以将每个物品都推荐给至少一个用户。</p><p>除了推荐物品的占比，还可以通过研究物品在推荐列表中出现的次数分布，更好的描述推荐系统的挖掘长尾的能力。</p><p>如果分布比较平，说明推荐系统的覆盖率很高；如果分布陡峭，说明分布系统的覆盖率较低。</p><p>信息论和经济学中有两个著名指标，可以定义覆盖率：</p><p><strong>信息熵</strong></p><p><img src="/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/2020-05-20-21-52-40.png" alt></p><p>p(i)是物品i的流行度除以所有物品流行度之和。</p><p><strong>基尼系数（Gini Index）</strong></p><p><img src="/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/2020-05-20-21-52-57.png" alt></p><p>p(ij)是按照物品流行度p()从小到大排序的物品列表中第j个物品。</p><p><img src="/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/2020-05-20-21-53-09.png" alt></p><p><strong>评测马太效应</strong></p><p>马太效应，是指强者越强，弱者越弱的效应。推荐系统的初衷是希望消除马太效应，使得各物品都能被展示给对它们感兴趣的人群。</p><p>但是，很多研究表明，现在的主流推荐算法（协同过滤）是具有马太效应的。评测推荐系统是否具有马太效应可以使用基尼系数。</p><p>如，G1是从初始用户行为中计算出的物品流行度的基尼系数，G2是从推荐列表中计算出的物品流行度的基尼系数，那么如果G1&gt;G2，就说明推荐算法具有马太效应。</p><h4 id="4）多样性"><a href="#4）多样性" class="headerlink" title="4）多样性"></a>4）多样性</h4><p>为了满足用户广泛的兴趣，推荐列表需要能够覆盖用户不同兴趣的领域，即需要具有多样性。</p><p>多样性描述了推荐列表中物品两两之间的不相似性。假设s(i,j)在[0,1]区间定义了物品i和j之间的相似度，那么用户u的推荐列表R(u)的多样性定义如下：</p><p><img src="/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/2020-05-20-21-53-44.png" alt></p><p>推荐系统整体多样性可以定义为所有用户推荐列表多样性的平均值：</p><p><img src="/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/2020-05-20-21-53-51.png" alt></p><h4 id="5）新颖性"><a href="#5）新颖性" class="headerlink" title="5）新颖性"></a>5）新颖性</h4><p>新颖性也是影响用户体验的重要指标之一。它指的是向用户推荐非热门非流行物品的能力。</p><p>评测新颖度最简单的方法，是利用推荐结果的平均流行度，因为越不热门的物品，越可能让用户觉得新颖。</p><p>此计算比较粗糙，需要配合用户调查准确统计新颖度。</p><h4 id="6）惊喜度"><a href="#6）惊喜度" class="headerlink" title="6）惊喜度"></a>6）惊喜度</h4><p>推荐结果和用户的历史兴趣不相似，但却让用户满意，这样就是惊喜度很高。</p><p>目前惊喜度还没有公认的指标定义方式，最近几年研究的人很多，深入研究可以参考一些论文。</p><h4 id="7）信任度"><a href="#7）信任度" class="headerlink" title="7）信任度"></a>7）信任度</h4><p>如果用户信任推荐系统，就会增加用户和推荐系统的交互。</p><p>提高信任度的方式有两种：</p><ul><li>增加系统透明度：提供推荐解释，让用户了解推荐系统的运行机制。</li><li>利用社交网络，通过好友信息给用户做推荐</li></ul><p>度量信任度的方式，只能通过问卷调查。</p><h4 id="8）实时性"><a href="#8）实时性" class="headerlink" title="8）实时性"></a>8）实时性</h4><p>推荐系统的实时性，包括两方面：</p><ul><li>实时更新推荐列表满足用户新的行为变化；</li><li>将新加入系统的物品推荐给用户；</li></ul><h4 id="9）健壮性"><a href="#9）健壮性" class="headerlink" title="9）健壮性"></a>9）健壮性</h4><p>任何能带来利益的算法系统都会被攻击，最典型的案例就是搜索引擎的作弊与反作弊斗争。</p><p>健壮性（robust，鲁棒性）衡量了推荐系统抗击作弊的能力。</p><p>2011年的推荐系统大会专门有一个推荐系统健壮性的教程，作者总结了很多作弊方法，最著名的是行为注入攻击（profile injection attack）。</p><p>就是注册很多账号，用这些账号同时购买A和自己的商品。此方法针对亚马逊的一种推荐方法，“购买商品A的用户也经常购买的其他商品”。</p><p>评测算法的健壮性，主要利用模拟攻击：</p><ul><li>a）给定一个数据集和算法，用算法给数据集中的用户生成推荐列表；</li><li>b）用常用的攻击方法向数据集中注入噪声数据；</li><li>c）利用算法在有噪声的数据集上再次生成推荐列表；</li><li>d）通过比较攻击前后推荐列表的相似度评测算法的健壮性。</li></ul><p>提高系统健壮性的方法：</p><ul><li>选择健壮性高的算法；</li><li>选择代价较高的用户行为，如购买行为比浏览行为代价高；</li><li>在使用数据前，进行攻击检测，从而对数据进行清理。</li></ul><h4 id="10）商业目标"><a href="#10）商业目标" class="headerlink" title="10）商业目标"></a>10）商业目标</h4><p>设计推荐系统时，需要考虑最终的商业目标。不同网站具有不同的商业目标，它与网站的盈利模式息息相关。</p><p>总结：</p><p><img src="/2020/05/20/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F/2020-05-20-21-54-09.png" alt></p><p>作者认为，对于可以离线优化的指标，在给定覆盖率、多样性、新颖性等限制条件下，应尽量优化预测准确度。</p><h3 id="1-3-评测维度"><a href="#1-3-评测维度" class="headerlink" title="1.3 评测维度"></a>1.3 评测维度</h3><p>如果推荐系统的评测报告中，包含了不同维度下的系统评测指标，就能帮我们全面了解系统性能。一般评测维度分3种：</p><p>用户维度，主要包括用户的人口统计学信息、活跃度以及是不是新用户等；<br>物品维度，包括物品的属性信息、流行度、平均分以及是不是新加入的物品等；<br>时间维度，包括季节，是工作日还是周末，白天还是晚上等；</p><h2 id="2-评价智能系统"><a href="#2-评价智能系统" class="headerlink" title="2. 评价智能系统"></a>2. 评价智能系统</h2><p>【未完成】</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>testing</tag>
      
      <tag>Recommendation System</tag>
      
      <tag>Metrics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Datawhale——SVHN——Task01：赛题理解</title>
    <link href="/2020/05/20/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/"/>
    <url>/2020/05/20/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p>本次新人赛是Datawhale与天池联合发起的0基础入门系列赛事第二场 —— 零基础入门CV之街景字符识别比赛。<br><!--more---></p><h1 id="Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解"><a href="#Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解" class="headerlink" title="Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解"></a>Datawhale小组学习之街景字符编码识别任务——Task01：赛题理解</h1><h2 id="1-大赛简介"><a href="#1-大赛简介" class="headerlink" title="1. 大赛简介"></a>1. 大赛简介</h2><p>本次新人赛是Datawhale与天池联合发起的0基础入门系列赛事第二场 —— 零基础入门CV之街景字符识别比赛。</p><h3 id="1-1-赛题数据介绍"><a href="#1-1-赛题数据介绍" class="headerlink" title="1.1 赛题数据介绍"></a>1.1 赛题数据介绍</h3><p>赛题来源自Google街景图像中的门牌号数据集（The Street View House Numbers Dataset, SVHN），该数据来自真实场景的门牌号。</p><p>训练集数据包括3W张照片，验证集数据包括1W张照片，每张照片包括颜色图像和对应的编码类别和具体位置</p><h3 id="1-2-参赛规则"><a href="#1-2-参赛规则" class="headerlink" title="1.2 参赛规则"></a>1.2 参赛规则</h3><ul><li>比赛允许使用CIFAR-10和ImageNet数据集的预训练模型，不允许使用其他任何预训练模型和任何外部数据；</li><li>报名成功后，选手下载数据，在本地调试算法，提交结果；</li><li>提交后将进行实时评测；每天排行榜更新时间为12:00和20:00，按照评测指标得分从高到低排序；排行榜将选择历史最优成绩进行展示。</li></ul><h3 id="1-3-数据集简介"><a href="#1-3-数据集简介" class="headerlink" title="1.3 数据集简介"></a>1.3 数据集简介</h3><p>所有的数据（训练集、验证集和测试集）的标注使用JSON格式，并使用文件名进行索引。</p><div class="table-container"><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td>top</td><td>左上角坐标X</td></tr><tr><td>height</td><td>字符高度</td></tr><tr><td>left</td><td>左上角最表Y</td></tr><tr><td>width</td><td>字符宽度</td></tr><tr><td>label</td><td>字符编码</td></tr></tbody></table></div><p>字符的坐标具体如下所示：<br><img src="/2020/05/20/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/字符坐标.png" alt="坐标">  </p><p>在比赛数据（训练集和验证集）中，同一张图片中可能包括一个或者多个字符，因此在比赛数据的JSON标注中，会有两个字符的边框信息：<br>|原始图片|图片JSON标注|<br>|——|——-|<br><img src="/2020/05/20/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/原始图片.png" alt="19">    | <img src="/2020/05/20/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/原始图片标注.png" alt="标注">  |</p><p>在<a href="http://ufldl.stanford.edu/housenumbers/">SVHN官网</a>上下载数据集的话，有两种格式可供选择：</p><h4 id="第一种格式"><a href="#第一种格式" class="headerlink" title="第一种格式"></a>第一种格式</h4><p>第一种格式的数据集，除了原始图像之外还包括边界框信息。每个tar.gz文件都包含png格式的原始图像，以及一个digitStruct.mat文件。边界框信息存储在digitStruct.mat文件中，而不是直接绘制在数据集中的图像上。以<code>.mat</code>结尾的文件是matlab专用文件，可以使用python的scipy库打开，内部装的是类似json的字典。</p><p>digitStruct中的每个元素都有以下字段：</p><ul><li>name是一个字符串，其中包含相应图像的文件名。</li><li>bbox是一个结构数组，包含图像中每个数字边界框的位置，大小和标签。</li></ul><p>例如：digitStruct（300）.bbox（2）.height则是第300张图片中第二个数字边界框的高度。</p><h4 id="第二种格式"><a href="#第二种格式" class="headerlink" title="第二种格式"></a>第二种格式</h4><p>为了降低难度，将上述第一种格式的图片都被剪切到只剩下有效字符，并被缩放至32*32像素的标准大小，以方便识别。图片的边界框经过适当的选择，避免出现扭曲等状况。尽管如此，这种处理还是可能导致引入一些错误信息。</p><p>train_32x32.mat和test_32x32.mat，<code>.mat</code>文件中包含两个变量,X是一个4D的矩阵，维度是(32,32,3,n),n是数据个数,y是label变量。看一下前十张图：</p><pre><code class="lang-python">import scipy.io as sioimport matplotlib.pyplot as pltprint (&#39;Loading Matlab data.&#39;)mat = sio.loadmat(&#39;train_32x32.mat&#39;)data = mat[&#39;X&#39;]label = mat[&#39;y&#39;]for i in range(10):    plt.subplot(2,5,i+1)    plt.title(label[i][0])    plt.imshow(data[...,i])    plt.axis(&#39;off&#39;)plt.show()</code></pre><p><img src="/2020/05/20/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/2020-05-30-22-17-20.png" alt></p><h3 id="1-4-成绩评定方式"><a href="#1-4-成绩评定方式" class="headerlink" title="1.4 成绩评定方式"></a>1.4 成绩评定方式</h3><p>评价标准为准确率。<br>选手提交结果与实际图片的编码进行对比，以编码整体识别准确率为评价指标，结果越大越好，具体计算公式如下：</p><p> Score=编码识别正确的数量/测试集图片数量   </p><h3 id="1-5-结果提交格式"><a href="#1-5-结果提交格式" class="headerlink" title="1.5 结果提交格式"></a>1.5 结果提交格式</h3><p>提交前请确保预测结果的格式与sample_submit.csv中的格式一致，以及提交文件后缀名为csv。<br>形式如下：<br>file_name, file_code<br>0010000.jpg,451<br>0010001.jpg,232<br>0010002.jpg,45<br>0010003.jpg,67<br>0010004.jpg,191<br>0010005.jpg,892 </p><h2 id="2-数据读取"><a href="#2-数据读取" class="headerlink" title="2. 数据读取"></a>2. 数据读取</h2><p>JSON中标签的读取方式：  </p><pre><code class="lang-python">import jsontrain_json = json.load(open(&#39;../input/train.json&#39;))# 数据标注处理def parse_json(d):    arr = np.array([        d[&#39;top&#39;], d[&#39;height&#39;], d[&#39;left&#39;],  d[&#39;width&#39;], d[&#39;label&#39;]    ])    arr = arr.astype(int)    return arrimg = cv2.imread(&#39;../input/train/000000.png&#39;)arr = parse_json(train_json[&#39;000000.png&#39;])plt.figure(figsize=(10, 10))plt.subplot(1, arr.shape[1]+1, 1)plt.imshow(img)plt.xticks([]); plt.yticks([])for idx in range(arr.shape[1]):    plt.subplot(1, arr.shape[1]+1, idx+2)    plt.imshow(img[arr[0, idx]:arr[0, idx]+arr[1, idx],arr[2, idx]:arr[2, idx]+arr[3, idx]])    plt.title(arr[4, idx])    plt.xticks([]); plt.yticks([])</code></pre><p><img src="/2020/05/20/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/19.png" alt="19"></p><h2 id="3-解题思路"><a href="#3-解题思路" class="headerlink" title="3. 解题思路"></a>3. 解题思路</h2><p>赛题思路分析：赛题本质是分类问题，需要对图片的字符进行识别。但赛题给定的数据图片中不同图片中包含的字符数量不等，如下图所示。有的图片的字符个数为2，有的图片字符个数为3，有的图片字符个数为4。 </p><div class="table-container"><table><thead><tr><th>字符属性</th><th>图片</th></tr></thead><tbody><tr><td>字符：42   字符个数：2</td><td><img src="/2020/05/20/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/42.png" alt="标注"></td></tr><tr><td>字符：241   字符个数：3</td><td><img src="/2020/05/20/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/2411.png" alt="标注"></td></tr><tr><td>字符：7358   字符个数：4</td><td><img src="/2020/05/20/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/7358.png" alt="标注"></td></tr></tbody></table></div><p>因此本次赛题的难点是需要对不定长的字符进行识别，与传统的图像分类任务有所不同。为了降低参赛难度，我们提供了一些解题思路供大家参考：</p><ul><li>简单入门思路：定长字符识别    </li></ul><p>可以将赛题抽象为一个定长字符识别问题，在赛题数据集中大部分图像中字符个数为2-4个，最多的字符    个数为6个。<br>因此可以对于所有的图像都抽象为6个字符的识别问题，字符23填充为23XXXX，字符231填充为231XXX。<br><img src="/2020/05/20/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/23xxxxxx.png" alt="标注">   </p><p>经过填充之后，原始的赛题可以简化了6个字符的分类问题。在每个字符的分类中会进行11个类别的分类，假如分类为填充字符，则表明该字符为空。    </p><ul><li>专业字符识别思路：不定长字符识别 </li></ul><p><img src="/2020/05/20/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/不定长字符识别.png" alt="标注"> </p><p>在字符识别研究中，有特定的方法来解决此种不定长的字符识别问题，比较典型的有CRNN字符识别模型。<br>在本次赛题中给定的图像数据都比较规整，可以视为一个单词或者一个句子。   </p><ul><li>专业分类思路：检测再识别</li></ul><p>在赛题数据中已经给出了训练集、验证集中所有图片中字符的位置，因此可以首先将字符的位置进行识别，利用物体检测的思路完成。   </p><p><img src="/2020/05/20/Datawhale%E2%80%94%E2%80%94SVHN%E2%80%94%E2%80%94Task01%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/检测.png" alt="IMG"> </p><p>此种思路需要参赛选手构建字符检测模型，对测试集中的字符进行识别。选手可以参考物体检测模型SSD或者YOLO来完成。    </p><h2 id="4-Baseline思路：将不定长字符转换为定长字符的识别问题，并使用CNN完成训练和验证"><a href="#4-Baseline思路：将不定长字符转换为定长字符的识别问题，并使用CNN完成训练和验证" class="headerlink" title="4. Baseline思路：将不定长字符转换为定长字符的识别问题，并使用CNN完成训练和验证"></a>4. Baseline思路：将不定长字符转换为定长字符的识别问题，并使用CNN完成训练和验证</h2><h3 id="4-1-运行环境及安装示例"><a href="#4-1-运行环境及安装示例" class="headerlink" title="4.1 运行环境及安装示例"></a>4.1 运行环境及安装示例</h3><ul><li>运行环境要求：Python2/3，Pytorch1.x，内存4G，有无GPU都可以。         </li></ul><p>下面给出python3.7+ torch1.3.1gpu版本的环境安装示例：      </p><ul><li><p>首先在Anaconda中创建一个专门用于本次天池练习赛的虚拟环境。          </p><blockquote><p>$conda create -n py37_torch131 python=3.7      </p></blockquote></li><li><p>激活环境，并安装pytorch1.3.1                                     </p><blockquote><p>$source activate py37_torch131<br>$conda install pytorch=1.3.1 torchvision cudatoolkit=10.0                     </p></blockquote></li><li><p>通过下面的命令一键安装所需其它依赖库     </p><blockquote><p>$pip install jupyter tqdm opencv-python matplotlib pandas                                  </p></blockquote></li><li><p>启动notebook，即可开始baseline代码的学习                  </p><blockquote><p>$jupyter-notebook   </p></blockquote></li><li><p>假设所有的赛题输入文件放在../input/目录下，首先导入常用的包：</p></li></ul><pre><code class="lang-python">import os, sys, glob, shutil, jsonos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &#39;0&#39;import cv2from PIL import Imageimport numpy as npfrom tqdm import tqdm, tqdm_notebookimport torchtorch.manual_seed(0)torch.backends.cudnn.deterministic = Falsetorch.backends.cudnn.benchmark = Trueimport torchvision.models as modelsimport torchvision.transforms as transformsimport torchvision.datasets as datasetsimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.autograd import Variablefrom torch.utils.data.dataset import Dataset</code></pre><h3 id="4-2-步骤"><a href="#4-2-步骤" class="headerlink" title="4.2 步骤"></a>4.2 步骤</h3><ul><li>赛题数据读取（封装为Pytorch的Dataset和DataLoder）</li><li>构建CNN模型（使用Pytorch搭建）</li><li>模型训练与验证</li><li>模型结果预测</li></ul><h4 id="步骤1：定义好读取图像的Dataset"><a href="#步骤1：定义好读取图像的Dataset" class="headerlink" title="步骤1：定义好读取图像的Dataset"></a>步骤1：定义好读取图像的Dataset</h4><pre><code class="lang-python">class SVHNDataset(Dataset):    def __init__(self, img_path, img_label, transform=None):        self.img_path = img_path        self.img_label = img_label         if transform is not None:            self.transform = transform        else:            self.transform = None    def __getitem__(self, index):        img = Image.open(self.img_path[index]).convert(&#39;RGB&#39;)        if self.transform is not None:            img = self.transform(img)        # 设置最长的字符长度为5个        lbl = np.array(self.img_label[index], dtype=np.int)        lbl = list(lbl)  + (5 - len(lbl)) * [10]        return img, torch.from_numpy(np.array(lbl[:5]))    def __len__(self):        return len(self.img_path)</code></pre><h4 id="步骤2：定义好训练数据和验证数据的Dataset"><a href="#步骤2：定义好训练数据和验证数据的Dataset" class="headerlink" title="步骤2：定义好训练数据和验证数据的Dataset"></a>步骤2：定义好训练数据和验证数据的Dataset</h4><pre><code class="lang-python">train_path = glob.glob(&#39;../input/train/*.png&#39;)train_path.sort()train_json = json.load(open(&#39;../input/train.json&#39;))train_label = [train_json[x][&#39;label&#39;] for x in train_json]print(len(train_path), len(train_label))train_loader = torch.utils.data.DataLoader(    SVHNDataset(train_path, train_label,                transforms.Compose([                    transforms.Resize((64, 128)),                    transforms.RandomCrop((60, 120)),                    transforms.ColorJitter(0.3, 0.3, 0.2),                    transforms.RandomRotation(5),                    transforms.ToTensor(),                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    ])),     batch_size=40,     shuffle=True,     num_workers=10,)val_path = glob.glob(&#39;../input/val/*.png&#39;)val_path.sort()val_json = json.load(open(&#39;../input/val.json&#39;))val_label = [val_json[x][&#39;label&#39;] for x in val_json]print(len(val_path), len(val_label))val_loader = torch.utils.data.DataLoader(    SVHNDataset(val_path, val_label,                transforms.Compose([                    transforms.Resize((60, 120)),                    # transforms.ColorJitter(0.3, 0.3, 0.2),                    # transforms.RandomRotation(5),                    transforms.ToTensor(),                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    ])),     batch_size=40,     shuffle=False,     num_workers=10,)</code></pre><h4 id="步骤3：定义好字符分类模型，使用renset18的模型作为特征提取模块"><a href="#步骤3：定义好字符分类模型，使用renset18的模型作为特征提取模块" class="headerlink" title="步骤3：定义好字符分类模型，使用renset18的模型作为特征提取模块"></a>步骤3：定义好字符分类模型，使用renset18的模型作为特征提取模块</h4><pre><code class="lang-python">class SVHN_Model1(nn.Module):    def __init__(self):        super(SVHN_Model1, self).__init__()        model_conv = models.resnet18(pretrained=True)        model_conv.avgpool = nn.AdaptiveAvgPool2d(1)        model_conv = nn.Sequential(*list(model_conv.children())[:-1])        self.cnn = model_conv        self.fc1 = nn.Linear(512, 11)        self.fc2 = nn.Linear(512, 11)        self.fc3 = nn.Linear(512, 11)        self.fc4 = nn.Linear(512, 11)        self.fc5 = nn.Linear(512, 11)    def forward(self, img):                feat = self.cnn(img)        # print(feat.shape)        feat = feat.view(feat.shape[0], -1)        c1 = self.fc1(feat)        c2 = self.fc2(feat)        c3 = self.fc3(feat)        c4 = self.fc4(feat)        c5 = self.fc5(feat)        return c1, c2, c3, c4, c5</code></pre><h4 id="步骤4：定义好训练、验证和预测模块"><a href="#步骤4：定义好训练、验证和预测模块" class="headerlink" title="步骤4：定义好训练、验证和预测模块"></a>步骤4：定义好训练、验证和预测模块</h4><pre><code class="lang-python">def train(train_loader, model, criterion, optimizer):    # 切换模型为训练模式    model.train()    train_loss = []    for i, (input, target) in enumerate(train_loader):        if use_cuda:            input = input.cuda()            target = target.cuda()        c0, c1, c2, c3, c4 = model(input)        loss = criterion(c0, target[:, 0]) + \                criterion(c1, target[:, 1]) + \                criterion(c2, target[:, 2]) + \                criterion(c3, target[:, 3]) + \                criterion(c4, target[:, 4])        # loss /= 6        optimizer.zero_grad()        loss.backward()        optimizer.step()        if i % 100 == 0:            print(loss.item())        train_loss.append(loss.item())    return np.mean(train_loss)def validate(val_loader, model, criterion):    # 切换模型为预测模型    model.eval()    val_loss = []    # 不记录模型梯度信息    with torch.no_grad():        for i, (input, target) in enumerate(val_loader):            if use_cuda:                input = input.cuda()                target = target.cuda()            c0, c1, c2, c3, c4 = model(input)            loss = criterion(c0, target[:, 0]) + \                    criterion(c1, target[:, 1]) + \                    criterion(c2, target[:, 2]) + \                    criterion(c3, target[:, 3]) + \                    criterion(c4, target[:, 4])            # loss /= 6            val_loss.append(loss.item())    return np.mean(val_loss)def predict(test_loader, model, tta=10):    model.eval()    test_pred_tta = None    # TTA 次数    for _ in range(tta):        test_pred = []        with torch.no_grad():            for i, (input, target) in enumerate(test_loader):                if use_cuda:                    input = input.cuda()                c0, c1, c2, c3, c4 = model(input)                output = np.concatenate([                    c0.data.numpy(),                     c1.data.numpy(),                    c2.data.numpy(),                     c3.data.numpy(),                    c4.data.numpy()], axis=1)                test_pred.append(output)        test_pred = np.vstack(test_pred)        if test_pred_tta is None:            test_pred_tta = test_pred        else:            test_pred_tta += test_pred    return test_pred_tta</code></pre><h4 id="步骤5：迭代训练和验证模型"><a href="#步骤5：迭代训练和验证模型" class="headerlink" title="步骤5：迭代训练和验证模型"></a>步骤5：迭代训练和验证模型</h4><pre><code class="lang-python">model = SVHN_Model1()criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), 0.001)best_loss = 1000.0use_cuda = Falseif use_cuda:    model = model.cuda()for epoch in range(2):    train_loss = train(train_loader, model, criterion, optimizer, epoch)    val_loss = validate(val_loader, model, criterion)    val_label = [&#39;&#39;.join(map(str, x)) for x in val_loader.dataset.img_label]    val_predict_label = predict(val_loader, model, 1)    val_predict_label = np.vstack([        val_predict_label[:, :11].argmax(1),        val_predict_label[:, 11:22].argmax(1),        val_predict_label[:, 22:33].argmax(1),        val_predict_label[:, 33:44].argmax(1),        val_predict_label[:, 44:55].argmax(1),    ]).T    val_label_pred = []    for x in val_predict_label:        val_label_pred.append(&#39;&#39;.join(map(str, x[x!=10])))    val_char_acc = np.mean(np.array(val_label_pred) == np.array(val_label))    print(&#39;Epoch: &#123;0&#125;, Train loss: &#123;1&#125; \t Val loss: &#123;2&#125;&#39;.format(epoch, train_loss, val_loss))    print(val_char_acc)    # 记录下验证集精度    if val_loss &lt; best_loss:        best_loss = val_loss        torch.save(model.state_dict(), &#39;./model.pt&#39;)</code></pre><p>训练两个2 Epoch后，输出的训练日志为：</p><p>Epoch: 0, Train loss: 3.1      Val loss: 3.4 验证集精度：0.3439<br>Epoch: 1, Train loss: 2.1      Val loss: 2.9 验证集精度：0.4346     </p><h4 id="步骤6：对测试集样本进行预测，生成提交文件"><a href="#步骤6：对测试集样本进行预测，生成提交文件" class="headerlink" title="步骤6：对测试集样本进行预测，生成提交文件"></a>步骤6：对测试集样本进行预测，生成提交文件</h4><pre><code class="lang-python">test_path = glob.glob(&#39;../input/test_a/*.png&#39;)test_path.sort()test_label = [[1]] * len(test_path)print(len(val_path), len(val_label))test_loader = torch.utils.data.DataLoader(    SVHNDataset(test_path, test_label,                transforms.Compose([                    transforms.Resize((64, 128)),                    transforms.RandomCrop((60, 120)),                    # transforms.ColorJitter(0.3, 0.3, 0.2),                    # transforms.RandomRotation(5),                    transforms.ToTensor(),                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    ])),     batch_size=40,     shuffle=False,     num_workers=10,)test_predict_label = predict(test_loader, model, 1)test_label = [&#39;&#39;.join(map(str, x)) for x in test_loader.dataset.img_label]test_predict_label = np.vstack([    test_predict_label[:, :11].argmax(1),    test_predict_label[:, 11:22].argmax(1),    test_predict_label[:, 22:33].argmax(1),    test_predict_label[:, 33:44].argmax(1),    test_predict_label[:, 44:55].argmax(1),]).Ttest_label_pred = []for x in test_predict_label:    test_label_pred.append(&#39;&#39;.join(map(str, x[x!=10])))import pandas as pddf_submit = pd.read_csv(&#39;../input/test_A_sample_submit.csv&#39;)df_submit[&#39;file_code&#39;] = test_label_preddf_submit.to_csv(&#39;renset18.csv&#39;, index=None)</code></pre><p><strong>在训练完成2个Epoch后，模型在测试集上的成绩应该在0.33左右。</strong>    </p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>datawhale</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>numpy中的einsum使用方法</title>
    <link href="/2020/05/19/numpy%E4%B8%AD%E7%9A%84einsum%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <url>/2020/05/19/numpy%E4%B8%AD%E7%9A%84einsum%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>本文将介绍爱因斯坦求和约定，以及在numpy中的使用<br><!--more---></p><p>numpy里面有很多奇技淫巧，爱因斯坦求和约定就是其中之一。</p><p>爱因斯坦求和约定能够很方便和简介地表示点积、外积、转置、矩阵-向量乘法、矩阵-矩阵乘法等，这在深度学习公式推导中的用处很大。</p><p>其实我不认为einsum在numpy中用处很大，我认为其顶多就是一种统一的矩阵运算写法罢了。这种技巧，是在牺牲可读性基础上，对代码的简化。而且由于numpy对其他运算也有进行优化，所以仅凭借爱因斯坦乘数法还不一定能提升代码执行效率。</p><p>可能是我还没有体会到高维张量相互计算时的痛苦吧。</p><p>先看一下einsum的api：</p><pre><code class="lang-python">np.einsum(equation, *arr)</code></pre><p>最开始需要一个字符串，用以描述想要完成的计算。后面是计算需要的操作数，也就是你的矩阵等。</p><p>来看具体的例子：</p><h3 id="对于向量"><a href="#对于向量" class="headerlink" title="对于向量"></a>对于向量</h3><pre><code class="lang-python">arr1 = np.arange(5) # 0,1,2,3,4arr2 = np.arange(5) # 0,1,2,3,4</code></pre><ol><li>计算向量所有分量的和，即<code>np.sum(arr)</code>。如何利用einsum完成？</li></ol><pre><code class="lang-python">np.einsum(&quot;i-&gt;&quot;, arr) # 10</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c = \sum_{i} a_i,\quad i = 1, 2, \dots</script><ol><li>计算两向量内积，即<code>np.dot(arr1, arr2)</code>或<code>np.inner(arr1, arr2)</code></li></ol><pre><code class="lang-python"># 0*0 + 1*1 + 2*2 + 3*3 + 4*4np.einsum(&quot;i,i-&gt;&quot;, arr1, arr2) # 30</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c = \sum_{i} a_i \times b_i,\quad i = 1, 2, \dots</script><ol><li>计算两向量逐元素乘积，即<code>arr1 * arr2</code></li></ol><pre><code class="lang-python">np.einsum(&quot;i,i-&gt;i&quot;, arr1, arr2) # 0,1,4,9,16</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_i = a_i \times b_i,\quad i = 1, 2, \dots</script><ol><li>计算两向量外积，即<code>np.outer(arr1, arr2)</code></li></ol><pre><code class="lang-python">[[ 0  0  0  0  0] [ 0  1  2  3  4] [ 0  2  4  6  8] [ 0  3  6  9 12] [ 0  4  8 12 16]]np.einsum(&quot;i,j-&gt;ij&quot;, arr1, arr2)</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i,j} = a_i \times b_j,\quad i,j = 1, 2, \dots</script><h3 id="对于矩阵"><a href="#对于矩阵" class="headerlink" title="对于矩阵"></a>对于矩阵</h3><pre><code class="lang-python">A = np.arange(4).reshape(2,2)B = np.arange(4,8).reshape(2,2)[[0 1] [2 3]][[4 5] [6 7]]</code></pre><ol><li>计算矩阵转置，即<code>A.T</code></li></ol><pre><code class="lang-python">[[0 2] [1 3]]print(np.einsum(&quot;ij-&gt;ji&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i,j} = a_{j,i},\quad i,j = 1, 2, \dots</script><ol><li>计算矩阵各元素求和，即<code>np.sum(A)</code></li></ol><pre><code class="lang-python">6print(np.einsum(&quot;ij-&gt;&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c = \sum_{i}\sum_{j}a_{i,j},\quad i,j = 1, 2, \dots</script><ol><li>计算矩阵按列求和，即<code>np.sum(A, axis=0)</code></li></ol><pre><code class="lang-python">[2 4]print(np.einsum(&quot;ij-&gt;j&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{j} = \sum_{i}a_{i,j},\quad i,j = 1, 2, \dots</script><ol><li>计算矩阵按行求和，即<code>np.sum(A, axis=1)</code></li></ol><pre><code class="lang-python">[1 5]print(np.einsum(&quot;ij-&gt;i&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i} = \sum_{j}a_{i,j},\quad i,j = 1, 2, \dots</script><ol><li>求矩阵对角线元素，即<code>np.diag(A)</code></li></ol><pre><code class="lang-python">[0 3]print(np.einsum(&quot;ii-&gt;i&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i} = a_{i,i},\quad i = 1, 2, \dots</script><ol><li>计算矩阵的迹，即对角线元素和，即<code>np.trace(A)</code></li></ol><pre><code class="lang-python">3print(np.einsum(&quot;ii-&gt;&quot;, A))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c = \sum_{i}a_{i,i},\quad i = 1, 2, \dots</script><ol><li>计算两矩逐元素乘积，即<code>A*B</code></li></ol><pre><code class="lang-python">[[ 0  5] [12 21]] print(np.einsum(&quot;ij,ij-&gt;ij&quot;, A, B))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i,j} = a_{i,j} \times b_{i,j}, i,j = 1, 2, \dots</script><ol><li>计算<code>A*B.T</code></li></ol><pre><code class="lang-python">[[ 0  6] [10 21]]print(np.einsum(&quot;ij,ji-&gt;ij&quot;, A, B))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i,j} = a_{i,j} \times b_{j,i}, i,j = 1, 2, \dots</script><ol><li>计算两矩阵乘积<code>np.dot(A, B)</code></li></ol><pre><code class="lang-python">[[ 6  7] [26 31]]print(np.einsum(&quot;ij,jk-&gt;ik&quot;, A, B))</code></pre><p>在数学上相当于：</p><script type="math/tex; mode=display">c_{i,k} = a_{i,j} \times b_{j,k}, i,j = 1, 2, \dots</script><p>停一下，停一下。</p><p><img src="/2020/05/19/numpy%E4%B8%AD%E7%9A%84einsum%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/2020-05-20-09-45-48.png" alt></p><p>你们懂了吗？反正我没有。网上的文章指望着我们光看例子就能学会，这是把我们都当成模型训练了吗？</p><p>仔细看一下上面的两个例子，其实每个equation都拥有一个箭头<code>-&gt;</code>。对应数学公式不难得出，箭头左边对应数学公式右边，箭头右边对应数学公式左边。</p><p>比如这个式子：</p><pre><code class="lang-python">np.einsum(&quot;ij,ji-&gt;i&quot;, A, B)</code></pre><p><code>&quot;ij,ji-&gt;i&quot;</code>解释成自然语言：将A中第<code>&#123;i,j&#125;</code>个元素与B中第<code>&#123;j,i&#125;</code>个元素相乘（逗号理解成相乘），结果中没有j分量，只有i分量，所以所有j分量求和。</p><p>就是对应这个数学公式：</p><script type="math/tex; mode=display">c_i = \sum_{j}a_{i,j}\times b_{j,i}</script><p>实际含义代表：<code>np.sum(A*B.T, axis=1)</code></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>numpy</tag>
      
      <tag>einsum</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>numpy中axis的简单理解</title>
    <link href="/2020/05/19/numpy%E4%B8%ADaxis%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/"/>
    <url>/2020/05/19/numpy%E4%B8%ADaxis%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p>本文将介绍numpy中的axis<br><!--more---></p><p>我对于numpy中的axis的理解，一直处于似懂非懂、似是而非的状态。看到网上大神的文章，也只能点个赞之后，该不会还是不会。每次看完博客，都会觉得自己懂了；但是每次使用的时候，又要想老半天才行。因此今天我想借此机会，彻底扫清使用numpy时，axis的障碍。</p><p>在numpy中，数据的基本类型是array。array有个基本的数据属性，是它的维度。</p><p>比如下面的这个array，在逻辑上来看这就是个2维的数据，是一个矩阵。</p><pre><code class="lang-python">A = np.random.randint(0, 19, 9).reshape(3, 3)print(A)[[12 15  0] [ 3  3  7] [ 9 18  4]]</code></pre><p>接下来我要对其中的元素进行求和。</p><pre><code class="lang-python">print(np.sum(A))print(np.sum(A, axis=0))print(np.sum(A, axis=1))71[24 36 11][27 13 31]</code></pre><p>显然，第一个sum是对所有元素累加。第二个参数为axis=0的求和，则是这样计算的：</p><p><code>A[0][X] + A[1][X] + A[2][X]</code><br><code>--|---------|---------|----</code></p><p>也就是说，axis=0意味着在求和的过程中，只有A的第0个分量会变化，将第0个分量的所有情况穷举出来，再作为被操作元素，求和之。</p><p>第0个分量的元素计算完毕、得到一个结果时，计算并没有结束，因为我们的X还有很多种可能。</p><p>同理，axis=1时，变化的只有A的第1个（从逻辑上讲是第二个）分量有变化：</p><p><code>A[X][0] + A[X][1] + A[X][2]</code><br><code>-----|---------|---------|-</code></p><p>把该结论推广到更高维度的数据也不会有问题。我们看一个4维的张量是如何指定axis求和的：</p><pre><code class="lang-python">np.random.seed(0)A = np.random.randint(0, 9, 16).reshape(2, 2, 2, 2)print(&quot;orignal A&quot;, A)orignal A [[[[5 0]   [3 3]]  [[7 3]   [5 2]]] [[[4 7]   [6 8]]  [[8 1]   [6 7]]]]</code></pre><pre><code class="lang-python">print(np.sum(A))75</code></pre><pre><code class="lang-python">print(np.sum(A, axis=0))# 相当于print(A[0,:,:,:]+A[1,:,:,:])[[[ 9  7]  [ 9 11]] [[15  4]  [11  9]]]</code></pre><pre><code class="lang-python">print(np.sum(A, axis=1))# 相当于print(A[:,0,:,:] + A[:,1,:,:])[[[12  3]  [ 8  5]] [[12  8]  [12 15]]]</code></pre><pre><code class="lang-python">print(np.sum(A, axis=2))# 相当于print(A[:,:,0,:] + A[:,:,1,:])[[[ 8  3]  [12  5]] [[10 15]  [14  8]]]</code></pre><pre><code class="lang-python">print(np.sum(A, axis=3))# 相当于print(A[:,:,:,0]+A[:,:,:,1])[[[ 5  6]  [10  7]] [[11 14]  [ 9 13]]]</code></pre>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>numpy</tag>
      
      <tag>axis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>闭包的迷思</title>
    <link href="/2020/05/19/%E9%97%AD%E5%8C%85%E7%9A%84%E8%BF%B7%E6%80%9D/"/>
    <url>/2020/05/19/%E9%97%AD%E5%8C%85%E7%9A%84%E8%BF%B7%E6%80%9D/</url>
    
    <content type="html"><![CDATA[<p>闭包是什么？如果你与我有同样的疑问，敬请阅读。<br><!--more---></p><h2 id="什么是闭包？"><a href="#什么是闭包？" class="headerlink" title="什么是闭包？"></a><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-closure/#note_1">什么是闭包？</a></h2><p>这个问题困扰了我很长时间。</p><p>第一次接触闭包这个概念，是在“形式语言”这门课上。好像“离散数学”这门课上也教过闭包，但是这都不重要，因为我们这里讨论的闭包与数学上的闭包没什么关系。本文讨论的闭包，是程序设计语言中的闭包。</p><h3 id="专业概念："><a href="#专业概念：" class="headerlink" title="专业概念："></a>专业概念：</h3><p><img src="/2020/05/19/%E9%97%AD%E5%8C%85%E7%9A%84%E8%BF%B7%E6%80%9D/2020-05-19-15-20-04.png" alt></p><p>闭包是在其词法上下文中引用了自由变量的<strong>函数</strong>，自由变量是指除局部变量以外的变量。</p><p>又有一种说法是闭包<strong>不是函数</strong>，而是由函数和与其相关的引用环境组合而成的实体。</p><p><a href="https://zh.wikipedia.org/wiki/%E9%97%AD%E5%8C%85_(%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6">维基百科</a>)的解释：闭包在实现上是一个结构体，它存储了一个函数（通常是其入口地址）和一个关联的环境（相当于一个符号查找表）。</p><p>看到这里我彻底懵逼了。是是是，你们说的都对！<br><img src="/2020/05/19/%E9%97%AD%E5%8C%85%E7%9A%84%E8%BF%B7%E6%80%9D/2020-05-18-14-59-42.png" alt></p><p>身为新手小白，我需要通过判断闭包是做什么的，之后再讨论为什么叫做闭包。</p><h2 id="闭包有什么用？"><a href="#闭包有什么用？" class="headerlink" title="闭包有什么用？"></a>闭包有什么用？</h2><p>如果你是从C++来的，那么阅读下面没有什么障碍。如果不是也没有关系，反正各种语言的设计原理都是类似的，只要你掌握的语言有<strong>匿名函数</strong>的功能即可。</p><p>我们都知道，C++11标准引入了lambda表达式，就是一个匿名函数。这个函数长成这样：</p><pre><code class="lang-cpp">[](const string&amp;a, const string&amp;b) &#123;    return a.size() &lt; b.size();&#125;;</code></pre><p>上面的这个匿名函数负责比较两个字符串的大小。匿名函数的好处就是节省代码。</p><p>比如我现在想要实现自定义字符串排序函数，按照字符串长度从小到大排序，而不是按照字典排序。这个排序函数就可以用lambda表达式定义。</p><pre><code class="lang-cpp">stable_sort(words.begin(), words.end(),             [](const string&amp;a, const string&amp;b) &#123;                return a.size() &lt; b.size();&#125;;)</code></pre><p>lambda前面的中括号是干啥的？是用来捕获外部变量的。比如我想判断字符串长度有没有大于阈值threshold，这个threshold是在函数外面定义的。按照C++的语法，一般的函数不能访问函数外部的变量。但是lambda可以把外部的变量“捕获”，就像下面这样：</p><pre><code class="lang-cpp">int threshold = 10;[threshold](const string&amp; a) &#123;    return a.size() &gt; threshold;&#125;;</code></pre><p>可以看到，这个lambda不但使用了lambda内部的变量和参数，而且还“偷取”了不属于它的全局变量threshold。<strong>我们把lambda表达式定义的这种函数叫做闭包。</strong></p><h2 id="为什么叫做闭包？"><a href="#为什么叫做闭包？" class="headerlink" title="为什么叫做闭包？"></a>为什么叫做闭包？</h2><p>有人说这不是脑子有坑吗，闭包哪里“闭”了？这明明比普通函数更“开放”好吧？是不是名字起错了？</p><p>其实不然。闭包并不是对内部封闭，而是给当前外部环境取了个快照，相当于封闭了外部状态。下面是著名营养快线经销商vczh的回答：</p><p><img src="/2020/05/19/%E9%97%AD%E5%8C%85%E7%9A%84%E8%BF%B7%E6%80%9D/2020-05-19-15-40-47.png" alt></p><h2 id="Python中的闭包"><a href="#Python中的闭包" class="headerlink" title="Python中的闭包"></a>Python中的闭包</h2><p>Python中写闭包就要方便多了，毕竟Python的设计哲学就是“一切皆对象”，函数都是对象。</p><p>我们来看这样一个问题：利用闭包和生成器返回一个计数器函数，每次调用它返回递增整数。</p><pre><code class="lang-python"># 利用闭包和生成器返回一个计数器函数，每次调用它返回递增整数。def createCounter():        [...]# 检验部分counterA = createCounter()print(counterA(), counterA(), counterA(), counterA(), counterA()) # 1 2 3 4 5counterB = createCounter()if [counterB(), counterB(), counterB(), counterB()] == [1, 2, 3, 4]:    print(&#39;测试通过!&#39;)else:    print(&#39;测试失败!&#39;)</code></pre><p>你想怎么写？我能想到的，就是在函数内部定义一个生成器，每次调用生成一个整数；然后利用next函数构造一个迭代器，每次调用让这个整数+1，最后返回这个迭代器。</p><pre><code class="lang-python">def createCounter():        def counter():        &#39;&#39;&#39;定义一个生成器        &#39;&#39;&#39;        n = 0        while 1:            n += 1            yield n    g = counter() # 取生成器    def g_fn():        &#39;&#39;&#39;定义一个迭代器，利用next迭代生成器g        &#39;&#39;&#39;        return next(g)    return g_fn # 返回这个迭代器</code></pre><p>我们看一下上面这个函数，函数内部定义的<code>g_fn</code>函数，它使用了外部变量<code>g</code>，也就是说<code>g_fn</code>是个闭包。</p><h2 id="总结一下："><a href="#总结一下：" class="headerlink" title="总结一下："></a>总结一下：</h2><p><strong>引用了自由变量的函数，就是闭包。</strong></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>C++</tag>
      
      <tag>Python</tag>
      
      <tag>closure</tag>
      
      <tag>lambda</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python装饰器为什么这么难以理解</title>
    <link href="/2020/05/18/Python%E8%A3%85%E9%A5%B0%E5%99%A8%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E9%9A%BE%E4%BB%A5%E7%90%86%E8%A7%A3/"/>
    <url>/2020/05/18/Python%E8%A3%85%E9%A5%B0%E5%99%A8%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E9%9A%BE%E4%BB%A5%E7%90%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p>本文将介绍Python中的装饰器，以及设计模式中的装饰模式。<br><!--more---></p><p>从C/C++或Java迁移来的新Python程序员一定会对Python的装饰器功能感到陌生，尤其是在函数定义前加<code>@func</code>这一功能感到困惑。装饰器到底是什么？Python背后做了什么？在仔细研究网上的资料之后，我总结了此文，与大家分享。</p><p><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017451662295584">参考文章</a></p><h2 id="1-提出需求"><a href="#1-提出需求" class="headerlink" title="1. 提出需求"></a>1. 提出需求</h2><p>我们想在函数增加一点功能，比如每次函数执行之前打印一段话，但是又不想更改函数的定义。</p><p>这种想要给原来函数增加需求的同时，不修改原来代码的行为，非常有“面向对象编程思想”内味儿，因为它符合“开放封闭原则”。</p><p>现在就有请大名鼎鼎的设计模式之——装饰器模式登场！</p><blockquote><p>装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。</p></blockquote><p><img src="/2020/05/18/Python%E8%A3%85%E9%A5%B0%E5%99%A8%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E9%9A%BE%E4%BB%A5%E7%90%86%E8%A7%A3/2020-05-19-17-27-00.png" alt></p><h2 id="2-Python中的装饰器模式"><a href="#2-Python中的装饰器模式" class="headerlink" title="2. Python中的装饰器模式"></a>2. Python中的装饰器模式</h2><p>在Python中实现装饰器模式很方便。在Python中，有个功能模块直接就叫装饰器。在Python中的装饰器是指一个返回其他函数的函数。外部的高阶函数在执行内部的原函数的前后，再私藏一点干货，然后把修改后的函数对象赋值给原来的函数变量。这样就能在不修改原函数的基础上，增加一些功能。</p><p>总结下来，实现装饰器三步走：</p><ol><li>定义原函数</li><li>定义高阶函数，在里面除了执行原函数之外，再添加一些功能</li><li>将高阶函数对象赋值为原函数变量，以后调用原函数的时候都会执行高阶函数了</li></ol><pre><code class="lang-python">def log(func):    def wrapper(*args, **kw):        print(&#39;call %s():&#39; % func.__name__)        return func(*args, **kw)    return wrapper</code></pre><p>上面的函数，输入参数为原函数变量，在内部构造了一个高阶函数对象wrapper，wrapper里面负责执行一个print语句。最后返回构造好的wrapper。</p><p>以后我们使用<code>func</code>的时候，只要使用<code>log(func)</code>就可以在执行<code>func</code>的同时，打印一段话了。</p><p>看起来不咋地啊，毕竟我们还是修改了代码，把<code>func</code>全都替换成<code>log(func)</code>才能执行。</p><p>或者我们来这样一句：</p><pre><code class="lang-python">func = log(func)</code></pre><p>这个log函数就是一个装饰器，它现在装饰的是func函数。</p><h2 id="3-Python的语法糖"><a href="#3-Python的语法糖" class="headerlink" title="3. Python的语法糖"></a>3. Python的语法糖</h2><p>借助Python的@语法，把decorator置于函数的定义处，我们可以直接完成<code>func = log(func)</code>的操作。</p><pre><code class="lang-python">@logdef basic_fun():    print(&quot;basic_func&quot;)</code></pre><p>以后使用basic_func就会默认执行log(basic_func)了。</p><h2 id="4-改函数名"><a href="#4-改函数名" class="headerlink" title="4. 改函数名"></a>4. 改函数名</h2><p>Python的设计思想就是“一切皆对象”，就连函数也不例外。既然是对象，那么对象可以赋值给一个变量，也可以直接使用。通过变量也可以调用该函数对象。</p><pre><code class="lang-python">def f():    return 0f_obj = f # 注意，这里f为函数名，不加括号则为将函数对象赋值为变量f_res = f() # f后面跟了括号，则此时执行函数，并把返回值赋值给变量</code></pre><p>Python有个特别方便的功能，那就是函数对象可以在运行时打印自己的名字。接上面的代码：</p><pre><code class="lang-python">print(f.__name__) # fprint(f_obj.__name__) # 本质上还是调用上面的函数对象，结果仍为f</code></pre><p>前面我们做了赋值操作<code>func = log(func)</code>，但是其变量代表的函数名称发生了变化。</p><pre><code class="lang-python">print(func.__name__) # funcfunc = log(func)print(func.__name__) # wrapper</code></pre><p>我们希望装饰器完全包裹原函数，也就是说令外界环境感觉不到内部逻辑的变化。那么就需要我们把函数名字也给保持住。这个功能不难，我们使用<code>functools</code>库中自带的装饰器<code>wraps</code>就可以保持函数名称了。</p><pre><code class="lang-python">import functoolsdef log(func):    @functools.wraps(func) # 将被装饰函数名变成参数中函数名    def wrapper(*args, **kw):        print(&#39;call %s():&#39; % func.__name__)        return func(*args, **kw)    return wrapper</code></pre><h2 id="5-带参数的装饰器"><a href="#5-带参数的装饰器" class="headerlink" title="5. 带参数的装饰器"></a>5. 带参数的装饰器</h2><p>在上面我们可以看到，装饰器也是可以带参数的。这是怎么做到的呢？</p><p>其实我们不难想到，只需装饰一个装饰器即可。比如下面这个问题：</p><p><strong>实现log(str)：在函数每次执行前打印str和函数名</strong></p><pre><code class="lang-python">@log(&#39;end&#39;)def now():    print(np.datetime64(&#39;today&#39;, &#39;D&#39;))&gt;&gt;&gt; now()end now():2019-10-13</code></pre><p>解法如下：</p><pre><code class="lang-python">import functoolsdef log(text):    def decorator(func):        @functools.wraps(func)        def wrapper(*args, **kw):            print(&#39;%s %s():&#39; % (text, func.__name__))            return func(*args, **kw)        return wrapper    return decorator</code></pre><p>相当于<code>fun = log(&#39;text&#39;)(fun)</code>，实际上函数变成了<code>wrapper</code><br>但是由于<code>@functools.wraps(func)</code>，函数的<code>__name__</code>不变</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>decorator</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习工作站调研--结合政府采购网信息</title>
    <link href="/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%E8%B0%83%E7%A0%94-%E7%BB%93%E5%90%88%E6%94%BF%E5%BA%9C%E9%87%87%E8%B4%AD%E7%BD%91%E4%BF%A1%E6%81%AF/"/>
    <url>/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%E8%B0%83%E7%A0%94-%E7%BB%93%E5%90%88%E6%94%BF%E5%BA%9C%E9%87%87%E8%B4%AD%E7%BD%91%E4%BF%A1%E6%81%AF/</url>
    
    <content type="html"><![CDATA[<h1 id="服务器调研-2020年5月10日"><a href="#服务器调研-2020年5月10日" class="headerlink" title="服务器调研 2020年5月10日"></a>服务器调研 2020年5月10日</h1><p><strong>调研目标：</strong></p><ul><li>目前典型的计算机，包括商用台式机、工作站、服务器</li><li>搭配目前典型的GPU卡</li><li>GPU适配计算机，需要厂家网站公开的列表，特别是对于服务器。如果厂家没有，需要致电厂商（非销售商）的技术支持。</li></ul><h2 id="1-制约性能的典型项目"><a href="#1-制约性能的典型项目" class="headerlink" title="1. 制约性能的典型项目"></a>1. 制约性能的典型项目</h2><h3 id="1-1-主板"><a href="#1-1-主板" class="headerlink" title="1.1 主板"></a>1.1 主板</h3><p>Intel部分芯片组不支持PCIe 3.0接口，无法发挥显卡的最佳速度。</p><h4 id="名词解释："><a href="#名词解释：" class="headerlink" title="名词解释："></a>名词解释：</h4><p><img src="/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%E8%B0%83%E7%A0%94-%E7%BB%93%E5%90%88%E6%94%BF%E5%BA%9C%E9%87%87%E8%B4%AD%E7%BD%91%E4%BF%A1%E6%81%AF/2020-05-10-21-33-30.png" alt><br><strong>PCI Express / PCI-e</strong><br>PCI-E的全名叫PCI Express，简称PCI-E，官方简称PCIe，他是计算机内部的一种高速总线。PCI-E既是通道，也是接口，当他以接口形式存在的时候，就是我们主板上那长长的槽。PCI-E接口目前最大的作用就是插显卡<br><img src="/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%E8%B0%83%E7%A0%94-%E7%BB%93%E5%90%88%E6%94%BF%E5%BA%9C%E9%87%87%E8%B4%AD%E7%BD%91%E4%BF%A1%E6%81%AF/2020-05-10-21-32-30.png" alt></p><p><strong>PCI Express 修订版 / PCIe版本</strong><br>PCIe所能承受的带宽一般以版本和长度来区分，目前最流行的PCIe版本是3.0，最新的版本是4.0，目前只有高端主板支持4.0，只有比2080ti还要高端的显卡才需要4.0。</p><p><strong>PCI Express 配置</strong><br>通俗的说就是插槽长度。X1长度是最短的，所能承受的带宽大约是986MB/S。X2长度就是2GB/S，X4长度就是4GB/S，那X16长度就是16GB/S。当前主流显卡，均采用PCIE×16插槽结构。只要具有PCIE×16插槽的主板，都是可以安装独立显卡的。<br>英特尔官网的意义没大看懂，真正有意义的是“支持的处理器 PCI Express 端口配置”这一项。</p><p><strong>支持的处理器 PCI Express 端口配置</strong><br>以Z390主板为例，该主板1x16 or 2x8 or 1x8+2x4，意思就是可以插1个长度为16X的显卡，也可以插两个长度为8X的固态硬盘之类的，但是如果同时插上显卡和固态硬盘，就会出现抢通道的现象：显卡占用16个通道，两个固态占用16个通道，然而<strong>PCI Express 通道数的最大值</strong>就只有24个，通道不够用就会导致限速，甚至无法正常运转。</p><p><img src="/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%E8%B0%83%E7%A0%94-%E7%BB%93%E5%90%88%E6%94%BF%E5%BA%9C%E9%87%87%E8%B4%AD%E7%BD%91%E4%BF%A1%E6%81%AF/2020-05-10-22-13-34.png" alt><br><img src="/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%E8%B0%83%E7%A0%94-%E7%BB%93%E5%90%88%E6%94%BF%E5%BA%9C%E9%87%87%E8%B4%AD%E7%BD%91%E4%BF%A1%E6%81%AF/2020-05-10-22-13-57.png" alt><br><img src="/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%E8%B0%83%E7%A0%94-%E7%BB%93%E5%90%88%E6%94%BF%E5%BA%9C%E9%87%87%E8%B4%AD%E7%BD%91%E4%BF%A1%E6%81%AF/2020-05-10-22-14-07.png" alt><br><img src="/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%E8%B0%83%E7%A0%94-%E7%BB%93%E5%90%88%E6%94%BF%E5%BA%9C%E9%87%87%E8%B4%AD%E7%BD%91%E4%BF%A1%E6%81%AF/2020-05-10-22-14-15.png" alt></p><p>以上是当前在售处理器搭配主板（芯片组）的特性支持情况，仅供参考，并不是说某块主板用了上述某个芯片组芯片就会具备这么多的扩展接口及能力，具体还要看主板厂商针对这个版型作出什么样的“阉割”调整。</p><h3 id="1-2-电源"><a href="#1-2-电源" class="headerlink" title="1.2 电源"></a>1.2 电源</h3><p><img src="/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%E8%B0%83%E7%A0%94-%E7%BB%93%E5%90%88%E6%94%BF%E5%BA%9C%E9%87%87%E8%B4%AD%E7%BD%91%E4%BF%A1%E6%81%AF/2020-05-10-22-11-24.png" alt="顶级游戏显卡及需要的电源功率大小"></p><h2 id="2-目前典型计算机"><a href="#2-目前典型计算机" class="headerlink" title="2. 目前典型计算机"></a>2. 目前典型计算机</h2><p>服务器对显卡的支持不如工作站，台式机的性能过低，因此本调查汇聚于工作站查询。</p><h3 id="2-1-服务器"><a href="#2-1-服务器" class="headerlink" title="2.1 服务器"></a>2.1 服务器</h3><p>服务器按外形划分可以划分为：塔式服务器、机架式服务器、刀片式服务器。<br>服务器除了一些低端的塔式机能用显卡以外，其他的都不支持显度卡，当然机架式服务器很薄根本就没有显卡的空间。<br>如果购买服务器，官方售后将不会主动为你安装个人家用系列显卡，转而推销商业计算卡。<br><img src="/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%E8%B0%83%E7%A0%94-%E7%BB%93%E5%90%88%E6%94%BF%E5%BA%9C%E9%87%87%E8%B4%AD%E7%BD%91%E4%BF%A1%E6%81%AF/2020-05-10-15-26-49.png" alt></p><p>截至2020年5月，服务器热销品牌Top-10（取自<a href="http://top.zol.com.cn/compositor/server.html">ZOL网</a>）：<br>Dell、华为、浪潮、联想、惠普、H3C、ThinkServer、中科曙光、宝德、IBM。</p><h3 id="2-2-工作站"><a href="#2-2-工作站" class="headerlink" title="2.2 工作站"></a>2.2 工作站</h3><p>工作站的机箱主要以塔式为主，和一般家用主机机箱差距不大。<br>工作站对显卡的支持比服务器强很多，具体来说，工作站的主板对PCI-E的接口支持更好。</p><p>以下价格和资料全部取自于北京市政府采购网。</p><p><strong>神舟</strong><br>HFMPB2O8型号支持双路2080ti或TITAN<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=1r6e15444412038777n5&amp;tiitPk=BG_002X&amp;tioPk=">HFMPB2O8</a>    78,016.00 自带2080ti<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=2g0v15681729710791z0&amp;tiitPk=BG_002X&amp;tioPk=">HFMPB99K</a>  55,691.28   自带2080ti<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=6u7m15621197126648s6&amp;tiitPk=BG_002X&amp;tioPk=">HFMPB3IR</a>  32,870.00   C422可更换更高级显卡<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=8n5x15444417182251i6&amp;tiitPk=BG_002X&amp;tioPk=">HFMPB3J9</a>  29,980.00   自带2080</p><p><strong>联想</strong><br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=6h7y15571256822209c5&amp;tiitPk=BG_002X&amp;tioPk=">Think Station P520</a>   46,920.00    C422可更换更高级显卡<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=1s8z15571265520623u2&amp;tiitPk=BG_002X&amp;tioPk=">ThinkStation P720</a> 35,000.00   C622可更换更高级 </p><p><strong>宏碁</strong><br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=7w0g15281019011939v9&amp;tiitPk=BG_002X&amp;tioPk=">AP150 F4</a> 38,500.00    C622可更换更高级 </p><p><strong>浪潮</strong><br>浪潮是自研主板，不过其主板支持PCIe 16x，理论上只要供电足够即可安装包括2080Ti在内的显卡<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=3f4m15287143876139q2&amp;tiitPk=BG_002X&amp;tioPk=">P8000</a>    37,260.00</p><p><strong>惠普</strong><br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=4g2q15281081290990x3&amp;tiitPk=BG_002X&amp;tioPk=">HP Z4 G4</a>  15,900.00   C622可更换更高级<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=4v9p15281082398434h6&amp;tiitPk=BG_002X&amp;tioPk=">HP Z6 G4</a>  23,500.00   C622可更换更高级<br><a href="http://114.255.53.119:81/bgpc_office_manage/produce/parmsInfo.htm?topPk=0z0m15314586871347z2&amp;tiitPk=BG_002X&amp;tioPk=">HP Z8 G4</a>  35,800.00   C622可更换更高级 </p><p><strong>苹果</strong><br>苹果的主板仅支持AMD的显卡，A卡不能用作深度学习。</p><h3 id="2-3-商用台式机"><a href="#2-3-商用台式机" class="headerlink" title="2.3 商用台式机"></a>2.3 商用台式机</h3><p>即普通台式机。普通台式机难以支撑深度学习任务。</p><h2 id="3-显卡介绍"><a href="#3-显卡介绍" class="headerlink" title="3. 显卡介绍"></a>3. 显卡介绍</h2><p>显卡分为Nvidia显卡和AMD显卡，其中Nvidia显卡可以用来深度学习训练和推理。</p><p>比较显卡性能，可以去<a href="https://versus.com/cn">这个网站</a></p><h3 id="3-1-Nvidia显卡简介"><a href="#3-1-Nvidia显卡简介" class="headerlink" title="3.1 Nvidia显卡简介"></a>3.1 Nvidia显卡简介</h3><p><a href="https://www.bybusa.com/gpu-rank">2020年显卡天梯图</a></p><p><img src="/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%E8%B0%83%E7%A0%94-%E7%BB%93%E5%90%88%E6%94%BF%E5%BA%9C%E9%87%87%E8%B4%AD%E7%BD%91%E4%BF%A1%E6%81%AF/2020-05-10-22-23-18.png" alt="2020年显卡天梯图"></p><p>目前最强的显卡是2080ti。预计在2020年底的3080ti发布之前，2080ti还会持续称霸显卡江湖。</p><h3 id="3-2-游戏显卡"><a href="#3-2-游戏显卡" class="headerlink" title="3.2 游戏显卡"></a>3.2 游戏显卡</h3><p>对游戏显卡的调研，参考<a href="https://post.smzdm.com/p/a6lrwk3e/">“什么值得买”上的调研</a>以及<a href="https://www.cnblogs.com/xiaozhi_5638/p/10923351.html">这个网址</a>。</p><p><img src="/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%E8%B0%83%E7%A0%94-%E7%BB%93%E5%90%88%E6%94%BF%E5%BA%9C%E9%87%87%E8%B4%AD%E7%BD%91%E4%BF%A1%E6%81%AF/2020-05-10-22-09-38.png" alt="游戏显卡一览"></p><p><strong>Geforce系列</strong></p><p>这个系列是销量最多、大众最为熟悉的显卡，一般用来打游戏。价格便宜，最新出来的旗舰卡RTX 2080Ti京东售价大概1w左右，根据不同的品牌，价格有所波动。低配置的便宜的一千就能买到。官方定位是消费级，但是它在深度学习上的表现也非常不错，很多人用来做推理、训练，单张卡的性能跟深度学习专业卡Tesla系列比起来其实差不太多，但是性价比却高很多。比如已经停产的GTX 1080显卡的参数基本和深度学习入门级显卡Tesla P4一样，用来做训练和推理的效果比Tesla P4还要好，可是GTX 1080一张卡才卖5000~6000左右，而Tesla P4要卖到1.4w。</p><p>究其原因，很大程度上在于英伟达官方禁止使用GTX、RTX系列显卡用于深度学习等用途，一经使用，自动过保。除了商业考虑外，还包括：Tesla多块显卡合起来的性能不会受很大影响，且Tesla系列显卡功耗优化非常明显，基本都是被动散热，不提供风扇，更适合数据中心机房工作环境等。</p><h3 id="3-3-计算显卡"><a href="#3-3-计算显卡" class="headerlink" title="3.3 计算显卡"></a>3.3 计算显卡</h3><p>专业级显卡的介绍参考<a href="https://product.pconline.com.cn/itbk/diy/graphics/1802/10846244.html">“什么值得买”上的调研</a>以及<a href="https://www.cnblogs.com/xiaozhi_5638/p/10923351.html">这个网址</a>。</p><p><strong>Quadro系列</strong><br>Quadro系列显卡一般用于特定行业，比如设计、建筑等，图像处理专业显卡，比如CAD、Maya等软件，一般人很少用到，价格相对来讲也稍微贵一些，最新的包括RTX 3000/4000/6000/8000型号。</p><p><strong>Tesla系列</strong><br>Tesla系列显卡定位并行计算，一般用于数据中心，具体点，比如用于深度学习，做训练、推理等。阿里云、Amazon云有非常多的GPU服务器，基本都采用Tesla系列显卡。这个系列显卡有个特别明显的特征，那就是贵。Tesla系列入门级显卡 Tesla P4，前面提到过，用来做深度学习的效果比GTX 1080还差，但是价格是后者的3倍多。像其他更高级别的Tesla V100、Tesla P100 价格高达8w、4w，这种价位的显卡虽然性能强劲，但是一般人是买不起的，只有企业数据中心才会部署这种显卡。</p><h3 id="3-4-显卡性能指标"><a href="#3-4-显卡性能指标" class="headerlink" title="3.4 显卡性能指标"></a>3.4 显卡性能指标</h3><p>本部分请参考<a href="https://www.cnblogs.com/xiaozhi_5638/p/10923351.html">这里</a>。</p><h3 id="3-4-显卡罗列"><a href="#3-4-显卡罗列" class="headerlink" title="3.4 显卡罗列"></a>3.4 显卡罗列</h3><p>政府采购网上，值得采购的显卡如下</p><div class="table-container"><table><thead><tr><th>型号</th><th>价格</th></tr></thead><tbody><tr><td>p5000</td><td>27000</td></tr><tr><td>p6000</td><td>43500</td></tr><tr><td>2080</td><td>13500</td></tr><tr><td>k4000</td><td>42450</td></tr><tr><td>p4000</td><td>6800</td></tr><tr><td>2070s</td><td>8000</td></tr><tr><td>8000</td><td>93350</td></tr><tr><td>2080ti</td><td>16000</td></tr><tr><td>1080ti</td><td>8620</td></tr><tr><td>p4</td><td>28000</td></tr><tr><td>2080</td><td>10290</td></tr><tr><td>2060</td><td>5000</td></tr><tr><td>titan rtx</td><td>30000</td></tr><tr><td>p1000</td><td>3500</td></tr><tr><td>2070</td><td>7500</td></tr><tr><td>m2000</td><td>2982</td></tr><tr><td>titan v</td><td>37500</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PCIe</tag>
      
      <tag>显卡</tag>
      
      <tag>工作站</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>chrome升级版本失败解决办法</title>
    <link href="/2020/05/11/chrome%E5%8D%87%E7%BA%A7%E7%89%88%E6%9C%AC%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <url>/2020/05/11/chrome%E5%8D%87%E7%BA%A7%E7%89%88%E6%9C%AC%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="错误描述："><a href="#错误描述：" class="headerlink" title="错误描述："></a>错误描述：</h2><p>在Win7电脑上试图将Chrome从32位的72版本升级到64位的80版本时发生问题，升级进度到62%报错：<br>Chrome安装 未知错误导致安装失败  “0x80040902”</p><p>从chrome官网下载“chromesetup.exe”，打开梯子之后下载成功，在安装过程中也出现未知错误。<br>从Chrome官网下载“Chromestandalonesetup64.exe”，即离线安装包，最后也出现同样的错误。<br>重新启动、进入安全模式、试图结束所有有关google的进程的方法对我都没用。</p><h2 id="最后有效的方法："><a href="#最后有效的方法：" class="headerlink" title="最后有效的方法："></a>最后有效的方法：</h2><p>把原来的Chrome从控制面板的“添加删除程序”中卸载；</p><p>按住windows+R，在“开始”运行中输入“regedit”，打开注册表编辑器，依次进入HKEY_CURRENT_USER\Software\Google\Chrome；</p><p>把Chrome这一项删除，然后重启。再安装就不会存在问题了。</p>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>chrome</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读笔记】Coverage Guided Testing for Recurrent Neural Networks</title>
    <link href="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Coverage-Guided-Testing-for-Recurrent-Neural-Networks/"/>
    <url>/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Coverage-Guided-Testing-for-Recurrent-Neural-Networks/</url>
    
    <content type="html"><![CDATA[<p>本文类比软件测试中覆盖率的概念，提出LSTM网络的覆盖率，并为带有LSTM网络结构的深度学习模型开发了一种覆盖率引导的模糊测试方法，并整合成测试工具testRNN。该测试主要是为了衡量网络的鲁棒性。<br><!--more---></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>本文类比软件测试中覆盖率的概念，提出LSTM网络的覆盖率，为带有LSTM网络结构的深度学习模型开发了一种覆盖率引导的模糊测试方法，并整合成测试工具testRNN。</p><p>另外，本研究还发现对抗样本生成率和覆盖率有相关性，这证明覆盖率是评估带LSTM结构的神经网络的鲁棒性的良好指标，也说明了testRNN能够捕获RNN的错误。</p><p>经实验评估证明，该测试方法能有效生成使模型出错的变异样本，便于测试人员及早发现神经网络模型的问题。</p><p>本文最后解释了上面覆盖率和鲁棒性正相关的原因，认为覆盖率可提高网络的可解释性。</p><h1 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h1><h2 id="回顾RNN、LSTM"><a href="#回顾RNN、LSTM" class="headerlink" title="回顾RNN、LSTM"></a>回顾RNN、LSTM</h2><p>这一部分可以看<a href="https://superlova.github.io/2020/06/20/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91LSTM%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AF%B9%E5%BA%94%E7%9A%84keras%E5%AE%9E%E7%8E%B0/">这里</a>。</p><h2 id="什么是模糊测试"><a href="#什么是模糊测试" class="headerlink" title="什么是模糊测试"></a>什么是模糊测试</h2><p>这一部分可以看<a href="https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Fuzzing-A-Survey/">这里</a>。</p><p>用覆盖率引导测试用例生成。一旦非目标随机化难以提高覆盖率，就以满足未达标测试条件的距离为目标进行突变，以生成corner测试案例。</p><h2 id="关于覆盖率"><a href="#关于覆盖率" class="headerlink" title="关于覆盖率"></a>关于覆盖率</h2><p>在软件测试中，覆盖率是用来度量测试完整性的一个手段，同时也是测试技术有效性的一个度量。</p><p>在软件测试中，覆盖率是基于程序的执行情况来计算的。如果要计算语句覆盖率，覆盖率=（至少被执行一次的程序语句数）/程序中语句的总数；如果要计算判定覆盖率，判定覆盖率=（判定结果被评价的次数）/（判定结果的总数），诸如此类。</p><p>通过覆盖率数据，可以检测我们的测试是否充分，分析出测试方案的弱点在哪方面；也可以指导我们设计能够增加覆盖率的测试用例，有效提高测试质量。</p><p>但是，测试成本随覆盖率的增加而增加，因此不能一味追求构建高覆盖率的测试用例。</p><p>软件覆盖率的概念很早就有人提出来了。但是将其应用到深度学习系统中来，还是最近几年的事情。最早提出深度学习系统覆盖率的论文是<a href="https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91DeepXplore-Automated-Whitebox-Testing-of-Deep-Learning-Systems/">这篇</a>，可以看一下我的阅读笔记。</p><p>随后又有人定义了五花八门的覆盖率，可以看我的<a href="https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Deep-Gauge-Multi-Granularity-Testing-Criteria-for-Deep-Learning-Systems/">这篇</a>论文、<a href="https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91DeepTest-Automated-Testing-of-Deep-Neural-Network-driven-Autonomous-Cars/">这篇</a>论文和<a href="https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91DeepCT-Tomographic-Combinatorial-Testing-for-Deep-Learning-Systems/">这篇</a>论文的阅读笔记。</p><p>关于RNN的覆盖率，当前研究的不多，本文就算一个。</p><h2 id="本文主要贡献"><a href="#本文主要贡献" class="headerlink" title="本文主要贡献"></a>本文主要贡献</h2><ul><li><p>提出三个LSTM的结构覆盖率指标。</p></li><li><p>发现覆盖率越高的样本越容易令模型出错。也就是说，错误样本生成率和本文定义的覆盖率之间呈正相关。</p></li><li><p>提出的三个覆盖率指标不仅都能有效地捕捉LSTM的错误（产生令模型出错的样本），且三个覆盖率的测试目标有所不同。三者相互补充，扩展了测试的多样性。</p></li><li><p>利用testRNN工具生成的测试用例能在目标RNN模型上达到高覆盖率。</p></li><li><p>解释了测试指标背后的语义含义（semantic metrics），能帮助理解LSTM。</p></li></ul><h1 id="二、LSTM结构性覆盖率指标"><a href="#二、LSTM结构性覆盖率指标" class="headerlink" title="二、LSTM结构性覆盖率指标"></a>二、LSTM结构性覆盖率指标</h1><h3 id="LSTM结构信息"><a href="#LSTM结构信息" class="headerlink" title="LSTM结构信息"></a>LSTM结构信息</h3><p>LSTM中有哪些信息值得关注？</p><p>隐状态$\boldsymbol{h}$可以代表LSTM的短期记忆，隐状态$\boldsymbol{c}$则代表相对长期的记忆。</p><p>除此之外，$\boldsymbol{f}$、$\boldsymbol{i}$、$\boldsymbol{o}$三个门的信息也值得关注。</p><ul><li>遗忘门$\boldsymbol{f}<em>{t}$控制上一个时刻的内部状态 $\boldsymbol{c}</em>{t-1}$ 需要遗忘多少信息；</li><li>输入门$\boldsymbol{i}<em>{t}$控制当前时刻的候选状态 ̃$\tilde{\boldsymbol{c}}</em>{t}$ 有多少信息需要保存。</li><li>输出门 $\boldsymbol{o}<em>{t}$ 控制当前时刻的内部状态 $\boldsymbol{c}</em>{t}$ 有多少信息需要输出给外部状态 $\boldsymbol{h}_{t}$</li></ul><p>$\boldsymbol{i}<em>{t}$越大、$\boldsymbol{f}</em>{t}$越小，代表此时的输入对长期记忆影响越大，模型整体上倾向于遗忘过去记住的长期记忆$\boldsymbol{c}<em>{t-1}$，而代替以最新学到的短期记忆$\tilde{\boldsymbol{c}}</em>{t}$。</p><p>如果$\boldsymbol{i}<em>{t}$所有分量都为1，$\boldsymbol{f}</em>{t}$都为0，则代表在该时刻$t$的输入对模型记忆影响很大，模型选择忘记长期记忆而全盘接受此时学到的短期记忆。记忆单元将历史信息清空，并将候选状态向量$\tilde{\boldsymbol{c}}_{t}$写入。</p><p>当$\boldsymbol{f}<em>{t}=\mathbf{1}, \boldsymbol{i}</em>{t}=\mathbf{0}$时，记忆单元将复制上一时刻的内容，不写入新的信息。</p><p>为了量化从LSTM单元的结构中抽象出来的信息值，定义结构信息表示符号：</p><ul><li>定义$\mathcal{S}={f, i, o, c, h}$为五种不同的结构信息集合，代表五个可学习的网络参数；</li><li>$s$代表$\mathcal{S}$中的任意一个，表明测试目标；</li><li>定义$\mathcal{A}={+,-, \mathrm{avg}}$为取正、取负和取平均三种计算方法构成的集合；</li><li>$a$代表$\mathcal{A}$中的任意一个抽象函数；</li><li>$t$代表时间步；</li><li>$x$代表输入的测试用例。</li></ul><p>根据测试目标不同、计算方法不同，构建结构信息表示符号如下：</p><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Coverage-Guided-Testing-for-Recurrent-Neural-Networks/2020-06-20-17-15-06.png" alt></p><p>该符号表示，模型输入$x$，在$t$时间步，被测参数$s$经过$a$的计算方法取得的结果。</p><p><strong>例子1：</strong><br>$\xi<em>{t}^{h,+}$代表$t$时间步时，隐状态$h$的分量中所有正分量的和；$\xi</em>{t}^{h,-}$代表$t$时间步时，隐状态$h$的分量中所有负分量的和。</p><script type="math/tex; mode=display">\begin{array}{l}\xi_{t}^{h,+}=\sum\left\{h_{t}(i)\left|i \in\left\{1, \ldots,\left|h_{t}\right|\right\}, h_{t}(i)>0\right\}\right. \\\xi_{t}^{h,-}=\sum\left\{h_{t}(i)\left|i \in\left\{1, \ldots,\left|h_{t}\right|\right\}, h_{t}(i)<0\right\}\right.\end{array}</script><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Coverage-Guided-Testing-for-Recurrent-Neural-Networks/2020-06-20-20-27-56.png" alt></p><p><strong>例子2：</strong><br>$\xi_{t}^{f, \mathrm{avg}}$则代表$t$时间步时，遗忘门$f$的所有分量的平均值。</p><script type="math/tex; mode=display">\xi_{t}^{f, a v g}=\frac{1}{\left|f_{t}\right|} \sum_{i=1}^{\left|f_{t}\right|} f_{t}(i)</script><p><strong>例子3：</strong><br>以输入门$\boldsymbol{i}$举例来说，$\xi_{x,t}^{i, \mathrm{avg}}&gt;0.9$就是指输入测试用例$x$到模型，在$t$时间步时，将此时$\boldsymbol{i}$向量的所有分量取平均，得到平均值大于0.9的那些时间步对应的向量。即：</p><script type="math/tex; mode=display">\{ \boldsymbol{i}_{t} | \frac{1}{\boldsymbol{D}}\sum_{k=1}^{D}\boldsymbol{i}_{t,k} > 0.9, t=1,\dots,|x|\}</script><p>含义为{$\xi$ | $t$时刻$\boldsymbol{i}$的分量平均值大小$&gt;0.9$}</p><p>由此我们得到了三个覆盖率的定义：</p><h3 id="Boundary-Coverage-BC"><a href="#Boundary-Coverage-BC" class="headerlink" title="Boundary Coverage (BC)"></a>Boundary Coverage (BC)</h3><p>Boundary Coverage衡量结构信息$\xi^{s,a}_{x,t}$中极端情况的出现频度。</p><script type="math/tex; mode=display">\mathrm{BC}=\frac{|\left\{\xi_{t}^{s, a} \geq v_{\max }, \quad \xi_{t}^{s, a} \leq v_{\min } | t \in\{1,\dots,n\}\right\}|}{|\left\{\xi^{s,a}_{t} | t \in\{1,\dots,n\} \right\}|}</script><p>式中，$v<em>{max}$和$v</em>{min}$是阈值，可以取训练时产生的最大和最小值。当然这只是其中一种取法，完全可以自己选择最大阈值和最小阈值。</p><h3 id="Step-wise-Coverage-SC"><a href="#Step-wise-Coverage-SC" class="headerlink" title="Step-wise Coverage (SC)"></a>Step-wise Coverage (SC)</h3><p>量化单步时间语义，即研究一段时间的改变。首先定义</p><script type="math/tex; mode=display">\Delta \xi_{t}^{s}=\left|\xi_{t}^{s,+}-\xi_{t-1}^{s,+}\right|+\left|\xi_{t}^{s,-}-\xi_{t-1}^{s,-}\right|</script><p>分别计算前后时间步的正$\xi$之差与负$\xi$之差，也就是衡量短期记忆的改变程度。该值越大代表该时间步时短期记忆变化过大。由此定义SC：</p><script type="math/tex; mode=display">\mathrm{SC}=\frac{|\left\{\Delta \xi_{t}^{s} \geq v_{\mathrm{SC}} | t \in\{1,\dots,n\}\right\}|}{|\left\{\xi^{s}_{t} | t \in\{1,\dots,n\} \right\}|}</script><p>式中，$v_{\mathrm{sc}}$也是可以自定义的阈值。一般情况下，选取训练过程中能达到的最大值，当做测试时的阈值。</p><h3 id="Temporal-Coverage-TC"><a href="#Temporal-Coverage-TC" class="headerlink" title="Temporal Coverage (TC)"></a>Temporal Coverage (TC)</h3><p>量化整体时间语义。首先定义所有时间步的$\xi$组成的向量集合为：</p><script type="math/tex; mode=display">\xi^{s, a}=\left\{\xi_{t}^{s, a}\right\}_{t=1}^{n}</script><script type="math/tex; mode=display">\xi^{s, a}\in \mathbb{R}^{|x|\times \boldsymbol{D}}</script><p>然后通过统计训练时模型的$\xi^{s, a}$值，将数据从实数域压缩成n分类数据：</p><script type="math/tex; mode=display">\xi^{s, a}\in [1,2,\dots,n]^{|x|\times \boldsymbol{D}}</script><p>在本文中用到的压缩手段是通过拟合训练集得到的分布，然后将拟合的正态分布切成三个区间。当输入测试集时，得到的$\xi$中的各个分量落到哪个区间就变成那个值。</p><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Coverage-Guided-Testing-for-Recurrent-Neural-Networks/2020-06-20-20-27-17.png" alt></p><p>这样做的目的是简化数据计算，方便分析时间步极长的时序数据。</p><p>TC即为落到对应区间的向量占总时间步的比例。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="主体算法"><a href="#主体算法" class="headerlink" title="主体算法"></a>主体算法</h3><p>算法整体架构：</p><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Coverage-Guided-Testing-for-Recurrent-Neural-Networks/2020-06-20-20-24-03.png" alt></p><p>伪代码：</p><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Coverage-Guided-Testing-for-Recurrent-Neural-Networks/2020-06-20-20-19-35.png" alt></p><p>算法1的主体是一个迭代循环（第5-10行），只要没有达到目标覆盖水平（第4行），该循环就会继续。</p><p>在每次迭代中，从输入队列（第5行）中选择一个测试输入x，并按照预定义的变异函数m（第6行）对其进行变异。 </p><p>新生成的测试输入将添加到输入语料库T（第7行）中，在此处进一步选择它们并将它们排序以进行下一次迭代。</p><p>此后，将调用Test Oracle来确定生成的测试用例是否代表网络错误（第9-10行）。</p><h3 id="Step-1-Selection-policies-and-queuing"><a href="#Step-1-Selection-policies-and-queuing" class="headerlink" title="Step 1: Selection policies and queuing"></a>Step 1: Selection policies and queuing</h3><p>所有输入样本的重要性并不相同，有些样本更容易突变产生覆盖率高的变异样本。因此要给输入样本集合排序。</p><p>testRNN优先考虑突变生成以下两种测试输入：</p><ul><li>满足更多测试条件（test conditions）的样本（即，使得覆盖率上升的测试样本）</li><li>更可能导致模型出错的样本，比如对抗样本</li></ul><p>本文采用的突变算法，使用的是其他的生成对抗样本的算法。</p><h3 id="Step-2-Mutation"><a href="#Step-2-Mutation" class="headerlink" title="Step 2: Mutation"></a>Step 2: Mutation</h3><p>TESTRNN测试用例生成的核心是其Mutator引擎。</p><p>随机变异和根据RNN内部信息而特定设计的变异策略均已通过算法1中的变异函数m实现。在下文中，我们将详细讨论TESTRNN中的变异策略。</p><h4 id="Random-Mutation"><a href="#Random-Mutation" class="headerlink" title="Random Mutation"></a>Random Mutation</h4><p>对于连续输入的问题，例如MNIST手写图像识别，我们向输入中添加了具有固定均值和方差的高斯噪声，并期望变异后的输入保持其原始分类。</p><p>然而，对于离散输入的问题，例如对IMDB电影评论的情感分析，IMDB模型的输入是单词序列，其上的随机变化可能导致无法识别（且无效）的文本段落。 </p><p>为了避免这种情况，我们从<a href="https://superlova.github.io/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91EDA-Easy-Data-Augmentation-Techniques-for-Boosting-Performance-on-Text-Classification-Tasks/">EDA工具包</a>中选取了一组突变体，该突变体最初旨在增加训练数据以改进文本分类任务。 这样做的好处是，我们始终可以确保变异的文本段落有效。 </p><p>在我们的实验中，我们考虑了四个突变操作，即M包括（1）同义词替换，（2）随机插入，（3）随机交换，（4）随机删除。 文本含义在所有突变中均保留。</p><p>我们定义了一组突变函数。在新迭代的开始（算法1中的5-10行），m由M中的一个函数随机实例化。</p><h4 id="Targeted-Mutation"><a href="#Targeted-Mutation" class="headerlink" title="Targeted Mutation"></a>Targeted Mutation</h4><p>当随机突变难以提高覆盖率时，我们可以使用目标突变，仅当新测试用例在某些预定义的覆盖损失函数上优于现有测试用例时，才选择新的测试用例。 </p><p>对于关于某些$s\in\mathcal{S}$和$a\in\mathcal{A}$的三类测试条件（BC，SC，TC），我们将覆盖损失函数定义为到它们各自目标的距离，例如</p><script type="math/tex; mode=display">\begin{array}{l}J_{B C}(x)=\xi_{x, t}^{s, a}-v_{\max } \\J_{S C}(x)=\Delta \xi_{x, t}^{s}-v_{\mathrm{SC}} \\J_{T C}(x)=\left\|u_{x}^{\left[t_{1}, t_{2}\right]}-u_{\text {target}}\right\|_{0}\end{array}</script><p>，其中$t$，$t<em>1$，$t_2$为 可以从上下文中推断出的时间步长，$u</em>{x}^{[t1，t2]}$表示输入为$x$时，在时间段[t1，t2]内所有$\xi^{s,a}$的符号表示，而$u_{\text{target}}$是目标符号表示。 $\left|u_1−u_2\right|_0$是汉明距离，计算两个符号表示形式$u_1$和$u_2$之间不同元素的数量。 </p><p>直觉上，覆盖损失$J(x)$用来估计达到未满足测试条件的距离。 </p><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Coverage-Guided-Testing-for-Recurrent-Neural-Networks/2020-06-22-20-01-26.png" alt></p><p>通过生成旨在逐步减少损失的测试用例，目标突变是一种贪婪的搜索算法。</p><h3 id="Step-3-Test-Oracle"><a href="#Step-3-Test-Oracle" class="headerlink" title="Step 3: Test Oracle"></a>Step 3: Test Oracle</h3><p>oracle的中文术语为“结果参照物”，在软件测试中也就是程序预期输出。</p><p>通常使用测试预言 (Test Oracle) 来确定测试用例是否通过。给定一个系统的输入，如何区分相应的期望的、正确的行为和潜在的不正确行为，被称为 “Test Oracle Problem”（测试预言问题）。之所以测试预言在软件测试中是一个问题，主要还是其难以自动化，导致软件测试人员采用肉眼人工测试，效率极低。</p><p>对于testRNN，测试预言包含两个条件：</p><p>(1) 与x’原测试用例x相差小于指定距离，即$\left|x-x’\right|<em>p\leq\alpha</em>{oracle}$;<br>(2) 模型给出的分类与原阿来不同，即$\phi(x)\neq\phi(x’)$;</p><p>我们就把$x’$这种没能通过测试预言的测试用例，成为<strong>对抗样本</strong>。</p><p>条件(1)使用一个限定条件(constraint)来确定生成的测试用例不会被人类察觉，条件(2)则确保生成的测试用例能够令模型出错。</p><blockquote><p>！！注意，这里其实默认了一件事，那就是对深度学习模型的测试过程，就是找对抗样本的过程。但我个人对这种目的存疑。</p></blockquote><p>给定一个LSTM网络和一个特定的测试指标，一旦生成了测试用例，那么<strong>攻击率</strong>就是未通过预言的测试用例的百分比。</p><p><strong>adversary rate:</strong><br>生成测试样例中对抗样本所占比例。</p><p><strong>coverage rate:</strong><br>生成测试样例中满足覆盖率要求的样例所占比例。</p><h1 id="三、评估-Evaluate-Research-Questions"><a href="#三、评估-Evaluate-Research-Questions" class="headerlink" title="三、评估 Evaluate: Research Questions"></a>三、评估 Evaluate: Research Questions</h1><p>RQ1. 为什么我们需要新的覆盖率指标：NC不行<br>RQ2. 覆盖率和对抗样本的关系<br>RQ3. 覆盖率能够发现不同的RNN错误吗？<br>RQ4. 测试用例生成算法能达到高覆盖率吗<br>RQ5. 变异半径和对抗样本生成率的关系<br>RQ6. 这些测试指标和测试结果能帮助解释LSTM内部决策逻辑吗？</p><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><h3 id="待测模型"><a href="#待测模型" class="headerlink" title="待测模型"></a>待测模型</h3><p>作者训练了四种不同的模型，分别对应手写数字识别任务、情感分类任务、化学分子式亲油性预测任务、人体姿势识别任务。不同任务下，模型有些许不同，但都使用了LSTM层。</p><h3 id="测试指标"><a href="#测试指标" class="headerlink" title="测试指标"></a>测试指标</h3><ul><li>BC ($\xi^{f, avg}_{t}$)</li><li>SC ($\Delta\xi^{h}_{t}$)</li><li>TC+ ($\xi^{h,+}_{t}$)</li><li>TC- ($\xi^{h,-}_{t}$)</li></ul><h3 id="变异方法"><a href="#变异方法" class="headerlink" title="变异方法"></a>变异方法</h3><ul><li>手写数字识别任务，在输入图像中随机加入高斯噪声。</li><li>情感分类任务，使用<a href="https://superlova.github.io/2020/06/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91EDA-Easy-Data-Augmentation-Techniques-for-Boosting-Performance-on-Text-Classification-Tasks/">EDA工具包</a>对文本进行变异。</li><li>化学分子式亲油性预测任务，采用Python化学信息学包RDkit对化学式进行变异。</li><li>人体姿势识别任务，在原始视频帧中添加高斯噪声。</li></ul><h3 id="变异距离限制"><a href="#变异距离限制" class="headerlink" title="变异距离限制"></a>变异距离限制</h3><p>为了防止生成的测试用例彻底改变语义，或者是为了降低变动被人类察觉的可能性，我们必须对变异本身加以限制。</p><p>$\alpha_{oracle}$越大，对变异的限制越宽，生成的测试用例与原测试用例差别也就越大。</p><ul><li>手写数字识别任务，欧氏距离限制$\alpha_{oracle} = 0.005$</li><li>情感分类任务，$\alpha_{oracle} = 0.05$</li><li>化学分子式亲油性预测任务，$\alpha_{oracle} = \infty$，对变异不加限制，能否通过Oracle完全取决于分类结果。</li><li>人体姿势识别任务，$\alpha_{oracle} = 0.1$</li></ul><h2 id="RQ1-为什么我们需要新的覆盖率指标"><a href="#RQ1-为什么我们需要新的覆盖率指标" class="headerlink" title="RQ1. 为什么我们需要新的覆盖率指标"></a>RQ1. 为什么我们需要新的覆盖率指标</h2><p>答：因为传统的NC不行。</p><p>首先要介绍一下NC是什么。</p><p>NC是<a href="https://superlova.github.io/2020/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91DeepXplore-Automated-Whitebox-Testing-of-Deep-Learning-Systems/">DeepXplore论文</a>中提出的一种朴素地计算神经网络覆盖率的方法，适用于前馈神经网络的全连接层。</p><p>输入测试用例x，某一层神经网络的神经元覆盖率为：</p><script type="math/tex; mode=display">\text{Neuron Coverage}=\frac{\text{Activated Neurons}}{\text{All Neurons}}</script><p>如果输入一批测试用例，此时只要激活过一次的神经元都算作Activated Neurons。因此随着测试用例变多，NC覆盖率会先升高、后趋近于100%。</p><p>在循环神经网络中如何计算覆盖率？如果我们将一个LSTM网络结构抽象为一层，那么该层的输出向量就是最后的隐变量$h$。</p><p>覆盖率的计算方法如下：</p><script type="math/tex; mode=display">\text{Neuron Coverage}=\frac{\left|\{ i | h(i) > \text{threshold} \}\right|}{\left|\{ i | h(i) \}\right|}</script><p>下面分别是三种不同任务下，随着输入测试用例变多，覆盖率的变化曲线。</p><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Coverage-Guided-Testing-for-Recurrent-Neural-Networks/2020-06-22-20-21-51.png" alt></p><p>可以看到覆盖率曲线不论在哪个任务，都很快就上升到100%。这种覆盖率没有区分度，也就是说，测试用例好与坏无法凭借覆盖率区分开。所以这种覆盖率无法满足我们引导测试用例生成的需要。</p><h2 id="RQ2-覆盖率和对抗样本的关系"><a href="#RQ2-覆盖率和对抗样本的关系" class="headerlink" title="RQ2. 覆盖率和对抗样本的关系"></a>RQ2. 覆盖率和对抗样本的关系</h2><blockquote><p>此处我又有疑问，生成了对抗样本就一定代表模型有缺陷吗？生成对抗样本的难度与模型的好坏有什么关系？这篇论文并没有回答这个问题，而是默认了“对抗样本生成就代表模型的错误”，并根据这个命题走下去了。</p></blockquote><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Coverage-Guided-Testing-for-Recurrent-Neural-Networks/2020-06-22-20-40-25.png" alt></p><p>上表中，分别以三个模型的测试任务，说明了对抗样本的确能够使得覆盖率上升。</p><ul><li>200(O)代表Original，未加扰动的原始样本</li><li>200(N)代表Normal，加了扰动，但是没有改变标签的良性变异样本</li><li>200(A)代表Adversarial，加了扰动并且令模型错误的对抗样本</li></ul><p>SC/BC/TC+/TC-分别是之前定义的四种不同的覆盖率。</p><p>在表格中，良性变异样本的覆盖率比未加扰动的覆盖率高，对抗样本的覆盖率比良性变异样本的覆盖率又高。因此可以说明，对抗样本的确能使得覆盖率上升。</p><blockquote><p>其实相关性也不是特别显著。。。</p></blockquote><h2 id="RQ3-覆盖率能够发现不同的RNN错误吗？"><a href="#RQ3-覆盖率能够发现不同的RNN错误吗？" class="headerlink" title="RQ3. 覆盖率能够发现不同的RNN错误吗？"></a>RQ3. 覆盖率能够发现不同的RNN错误吗？</h2><p>这个问题是想问，不同的覆盖率的测试目标是否有差别？毕竟如果只是针对同一目标进行测试，那么不同的覆盖率只是数学游戏罢了。</p><p>设计实验，分别以SC/BC/TC+/TC-为指标，力图在保证高覆盖率的同时，最小化测试用例数目。</p><p>那么如果以SC为目标最小化的测试用例集合，不能使得其他覆盖率升高，就能在某种程度上说明，SC的测试目标与其他三种覆盖率有所不同。</p><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Coverage-Guided-Testing-for-Recurrent-Neural-Networks/2020-06-22-21-02-39.png" alt></p><blockquote><p>这个实验设计还是比较精妙的，值得借鉴其思路</p></blockquote><p>从表中我们可以看出，以各自覆盖率为优化目标的最小测试集合，都不能使得其他覆盖率得到很大提升。从而可以从侧面佐证作者的想法。</p><p>实际上，BC考虑的是抽象信息的值，SC考虑的是值的单步变化，TC考虑的是值的多步变化。</p><p>它们的设计涵盖了RNN的时间语义的不同方面。</p><h2 id="RQ4-测试用例生成算法能达到高覆盖率的同时生成对抗样本吗"><a href="#RQ4-测试用例生成算法能达到高覆盖率的同时生成对抗样本吗" class="headerlink" title="RQ4. 测试用例生成算法能达到高覆盖率的同时生成对抗样本吗"></a>RQ4. 测试用例生成算法能达到高覆盖率的同时生成对抗样本吗</h2><p>这个问题是想问该方法的有效性。从以下几个方面证明测试案例生成的有效性：(1) 实现高覆盖率并非易事（证明其他方法的无效性）; (2) 在生成的测试用例中有相当数量的对抗样本。</p><p>随机突变不能提升测试用例的覆盖率，需要有针对性的突变（即通过覆盖知识增强的随机突变）来提高覆盖率</p><p>考虑了三种测试用例生成方法：</p><ul><li>（SI）对来自训练数据集的500个种子输入进行采样；</li><li>（RM）通过使用随机突变 (random mutation) 从500个种子中生成2000个测试案例；</li><li>（TM）通过使用针对性突变 (target mutation) 从500个种子中生成2000个测试案例。</li></ul><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Coverage-Guided-Testing-for-Recurrent-Neural-Networks/2020-06-22-21-14-17.png" alt></p><p>表格中表明只有target mutation得到的测试用例，才能够使得覆盖率达到最高值。</p><p>另外从表格可以看到，target mutation比其他方法更高效地生成了对抗样本。</p><h2 id="RQ5-变异半径和对抗样本生成率的关系"><a href="#RQ5-变异半径和对抗样本生成率的关系" class="headerlink" title="RQ5. 变异半径和对抗样本生成率的关系"></a>RQ5. 变异半径和对抗样本生成率的关系</h2><p>如果将每个测试用例当作某个高维空间中的一个点，那么以原测试用例为圆心、以变异距离$\alpha_{oracle}$为半径画一个球，这个球的内部就是我们能忍受的变异样本的生成区域。</p><p>直观上，对抗样本生成率随变异半径单调增加。实验结果也证明这一点。</p><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Coverage-Guided-Testing-for-Recurrent-Neural-Networks/2020-06-22-21-21-33.png" alt></p><p>根据变异距离与对抗样本生成率的变化曲线，我们可以得到有用的结论：</p><p>曲线的斜率、面积等可用于衡量模型鲁棒性。陡峭曲线说明鲁棒性差，大面积说明鲁棒性差。</p><h2 id="RQ6-这些测试指标和测试结果能帮助解释LSTM内部决策逻辑吗？"><a href="#RQ6-这些测试指标和测试结果能帮助解释LSTM内部决策逻辑吗？" class="headerlink" title="RQ6. 这些测试指标和测试结果能帮助解释LSTM内部决策逻辑吗？"></a>RQ6. 这些测试指标和测试结果能帮助解释LSTM内部决策逻辑吗？</h2><h3 id="MNIST手写数字识别模型的覆盖率分析"><a href="#MNIST手写数字识别模型的覆盖率分析" class="headerlink" title="MNIST手写数字识别模型的覆盖率分析"></a>MNIST手写数字识别模型的覆盖率分析</h3><p>下图是MNIST手写数字识别任务利用LSTM模型实现，横坐标是28个Input Feature（其实是每张照片对应28行像素向量），LSTM层将每个Input Feature依次读入，比如时间步t=1的时候读入第一行，t=2读入第二行，以此类推。</p><p>纵坐标则是覆盖次数。Step-wise Coverage的计算方法还记得吗？</p><script type="math/tex; mode=display">\mathrm{SC}=\left\{\Delta \xi_{t}^{s} \geq v_{\mathrm{SC}} | t \in\{1,\dots,n\}\right\}</script><p>当我们输入一张图片，这28个Input Feature就会分成28个时间步，每个时间步都会计算是否满足SC。当我们继续输入整个测试集，每个时间步的满足次数累加起来，就是纵坐标。</p><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Coverage-Guided-Testing-for-Recurrent-Neural-Networks/2020-06-22-21-22-22.png" alt></p><p>同理，Boundary Coverage也是如此计算Coverage Times。</p><p>我们可以分析左图SC的变化：</p><ul><li>开始和结尾的Feature代表图像边缘，SC覆盖率低代表模型不学习；</li><li>第4和第7个Feature往往容易出现敏感像素；</li><li>模型在最后几个时间步拒绝学习新知识（只用老知识就判断这张图是什么数字）。</li></ul><p>右图是BC的变化（$v_{\max}$故意选择比较高的0.85）：</p><ul><li>开始时学新的、忘旧的；</li><li>长期记忆$c_t$往往在时间序列结束时懒散地更新</li></ul><p>这也与BC反应长期记忆、SC反应短期记忆相吻合。</p><h3 id="IMDb情感分析模型的覆盖率分析"><a href="#IMDb情感分析模型的覆盖率分析" class="headerlink" title="IMDb情感分析模型的覆盖率分析"></a>IMDb情感分析模型的覆盖率分析</h3><p>下图是IMDb情感分析模型的覆盖率图：</p><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91Coverage-Guided-Testing-for-Recurrent-Neural-Networks/2020-06-22-21-40-00.png" alt></p><p>IMDb情感分析模型的输入是长度为500的整型数组，每个整数代表一个单词每个单词为一个feature，上面两个是最后50个feature的图，底下两张图是所有500个feature的图。</p><p>关于<a href="https://superlova.github.io/2020/06/09/%E3%80%90%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E3%80%91IMDb%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86/">IMDb影评情感分类任务的LSTM模型解决方案</a>，可以参考我的这篇博客文章。</p><p>比较MNIST任务可知，IMDb任务模型不像MNIST任务模型那样有固定的工作模式，文本中的敏感词可能会出现在文本的任何地方</p><h1 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h1><p>这篇文献阅读报告我鸽了好久，从3月25号决定写，到现在6月22号初步写完。其实这篇论文我在去年的时候就已经打印出来，粗略地读过一遍；去年年底要开题的时候又仔细地读了一遍，但是那次并没有像这次一样，读懂内部的所有细节。</p><p>通过这次编写文献阅读报告的过程，我能感觉到自己对这篇论文的理解更深入了，一个重要的标志就是我能够体会这篇论文内部的缺陷和漏洞，以及确定了值得深挖的方向。</p><p>显然不可能每篇论文都如此阅读，鉴于这篇论文对我的工作有指导意义，所以之前的工作不会白费。</p>]]></content>
    
    
    <categories>
      
      <category>paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RNN</tag>
      
      <tag>testing</tag>
      
      <tag>testRNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读笔记】RNN-Test Adversarial Testing Framework for Recurrent Neural Network Systems</title>
    <link href="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91RNN-Test-Adversarial-Testing-Framework-for-Recurrent-Neural-Network-Systems/"/>
    <url>/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91RNN-Test-Adversarial-Testing-Framework-for-Recurrent-Neural-Network-Systems/</url>
    
    <content type="html"><![CDATA[<p>清华大学和北邮作者共著的大作，思想挺新颖但是至今为止都是挂在arxiv上。读完之后给人感觉像是草稿，机翻色彩严重……<br><!--more---></p><h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h1><p>近年来对于对抗样本的研究主要集中于图像领域，少部分集中在音频领域。而这两种领域的突变攻击，都局限于DNN模型，RNN模型的对抗样本生成则鲜有研究（当时）。</p><p>RNN的对抗测试面临某些挑战，概括为三方面：</p><ul><li>首先，没有明显的类别标签，就没有规则来识别对抗样本。因此传统的对抗性测试对于文本生成模型等无能为力。</li><li>其次，人类能够察觉在文本上的微小修改，且文本扰动很难保证语义不发生变化。</li><li>第三，现有的基于CNN的基于神经元的覆盖度量未能考虑RNN结构的特征，因此无法直接借鉴。</li></ul><p>本文贡献主要有以下四点：</p><ul><li>针对RNN的特定计算逻辑，定义三个覆盖率指标；</li><li>采用覆盖率指南来获取扰动，提出state inconsistency orientation作为引导对抗样本生成的方向，生成对抗样本；</li><li>提出了一种针对RNN的通用对抗测试框架RNN-Test；</li><li>利用RNN-Test测试并优化了PTB语言模型和拼写检查器模型。</li></ul><p>本文提出的新颖思想有：</p><ul><li>在整个输入中，我们仅保留一个单词/字符的扰动即可进行修改，从而确保了微小的修改。</li><li>通过利用测试模型的性能指标来评估对抗性输入的质量</li><li>实验发现，覆盖率值与对抗性输入的质量之间没有线性关系，因此应该付出更多的努力来提高输入质量，而不是覆盖率的值</li></ul><h1 id="2-RNN-Test"><a href="#2-RNN-Test" class="headerlink" title="2. RNN-Test"></a>2. RNN-Test</h1><h2 id="2-1-模块简介"><a href="#2-1-模块简介" class="headerlink" title="2.1 模块简介"></a>2.1 模块简介</h2><p>RNN-Test工作流程如图所示：</p><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91RNN-Test-Adversarial-Testing-Framework-for-Recurrent-Neural-Network-Systems/2020-06-23-23-50-28.png" alt></p><p>RNN-Test三个重要组成部分：</p><ul><li>RNN wrapper</li><li>adversary orientation maximizing</li><li>coverage boosting.</li></ul><p><strong>RNN Wrapper模块</strong>负责提取RNN模型中的hidden states和cell states。</p><p><strong>adversary orientation maximizing模块</strong>将三个orientation方法整合，这三个orientation方法分别是</p><ul><li>本文提出的state inconsistency orientation方法，</li><li>FGSM提到的优化方法，</li><li>DLFuzz提到的优化方法。</li></ul><p><strong>coverage boosting模块</strong>则通过提升覆盖率，以图在RNN的未发现空间中搜索对抗样本</p><p>adversary orientation maximizing模块和coverage boosting模块的结果将集成起来，产生一个联合目标。对这个目标进行梯度上升联合优化求最优解。</p><p>在这里，我们只是从整个顺序输入中随机修改一个单词或字符，以确保所做的修改不足以保持原始语义。</p><p>单词在词嵌入空间中，因此我们以词嵌入空间中两点的距离作为辅助判断，选取突变距离最小的输入作为对抗样本。</p><p>RNN-Test不仅可以用于分类模型，而且可以应用于序列模型。</p><h1 id="3-覆盖率"><a href="#3-覆盖率" class="headerlink" title="3. 覆盖率"></a>3. 覆盖率</h1><p>为了方便后面解释，有如下定义：</p><ul><li>S: 时间步个数</li><li>L: 层数</li><li>B: 批数</li><li>E: 隐状态集合大小</li><li>h: 隐状态集合，|h|=E</li><li>c: cell集合</li><li>g: gate集合</li><li>e: 一个隐状态，e∈h</li></ul><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91RNN-Test-Adversarial-Testing-Framework-for-Recurrent-Neural-Network-Systems/LSTM简介图.png" alt></p><h2 id="3-1-状态覆盖率的意义"><a href="#3-1-状态覆盖率的意义" class="headerlink" title="3.1 状态覆盖率的意义"></a>3.1 状态覆盖率的意义</h2><p>每个RNN单元在每个时间步$t$的输出都是隐藏状态向量$h<em>t$。当输入$x$到了最后一个单词，即执行到最后一个时间步$tp$的时候，输出的还是隐状态$h</em>{tp}$，只不过此时的$h_{tp}$能够真正代表整个序列$x$的所有信息了。之前的$h_t$是在没能读完$x$的情况下做出的草率判断。</p><p>对于LSTM单元，输出还包含表示为$c$的单元状态向量，长度与$h$相同。如果说$h$代表短期记忆，那么$c$就代表长期记忆。</p><p>对于一个输入$x$，如果特定的隐藏状态具有RNN单元输出的最大值，则其映射部分在预测结果中的概率也将更高。</p><blockquote><p>其实这句话我没看懂，原文如下：<br>For one input, if a <strong><em>specific hidden state</em></strong> has <strong><em>the maximum value of the RNN cell outputs</em></strong>, the probabilities of <strong><em>its mapping part of the prediction result</em></strong> tend to be higher as well.<br>As covering the permutations of each hidden state of the RNN cell is extremely time-consuming, covering all <strong>the maximum hidden states</strong> is a feasible solution.</p></blockquote><p>猜想作者应该是想要在所有时间步的$h<em>t$中选择一个最大的$h</em>{t’}$，计算覆盖率时只要覆盖该$h_{t’}$的就算是覆盖了。</p><p>$h$和$c$的所有分量大小在[-1,1]内，都是通过tanh函数计算得到的。本文将这个值域分割成等距离的五个部分，[-1，-0.8，-0.2、0.2、0.8、1]。通过将值范围划分为多个部分，然后记录每个部分的覆盖范围。</p><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91RNN-Test-Adversarial-Testing-Framework-for-Recurrent-Neural-Network-Systems/2020-06-24-21-53-12.png" alt></p><h2 id="3-2-门覆盖率的意义"><a href="#3-2-门覆盖率的意义" class="headerlink" title="3.2 门覆盖率的意义"></a>3.2 门覆盖率的意义</h2><p>采用与状态覆盖相同的机制来计算门的覆盖范围。由于门的值域在[0,1]，因此划分区间[0、0.1、0.4、0.6 ，0.9，1]。</p><h2 id="3-3-覆盖率定义"><a href="#3-3-覆盖率定义" class="headerlink" title="3.3  覆盖率定义"></a>3.3  覆盖率定义</h2><h3 id="Hidden-state-coverage"><a href="#Hidden-state-coverage" class="headerlink" title="Hidden state coverage"></a><strong>Hidden state coverage</strong></h3><p>RNN中的所有Hidden State集合表示成H，H的形状为[S, L, B, E]。S为时间步个数，L为层数，B为batch数，E是每个状态向量的长度。</p><p>对于一个具体地隐状态h向量，形状为(s,l,b)，其中$s\in{1,2,\dots,S}$，$l\in{1,2,\dots,L}$，$b\in{1,2,\dots,B}$，$E=|h|$。</p><p>如果一个状态分量$e\in h$且$e=\max{(h)}$，也就是说$e$是该状态分量中的最大值，那么可以说该状态$e$被覆盖了。覆盖率计算公式如下：</p><script type="math/tex; mode=display">\text{HS\_C}=\frac{|\{e\in h|e=\max{(h)}\}|}{S\times L\times B\times E}</script><h3 id="Cell-state-coverage"><a href="#Cell-state-coverage" class="headerlink" title="Cell state coverage"></a><strong>Cell state coverage</strong></h3><p>RNN中的所有Cell State集合表示成C，C的形状也为[S,L,B,E]。cell state的值域[-1,1]被分割成Sec块，分别是$sec<em>1, sec_2, \dots, sec</em>{Sec}$。每个sec可以表示成$sec<em>i=[v</em>{i-1}, v_i],-1\leq v_i\leq 1$。</p><p>如果一个cell状态分量$e\in c$而且$\tanh(e)\in sec_i, i\in{1,2,\dots, Sec}$，就说状态分量$e$在$sec_i$中被覆盖了（$e$ is covered in $sec_i$）。覆盖率公式如下：</p><script type="math/tex; mode=display">\text{CS\_C}=\frac{|\{e\in c|\tanh(e)\in sec_i\}|}{S\times L\times B\times E}</script><h3 id="Gate-coverage"><a href="#Gate-coverage" class="headerlink" title="Gate coverage"></a><strong>Gate coverage</strong></h3><p>用$g$来代表门，$g\in {i,o,f}$。用来计算这些gate的状态定义为$G_g$，$G_g$的形状是[S, L, B, $E_g$]，$E_g$是状态大小。</p><p>如果一个状态$e\in g$并且$\text{activation}(e)\in sec_i$，$\text{activation}$代表计算该gate所需的激活函数，那么就称状态$e$在$sec_i$中被覆盖了。门覆盖率的计算公式如下：</p><script type="math/tex; mode=display">G_g\_C_{sec_i}=\frac{|\{e\in g|\text{activation}(e)\in sec_i\}|}{S\times L\times B\times E_g}</script><h3 id="DX-coverage"><a href="#DX-coverage" class="headerlink" title="DX coverage"></a><strong>DX coverage</strong></h3><p>这个覆盖率是仿照DeepXplore在DNN上定义的覆盖率，将其迁移到RNN上的结果。</p><p>对于CNN，DeepXplore将每个特征图（卷积层的输出，数百个值的矩阵）视为神经元，并将平均值作为输出。</p><p>如果我们将每个单元格的隐藏状态向量$h$视为神经元，此时像PTB模型这样的普通RNN将仅由一层少于100个神经元组成。这样只需几次输入，覆盖率值便可达到100%。因此，我们将每个隐藏状态向量分量$e$视为神经元。</p><p>对于所有的隐状态集合$H$，有状态$e\in H$。如果输出值在经过max-min normalization之后$\text{out}(e)&gt;t$，t是某个阈值，此时就称状态$e$被覆盖。计算公式如下所示：</p><script type="math/tex; mode=display">DX\_C=\frac{|\{e|\text{out}(e)>t\}|}{S\times L\times B\times E}</script><h1 id="4-对抗定向优化方法"><a href="#4-对抗定向优化方法" class="headerlink" title="4 对抗定向优化方法"></a>4 对抗定向优化方法</h1><p>与训练过程相反，该过程通过使得输入变异，试图升高预先定义的orientation，来引导变异过程，最终使得模型判断错误。</p><p>可以选择的orientation有以下三个：</p><h2 id="1-state-inconsistency-orientation"><a href="#1-state-inconsistency-orientation" class="headerlink" title="1) state inconsistency orientation"></a>1) state inconsistency orientation</h2><script type="math/tex; mode=display">\text {obj\_orient}=h_{t-1}^{l}+c_{t}^{l}-h_{t}^{l}</script><p>t时刻的orientation，与t-1时刻的hidden state正相关，与t时刻的cell state正相关，与t时刻的hidden state负相关。</p><p>也就是说如果想优化state inconsistency orientation，就必须找到一个$x<em>t$，令$h</em>{t-1}$和$c_t$越大越好，$h_t$越小越好。</p><script type="math/tex; mode=display">\left[\begin{array}{c}\tilde{\boldsymbol{c}}_{t} \\\boldsymbol{o}_{t} \\\boldsymbol{i}_{t} \\\boldsymbol{f}_{t}\end{array}\right]=\left[\begin{array}{c}\tanh \\\sigma \\\sigma \\\sigma\end{array}\right]\left(\boldsymbol{W}\left[\begin{array}{c}\boldsymbol{x}_{t} \\\boldsymbol{h}_{t-1}\end{array}\right]+\boldsymbol{b}\right)</script><script type="math/tex; mode=display">\boldsymbol{c}_{t}=\boldsymbol{f}_{t} \odot \boldsymbol{c}_{t-1}+\boldsymbol{i}_{t} \odot \tilde{\boldsymbol{c}}_{t}</script><script type="math/tex; mode=display">\boldsymbol{h}_{t}=\boldsymbol{o}_{t} \odot \tanh \left(\boldsymbol{c}_{t}\right)</script><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91RNN-Test-Adversarial-Testing-Framework-for-Recurrent-Neural-Network-Systems/2020-06-26-21-20-16.png" alt></p><p>基于LSTM的设计理念，c代表较长的记忆，h代表短期记忆。</p><p>按公式可看出$c<em>t$、$h_t$、$h</em>{t-1}$应该是同增同减、状态一致。因此使$c<em>t$、$h</em>{t-1}$和$h_t$分开，就是使状态不一致程度上升。</p><h2 id="2-Cost-orientation"><a href="#2-Cost-orientation" class="headerlink" title="2) Cost orientation"></a>2) Cost orientation</h2><p>FGSM论文中提到的方法。全称为Fast Gradient Sign Method。该方法的优化目标是模型训练时使用的loss函数，然后利用一遍优化，计算对抗样本。</p><p>一个对抗样本攻击过程，与训练过程可以对比如下：</p><script type="math/tex; mode=display">L_{t r a i n}(\theta)=C\left(y^{0}, y^{t r u e}\right)</script><p>训练时loss函数作用是，在输入x固定的条件下，通过改变模型参数$\theta$来优化loss使之最小化。</p><p>对抗样本攻击则是在参数$\theta$固定的条件下，通过改变输入x，来优化loss使之最大化。</p><p>如果只是想要让模型出错，这种攻击手段成为无目标攻击 (Non-target Attack)</p><script type="math/tex; mode=display">L\left(x^{\prime}\right)=-C\left(y^{\prime}, y^{t r u e}\right)</script><p>如果指定了目标，试图让x向着指定的类别突变，这种攻击就被称为 (Target Attack)。</p><script type="math/tex; mode=display">L\left(x^{\prime}\right)=-C\left(y^{\prime}, y^{t r u e}\right)+C\left(y^{\prime}, y^{f a l s e}\right)</script><p>但是突变程度有一定限制，要求突变前后图片变化小于一个阈值，意思是不要被发现。</p><script type="math/tex; mode=display">d\left(x^{0}, x^{\prime}\right) \leq \varepsilon</script><p>总的来说，一个突变算法可以选择不同的优化方法，也许要选择不同的限制方法。<br><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91RNN-Test-Adversarial-Testing-Framework-for-Recurrent-Neural-Network-Systems/2020-06-26-21-38-26.png" alt></p><p>而FGSM算法选取的攻击手段是“一次求导”，通过依次计算损失函数对输入x的每个分量的偏导，如果为正数则值=1，负数则=-1，无论其大小。</p><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91RNN-Test-Adversarial-Testing-Framework-for-Recurrent-Neural-Network-Systems/2020-06-26-21-41-53.png" alt></p><p>效果就是每个像素点要不就+$\epsilon$，要不就-$\epsilon$。</p><h2 id="3-Decision-boundary-orientation"><a href="#3-Decision-boundary-orientation" class="headerlink" title="3) Decision boundary orientation"></a>3) Decision boundary orientation</h2><p>决策边界定向旨在降低原始预测标签的概率，并增加其他前k个标签在预测中的概率。</p><script type="math/tex; mode=display">\text {obj\_orient}=(\sum^{k}_{i=0}\hat{y}_{t_i})-\hat{y}_{t}</script><p>$\hat{y}_{t}$是softmax层的输出向量。</p><h1 id="5-对抗样本搜索的过程"><a href="#5-对抗样本搜索的过程" class="headerlink" title="5. 对抗样本搜索的过程"></a>5. 对抗样本搜索的过程</h1><h2 id="5-1-RNN-Test整体算法"><a href="#5-1-RNN-Test整体算法" class="headerlink" title="5.1 RNN-Test整体算法"></a>5.1 RNN-Test整体算法</h2><p>在keras的Sequence模型中，可以通过修改参数设置，使模型输出所有时间步的hidden state和cell state。</p><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91RNN-Test-Adversarial-Testing-Framework-for-Recurrent-Neural-Network-Systems/Alg1_1.png" alt></p><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91RNN-Test-Adversarial-Testing-Framework-for-Recurrent-Neural-Network-Systems/Alg1_2.png" alt></p><p>在1~8行，通过<code>retrieve_state</code>获得模型此时的state信息。其实我比较感兴趣<code>retrieve_state</code>方法是如何实现的</p><p>首先获得模型内部信息，根据优化目标计算能够最大化提升该目标的梯度方向。</p><p>然后第9~10行，根据梯度上升法，优化并生成新的变异体x。</p><p>11~13行，判断是否真的生成了能够使模型出错的对抗样本。</p><p>在15行，衡量此时模型的性能，如果是分类模型就看loss大小，如果是生成文本模型就看perplexity。</p><p>在第22~27行，生成对抗样本的过程中，对一段文本的变异方法，是通过在词嵌入空间中搜索最近的单词/字符，替换该时间步t对应的字符。</p><h2 id="5-2-覆盖率提升方法"><a href="#5-2-覆盖率提升方法" class="headerlink" title="5.2 覆盖率提升方法"></a>5.2 覆盖率提升方法</h2><p>选择n个没被覆盖到的state，RNN-Test会尽可能提升他们的值。</p><script type="math/tex; mode=display">o b j_{-} c o v=\sum_{i=0}^{n} e, e \in\{h, c\}</script><p>RNN-Test还会挑选在边界或者端点的state，将它们的值设成边界值。</p><p><img src="/2020/03/25/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91RNN-Test-Adversarial-Testing-Framework-for-Recurrent-Neural-Network-Systems/Alg2.png" alt></p><blockquote><p>这一块我看的很乱，可能不是我的问题而是作者写作的问题。</p></blockquote><h1 id="6-总结"><a href="#6-总结" class="headerlink" title="6.总结"></a>6.总结</h1><ol><li>本文肯定不是作者的终稿，还有很多没说明白的地方，遣词造句也有一股中式英语的风味。</li><li>新颖的地方包括覆盖率，但是这一块没看懂，作者也没把公式定义明白。</li><li>其二是采用状态不一致程度来衡量模型不稳定性，并以此引导生成对抗样本，本来思想很新颖，但是突变样本的方法就是最简单的文本替换，在词嵌入空间中搜一个最近的单词向量。杀鸡焉用牛刀？</li></ol>]]></content>
    
    
    <categories>
      
      <category>paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RNN</tag>
      
      <tag>testing</tag>
      
      <tag>RNN-Test</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>lintcode-138 子数组求和问题</title>
    <link href="/2020/03/24/lintcode-138-%E5%AD%90%E6%95%B0%E7%BB%84%E4%B9%8B%E5%92%8C/"/>
    <url>/2020/03/24/lintcode-138-%E5%AD%90%E6%95%B0%E7%BB%84%E4%B9%8B%E5%92%8C/</url>
    
    <content type="html"><![CDATA[<ul><li>给定一个整数数组，找到和为零的子数组。</li><li>你的代码应该返回满足要求的子数组的起始位置和结束位置<span id="more"></span><h1 id="lintcode-138：子数组之和"><a href="#lintcode-138：子数组之和" class="headerlink" title="lintcode 138：子数组之和"></a>lintcode 138：子数组之和</h1><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2></li></ul><p>给定一个整数数组，找到和为零的子数组。你的代码应该返回满足要求的子数组的起始位置和结束位置<br><strong>样例 1:</strong><br>输入: [-3, 1, 2, -3, 4]<br>输出: [0,2] 或 [1,3]<br>样例解释： 返回任意一段和为0的区间即可。<br><strong>样例 2:</strong><br>输入: [-3, 1, -4, 2, -3, 4]<br>输出: [1,5]<br><strong>注意事项</strong><br>至少有一个子数组的和为 0</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>子数组之和问题。看看哪个区间段，段内所有元素加起来刚好等于0（或者某个值）。类似这种求区间段，段内元素满足什么条件的问题。</p><p>关键是下面这个结论：</p><p>准备一个数组array，其中第i个元素保存num[0]~num[i]之和。那么index_value中一旦出现两个元素其值相同，就说明这两个下标之间所有元素加起来等于0。</p><script type="math/tex; mode=display">\begin{aligned}& if & \sum_{i=0}^{\operatorname{index_1}}nums(i) = \sum_{i=0}^{\operatorname{index_2}}nums(i) \\ & then\quad & return \left[ \operatorname{index_1}+1, \operatorname{index_2} \right] \end{aligned}</script><p>举个例子：对于数组<code>num = [-3, 1, 2, -3, 4]</code>，我们可以构建array数组如下：</p><div class="table-container"><table><thead><tr><th>index</th><th>nums[index]</th><th>$\sum_{i=0}^{index}nums(i)$</th></tr></thead><tbody><tr><td>0</td><td>-3</td><td>-3</td></tr><tr><td>1</td><td>1</td><td>-2</td></tr><tr><td>2</td><td>2</td><td>0</td></tr><tr><td>3</td><td>-3</td><td>-3</td></tr><tr><td>4</td><td>4</td><td>1</td></tr></tbody></table></div><p>返回 [0, 2] 或 [1, 3]</p><p>在代码实现中，当我们采用数组实现array时，会受限于查询array内元素的线型时间复杂度，为了找某个值对应的下标，遍历array数组的过程，可能耗费线性复杂度的时间，导致代码TLE超时。</p><p>因此我们采用散列，将散列的key设置为前i个元素的和值，value为该值对应的下标位置。</p><p>在Python中查找元素，用<strong>字典</strong>可以大大加快查找速度。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><pre><code class="lang-python">class Solution:    &quot;&quot;&quot;    @param nums: A list of integers    @return: A list of integers includes the index of the first number and the index of the last number    &quot;&quot;&quot;    def subarraySum(self, nums):        index_value = &#123;&#125;        accumulator = 0        for i in range(len(nums)):            accumulator += nums[i]            if accumulator in index_value:                return [index_value[accumulator] + 1, i]            else:                index_value[accumulator] = i        else:            if accumulator == 0:                return [0, i]        return [0, 0]</code></pre><h2 id="变种：子数组元素之和等于k"><a href="#变种：子数组元素之和等于k" class="headerlink" title="变种：子数组元素之和等于k"></a>变种：子数组元素之和等于k</h2><script type="math/tex; mode=display">\begin{aligned}& if & \sum_{i=0}^{\operatorname{index_1}}nums(i) - \bold{k} = \sum_{i=0}^{\operatorname{index_2}}nums(i) \\ & then\quad & return \left[ \operatorname{index_1}+1, \operatorname{index_2} \right] \end{aligned}</script><pre><code class="lang-python">class Solution:    &quot;&quot;&quot;    @param nums: A list of integers    @return: A list of integers includes the index of the first number and the index of the last number    &quot;&quot;&quot;    def subarraySum(self, nums, obj_num):        index_value = &#123;&#125;        accumulator = 0        for i in range(len(nums)):            accumulator += nums[i]            if accumulator - obj_num in index_value:                return [index_value[accumulator - obj_num] + 1, i]            else:                index_value[accumulator] = i        else:            if accumulator == 0:                return [0, i]        return [0, 0]</code></pre>]]></content>
    
    
    <categories>
      
      <category>code_exercises</category>
      
    </categories>
    
    
    <tags>
      
      <tag>algorithm</tag>
      
      <tag>lintcode</tag>
      
      <tag>array</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】C++字符串高级操作总结</title>
    <link href="/2020/03/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91C-%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/"/>
    <url>/2020/03/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91C-%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>C++的字符串操作非常多，功能也非常多样化，熟练使用标准库提供的字符串操作函数能够高效提升我们编写代码的效率和可读性。除了常用的<string>库中包含的几项基本操作之外，本文总结了几项特别好用而又不为人所知的高级操作，包括\<regex\>、string_view等。<br><span id="more"></span></regex\></string></p><h2 id="一、常见的基本操作回顾"><a href="#一、常见的基本操作回顾" class="headerlink" title="一、常见的基本操作回顾"></a>一、常见的基本操作回顾</h2><p>必须指明，<string>中字符串方法可以按照输入参数的类型不同调用不同的重载方法，这些函数名相同，但是参数类型和顺序完全不同，返回值也略有差别。</string></p><p>部分函数，包括insert和erase等函数可分为两类，如果输入的位置参数<code>pos</code>为整数<code>int</code>，此时返回值为<strong>被插入字符串</strong>的引用；而输入的位置参数类型为迭代器<code>iterator</code>，则会调用返回迭代器的函数，该迭代器<strong>指向被插入部分的头部</strong>。</p><p>而且<string>部分函数为了兼容C原生字符串，提供了一批适用于C String构造接口，这又产生了一大批只有参数顺序不同的同名函数。比如对于string构造方法上，输入原生string和输入C String的参数含义完全不同。</string></p><p>这些同名、功能相似但不同参数的函数使得C++新人学习标准库时容易产生极大的困扰。</p><h3 id="1-构造string"><a href="#1-构造string" class="headerlink" title="1. 构造string"></a>1. 构造string</h3><pre><code class="lang-cpp">const char * cp = &quot;Hello World!!!&quot;;char noNull[] = &#123;&#39;H&#39;, &#39;i&#39;&#125;;</code></pre><p>输入一个char类型的指针，以及偏移量。转换从指针开始的偏移量个字符。<br>如果未指定偏移量，则默认转化到碰到’\0’为止。<br>没有’\0’结尾则该行为未定义。  </p><pre><code class="lang-cpp">string s1(cp); // 从C风格字符串转化string s2(noNull, 2); // 指定转化的字符个数string s3(noNull); // 未定义，因为noNull不是以空字符结尾string s4(cp + 6, 5);</code></pre><p>拷贝构造函数，从其他string拷贝<br>指定拷贝位置和拷贝字符个数<br>不指定pos则默认从头拷贝<br>不指定len则默认从pos开始全拷贝<br>pos越界则抛出异常<br>len越界没问题，只到’\0’  </p><pre><code class="lang-cpp">string s5(s1, 6, 5);string s6(s1, 6);string s7(s1, 6, 20);string s8(s1, 16);</code></pre><p>substr函数，输入pos和len<br>返回由该字符串的第pos位置拷贝len个字符组成的新子串  </p><pre><code class="lang-cpp">string s9 = s1.substr(0, 5);</code></pre><h3 id="2-改变string"><a href="#2-改变string" class="headerlink" title="2. 改变string"></a>2. 改变string</h3><p>以insert为例，简单介绍不同参数重载的不同insert。</p><p>insert除了接受迭代器的版本之外，还有直接接受下标的版本。返回值为被插入字符串的引用。<br>s.insert(pos, count, char)<br>s.insert(pos, char_ptr, len)<br>s.insert(pos, string, pos, len)</p><p>第一个位置总会是pos，表示被插入位置；</p><p>第二个参数如果是个数，那么你调用的是第一个insert函数，其含义为重复插入第三个参数char所制定的内容；</p><p>第二个参数如果是C风格字符串，那么第三个参数可以指出插入长度，不指名就默认把该C风格字符串全插到pos的位置；</p><p>第二个参数如果是string，那么你还需在string参数后指定从哪个pos开始插，并且指定len表示插入多少个。相对于插入C风格字符串的insert来说，插入string更灵活。</p><p>其他函数及其说明见下表。</p><p><img src="/2020/03/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91C-%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/2020-03-24-20-38-14.png" alt><br><img src="/2020/03/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91C-%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/2020-03-25-08-58-36.png" alt></p><h2 id="二、搜索字符串"><a href="#二、搜索字符串" class="headerlink" title="二、搜索字符串"></a>二、搜索字符串</h2><p><string>定义了六种不同的搜索方法，每种方法拥有四个重载版本。</string></p><p><img src="/2020/03/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91C-%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/2020-03-25-09-00-50.png" alt><br><img src="/2020/03/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91C-%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/2020-03-25-09-02-09.png" alt></p><p>需要注意的有两点：</p><ol><li>搜索函数返回类型为string::size_type，为无符号整数类型。</li><li>搜索失败时，返回string::npos，该值为-1，也就是无符号整数最大的值。</li></ol><h2 id="三、正则表达式库"><a href="#三、正则表达式库" class="headerlink" title="三、正则表达式库"></a>三、正则表达式库</h2><p>正则表达式是字符串匹配的有力工具。C++11加入了对正则表达式的支持，具体定义位于<regex>头文件中。</regex></p><p>在C++中，正则表达式可以做的工作有：</p><ul><li>Match 将整个输入拿来比对（匹配）某个正则表达式</li><li>Search 查找与正则表达式吻合的子串</li><li>Tokenize 根据正则表达式切分字符串</li><li>Replace 根据正则表达式替换字符串</li></ul><h3 id="第一种应用：Match和Search"><a href="#第一种应用：Match和Search" class="headerlink" title="第一种应用：Match和Search"></a>第一种应用：Match和Search</h3><p>具体流程可概括为：定义、匹配、判断</p><pre><code class="lang-cpp">regex reg(&quot;&lt;.*&gt;.*&lt;/.*&gt;&quot;); // 定义regex reg_num(R&quot;(\d)&quot;); // 以 R&quot;(...)&quot; 的方式定义，相当于 Python 中的 r&quot;...&quot; ，特点是内部反斜杠不需要写两遍regex reg(R&quot;(\S+)&quot;, regex::icase); // icase 表示忽略大小写bool isExist = regex_match(string, reg); // 匹配整体，全部匹配则返回 true，注意参数顺序//orbool isExist = regex_search(string, reg); // 匹配部分，存在子串匹配则返回 true。注意参数顺序cout &lt;&lt; boolalpha &lt;&lt; isExist &lt;&lt; endl; // 判断，boolalpha 为 cout 的参数，表示将 bool 变量按照 true/false 输出而不是输出 0 和 1</code></pre><p>你可能已经注意到了，regex_match和regex_search返回的仅仅是一个bool值，表明是否匹配。我们还需要匹配的位置。此时我们需要一个<code>match</code>对象来保存结果。</p><p>需要提取结果的场合，我们使用<code>regex_search()</code>和<code>regex_match()</code>的另一个版本，参数多了一个 match，运行完毕后 match 会保存有结果。</p><p><code>match</code>对象的方法如下所示。</p><pre><code class="lang-cpp">smatch m;bool isExist = regex_search(string, m, reg); // 结果保存在m中m.empty()m.size() // 返回匹配个数m.str(i) // 类似于python中group，返回第i个匹配位置的字符串。i=0则返回全部m.length(i) // 同上，返回第i个匹配字符串的长度m.position(i) // 同上，返回第i个匹配字符串的位置m.prefix().str() // 已匹配位置之前的字符串，字符串前缀m.suffix().str() // 已匹配位置之后的字符串，字符串后缀for (auto pos = m.begin(); pos != m.end(); ++pos) &#123;    cout &lt;&lt; *pos &lt;&lt; endl;&#125;代码中的参数i，代表了正则表达式中存在分组，i为提取分组i的被匹配内容。类似python中的group。分组操作是正则表达式的语法，本文不再赘述。</code></pre><p><code>match</code>对象根据保存内容类型不同分成</p><ul><li><code>smatch</code> 匹配string</li><li><code>cmatch</code> 匹配C风格字符串</li><li><code>wsmatch</code> 匹配wstring</li><li><code>wcmatch</code> 匹配const wchar_t*</li></ul><h3 id="第二种应用：Regex-Iterator"><a href="#第二种应用：Regex-Iterator" class="headerlink" title="第二种应用：Regex Iterator"></a>第二种应用：Regex Iterator</h3><p>data可能很长，reg可能会多次匹配。为了迭代所有的匹配子串，我们可以使用<code>regex_iterator</code>。</p><p>根据类型不同，分别是</p><ul><li><code>sregex_iterator</code></li><li><code>cregex_iterator</code></li><li><code>wsregex_iterator</code></li><li><code>wcregex_iterator</code></li></ul><pre><code class="lang-cpp">regex reg(&quot;...&quot;);sregex_iterator pos(data.cbegin(), data.cend(), reg);sregex_iterator end; // 空迭代器即可表示 endfor (; pos != end; ++pos) &#123;    cout &lt;&lt; pos-&gt;str() &lt;&lt; endl;&#125;</code></pre><p>如何统计匹配个数？可以借助<iterator> 中的<code>distance()</code>来计算两个迭代器之间的距离。</iterator></p><p>比如要统计一句话中单词的个数：</p><pre><code class="lang-cpp">regex reg(R&quot;(\S+)&quot;);string text = &quot;The cat sat on the mat&quot;;sregex_iterator begin&#123;text.cbegin(), text.cend(), reg&#125;;sregex_iterator end;#include &lt;iterator&gt;auto count = std::distance(begin, end); // 专门计算迭代器之间的距离cout &lt;&lt; count &lt;&lt; endl;  // output: 6</code></pre><h3 id="第三种应用：Regex-Token-Iterator"><a href="#第三种应用：Regex-Token-Iterator" class="headerlink" title="第三种应用：Regex Token Iterator"></a>第三种应用：Regex Token Iterator</h3><p>你可能关注的不是被匹配的字符串，而是其余的字符串。此时正则表达式串就像是切割刀一样，将data分割成不含被匹配串的几部分。我们可以利用此功能实现C++中一直没能实现的字符串分割函数split。</p><pre><code class="lang-cpp">string data = &quot;qqq www    eee rrr&quot;;regex r(&quot;\\s+&quot;);sregex_token_iterator pos(data.cbegin(), data.cend(), r, -1); // -1代表你对正则表达式匹配的内容不感兴趣sregex_token_iterator end;for (; pos != end; ++pos) &#123;    cout &lt;&lt; *pos &lt;&lt; endl;&#125;</code></pre><h3 id="第四种应用：替换"><a href="#第四种应用：替换" class="headerlink" title="第四种应用：替换"></a>第四种应用：替换</h3><p>下面的代码将</p><pre><code class="lang-html">&lt;person&gt;&lt;first&gt;Nico&lt;/first&gt;&lt;/person&gt;</code></pre><p>替换成</p><pre><code class="lang-html">&lt;person&gt;&lt;first value=&quot;Nico&quot;/&gt;&lt;/person&gt;</code></pre><pre><code class="lang-cpp">string data = &quot;&lt;person&gt;&lt;first&gt;Nico&lt;/first&gt;&lt;/person&gt;&quot;regex reg(&quot;&lt;(.*)&gt;(.*)&lt;/(\\1)&gt;&quot;);string replace_pattern = &quot;&lt;$1 value=\&quot;$2\&quot;/&gt;&quot;;cout &lt;&lt; regex_replace(data, reg, replace_pattern) &lt;&lt; endl;</code></pre><p>模式替换串用$n指定第几个匹配部分group(n)<br>$1 value=$2 含义即为原来是group(1)的部分替换成group(2)的内容。</p><p><img src="/2020/03/23/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91C-%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/2020-03-25-10-21-38.png" alt></p><h2 id="四、string-view"><a href="#四、string-view" class="headerlink" title="四、string_view"></a>四、string_view</h2><p>C++17加入了string_view对象，能够避免string类型的复制临时对象操作。</p><ul><li>string_view对象由两部分组成，分别是<strong>数据的起始指针</strong>和<strong>数据的长度</strong>。有点类似于带其他语言表示字符串的方法，不依赖<code>&#39;\0&#39;</code>在结尾，而是通过一个变量记忆长度。</li><li>string_view只读，不能修改。可以很好地作为函数的参数和返回值</li></ul><pre><code class="lang-cpp">//使用string的拷贝操作string s(1000, &#39;0&#39;);string sub_s = s.substr(100, 200); // O(n)//使用view则不需要拷贝string_view sv(s); // no copystring_view sv2 = sv.substr(100, 200); // O(1)</code></pre>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>C++</tag>
      
      <tag>STL</tag>
      
      <tag>string</tag>
      
      <tag>regex</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>C++关联容器学习笔记</title>
    <link href="/2020/03/20/C-%E5%85%B3%E8%81%94%E5%AE%B9%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2020/03/20/C-%E5%85%B3%E8%81%94%E5%AE%B9%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="1-关联容器特点简介"><a href="#1-关联容器特点简介" class="headerlink" title="1. 关联容器特点简介"></a>1. 关联容器特点简介</h2><p>关联容器和顺序容器是两种适用范围不同的容器。许多C++程序员只用过顺序容器诸如vector和string，但他们从未使用过set和map等关联数据结构。</p><p><img src="/2020/03/20/C-%E5%85%B3%E8%81%94%E5%AE%B9%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2020-03-20-09-47-21.png" alt></p><p><code>set</code>是元素的简单集合，用来保存类型相同的一组元素。当你只是想知道一个值<strong>是否存在</strong>时，<code>set</code>是最有用的。</p><ul><li><code>set</code><strong>不会出现重复元素</strong></li><li>内部元素永远<strong>有序</strong></li><li><code>set</code>中的元素一经添加就<strong>不能修改</strong>。</li></ul><p><code>map</code>可以看做特殊的<code>vector</code>，其特殊之处在于此<code>map</code>中的每个元素都由两部分 <code>(key, value)</code> 构成，C++将每个这样的 <code>(key, value)</code> 封装成一个对象，其类别为<code>pair</code>。<code>map</code>内部元素都为<code>pair</code>类型。</p><ul><li><code>map</code><strong>不会出现重复<code>key</code></strong></li><li>内部元素永远有序，按照<code>pair</code>类型元素的key字段排序（key字段必须能够被排序）。</li><li><code>map</code>中的key一经添加就<strong>不能修改</strong>。</li></ul><h2 id="2-有序容器基本操作"><a href="#2-有序容器基本操作" class="headerlink" title="2. 有序容器基本操作"></a>2. 有序容器基本操作</h2><p>以下所有操作需要添加头文件：</p><pre><code class="lang-cpp">#include &lt;map&gt;#include &lt;set&gt;</code></pre><h3 id="2-1-初始化"><a href="#2-1-初始化" class="headerlink" title="2.1 初始化"></a>2.1 初始化</h3><h4 id="2-1-1-map"><a href="#2-1-1-map" class="headerlink" title="2.1.1 map"></a>2.1.1 map</h4><pre><code class="lang-cpp">map&lt;int, int&gt; m; // 默认初始化map&lt;int, int&gt; m&#123;&#123;1, 2&#125;, &#123;2, 4&#125;&#125;; // 初始化列表map&lt;int, int&gt; m2(m); // 拷贝构造函数map&lt;int, int&gt; m = &#123;&#123;1, 2&#125;, &#123;2, 4&#125;&#125;; // 初始化列表 + 拷贝构造函数// pair和make_pair()在#include&lt;utility&gt;中map&lt;int, int&gt; m3 = &#123;    std::pair&lt;int, int&gt;(1, 2), std::pair&lt;int, int&gt;(2, 4)&#125;; // 初始化列表的方法展开来说就是这样map&lt;int, int&gt; m4&#123;    std::make_pair(1, 2), std::make_pair(2, 4)&#125;; // 或者使用make_pair函数，免得输入参数类型，效果等价。// std::begin()和end()在#include &lt;iterator&gt; 中map&lt;int, int&gt; m5 = &#123;std::begin(m), std::end(m)&#125;; // 迭代器，前提是被迭代的对象内部元素类型是pair</code></pre><h4 id="2-1-2-set"><a href="#2-1-2-set" class="headerlink" title="2.1.2 set"></a>2.1.2 set</h4><pre><code class="lang-cpp">set&lt;int&gt; s; // 默认初始化set&lt;int&gt; s&#123;1, 2, 3, 6, 9, 10&#125;; // 初始化列表set&lt;int&gt; s2(s); // 拷贝构造函数set&lt;int&gt; s = &#123;1, 2, 3, 6, 9, 10&#125;; // 初始化列表 + 拷贝构造函数// std::begin()和end()在#include &lt;iterator&gt; 中int arr[] = &#123;1, 2, 5, 8, 9&#125;;set&lt;int&gt; s2&#123;std::begin(arr), std::end(arr)&#125;; // 迭代器</code></pre><h4 id="2-1-3-定义排序方法"><a href="#2-1-3-定义排序方法" class="headerlink" title="2.1.3 定义排序方法"></a>2.1.3 定义排序方法</h4><p>可以给map或set初始化时输入一个比较器，用以替代原有的比较key大小的方法（原来一般是小于号）。这个比较器可以是一个函数，也可以是一个当做函数用的函数对象。下面是一个示例。</p><pre><code class="lang-cpp">struct comp &#123;    template&lt;typename T&gt;    bool operator() (const T&amp; l, const T&amp; r) const &#123;        return l &gt; r;    &#125;&#125;;int main() &#123;    map&lt;int, int, comp&gt; m = &#123;        &#123;1, 5&#125;, &#123;2, 3&#125;, &#123;7, 6&#125;    &#125;;    for (auto&amp;&amp; [key, val] : m) &#123;        cout &lt;&lt; key &lt;&lt; &quot; &quot; &lt;&lt; val &lt;&lt; endl;    &#125;&#125;output&gt;&gt; 7 6&gt;&gt; 2 3&gt;&gt; 1 5</code></pre><p>实现自己写的比较器，简单来说就是实现一个自定义的“&lt;”小于号。<br>原有的比较器<code>bool compare(object&amp; left, object&amp; right)</code>作用如下：</p><ol><li>compare输出true，map 认为 left 小于 right ，把left放在right前面。</li><li>compare输出false，map 认为 left 大于等于 right。</li><li>如果compare(left, right)为false，compare(right, left)也为false，就认为left==right。否则left放在right后面。</li></ol><blockquote><p>自己实现的比较器，必须让关键字集合满足以下性质：</p><ul><li>关键字自己不能小于自己</li><li>两个关键字不能互相小于对方</li><li>如果按照比较器，key1小于key2，key2小于key3，则key1小于key3</li></ul><p>学过离散数学的同学应该知道，上面描述的这种二元关系满足反自反性、反对称性和传递性。满足这三个性质的关系称之为“严格偏序关系”。我们日常生活中见到的数字比较的小于号、集合中“真包含于”都是这种关系。</p></blockquote><h3 id="2-2-添加元素"><a href="#2-2-添加元素" class="headerlink" title="2.2 添加元素"></a>2.2 添加元素</h3><h4 id="2-2-1-insert"><a href="#2-2-1-insert" class="headerlink" title="2.2.1 insert"></a>2.2.1 insert</h4><p>对于map</p><pre><code class="lang-cpp">map&lt;string, int&gt; m;m.insert(&#123;&quot;str&quot;, 1&#125;);m.insert(make_pair(&quot;ser&quot;, 1));m.insert(pair&lt;string, int&gt;(&quot;ssr&quot;, 1));m.insert(map&lt;string, int&gt;::value_type(&quot;sdr&quot;, 1));</code></pre><p>对于set</p><pre><code class="lang-cpp">vector&lt;int&gt; ivec = &#123;2, 4, 6, 8&#125;;set&lt;int&gt; set2;set2.insert(1);set2.insert(&#123;2, 4, 6, 8&#125;);set2.insert(ivec.cbegin(), ivec.cend());</code></pre><p>insert函数和emplace函数返回pair对象，pair.first为迭代器，指向刚插入的元素，pair.second为bool，表示插入是否成功。如果由于存在重复导致插入失败，则除了second为false之外，first指向那个重复元素。</p><pre><code class="lang-cpp">set&lt;int&gt; s;const auto [iter, success] = s.insert(x); // 返回值拆成两个</code></pre><h4 id="2-2-2-对map使用下标-操作"><a href="#2-2-2-对map使用下标-操作" class="headerlink" title="2.2.2 对map使用下标[]操作"></a>2.2.2 对map使用下标[]操作</h4><p>map使用下标操作首先会查找该key的元素，找不到就新建一个key的pair，将其初始化。最后执行赋值操作。</p><pre><code class="lang-cpp">map&lt;char, int&gt; mp;mp[&#39;a&#39;] = 5;mp[&#39;b&#39;] = 4;mp[&#39;c&#39;] = 3;</code></pre><h3 id="2-3-访问和查找元素"><a href="#2-3-访问和查找元素" class="headerlink" title="2.3 访问和查找元素"></a>2.3 访问和查找元素</h3><p>map除了使用下标操作访问元素之外，还可以用<code>at()</code>函数。</p><pre><code class="lang-cpp">map&lt;char, int&gt; mp;mp.at(k) // 查找关键字为k的元素，找不到就抛出异常</code></pre><p>关联容器内置的<code>find</code>函数和<code>count</code>函数可以执行查找操作</p><pre><code class="lang-cpp">//c为一个map容器c.find(k) // 返回一个迭代器，指向关键字为k的元素。若k不在容器中，则返回尾后迭代器c.count(k) // 返回关键字等于k的元素数量。对于map和set而言，返回值永远是0或1。</code></pre><p>当我们要在map容器中查找一个元素时，我们可以使用find函数查找。</p><pre><code class="lang-cpp">auto it = word_count.find(&quot;foobar&quot;);if(it==word_count.end())   cout&lt;&lt;&quot;foobar is not in the map&quot;&lt;&lt;endl;else   cout&lt;&lt;it-&gt;first&lt;&lt;&quot; &quot;&lt;&lt;it-&gt;second&lt;&lt;endl;</code></pre><p>在有序容器中，我们还可以找到关键字k附近的元素。</p><pre><code class="lang-cpp">s.lower_bound(k); // 返回迭代器，指向第一个关键字**不小于**k的元素s.upper_bound(k); // 返回迭代器，指向第一个关键字**大于**k的元素s.equal_range(k); // 返回pair&lt;iter, iter&gt;，表示关键字为k的元素范围。适用于multiset/multimap。若是没有k，则返回两个end()</code></pre><p>在对于允许重复关键字的容器来说，查找元素的过程稍微复杂些，因为一个关键字可能对应多个值，我们需要把这么对应的值都找出来。<br>如果multimap中有多个元素具有相同的关键字，则这些关键字在容器中会相邻存储。我们可以通过这一特性，将一个关键字对应的多个值全部找出来。</p><pre><code class="lang-cpp">//《C++ Primer》示例，查找某作者对应的所有书籍//authors是一个multimap容器string search_item(&quot;Alain&quot;);int numbers=authors.count(search_item);auto it=authors.find(search_item);while(numbers)&#123;   cout&lt;&lt;iter-&gt;second&lt;&lt;endl;   ++it;   numbers--;&#125;// 或者采用一种其他方式for (auto beg = authors.lower_bound(search_item),          end = authors.upper_bound(search_item);     beg != end; ++beg) &#123;    cout &lt;&lt; beg-&gt;second &lt;&lt; endl; &#125;// 或者采用一种更加直接的方式for (auto pos = authors.equal_range(search_item);     pos.first != pos.second; ++pos.first)&#123;    cout &lt;&lt; pos.first-&gt;second &lt;&lt; endl; // 打印每本书&#125;</code></pre><h3 id="2-4-删除元素"><a href="#2-4-删除元素" class="headerlink" title="2.4 删除元素"></a>2.4 删除元素</h3><p>使用erase</p><pre><code class="lang-cpp">// s为关联容器，可能为set/map/multiset/multimaps.erase(k); // 删除指定关键字的元素，返回删除的个数。s.erase(iter); // iter必须指向s中的一个真实元素，返回指向删除元素之后的元素的迭代器。s.erase(iter1, iter2); // 删除迭代器[iter1, iter2)，其中必须是真是的元素。iter2指向的元素不删除。返回iter2</code></pre><h2 id="3-无序容器特有操作"><a href="#3-无序容器特有操作" class="headerlink" title="3. 无序容器特有操作"></a>3. 无序容器特有操作</h2><h3 id="3-1-无序容器特点"><a href="#3-1-无序容器特点" class="headerlink" title="3.1 无序容器特点"></a>3.1 无序容器特点</h3><p>如果我们不关心容器中元素的次序，那么我们就可以使用无序容器。在无序容器中，元素没有明确的排列次序，当你迭代容器内的所有元素时，会发现他们的次序个有可能。我们唯一关心的是某个元素特定元素是否位于容器内。</p><p>无需容器，常常以Hash table实现出来，内部结构是一个类似于<code>vector&lt;list&gt;</code>的列表，列表的元素是链表<code>linked list</code>。通过某个hash函数的运算，确定元素落于这个列表的位置。</p><p>Hash函数的运算目标是让每个元素的落点（位置）有助于用户快速访问任何一个元素（前提则是哈希函数本身也必须够快）。</p><p>由于这样一个快速而完美的哈希函数不一定存在。抑或由于造成array耗费巨额内存而显得不切实际，因此退而求其次的哈希函数有可能让多个元素落于同一位置上，所以设计上就让vector的元素再被放进一个linked list中。如此一来，vector的每个位置就得以存放一个以上的元素。</p><p><img src="/2020/03/20/C-%E5%85%B3%E8%81%94%E5%AE%B9%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2020-03-20-18-01-59.png" alt></p><p>无序容器的主要优点是，当你打算查找一个特定值的元素，其速度甚至可能快过有序关联式容器（时间复杂度O(1)）。前提是你有一个良好的哈希函数。然而这样的哈希函数可能需要许多内存。</p><p>以下所有操作需要添加头文件：</p><pre><code class="lang-cpp">#include &lt;unordered_map&gt;#include &lt;unordered_set&gt;</code></pre><p><code>unordered_map</code>/<code>unordered_multimap</code>/<code>unordered_set</code>/<code>unordered_multiset</code>的初始化、插入、查找和删除的方法与普通的<code>map</code>/<code>multimap</code>/<code>set</code>/<code>multiset</code>没有大的区别。注意以下主要区别要点：</p><ol><li>无序容器内部不含比较器，因此你也不能提供自定义比较器。</li><li>每次对无序容器的添加操作可能会引起无序容器次序的改变。即便是相同元素，在不同的电脑上也可能得到不同的次序。</li><li>删除元素虽然不会引起无序容器次序改变，但是删除之后的第一次插入必然会引发次序改变。</li></ol><p>次序究竟会不会变化、怎样变化取决于使用的rehashing策略，该策略可由程序员自定义，就像在有序关联容器里定义排序函数那样。</p><h3 id="3-2-管理桶"><a href="#3-2-管理桶" class="headerlink" title="3.2 管理桶"></a>3.2 管理桶</h3><p>1.桶接口</p><pre><code class="lang-cpp">m.bucket_count()        正在使用的桶的数目m.max_bucket_count()    容器能容纳的最多的桶的数量m.bucket_size(n)        第n个桶中有多少个元素m.bucket(k)             关键字为k的元素在哪个桶</code></pre><p>2.桶迭代</p><pre><code class="lang-cpp">local_iterator            可以用来访问桶中元素的迭代器类型const_local_iterator      桶迭代器的const版本m.begin(n)、m.end(n)      桶n的首元素迭代器和尾后迭代器（n是什么类型？）m.cbegin(n)、m.cend(n)    与前两个函数类似，但返回const_local_iterator</code></pre><p>3.哈希策略</p><pre><code class="lang-cpp">//每个桶的平均元素数量，返回float值m.load_factor() //m试图维护的平均桶大小，返回float值，要求创建的新桶的load_factor&lt;=max_load_factor         m.max_load_factor() //重新存储，使得bucket_count&gt;=n，且bucket_count&gt;size/max_load_factor         m.rehash(n)  //重新存储，使得m可以保存n个元素且不必rehash m.reserve(n)</code></pre><h3 id="3-3-自定义哈希函数和比较函数"><a href="#3-3-自定义哈希函数和比较函数" class="headerlink" title="3.3 自定义哈希函数和比较函数"></a>3.3 自定义哈希函数和比较函数</h3><p>默认情况下，无序容器使用<code>==</code>来判断两key是否相等，并使用系统内置的哈希函数生成哈希值。不同类型的key会应用到不同的哈希函数，如下都是STL内置的哈希函数对象：</p><pre><code class="lang-cpp">struct hash&lt;char*&gt;struct hash&lt;const char*&gt;struct hash&lt;char&gt; struct hash&lt;unsigned char&gt; struct hash&lt;signed char&gt;struct hash&lt;short&gt;struct hash&lt;unsigned short&gt; struct hash&lt;int&gt; struct hash&lt;unsigned int&gt;struct hash&lt;long&gt; struct hash&lt;unsigned long&gt;</code></pre><p>如果key使用的是以上类型中的一种，可以使用缺省的hash函数。当然你程序员可以定义自己的hash函数。对于自定义对象，只能自定义hash函数。</p><p>下面是《C++ Primer》的一个自定义哈希函数的一个例子：</p><pre><code class="lang-cpp">/* 定义哈希函数和判等器 */size_t hasher(const Sales_data &amp;sd)&#123;    // 对书籍对象的哈希    return hash&lt;string&gt;() (sd.isbn()); // 返回其isbn编号的哈希，调用内置的string哈希函数&#125;bool eqOp(const Sales_data &amp;lhs, const Sales_data &amp;rhs)&#123;    // 如何判断两本书是否相等？    return lhs.isbn() == rhs.isbn(); // 判断两书的isbn编号是否相等&#125;/* 使用哈希函数和判等器 */using SD_multiset = unordered_multiset&lt;Sales_data, decltype(hasher)*, decltype(eqOp)*&gt;; // 类型名太长了，将类型名保存成别的变量名SD_multiset bookstore(42, hasher, eqOp);</code></pre><h2 id="4-小结"><a href="#4-小结" class="headerlink" title="4. 小结"></a>4. 小结</h2><p>对C++ 关联容器的总结到此告一段落。本篇文章从开始着手写作到完成，不间断地工作了八个小时，期间不断重温已经遗忘的知识，查阅资料，其中很多还是自己曾经收藏过的资料。</p><p>写作时，我多次问自己：写一篇不会有人看的文章值得吗？我也多次想要像我以前很多文章、像CSDN大多数的文章那样，随便水水，记录一下，反正只有自己看。</p><p>但是这次我觉得，我要为自己负责，要为已经付出的精力和时间负责。我相信大家都会有这种迷茫的时候，怀疑自己手头上的工作有没有意义，甚至想放弃。</p><p>不要轻言放弃，尤其是当你怀疑它的意义的时候。因为这个时候你可能是在为自己的懒惰找借口。将一件事情的意义贬低，这种想法出现的太容易，又太能让自己解脱了。这是一种让人没有负罪感的放弃方式。但是回头看，很多好想法，明明只要坚持一下就可以实现。因为对意义的评价，近乎于预测未来，我们大多数平凡人是没有这种本事的。</p><p>Be a better man, 每天进步一点点。大家共勉！</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>C++</tag>
      
      <tag>map</tag>
      
      <tag>set</tag>
      
      <tag>STL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读笔记】DeepStellar Model-Based Quantitative Analysis of Stateful Deep Learning Systems</title>
    <link href="/2020/03/19/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91DeepStellar%20Model-Based%20Quantitative%20Analysis%20of%20Stateful%20Deep%20Learning%20Systems/"/>
    <url>/2020/03/19/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%91DeepStellar%20Model-Based%20Quantitative%20Analysis%20of%20Stateful%20Deep%20Learning%20Systems/</url>
    
    <content type="html"><![CDATA[<p>DeepStellar 是少有的针对RNN进行测试的工具，它提出了RNN测试的新思路。<br>封面上的大神就是谢肖飞博士，他是DeepStellar一问的作者。<br><!--more---></p>]]></content>
    
    
    <categories>
      
      <category>paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RNN</tag>
      
      <tag>testing</tag>
      
      <tag>DeepStellar</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>循环神经网络模型的覆盖率调研</title>
    <link href="/2020/03/19/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%86%E7%9B%96%E7%8E%87%E8%B0%83%E7%A0%94/"/>
    <url>/2020/03/19/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%86%E7%9B%96%E7%8E%87%E8%B0%83%E7%A0%94/</url>
    
    <content type="html"><![CDATA[<h1 id="循环神经网络模型的覆盖率调研"><a href="#循环神经网络模型的覆盖率调研" class="headerlink" title="循环神经网络模型的覆盖率调研"></a>循环神经网络模型的覆盖率调研</h1><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><h3 id="1-1-循环神经网络"><a href="#1-1-循环神经网络" class="headerlink" title="1.1 循环神经网络"></a>1.1 循环神经网络</h3><p>循环神经网络（Recurrent neural network：RNN）是神经网络的一种。</p><p>单纯的RNN因为无法处理随着递归，权重指数级爆炸或梯度消失问题，难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题。循环神经网络可以描述动态时间行为。和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。手写识别是最早成功利用RNN的研究结果。</p><p>为了更好地理解循环神经网络，首先需要介绍前馈神经网络。</p><h4 id="1-前馈神经网络"><a href="#1-前馈神经网络" class="headerlink" title="1) 前馈神经网络"></a>1) 前馈神经网络</h4><p>前馈网络通过在网络的每个节点上做出的一系列操作传递信息。前馈网络每次通过每个层直接向后传递信息。这与循环神经网络不同。一般而言，前馈网络接受一个输入并据此产生输出，这也是大多数监督学习的步骤，输出结果可能是一个分类结果。输出可以是以猫狗等作为标签的类别。我们常见的卷积神经网络（CNN）就是一类经典的前馈网络。</p><p>前馈网络是基于一系列预先标注过的数据训练的。训练阶段的目的是减少前馈网络猜类别时的误差。一旦训练完成，我们就可以用训练后的权重对新批次的数据进行分类。</p><p><img src="/2020/03/19/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%86%E7%9B%96%E7%8E%87%E8%B0%83%E7%A0%94/2020-03-18-22-02-26.png" alt="img1"></p><p>在前馈网络中，在<strong>测试阶段</strong>无论展示给分类器的图像是什么，都不会改变权重，所以也不会影响第二个决策。这是前馈网络和循环网络之间一个非常大的不同。也就是说，前馈网络<strong>在测试时</strong>不会记得之前的输入数据。它们只会<strong>在训练阶段</strong>记得历史输入数据。</p><p>与前馈神经网络不同，循环网络不仅将当前的输入样例作为网络输入，还将它们之前感知到的一并作为输入。</p><h4 id="2-前馈网络到循环网络的转变"><a href="#2-前馈网络到循环网络的转变" class="headerlink" title="2) 前馈网络到循环网络的转变"></a>2) 前馈网络到循环网络的转变</h4><p>下图是一个多层感知机示意图，该图所示的模型只拥有一个隐藏层（Hidden Layer），其接受来自输入层经过ReLU处理后的信号，输出的信号再经过Softmax层，从而产生一个分类结果。</p><p><img src="/2020/03/19/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%86%E7%9B%96%E7%8E%87%E8%B0%83%E7%A0%94/2020-03-18-22-06-04.png" alt="img2"></p><p>如果在上述示例中的层数增加了，并且我们令隐藏层也接收输入，那么第一个隐藏层将激活传递到第二个隐藏层上，以此类推，最后到达输出层，每一层都有自己的权重（W）、偏置项（B）和激活函数（F）。</p><p><img src="/2020/03/19/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%86%E7%9B%96%E7%8E%87%E8%B0%83%E7%A0%94/2020-03-18-22-09-56.png" alt="img3"></p><p>我们令所有隐藏层的权重和偏置项替换成相同的值，从而能使得隐藏层在某种意义上“合并”，如下图所示（注意图中方框内部隐藏层的参数）：</p><p><img src="/2020/03/19/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%86%E7%9B%96%E7%8E%87%E8%B0%83%E7%A0%94/2020-03-18-22-12-56.png" alt="img4"></p><p>现在我们就可以将所有层合并在一起了。所有的隐藏层都可以结合在一个循环层中，如下图：</p><p><img src="/2020/03/19/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%86%E7%9B%96%E7%8E%87%E8%B0%83%E7%A0%94/2020-03-18-22-13-27.png" alt="img5"></p><p>我们在每一步都会向隐藏层提供输入。现在一个循环神经元存储了所有之前步的输入，并将这些信息和当前步的输入合并。因此，它还捕获到一些当前数据步和之前步的相关性信息。t-1 步的决策影响到第 t 步做的决策。</p><p>如果我们在向网络输入 7 个字母后试着找出第 8 个字母，隐藏层会经历 8 次迭代。如果展开网络的话就是一个 8 层的网络，每一层对应一个字母。所以一个普通的神经网络被重复了多次。展开的次数与它记得多久之前的数据是直接相关的。</p><h4 id="3-循环神经网络基本结构"><a href="#3-循环神经网络基本结构" class="headerlink" title="3) 循环神经网络基本结构"></a>3) 循环神经网络基本结构</h4><p>下图是一个简单的循环神经网络如，它由输入层、一个隐藏层和一个输出层组成：<br><img src="/2020/03/19/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%86%E7%9B%96%E7%8E%87%E8%B0%83%E7%A0%94/2020-03-18-23-31-00.png" alt></p><ul><li>x是一个向量，它表示输入层的值；<br>s是一个向量，它表示隐藏层的值（你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；<br>U是输入层到隐藏层的权重矩阵；<br>o也是一个向量，它表示输出层的值；<br>V是隐藏层到输出层的权重矩阵；<br>W是隐藏层上一次的值作为这一次的输入的权重。</li></ul><p>把上面的图展开，循环神经网络也可以画成下面这个样子：<br><img src="/2020/03/19/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%86%E7%9B%96%E7%8E%87%E8%B0%83%E7%A0%94/2020-03-18-23-33-05.png" alt></p><p>网络在t时刻接收到输入$x<em>t$之后，隐藏层的值是$s_t$，输出值是$o_t$。关键一点是，$s_t$的值不仅仅取决于$x_t$，还取决于$s</em>{t-1}$。</p><p>我们可以用下面的公式来表示循环神经网络的计算方法：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{o}_{t} &=g\left(V \mathbf{s}_{t}\right) \tag{1}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}\mathbf{s}_{t} &=f\left(U \mathbf{x}_{t}+W \mathbf{s}_{t-1}\right) \tag{2}\end{aligned}</script><p>(1)是输出层的计算公式，输出层是一个全连接层，它的每个节点都和隐藏层的各个节点相连。V是输出层的权重矩阵，g是激活函数。(2)是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次的值作为这一次的输入的权重矩阵，f是激活函数。</p><p>从上面的公式我们可以看出，循环层和全连接层的区别就是循环层多了一个权重矩阵 W。</p><h4 id="4-长短期记忆网络"><a href="#4-长短期记忆网络" class="headerlink" title="4) 长短期记忆网络"></a>4) 长短期记忆网络</h4><p>长短期记忆（英语：Long Short-Term Memory，LSTM）是一种时间循环神经网络（RNN），论文首次发表于1997年。由于独特的设计结构，LSTM适合于处理和预测时间序列中间隔和延迟非常长的重要事件。</p><p>LSTM是一种含有LSTM区块（blocks）或其他的一种类神经网络，文献或其他资料中LSTM区块可能被描述成智能网络单元，因为它可以记忆不定时间长度的数值，区块中有一个gate能够决定input是否重要到能被记住及能不能被输出output。</p><p>下图中，底下是四个S函数单元，最左边的单元为input，右边三个gate决定input是否能传入下个；左边第二个为input gate，如果这里gate近似于零，将把这里的值挡住，不会进到下一层。左数第三个是forget gate，当这产生值近似于零，将把过去记住的值忘掉。第四个也就是最右边的input为output gate，他可以决定在区块记忆中的input是否能输出 。</p><p><img src="/2020/03/19/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%86%E7%9B%96%E7%8E%87%E8%B0%83%E7%A0%94/2020-03-18-23-51-41.png" alt></p><p>LSTM有很多个版本，其中一个重要的版本是GRU（Gated Recurrent Unit），根据谷歌的测试表明，LSTM中最重要的是Forget gate，其次是Input gate，最次是Output gate。</p><h3 id="1-2-循环神经网络测试的机遇和挑战"><a href="#1-2-循环神经网络测试的机遇和挑战" class="headerlink" title="1.2 循环神经网络测试的机遇和挑战"></a>1.2 循环神经网络测试的机遇和挑战</h3><p>当前研究大多集中于针对前馈神经网络的测试，诸如DeepXplore、DeepGauge针对CNN的测试等，而对RNN鲜有研究。由于RNN的循环特性，适用于CNN的分析方法不能简单地迁移到RNN上。目前学者大多采用模糊测试的方法，通过随机干扰数据集产生对抗性样本，而后分析RNN内部状态信息，引导数据集的扰动方向的方法，快速生成能使得RNN模型判断错误的对抗测试用例集合，达到模型测试的目的。</p><p>根据此思路，一方面可以针对数据集添加干扰的方式进行优化，采用启发式搜索改善对测试样本的Mutation过程；另一方面则是针对RNN内部状态信息的分析，引导算法快速找到有效的、难以察觉变化的对抗样本，这个思路类似于模型攻击。</p><p>目前RNN的对抗性测试主要面临三方面的挑战：</p><ol><li>对于非分类模型而言，没有较好的标准识别对抗样本能不能让模型发生错误。</li></ol><blockquote><p>For the sequential outputs not then applied to classification, there is no standard to decide the outputs as wrong outputs with respect to the changing degree.</p></blockquote><ol><li>对于序列输入的Mutation来说，很难保证添加的扰动是最小的</li></ol><blockquote><p>Applying the perturbations to words in a discrete space always cannot obtain a legal input and the explicit modification is distinguishable for humans.</p></blockquote><ol><li>现有应用于CNN等的覆盖率指标没有考虑到RNN内部结构特性，因此不能直接应用到RNN上。</li></ol><h2 id="2-覆盖指标调研"><a href="#2-覆盖指标调研" class="headerlink" title="2. 覆盖指标调研"></a>2. 覆盖指标调研</h2><p>目前针对循环神经网络的测试 (testing) 和验证 (verification) 等工作的研究还十分有限，根据目前调研取得的结果，现有学者的研究思路大体分成两类：抽象替代模型法和门覆盖率法。下文分别对这两个模型进行简述。</p><h3 id="2-1-抽象替代模型法"><a href="#2-1-抽象替代模型法" class="headerlink" title="2.1 抽象替代模型法"></a>2.1 抽象替代模型法</h3><h4 id="2-1-1-代表论文："><a href="#2-1-1-代表论文：" class="headerlink" title="2.1.1 代表论文："></a>2.1.1 代表论文：</h4><blockquote><p>Du, X., Xie, X., Li, Y., Ma, L., Liu, Y., &amp; Zhao, J. (2019, August). <strong>Deepstellar: model-based quantitative analysis of stateful deep learning systems.</strong> In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 477-487).</p><p>Du, X., Xie, X., Li, Y., Ma, L., Liu, Y., &amp; Zhao, J. (2019, November). <strong>A Quantitative Analysis Framework for Recurrent Neural Network.</strong> In 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE) (pp. 1062-1065). IEEE.</p></blockquote><h4 id="2-1-2-关键方法"><a href="#2-1-2-关键方法" class="headerlink" title="2.1.2 关键方法"></a>2.1.2 关键方法</h4><p>由于直接分析RNN内部结构具有状态转移的特性，因此DeepStellar一文提出<strong>将RNN建模成马尔科夫链</strong>，来模拟其内部状态和动态行为特性。基于马尔科夫链的抽象，该文设计了两个相似度指标和五个覆盖率标准，来衡量输入测试用例差异和测试用例的测试充分性。</p><p><img src="/2020/03/19/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%86%E7%9B%96%E7%8E%87%E8%B0%83%E7%A0%94/2020-03-19-02-00-02.png" alt></p><ul><li><p>抽象模型结构模块：输入训练好的RNN，通过Profiling，分析其内部行为。一系列的RNN状态向量叫做trace。每个输入序列会通过Profiling分析得到一个trace。profiling结束之后就可以得到一系列的trace，记载着RNN训练过程访问过的和经过的状态。</p></li><li><p>分析内部状态空间和被训练集激活的trace的过程计算量很大，因此进一步抽象模型来简化状态和trace。首先对状态向量采用主成分分析，以保留其前k个主成分，并将这k个主成分等分成m部分。在状态转换方面，根据抽象状态将具体的转换概括成为抽象的转换。并且根据每个状态向不同方向转换的频率，导出了训练RNN的离散时间Markov链（DTMC）模型。</p></li><li><p>设计了两个相似度指标，用于衡量不同输入下激活的两个trace的相似度。分别是state-based trace similarity和transition-based trace similarity，简写为SBTSIM和TBTSIM。</p></li><li><p>五个覆盖率包括basic state coverage/n-step state boundary coverage, weighted state coverage/ basic transition coverage/weighted transition coverage，简写为BSCov/n-SBCov/WSCov/BTCov/WTCov。</p></li><li><p>将两个指标和五个标准应用于对抗样本检测和覆盖引导的测试上，来缓解来自对抗样本的威胁。运用两个相似度，即可在运行时检测对抗样本（<strong>Monitor</strong>）；运用五个覆盖率，我们将其用于指导测试用例生成上，生成的测试用例以提升覆盖率和找到更多的未被发现的defects为目标。这两个应用互相补充。</p></li></ul><h4 id="2-1-3-实验效果及评价"><a href="#2-1-3-实验效果及评价" class="headerlink" title="2.1.3 实验效果及评价"></a>2.1.3 实验效果及评价</h4><p>DeepStellar在精心的调参下，通过trace相似度检测算法能够很好地检测出当前输入样本是否为对抗样本（音频），准确度达到了89%。</p><p>DeepStellar提出的测试方法本质上是对一个等价模型进行分析的方法，抽象掉了RNN模型的很多细节，只保留了主干部分。思路值得借鉴。</p><p>DeepStellar的缺点也很明显，其高识别率的背后是精心的调参，并且作者也提到，面对更复杂的模型，结果未必会这么好。</p><blockquote><p>With finer-grained model, the result is not necessarily better.</p></blockquote><h3 id="2-2-门覆盖率法"><a href="#2-2-门覆盖率法" class="headerlink" title="2.2 门覆盖率法"></a>2.2 门覆盖率法</h3><h4 id="2-2-1-代表论文："><a href="#2-2-1-代表论文：" class="headerlink" title="2.2.1 代表论文："></a>2.2.1 代表论文：</h4><blockquote><p>Huang, W., Sun, Y., Huang, X., &amp; Sharp, J. (2019). <strong>testRNN: Coverage-guided Testing on Recurrent Neural Networks.</strong> arXiv preprint arXiv:1906.08557.</p></blockquote><h4 id="2-2-2-关键方法"><a href="#2-2-2-关键方法" class="headerlink" title="2.2.2 关键方法"></a>2.2.2 关键方法</h4><p>testRNN关注LSTM和其鲁棒性，鲁棒性指对输入添加小的扰动并不影响LSTM的判断结果的特性。注意，该工具只针对LSTM及相似的网络结构进行分析，原因在于其算法依赖于内部门结构的实现，而这种门结构只存在于LSTM类型的网络中。</p><p>testRNN的特色在于，其直接分析RNN内部结构并加以分析的思路非常类似于其前辈DeepXplore分析前馈神经网络的思路。但由于RNN网络内部关于“层”和“节点”的概念不同于CNN，因此对如何实现CNN中覆盖率迁移到RNN的应用中，testRNN提出了自己的方法。</p><ul><li><p>Cell覆盖率。Cell（后文称之为<strong>单元</strong>）覆盖旨在覆盖每个时间步的隐藏状态发生的显著变化$\Delta\xi_t$。当单元值$\Delta\xi_t$大于用户定义的阈值参数$\alpha_h$时，该单元将被激活并被测试用例覆盖。然后使用覆盖率来衡量由生成的测试用例激活至少一次的单元的百分比。（单元的隐藏状态变化大了，超过了用户定义的某个阈值，就算激活，测试用例激活的单元个数占总个数的比例就是该测试用例的覆盖率）</p></li><li><p>Gate覆盖率。门的覆盖率类似于单元覆盖率，但是信息是从LSTM单元的门中筛选的。上文提到，Google的研究团队发现，LSTM的四种门中最重要的门是忘记门（forget gate），因此testRNN专注于统计忘记门的覆盖率。忘记门的值$Rt(f，x)$表示可以从最后一个单元继承多少信息。由于LSTM以其长期的存储能力而闻名，因此检查一个单元格是否丢弃了从先前输入中学习到的适当数量的信息非常有意义。（忘记们忘记的信息量用Rt函数表示，则Rt太大了就激活？）</p></li></ul><p>下图是testRNN的具体处理流程。</p><p><img src="/2020/03/19/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A6%86%E7%9B%96%E7%8E%87%E8%B0%83%E7%A0%94/2020-03-19-02-43-32.png" alt></p><h4 id="2-2-3-实验效果"><a href="#2-2-3-实验效果" class="headerlink" title="2.2.3 实验效果"></a>2.2.3 实验效果</h4><p>testRNN着眼于门覆盖率，成功将分析CNN的那一套迁移了过来。但是testRNN的实验结果仅限于小数据集训练下的小网络，诸如MNIST分类数据集训练的双层LSTM网络等。因此能否将此方法推广到更加复杂的大型网络中，还有待探究。</p><h3 id="2-3-优化引导法"><a href="#2-3-优化引导法" class="headerlink" title="2.3 优化引导法"></a>2.3 优化引导法</h3><h4 id="2-3-1-代表论文："><a href="#2-3-1-代表论文：" class="headerlink" title="2.3.1 代表论文："></a>2.3.1 代表论文：</h4><blockquote><p>Guo, J., Zhao, Y., Han, X., Jiang, Y., &amp; Sun, J. (2019). <strong>RNN-Test: Adversarial Testing Framework for Recurrent Neural Network Systems.</strong> arXiv preprint arXiv:1911.06155.</p></blockquote><h4 id="2-3-2-关键方法"><a href="#2-3-2-关键方法" class="headerlink" title="2.3.2 关键方法"></a>2.3.2 关键方法</h4><p>RNN-Test一方面采用了和testRNN类似的“门覆盖率”方法来引导测试用例生成，另一方面采用了一种全新的优化函数思想，计算能同时使得扰动添加最小并且最有可能令模型发生判断错误的扰动方向。RNN-Test将二者结合起来，但正是因为该文仅仅是将这两种方法求得的偏移方向简单的加和，让人不禁怀疑其工作是否没有进行完全。</p><ul><li><p>状态不连续方向（State inconsistency orientation）：</p><script type="math/tex; mode=display">obj_{orient} = h_{t-1}^l + c_t^l - h_t^l</script><p>该优化函数的设计思想是，若一个样本能使得从隐状态t-1时刻输入的信息尽量大，而输出尽量小，那么这种样本更容易出现问题。这是因为t时刻$h<em>t^l$的值完全取决于$h</em>{t-1}^l $和$ c_t^l$，让这两部分产生大小差异更容易引发不确定行为。</p></li><li><p>损失函数优化方向（Cost orientation）:</p><script type="math/tex; mode=display">obj_{orient} = L_{seq}(y, \hat{y})</script><p>这一部分引用自FGSM的优化算法，该论文是对抗样本生成领域的开山之作。讲的是如何通过梯度上升算法求得添加扰动的方向，从而使得扰动最小的同时模型的变化最大。</p></li><li><p>决策边界方向（Decision boundary orientation）：</p><script type="math/tex; mode=display">obj_{orient} = (\sum_{i=0}^{k}\hat{y_{t_i}}) - \hat{y_t}</script><p>这一部分的灵感来自于RNN内部结构，由于每个时间步的隐状态事实上都会产生中间输出$y=o_t^l$，但一般我们认为只有最后阶段的输出向量才是有意义的。该优化函数将除了原来预测的最大值y之外前k个最大的y加起来，并减去原来的y。</p></li></ul><h4 id="2-3-3-实验效果"><a href="#2-3-3-实验效果" class="headerlink" title="2.3.3 实验效果"></a>2.3.3 实验效果</h4><p>该文章使用自己生成的对抗样本集合，对模型进行重新训练，模型的复杂度（Perplexity）有大约1.159%的降低。这说明模型更稳定了。</p><p>然而提升不大，运行效率却很低，计算代价较大。并且原文为了提升算法运行性能，Mutation过程采用了一遍Mutation，若不成功直接放弃的做法，原来的测试用例利用率较低、为了达到较高的突变利用率，算法不得不放宽添加干扰力度，这就导致生成太多无用的假测试用例。算法本身可以被优化。</p>]]></content>
    
    
    <categories>
      
      <category>paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RNN</tag>
      
      <tag>testing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何使用VS Code编写github pages博客</title>
    <link href="/2020/01/11/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8VS-Code%E7%BC%96%E5%86%99github-pages%E5%8D%9A%E5%AE%A2/"/>
    <url>/2020/01/11/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8VS-Code%E7%BC%96%E5%86%99github-pages%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<p>使用VS Code写博客，需要你按照我之前写的两篇博客，将github pages平台搭建起来。</p><p><a href="https://superlova.github.io/2019/04/14/%E9%85%8D%E7%BD%AEhexo+GitHub%20Pages%E7%BA%AA%E5%AE%9E/">配置hexo+GitHub Pages纪实</a><br><a href="https://superlova.github.io/2019/04/25/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/">hexo图片加载失败解决方案</a></p><p>之后我们安装VSCode。接下来介绍我一直使用的几个插件，和它们的配置小技巧。</p><p>第一个是<strong>Markdown Preview Enhanced</strong>，有了该插件，就可以提前预览markdown文件的渲染效果。方法是使用VSCode打开以md后缀名结尾的文件，右键点击<strong>Markdown Preview Enhanced： Open Preview To The Side</strong>，即可在侧边栏生成即时渲染的md效果文件。</p><p>第二个是<strong>Markdown PDF</strong>，该插件可以令写好的md文件打印成pdf格式。该插件需要安装chromium内核。</p><p>第三个是<strong><strong>Paste Image</strong></strong>插件，可以很方便地在md文章中粘贴位于剪切板的图片。</p><p>粘贴的快捷键是Ctrl+Alt+V。</p><p>在Paste Image插件的Path设置部分，改成如下所示：<br><img src="/2020/01/11/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8VS-Code%E7%BC%96%E5%86%99github-pages%E5%8D%9A%E5%AE%A2/2020-01-11-23-28-36.png" alt><br>这样图片粘贴的位置就变成了<strong>当前文章目录下，与该文章同名的文件夹内</strong>，方便我们进行进一步整理。</p>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>github pages</tag>
      
      <tag>hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>总结论文中常用的Matplotlib和Seaborn绘图技术</title>
    <link href="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/"/>
    <url>/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/</url>
    
    <content type="html"><![CDATA[<h1 id="一、使用matplotlib绘制图像"><a href="#一、使用matplotlib绘制图像" class="headerlink" title="一、使用matplotlib绘制图像"></a>一、使用matplotlib绘制图像</h1><p>matplotlib是一个Python的数据可视化2D图形库。matplotlib的特点是可以采用面向对象的方法，模仿MATLAB中的图形命令。matplotlib经常与numpy、pandas等库结合起来使用。<br>matplotlib可以采用MATLAB的命令风格使用，也可以采用面向对象的风格使用。</p><h2 id="matplotlib的图像中各组件名称"><a href="#matplotlib的图像中各组件名称" class="headerlink" title="matplotlib的图像中各组件名称"></a>matplotlib的图像中各组件名称</h2><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-11-23-42-39.png" alt></p><h2 id="新建图像"><a href="#新建图像" class="headerlink" title="新建图像"></a>新建图像</h2><pre><code class="lang-python">fig, axes = plt.subplots(2,1,figsize=(5,10)) #两行一列组成一张图，图像大小宽5高10</code></pre><p>上面的语句创建了一个figure，由两个ax组成。把它想象成一张画布上面的两个贴画，会比较容易理解。</p><p>plt.figure()函数的前两个参数是设置figure是由几行几列的ax组成。figure(2,1)说明figure是由两行一列的ax一共两个ax组成。</p><p>后面的figsize参数设置画布的宽和高，单位为英寸。</p><h1 id="二、使用Seaborn绘制图像"><a href="#二、使用Seaborn绘制图像" class="headerlink" title="二、使用Seaborn绘制图像"></a>二、使用Seaborn绘制图像</h1><p>首先确定我们需要可视化的数据的结构。以iris鸢尾花数据集为例，</p><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-07-35.png" alt></p><p>每一行代表一个数据对象，每一列代表数据对象的一个属性。但是现实生活的数据很多不长这样，只不过组织成一个表格的形式，内容大相径庭。因此在进行数据可视化时一定要保证你的数据也是<strong>用行代表数据对象，用列表示数据的属性</strong>。</p><h2 id="2-1-关联图"><a href="#2-1-关联图" class="headerlink" title="2.1 关联图"></a>2.1 关联图</h2><p>我们是用 <code>relplot</code>函数进行进一步绘制。实际上，<code>relplot</code> 可以看作是 <code>scatterplot</code> 和 <code>lineplot</code> 的结合版本。但是relplot包装层级更加高，这意味着它更适合快速应用，不适合自定义。如果你对它的效果不满意，恐怕还是得诉诸<code>scatterplot</code> 和 <code>lineplot</code>等与matplotlib结合更紧密的api，或者直接使用matplotlib。</p><pre><code class="lang-python">sns.relplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=iris)</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-10-26.png" alt></p><p>x为花萼长度，y为花萼宽度。这样分x，y其实有一定道理，我们的目的是能够把不同类型的数据对象在图上区分开。因为同类花朵一般个头差不多，花萼的长度和宽度聚集在图的一部分区域。但是在上图我们是看不出来的。我们希望给不同类别添加不同颜色。</p><pre><code class="lang-python">sns.relplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, hue=&quot;species&quot;, data=iris)</code></pre><p>可以看到我们添加了<code>hue</code>字段，并要求按照<code>species</code>进行进一步分类。<code>hue</code>字段就是进行二次分类的参数。</p><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-15-48.png" alt></p><p>如果是论文，则我们要使得读者在黑白打印的条件下也能发现区别。添加<code>stype</code>参数为<code>species</code>或许会有帮助。</p><pre><code class="lang-python">sns.relplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;,            hue=&quot;species&quot;, style=&quot;species&quot;, data=iris)</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-18-31.png" alt></p><p>不只是散点图，该方法还支持线形图，只需要指定 <code>kind=&quot;line&quot;</code> 参数即可。</p><pre><code class="lang-python">sns.relplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;,            hue=&quot;species&quot;, style=&quot;species&quot;, kind=&quot;line&quot;, data=iris)</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-20-55.png" alt></p><p>上图其实就是折线图，我们使用一个与matplotlib结合更紧密的api来探究花萼长度和花瓣长度之间的关系。</p><pre><code class="lang-python">sns.lineplot(x=&quot;sepal_length&quot;, y=&quot;petal_length&quot;,             hue=&quot;species&quot;, style=&quot;species&quot;, data=iris)</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-26-06.png" alt></p><h2 id="2-2-类别图"><a href="#2-2-类别图" class="headerlink" title="2.2 类别图"></a>2.2 类别图</h2><p>懒人函数是<code>catplot</code>，<code>catplot</code>是下面几个底层函数的封装：</p><ul><li><p>分类散点图:</p><ul><li><a href="https://seaborn.pydata.org/generated/seaborn.stripplot.html"><code>stripplot()</code></a> (<code>kind=&quot;strip&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.swarmplot.html"><code>swarmplot()</code></a> (<code>kind=&quot;swarm&quot;</code>)</li></ul></li><li><p>分类分布图:</p><ul><li><a href="https://seaborn.pydata.org/generated/seaborn.boxplot.html"><code>boxplot()</code></a> (<code>kind=&quot;box&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.violinplot.html"><code>violinplot()</code></a> (<code>kind=&quot;violin&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.boxenplot.html"><code>boxenplot()</code></a> (<code>kind=&quot;boxen&quot;</code>)</li></ul></li><li><p>分类估计图:</p><ul><li><a href="https://seaborn.pydata.org/generated/seaborn.pointplot.html"><code>pointplot()</code></a> (<code>kind=&quot;point&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.barplot.html"><code>barplot()</code></a> (<code>kind=&quot;bar&quot;</code>)</li><li><a href="https://seaborn.pydata.org/generated/seaborn.countplot.html"><code>countplot()</code></a> (<code>kind=&quot;count&quot;</code>)</li></ul></li></ul><p>我们想知道不同类别下花萼长度的散点图。</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&#39;strip&#39;,data=iris)</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-36-33.png" alt></p><p><code>kind=&quot;swarm&quot;</code> 可以让散点按照 beeswarm 的方式防止重叠，可以更好地观测数据分布。</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;swarm&quot;, data=iris)</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-38-46.png" alt></p><p>箱线图</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;box&quot;, data=iris)</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-41-06.png" alt><br>变种箱线图</p><pre><code class="lang-python">sns.catplot(x=&quot;species&quot;, y=&quot;sepal_length&quot;, kind=&quot;boxen&quot;, data=iris)</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-41-24.png" alt><br>提琴图</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;violin&quot;, data=iris)</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-41-16.png" alt><br>点线图</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;point&quot;, data=iris)</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-41-36.png" alt><br>柱状图</p><pre><code class="lang-python">sns.catplot(x=&quot;sepal_length&quot;, y=&quot;species&quot;, kind=&quot;bar&quot;, data=iris)</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-41-48.png" alt></p><h2 id="2-3-分布图"><a href="#2-3-分布图" class="headerlink" title="2.3 分布图"></a>2.3 分布图</h2><p>如果想看一个变量到底是正态分布、卡方分布还是指数分布，此时就要使用分布图进行可视化了。一维分布图比较常见，二维以上分布图不太直观。绘制分布图的函数有这几个：<code>jointplot</code> <code>pairplot</code> <code>distplot</code> <code>kdeplot</code>。</p><p><code>distplot</code>可以方便的查看单变量的分布图。</p><pre><code class="lang-python">sns.distplot(iris[&quot;sepal_length&quot;])</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-48-17.png" alt><br>图上那条曲线是根据数据拟合出来的核密度估计kde曲线（原理有待学习）。如果不想要这条线，可以在参数中设置<code>kde=False</code>。更可以只要kde曲线，设置<code>hist=False</code>即可。</p><p><code>jointplot</code>绘制二元变量的分布图，比如花瓣长度和宽度的关系。</p><pre><code class="lang-python">sns.jointplot(x=&quot;petal_length&quot;, y=&quot;petal_width&quot;, data=iris)</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-55-40.png" alt></p><p>kde估计图也可以在二元变量分布图中出现。还有蜂巢图<code>kind=&quot;hex&quot;</code>、回归图<code>kind=&quot;reg&quot;</code>等。</p><pre><code class="lang-python">sns.jointplot(x=&quot;petal_length&quot;, y=&quot;petal_width&quot;, data=iris, kind=&quot;kde&quot;)</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-57-09.png" alt></p><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-16-58-51.png" alt></p><p>最后注意到我们的鸢尾花数据集含有四组属性。我们想探究这四组属性两两之间的关系，就需要用到<code>pairplot</code></p><pre><code class="lang-python">sns.pairplot(iris, hue=&quot;species&quot;)</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-17-01-04.png" alt></p><h2 id="2-4-回归图"><a href="#2-4-回归图" class="headerlink" title="2.4 回归图"></a>2.4 回归图</h2><p><code>regplot</code> 绘制回归图，只会绘制一组回归曲线。</p><pre><code class="lang-python">sns.regplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, data=iris)</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-17-02-28.png" alt></p><p><code>lmplot</code> 可以引入<code>hue</code>变量，绘制不同类别数据的回归图</p><pre><code class="lang-python">sns.lmplot(x=&quot;sepal_length&quot;, y=&quot;sepal_width&quot;, hue=&quot;species&quot;, data=iris)</code></pre><p><img src="/2020/01/11/%E6%80%BB%E7%BB%93%E8%AE%BA%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Matplotlib%E7%BB%98%E5%9B%BE%E6%8A%80%E6%9C%AF/2020-01-12-17-03-57.png" alt></p><h2 id="2-5-矩阵图"><a href="#2-5-矩阵图" class="headerlink" title="2.5 矩阵图"></a>2.5 矩阵图</h2><p><code>heatmap</code>用来画热图，数据值大的格子颜色比较深。热力图在某些场景下非常实用，例如绘制出变量相关性系数热力图。<br><code>clustermap</code>用来画层次聚类结构图。对于iris数据集来说，这两类图没有用武之地。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>matplotlib</tag>
      
      <tag>seaborn</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>用于科学计算的GPU选购参考</title>
    <link href="/2019/07/01/%E7%94%A8%E4%BA%8E%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E7%9A%84GPU%E9%80%89%E8%B4%AD%E5%8F%82%E8%80%83/"/>
    <url>/2019/07/01/%E7%94%A8%E4%BA%8E%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E7%9A%84GPU%E9%80%89%E8%B4%AD%E5%8F%82%E8%80%83/</url>
    
    <content type="html"><![CDATA[<p>实验室最近要采购一批显卡，需要调研显卡的型号和价格。</p><h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><p>首先说一下需求：</p><ul><li>首先显卡的用途是科学计算，更具体一点是深度学习，有人做图像，有人做NLP；</li><li>其次预算有限，得买性价比最好的；</li><li>然后可能会有很多人要用服务器训练模型。</li></ul><p>然后这是戴尔服务器的售后人员发来的建议采购清单：<br><img src="/2019/07/01/%E7%94%A8%E4%BA%8E%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E7%9A%84GPU%E9%80%89%E8%B4%AD%E5%8F%82%E8%80%83/2019-07-01-18-19-43.png" alt></p><p>其中M10：19999￥ P100： 49999￥ V100：59999￥ P40：49999￥</p><p>值得一提的是，谷歌的Colab上面用的是这款：<br><img src="/2019/07/01/%E7%94%A8%E4%BA%8E%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E7%9A%84GPU%E9%80%89%E8%B4%AD%E5%8F%82%E8%80%83/2019-07-01-18-21-48.png" alt><br><img src="/2019/07/01/%E7%94%A8%E4%BA%8E%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E7%9A%84GPU%E9%80%89%E8%B4%AD%E5%8F%82%E8%80%83/2019-07-01-18-21-58.png" alt></p><p>但是一个从事深度学习研究的学长建议我买1080Ti。他好像提都没提过Tesla啊？难道显卡水这么深？</p><h2 id="GPU参数"><a href="#GPU参数" class="headerlink" title="GPU参数"></a>GPU参数</h2><p>GPU的性能主要由下面三个主要参数构成：</p><p><strong>计算能力</strong>。通常我们关心的是32位浮点计算能力。当然，对于高玩来说也可以考虑16位浮点用来训练，8位整数来预测。</p><p><strong>内存大小</strong>。神经网络越深，或者训练时批量大小越大，所需要的GPU内存就越多。</p><p><strong>内存带宽</strong>。内存带宽要足够才能发挥出所有计算能力。</p><p>此外，针对不同深度学习架构，GPU参数的选择优先级是不一样的，总体来说分两条路线：</p><p><strong>卷积网络和Transformer</strong>：张量核心&gt;FLOPs（每秒浮点运算次数）&gt;显存带宽&gt;16位浮点计算能力</p><p><strong>循环神经网络</strong>：显存带宽&gt;16位浮点计算能力&gt;张量核心&gt;FLOPs</p><p>这个排序背后有一套逻辑，下面将详细解释一下。</p><p>在说清楚哪个GPU参数对速度尤为重要之前，先看看两个最重要的张量运算：矩阵乘法和卷积。</p><p>举个栗子，以运算矩阵乘法A×B=C为例，将A、B复制到显存上比直接计算A×B更耗费资源。也就是说，如果你想用LSTM等处理大量小型矩阵乘法的循环神经网络，显存带宽是GPU最重要的属性。</p><p>矩阵乘法越小，内存带宽就越重要。</p><p>相反，卷积运算受计算速度的约束比较大。因此，要衡量GPU运行ResNets等卷积架构的性能，最佳指标就是FLOPs。张量核心可以明显增加FLOPs。</p><p>Transformer中用到的大型矩阵乘法介于卷积运算和RNN的小型矩阵乘法之间，16位存储、张量核心和TFLOPs都对大型矩阵乘法有好处，但它仍需要较大的显存带宽。</p><h2 id="性价比分析"><a href="#性价比分析" class="headerlink" title="性价比分析"></a>性价比分析</h2><p>下面总结了一张GPU和TPU的标准性能数据，值越高代表性能越好。RTX系列假定用了16位计算，Word RNN数值是指长度&lt;100的段序列的biLSTM性能。</p><p>这项基准测试是用PyTorch 1.0.1和CUDA 10完成的。</p><p><img src="/2019/07/01/%E7%94%A8%E4%BA%8E%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E7%9A%84GPU%E9%80%89%E8%B4%AD%E5%8F%82%E8%80%83/2019-07-01-19-09-53.png" alt="GPU和TPU的性能数据"></p><p>性价比可能是选择一张GPU最重要的考虑指标。</p><p>性价比可能是选择一张GPU最重要的考虑指标。在攻略中，小哥进行了如下运算测试各显卡的性能：</p><ul><li>用语言模型Transformer-XL和BERT进行Transformer性能的基准测试。</li><li>用最先进的biLSTM进行了单词和字符级RNN的基准测试。</li><li>上述两种测试是针对Titan Xp、Titan RTX和RTX 2080 Ti进行的，对于其他GPU则线性缩放了性能差异。</li><li>借用了现有的CNN基准测试。</li><li>用了亚马逊和eBay上显卡的平均售价作为GPU的参考成本。<br>最后，可以得出CNN、RNN和Transformer的归一化性能/成本比值，如下所示：</li></ul><p><img src="/2019/07/01/%E7%94%A8%E4%BA%8E%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E7%9A%84GPU%E9%80%89%E8%B4%AD%E5%8F%82%E8%80%83/2019-07-01-19-11-20.png" alt="CNN、RNN和Transformer的每美元性能"></p><p>在上面这张图中，数字越大代表每一美元能买到的性能越强。可以看出， RTX 2060比RTX 2070，RTX 2080或RTX 2080 Ti更具成本效益，<strong>甚至是Tesla V100性价比的5倍以上</strong>。</p><p>所以此轮的性价比之王已经确定，是RTX 2060无疑了。</p><p>下图是李沐老师画了900和1000系列里各个卡的32位浮点计算能力和价格的对比（价格是wikipedia的推荐价格，真实价格通常会有浮动）。</p><p><img src="/2019/07/01/%E7%94%A8%E4%BA%8E%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E7%9A%84GPU%E9%80%89%E8%B4%AD%E5%8F%82%E8%80%83/2019-07-01-19-06-00.png" alt></p><p>由于GPU的功耗，散热和体积，需要一些额外考虑。</p><ul><li>机箱体积<br>GPU尺寸较大，通常不考虑太小的机箱。而且机箱自带的风扇要好。</li><li>电源<br>购买GPU时需要查下GPU的功耗，50w到300w不等。因此买电源时需要功率足够的。</li><li>主板的PCIe卡槽<br>推荐使用PCIe 3.0 16x来保证足够的GPU到主内存带宽。如果是多卡的话，要仔细看主板说明，保证多卡一起使用时仍然是16x带宽。（有些主板插4卡时会降到8x甚至4x）</li></ul><h2 id="Tesla为什么那么贵？"><a href="#Tesla为什么那么贵？" class="headerlink" title="Tesla为什么那么贵？"></a>Tesla为什么那么贵？</h2><p>英伟达现在有一项非常坑爹的政策，如果在数据中心使用CUDA，那么只允许使用Tesla GPU而不能用GTX或RTX GPU。</p><p>由于担心法律问题，研究机构和大学经常被迫购买低性价比的Tesla GPU。<strong>然而，Tesla与GTX和RTX相比并没有真正的优势，价格却高出10倍。</strong></p><p>Nvidia卡有面向个人用户（例如GTX系列）和企业用户（例如Tesla系列）两种。企业用户卡通常使用被动散热和增加了内存校验从而更加适合数据中心。但计算能力上两者相当。<strong>企业卡通常要贵上10倍</strong>。</p><p>Tesla显卡那么贵，其实是贵在双精度浮点数运算能力上了，外加一个鸡肋的ECC校验功能，实在不值。</p><h2 id="总结建议："><a href="#总结建议：" class="headerlink" title="总结建议："></a>总结建议：</h2><p><strong>最佳GPU</strong>：RTX 2070</p><p><strong>避免的坑</strong>：所有Tesla、Quadro、创始人版（Founders Edition）的显卡，还有Titan RTX、Titan V、Titan XP</p><p><strong>高性价比</strong>：RTX 2070（高端），RTX 2060或GTX 1060 (6GB)（中低端）</p><p><strong>计算机视觉或机器翻译研究人员</strong>：采用鼓风设计的GTX 2080 Ti，如果训练非常大的网络，请选择RTX Titans</p><p><strong>NLP研究人员</strong>：RTX 2080 Ti</p><p><strong>已经开始研究深度学习</strong>：RTX 2070起步，以后按需添置更多RTX 2070</p><h2 id="其他配件要求："><a href="#其他配件要求：" class="headerlink" title="其他配件要求："></a>其他配件要求：</h2><p><img src="/2019/07/01/%E7%94%A8%E4%BA%8E%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E7%9A%84GPU%E9%80%89%E8%B4%AD%E5%8F%82%E8%80%83/2019-07-01-19-19-14.png" alt="各硬件性能要求"></p><ul><li><p>CPU:<br>因为主要使用显卡进行cuda计算，因此对CPU的要求并不是很高，频率越高、线程数越多越好，一般最低要求cpu核心数大于显卡个数。其中一个制约因素：cpu的最大PCI-E 通道数。每张显卡占用16条pcie通道才能达到最大性能，而单cpu最大支持40条pcie，也就是即使有4个pcie x16接口，只能最多达到2路x16加一路x8，插上的显卡不能发挥全部性能。不过，主板芯片组其实也可以扩充一部分pcie通道。（x99主板可以扩宽2.0的8lanes，z170可以扩充3.0的20lanes）</p></li><li><p>主板:<br>前面提到了cpu提供的pcie通道数的限制，如果要使用多块显卡，就需要主板提供额外的pcie通道，一般只有服务器级别的主板才会提供扩展pcie通道如x99、x299等主板，但是使用此类主板必须搭配具有该接口的服务器级cpu（xeon系列、i7 7900x以上、i9系列等），如果不需要三块以上的显卡，使用cpu提供的40lane pcie即可。</p></li><li><p>内存：<br>深度学习需要大量数据，中间计算过程也会临时储存大量数据，一般要求具有显存2~3倍的内存，32G或64G乃至更高。内存频率越高越好。<br>最低建议32G DDR4 3200MHz内存(16G*2)约2000元，预算宽裕可升级到64G（约4000元）</p></li><li><p>硬盘：<br>深度学习需要大量数据，和较快的访问速度，一般使用一个较大的固态硬盘作为系统盘和训练数据仓储盘，另外使用hdd机械硬盘作为仓储盘。<br>建议使用512G以上nVME固态硬盘（800元）搭配几TB(2TB约300元）Hdd作为储存空间</p></li><li><p>电源、机箱：电源其实还是要买个比较稳定的，因为要保证长期稳定运行会有“无休止”的training。一般使用大品牌的经过80PLUS金牌或铂金认证的电源。只搭配一张显卡700w即可，每多一张增加400w。4*titan V大概使用1600w电源。</p></li></ul><p>深度学习实验室共享服务器，7x24小时运行  2080ti或者4titan V ，预算充裕可以专门购置一台高性能多显卡深度学习服务器，24*7小时运行，其他用户可以在自己的笔记本电脑和台式机上编写和初步调试卷积神经网络，本地验证无误后，上传至服务器进行训练任务。这样做可以极大的节省设备开支，最大限度的利用计算资源，也避免了每个用户单独配置复杂的软件环境。</p>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>GPU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP学习笔记4</title>
    <link href="/2019/06/30/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04/"/>
    <url>/2019/06/30/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04/</url>
    
    <content type="html"><![CDATA[<h2 id="1-利用朴素贝叶斯模型进行文本分类"><a href="#1-利用朴素贝叶斯模型进行文本分类" class="headerlink" title="1. 利用朴素贝叶斯模型进行文本分类"></a>1. 利用朴素贝叶斯模型进行文本分类</h2><p>朴素贝叶斯是一种构建分类器的简单方法。该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。它不是训练这种分类器的单一算法，而是一系列基于相同原理的算法：<strong>所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关</strong>。</p><p>举个例子，如果一种水果其具有红，圆，直径大概3英寸等特征，该水果可以被判定为是苹果。尽管这些特征相互依赖或者有些特征由其他特征决定，然而朴素贝叶斯分类器认为这些属性在判定该水果是否为苹果的概率分布上独立的。</p><p>尽管是带着这些朴素思想和过于简单化的假设，但朴素贝叶斯分类器在很多复杂的现实情形中仍能够获取相当好的效果。2004年，一篇分析贝叶斯分类器问题的文章揭示了朴素贝叶斯分类器获取看上去不可思议的分类效果的若干理论上的原因。尽管如此，2006年有一篇文章详细比较了各种分类方法，发现更新的方法（如决策树和随机森林）的性能超过了贝叶斯分类器。</p><p>对于某些类型的概率模型，在监督式学习的样本集中能获取得非常好的分类效果。在许多实际应用中，朴素贝叶斯模型参数估计使用<strong>最大似然估计方法</strong>；换而言之，在不用到贝叶斯概率或者任何贝叶斯模型的情况下，朴素贝叶斯模型也能奏效。</p><p>朴素贝叶斯分类器的一个优势在于只需要根据少量的训练数据估计出必要的参数（变量的均值和方差）。由于变量独立假设，只需要估计各个变量的方法，而不需要确定整个协方差矩阵。</p><p>朴素贝叶斯分类器是与线性模型非常相似的一种分类器，但它的训练速度往往更快。这种高效率所付出的代价是，朴素贝叶斯模型的泛化能力要比线性分类器（如LogisticRegression 和LinearSVC）稍差。</p><p>朴素贝叶斯模型如此高效的原因在于，它通过单独查看每个特征来学习参数，并从每个特征中收集简单的类别统计数据。scikit-learn 中实现了三种朴素贝叶斯分类器：GaussianNB、BernoulliNB 和MultinomialNB。GaussianNB 可应用于任意连续数据， 而BernoulliNB 假定输入数据为二分类数据，MultinomialNB 假定输入数据为计数数据（即每个特征代表某个对象的整数计数，比如一个单词在句子里出现的次数）。BernoulliNB 和MultinomialNB 主要用于文本数据分类。</p><pre><code class="lang-python"># 从sklearn.datasets里导入20类新闻文本数据抓取器。from sklearn.datasets import fetch_20newsgroups# 从互联网上即时下载新闻样本,subset=&#39;all&#39;参数代表下载全部近2万条文本存储在变量news中。news = fetch_20newsgroups(subset=&#39;all&#39;)# 从sklearn.cross_validation导入train_test_split模块用于分割数据集。from sklearn.cross_validation import train_test_split# 对news中的数据data进行分割，25%的文本用作测试集；75%作为训练集。X_train, X_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25, random_state=33)# 从sklearn.feature_extraction.text里导入CountVectorizerfrom sklearn.feature_extraction.text import CountVectorizer# 采用默认的配置对CountVectorizer进行初始化（默认配置不去除英文停用词），并且赋值给变量count_vec。count_vec = CountVectorizer()# 只使用词频统计的方式将原始训练和测试文本转化为特征向量。#学习词汇的词典并返回文档矩阵。X_count_train = count_vec.fit_transform(X_train)#不进行学习直接转换文档document-term矩阵X_count_test = count_vec.transform(X_test)# 从sklearn.naive_bayes里导入朴素贝叶斯分类器。from sklearn.naive_bayes import MultinomialNB# 使用默认的配置对分类器进行初始化。mnb_count = MultinomialNB()# 使用朴素贝叶斯分类器，对CountVectorizer（不去除停用词）后的训练样本进行参数学习。mnb_count.fit(X_count_train, y_train)# 输出模型准确性结果。print (&#39;The accuracy of classifying 20newsgroups using Naive Bayes (CountVectorizer without filtering stopwords):&#39;, mnb_count.score(X_count_test, y_test))# 将分类预测的结果存储在变量y_count_predict中。y_count_predict = mnb_count.predict(X_count_test)# 从sklearn.metrics 导入 classification_report。from sklearn.metrics import classification_report# 输出更加详细的其他评价分类性能的指标。print (classification_report(y_test, y_count_predict, target_names = news.target_names))</code></pre><p><img src="/2019/06/30/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04/2019-06-30-20-42-49.png" alt></p><h2 id="2-利用SVM模型进行文本分类"><a href="#2-利用SVM模型进行文本分类" class="headerlink" title="2. 利用SVM模型进行文本分类"></a>2. 利用SVM模型进行文本分类</h2><h2 id="3-pLSA、共轭先验分布、LDA"><a href="#3-pLSA、共轭先验分布、LDA" class="headerlink" title="3. pLSA、共轭先验分布、LDA"></a>3. pLSA、共轭先验分布、LDA</h2><p>常用于文本数据的一种特殊技术是主题建模（topic modeling），这是描述将每个文档分配给一个或多个主题的任务（通常是无监督的）的概括性术语。这方面一个很好的例子是新闻数据，它们可以被分为“政治”“体育”“金融”等主题。如果为每个文档分配一个主题，那么这是一个文档聚类任务。如果每个文档可以有多个主题，那么这个任务与第3 章中的分解方法有关。我们学到的每个成分对应于一个主题，文档表示中的成分系数告诉我们这个文档与该主题的相关性强弱。通常来说，人们在谈论主题建模时，他们指的是一种叫作隐含狄利克雷分布（Latent Dirichlet Allocation，LDA）的特定分解方法</p><h3 id="隐含狄利克雷分布"><a href="#隐含狄利克雷分布" class="headerlink" title="隐含狄利克雷分布"></a>隐含狄利克雷分布</h3><p>从直观上来看，LDA 模型试图找出频繁共同出现的单词群组（即主题）。LDA 还要求，每个文档可以被理解为主题子集的“混合”。重要的是要理解，机器学习模型所谓的“主题”可能不是我们通常在日常对话中所说的主题，而是更类似于 PCA 或 NMF所提取的成分，它可能具有语义，也可能没有。即使 LDA“主题”具有语义，它可能也不是我们通常所说的主题。</p><p>举个自然语言处理的例子，我们可能有许多关于体育、政治和金融的文章，由两位作者所写。在一篇政治文章中，我们预计可能会看 到“州长”“投票”“党派”等词语，而在一篇体育文章中，我们预计可能会看到类似“队 伍”“得分”和“赛季”之类的词语。这两组词语可能会同时出现，而例如“队伍”和 “州长”就不太可能同时出现。但是，这并不是我们预计可能同时出现的唯一的单词群组。这两位记者可能偏爱不同的短语或者选择不同的单词。可能其中一人喜欢使用“划界”（demarcate）这个词，而另一人喜欢使用“两极分化”（polarize）这个词。其他“主题”可 能是“记者 A 常用的词语”和“记者 B 常用的词语”，虽然这并不是通常意义上的主题。</p><h2 id="4-使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类"><a href="#4-使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类" class="headerlink" title="4. 使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类"></a>4. 使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8">https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8</a></p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP学习笔记3</title>
    <link href="/2019/06/27/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03/"/>
    <url>/2019/06/27/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03/</url>
    
    <content type="html"><![CDATA[<h2 id="1-TF-IDF原理。"><a href="#1-TF-IDF原理。" class="headerlink" title="1. TF-IDF原理。"></a>1. TF-IDF原理。</h2><p>tf-idf（英语：term frequency–inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。tf-idf加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了tf-idf以外，互联网上的搜索引擎还会使用基于链接分析的评级方法，以确定文件在搜索结果中出现的顺序。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>在一份给定的文件里，词频（term frequency，tf）指的是某一个给定的词语在该文件中出现的频率。这个数字是对词数（term count）的归一化，以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词数，而不管该词语重要与否。）对于在某一特定文件里的词语 ${\displaystyle t_{i}!}$来说，它的重要性可表示为：</p><script type="math/tex; mode=display">{\displaystyle \mathrm {tf_{i,j}\!} ={\frac {n_{i,j}\!}{\sum _{k}n_{k,j}\!}\!}\!}</script><p>以上式子中 ${\displaystyle n<em>{i,j}!} n</em>{i,j}$是该词在文件 ${\displaystyle d<em>{j}!} d</em>{j}$中的出现次数，而分母则是在文件 ${\displaystyle d<em>{j}!} d</em>{j}$中所有字词的出现次数之和。</p><p>逆向文件频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到：</p><script type="math/tex; mode=display">{\displaystyle \mathrm {idf_{i}\!} =\lg {\frac {|D|}{|\{j:t_{i}\in d_{j}\}|}\!}\!}</script><p>其中</p><p>$|D|$：语料库中的文件总数</p><script type="math/tex; mode=display">{\displaystyle |\{j:t_{i}\in d_{j}\}|} $$：包含词语 ${\displaystyle t_{i}\!} t_{i}$的文件数目（即 ${\displaystyle n_{i,j}\neq 0} n_{i,j}\neq 0$的文件数目）如果词语不在数据中，就导致分母为零，因此一般情况下使用 $${\displaystyle 1+|\{j:t_{i}\in d_{j}\}|}</script><p>然后</p><script type="math/tex; mode=display">{\displaystyle \mathrm {tf{}idf_{i,j}\!} =\mathrm {tf_{i,j}\!} \times \mathrm {idf_{i}\!} }</script><p>某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的tf-idf。因此，tf-idf倾向于过滤掉常见的词语，保留重要的词语。</p><h3 id="1-2-例子"><a href="#1-2-例子" class="headerlink" title="1.2 例子"></a>1.2 例子</h3><p>有很多不同的数学公式可以用来计算tf-idf。这边的例子以上述的数学公式来计算。词频（tf）是一词语出现的次数除以该文件的总词语数。假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”一词在该文件中的词频就是3/100=0.03。而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现“母牛”一词的文件数。所以，如果“母牛”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是lg（10,000,000 / 1,000）=4。最后的tf-idf的分数为0.03 * 4=0.12。</p><h3 id="1-3-tf-idf的理论依据及不足"><a href="#1-3-tf-idf的理论依据及不足" class="headerlink" title="1.3 tf-idf的理论依据及不足"></a>1.3 tf-idf的理论依据及不足</h3><p>tf-idf算法是创建在这样一个假设之上的：对区别文档最有意义的词语应该是那些在文档中出现频率高，而在整个文档集合的其他文档中出现频率少的词语，所以如果特征空间坐标系取tf词频作为测度，就可以体现同类文本的特点。另外考虑到单词区别不同类别的能力，tf-idf法认为一个单词出现的文本频数越小，它区别不同类别文本的能力就越大。因此引入了逆文本频度idf的概念，以tf和idf的乘积作为特征空间坐标系的取值测度，并用它完成对权值tf的调整，调整权值的目的在于突出重要单词，抑制次要单词。但是在本质上idf是一种试图抑制噪声的加权，并且单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用，显然这并不是完全正确的。idf的简单结构并不能有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能，所以tf-idf法的精度并不是很高。</p><p>此外，在tf-idf算法中并没有体现出单词的位置信息，对于Web文档而言，权重的计算方法应该体现出HTML的结构特征。特征词在不同的标记符中对文章内容的反映程度不同，其权重的计算方法也应不同。因此应该对于处于网页不同位置的特征词分别赋予不同的系数，然后乘以特征词的词频，以提高文本表示的效果。</p><h2 id="2-文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。"><a href="#2-文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。" class="headerlink" title="2. 文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。"></a>2. 文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。</h2><p>TfidfVectorizer可以把原始文本转化为tf-idf的特征矩阵，从而为后续的文本相似度计算，主题模型(如LSI)，文本搜索排序等一系列应用奠定基础。基本应用如：</p><h3 id="第一步：分词"><a href="#第一步：分词" class="headerlink" title="第一步：分词"></a>第一步：分词</h3><p>采用著名的中文分词库jieba进行分词：</p><pre><code class="lang-python">import jiebatext = &quot;&quot;&quot;我是一条天狗呀！我把月来吞了，我把日来吞了，我把一切的星球来吞了，我把全宇宙来吞了。我便是我了！&quot;&quot;&quot;sentences = text.split()sent_words = [list(jieba.cut(sent0)) for sent0 in sentences]document = [&quot; &quot;.join(sent0) for sent0 in sent_words]print(document)</code></pre><h3 id="第二步：建模"><a href="#第二步：建模" class="headerlink" title="第二步：建模"></a>第二步：建模</h3><pre><code>理论上，现在得到的document的格式已经可以直接拿来训练了。让我们跑一下模型试试。</code></pre><pre><code class="lang-python">tfidf_model = TfidfVectorizer().fit(document)print(tfidf_model.vocabulary_)# &#123;&#39;一条&#39;: 1, &#39;天狗&#39;: 4, &#39;日来&#39;: 5, &#39;一切&#39;: 0, &#39;星球&#39;: 6, &#39;全宇宙&#39;: 3, &#39;便是&#39;: 2&#125;sparse_result = tfidf_model.transform(document)print(sparse_result)# (0, 4)    0.707106781187# (0, 1)    0.707106781187# (2, 5)    1.0# (3, 6)    0.707106781187# (3, 0)    0.707106781187# (4, 3)    1.0# (5, 2)    1.0</code></pre><h3 id="第三步：参数"><a href="#第三步：参数" class="headerlink" title="第三步：参数"></a>第三步：参数</h3><pre><code>查了一些资料以后，发现单字的问题是token_pattern这个参数搞的鬼。它的默认值只匹配长度≥2的单词，就像其实开头的例子中的&#39;I&#39;也被忽略了一样，一般来说，长度为1的单词在英文中一般是无足轻重的，但在中文里，就可能有一些很重要的单字词，所以修改如下：</code></pre><pre><code class="lang-python">tfidf_model2 = TfidfVectorizer(token_pattern=r&quot;(?u)\b\w+\b&quot;).fit(document)print(tfidf_model2.vocabulary_)# &#123;&#39;我&#39;: 8, &#39;是&#39;: 12, &#39;一条&#39;: 1, &#39;天狗&#39;: 7, &#39;呀&#39;: 6, &#39;把&#39;: 9, &#39;月&#39;: 13, &#39;来&#39;: 14, &#39;吞&#39;: 5, &#39;了&#39;: 2, &#39;日来&#39;: 10, &#39;一切&#39;: 0, &#39;的&#39;: 15, &#39;星球&#39;: 11, &#39;全宇宙&#39;: 4, &#39;便是&#39;: 3&#125;</code></pre><p>token_pattern这个参数使用正则表达式来分词，其默认参数为r”(?u)\b\w\w+\b”，其中的两个\w决定了其匹配长度至少为2的单词，所以这边减到1个。对这个参数进行更多修改，可以满足其他要求，比如这里依然没有得到标点符号，在此不详解了。</p><h2 id="3-互信息"><a href="#3-互信息" class="headerlink" title="3. 互信息"></a>3. 互信息</h2><p>在概率论和信息论中，两个随机变量的互信息（Mutual Information，简称MI）或转移信息（transinformation）是变量间相互依赖性的量度。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布$ p(X,Y) $和分解的边缘分布的乘积$ p(X)p(Y) $的相似程度。互信息是点间互信息（PMI）的期望值。互信息最常用的单位是bit。</p><p>一般地，两个离散随机变量$ X $和$ Y $的互信息可以定义为：</p><script type="math/tex; mode=display">{\displaystyle I(X;Y)=\sum _{y\in Y}\sum _{x\in X}p(x,y)\log {\left({\frac {p(x,y)}{p(x)\,p(y)}\!}\right)},\,\!}</script><p>其中$ p(x,y) $是 $X $和 $Y $的联合概率分布函数，而 ${\displaystyle p(x)} $和$ {\displaystyle p(y)}  $分别是 X 和 Y 的边缘概率分布函数。</p><p>在连续随机变量的情形下，求和被替换成了二重定积分：</p><script type="math/tex; mode=display">{\displaystyle I(X;Y)=\int _{Y}\int _{X}p(x,y)\log {\left({\frac {p(x,y)}{p(x)\,p(y)}\!}\right)}\;dx\,dy,} $$,其中 $p(x,y)$ 当前是 X 和 Y 的联合概率密度函数，而 ${\displaystyle p(x)} $和$ {\displaystyle p(y)}$分别是$ X $和$ Y $的边缘概率密度函数。如果对数以 2 为基底，互信息的单位是bit。直观上，互信息度量 X 和 Y 共享的信息：它度量知道这两个变量其中一个，对另一个不确定度减少的程度。例如，如果 X 和 Y 相互独立，则知道 X 不对 Y 提供任何信息，反之亦然，所以它们的互信息为零。在另一个极端，如果 X 是 Y 的一个确定性函数，且 Y 也是 X 的一个确定性函数，那么传递的所有信息被 X 和 Y 共享：知道 X 决定 Y 的值，反之亦然。因此，在此情形互信息与 Y（或 X）单独包含的不确定度相同，称作 Y（或 X）的熵。而且，这个互信息与 X 的熵和 Y 的熵相同。（这种情形的一个非常特殊的情况是当 X 和 Y 为相同随机变量时。）互信息是 X 和 Y 的联合分布相对于假定 X 和 Y 独立情况下的联合分布之间的内在依赖性。 于是互信息以下面方式度量依赖性：$I(X; Y) = 0$ 当且仅当 X 和 Y 为独立随机变量。从一个方向很容易看出：当 X 和 Y 独立时，$p(x,y) = p(x) p(y)$，因此：$${\displaystyle \log {\left({\frac {p(x,y)}{p(x)\,p(y)} \!}\right)}=\log 1=0.\,\!}</script><p>此外，互信息是非负的（即 $I(X;Y) ≥ 0$; 见下文），而且是对称的（即 $I(X;Y) = I(Y;X)$）。</p><p>互信息又可以等价地表示成</p><script type="math/tex; mode=display">{\displaystyle {\begin{aligned}I(X;Y)&{}=H(X)-H(X|Y)\\&{}=H(Y)-H(Y|X)\\&{}=H(X)+H(Y)-H(X,Y)\\&{}=H(X,Y)-H(X|Y)-H(Y|X)\end{aligned}\!}\!}</script><p>其中$ {\displaystyle \ H(X)} $ 和$ {\displaystyle \ H(Y)}  $是边缘熵，$H(X|Y) $和$ H(Y|X) $是条件熵，而 $H(X,Y) $是 X 和 Y 的联合熵。</p><p><strong>互信息越小，两个来自不同事件空间的随机变量彼此之间的关系性越低; 互信息越高，关系性则越高。</strong></p><h2 id="4-对特征矩阵使用互信息进行特征筛选"><a href="#4-对特征矩阵使用互信息进行特征筛选" class="headerlink" title="4. 对特征矩阵使用互信息进行特征筛选"></a>4. 对特征矩阵使用互信息进行特征筛选</h2><p><code>sklearn.metrics.mutual_info_score</code></p><pre><code class="lang-python">from sklearn import datasetsfrom sklearn import metrics as mriris = datasets.load_iris()x = iris.datalabel = iris.targetx0 = x[:, 0]x1 = x[:, 1]x2 = x[:, 2]x3 = x[:, 3]# 计算各特征与label的互信息print(mr.mutual_info_score(x0, label))print(mr.mutual_info_score(x1, label))print(mr.mutual_info_score(x2, label))print(mr.mutual_info_score(x3, label))</code></pre><p><img src="/2019/06/27/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03/2019-06-27-20-45-13.png" alt><br><code>sklearn.feature_selection.mutual_info_classif</code></p><pre><code class="lang-python">from sklearn import datasetsfrom sklearn.feature_selection import mutual_info_classifiris = datasets.load_iris()x = iris.datalabel = iris.targetmutual_info = mutual_info_classif(x, label, discrete_features= False)print(mutual_info)</code></pre><p><img src="/2019/06/27/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03/2019-06-27-20-45-28.png" alt></p><blockquote><p>参考文献<br><a href="https://zh.wikipedia.org/wiki/Tf-idf">https://zh.wikipedia.org/wiki/Tf-idf</a><br><a href="https://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF">https://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF</a><br><a href="https://blog.csdn.net/yyy430/article/details/88249709">https://blog.csdn.net/yyy430/article/details/88249709</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】机器学习之集成方法，随机森林和梯度提升树</title>
    <link href="/2019/06/27/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95%EF%BC%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91/"/>
    <url>/2019/06/27/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95%EF%BC%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<p>本文会介绍集成方法，bagging 和 boosting 两种思路，以及两种思路的代表性模型：随机森林和梯度提升树。</p><!--more---><h2 id="1-投票分类器"><a href="#1-投票分类器" class="headerlink" title="1. 投票分类器"></a>1. 投票分类器</h2><p>三个臭皮匠顶一个诸葛亮。即便是很多预测准确率仅强于随机的个体学习器的组合，经过一定的安排，也可以发挥令人惊讶的效果。在机器学习中，这种看起来没什么含金量的学习策略称之为<strong>集成学习</strong>。</p><p>集成学习首先需要一系列的个体学习器。之后采用某些策略结合它们的判断。</p><p>集成学习的要求：</p><ul><li>构成集成学习器的个体学习器，其性能不能太差，至少要为强于随机的<strong>弱学习器</strong>。</li></ul><p>当然强学习器更好。在最后的结果汇总阶段，也会更多听取强学习器的意见。</p><ul><li>个体学习器要具有一定的多样性。</li></ul><p>广泛吸收各种不同学习器的意见，做出的决策才有代表性。在机器学习中，体现出的要求就是模型之间的差别要尽可能的大。一方面可以通过划分不同的数据集，独立训练来得到差异性；另一方面我们也可以选取不同的训练模型，比如SVM、决策树、逻辑回归。</p><p>集成学习法的准确率比集成学习中表现最好的分类器准确率还高，这究竟是为什么？难道那一些不入流的臭鱼烂虾机器学习法，它们存在的意义就是提升集成学习中的准确率的吗？</p><p>我们来打个比方。假设我有一枚硬币，这枚硬币经过加工处理，正面朝上的可能性比背面要高那么一点点，51%的可能性是正面。问，如果我投掷1000次硬币，正面朝上次数大于背面朝上次数的可能性占比多少？一万次呢？</p><p>事实上，1000次投掷，最后正面次数比背面多的概率就达到了0.72，如果投掷10000次，那么就是0.94，几乎是必然事件。</p><p>将其类比到集成学习中来，如果相互独立的个体学习器足够多，那么我们得到正确结论的概率将大大提升。不过这里有一个最关键的点：<strong>模型之间相互独立</strong>。这个要求其实是蛮难达到的，因为即便是不同的机器学习模型，如果采用相同或者相似的数据集进行训练，那么他们之间必然存在某种相关性。更不用说连模型都是一模一样的情况了。</p><h2 id="2-Boosting算法"><a href="#2-Boosting算法" class="headerlink" title="2. Boosting算法"></a>2. Boosting算法</h2><p>Boosting算法的核心思想是分割训练集，用同一种机器学习算法得到差异化的一系列模型。</p><p>先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本的分布进行调整，是的先前基学习器做错的训练样本在后续收到更多地关注。然后基于调整后的样本分布来训练下一个基学习器。如此反复进行，训练T个基学习器，最终加权投票。</p><p><img src="/2019/06/27/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95%EF%BC%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91/2019-06-27-21-57-03.png" alt></p><p><img src="/2019/06/27/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95%EF%BC%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91/2019-06-27-21-57-17.png" alt></p><h3 id="梯度提升树"><a href="#梯度提升树" class="headerlink" title="梯度提升树"></a>梯度提升树</h3><h3 id="梯度提升树的实现：XGBoost"><a href="#梯度提升树的实现：XGBoost" class="headerlink" title="梯度提升树的实现：XGBoost"></a>梯度提升树的实现：XGBoost</h3><h3 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h3><h2 id="3-Bagging算法"><a href="#3-Bagging算法" class="headerlink" title="3. Bagging算法"></a>3. Bagging算法</h2><p>有放回采样被称为Bagging。采用Bagging的方法我们可以得到很多的可能有重复样本的数据子集。我们在之前的文章中已经提到，对于每个选取的子集，平均下来只有63%的训练实例被采样，剩下的37%正好当做测试集。</p><p>随机森林就采用了Bagging采样方法来训练很多的决策树。如果你观察单一的决策树，重要的特征会出现在更靠近根部的位置，不重要的特征会经常出现在靠近叶子的位置。因此我们可以通过计算一个特征在森林的全部树中出现的平均深度来预测特征的重要性。</p><h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
      <tag>boosting</tag>
      
      <tag>bagging</tag>
      
      <tag>random forest</tag>
      
      <tag>GBDT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP学习笔记2</title>
    <link href="/2019/06/24/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/"/>
    <url>/2019/06/24/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/</url>
    
    <content type="html"><![CDATA[<h2 id="1-分词的概念和实现细节"><a href="#1-分词的概念和实现细节" class="headerlink" title="1. 分词的概念和实现细节"></a>1. 分词的概念和实现细节</h2><p>NLP的底层任务可分为词法分析、句法分析和语义分析，分词是词法分析中最基本的任务。中文分词是在一个中文序列的词与此之间加上空格或者其他边界标志进行分割，从而方便接下来步骤的处理。</p><p>分词算法可分为两种，一种是基于词典的分词算法，另一种是基于字的分词算法。</p><h3 id="1-1-基于词典的分词算法："><a href="#1-1-基于词典的分词算法：" class="headerlink" title="1.1 基于词典的分词算法："></a>1.1 基于词典的分词算法：</h3><p><strong>最大匹配分词算法</strong>，有正向和反向两种。主要思路是将词典构造成一颗Trie树，也成为词典树。以“他说的确实在理”这句话为例，构造Trie树如图所示：</p><p><img src="/2019/06/24/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/2019-06-24-20-42-46.png" alt></p><p>Trie树由词的公共前缀构成节点，降低了存储空间的同时提升查找效率。最大（正向）匹配分词将句子与Trie树进行匹配，在匹配到根结点时由下一个字重新开始进行查找。比如正向（从左至右）匹配“他说的确实在理”，得出的结果为“他／说／的确／实在／理”。如果进行反向最大匹配，则为“他／说／的／确实／在理”。</p><p>单独依仗这种方法达不到很好的分词效果，而且分词时间复杂度为O(N)，即随着字符串长度线性上升。</p><p><strong>最短路径分词算法</strong>，讲一句话中所有的词匹配出来构成<strong>词图</strong>，词图是一个有向无环图。之后分词问题转化为求开始节点和结束节点之间的最短路径的问题。有迪杰斯特拉算法以及其他算法。不一定只保存最短的路径，有可能保存前N短的路径。图的边上也有可能按照不同词汇出现的概率大小不同安排不同的权值。</p><p><img src="/2019/06/24/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/2019-06-24-20-47-26.png" alt></p><p>如何构建不同权值的词图？有基于n-gram的分词算法。最后我们可以得到词的概率图。</p><p><img src="/2019/06/24/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/2019-06-24-20-49-14.png" alt></p><h3 id="1-2-基于字的分词算法"><a href="#1-2-基于字的分词算法" class="headerlink" title="1.2 基于字的分词算法"></a>1.2 基于字的分词算法</h3><p>与基于词典的分词不同的是，基于字的分词事先不对句子进行词的匹配，而是将分词看成序列标注问题，把一个字标记成B(Begin), I(Inside), O(Outside), E(End), S(Single)。因此也可以看成是每个字的分类问题，输入为每个字及其前后字所构成的特征，输出为分类标记。对于分类问题，可以用统计机器学习或神经网络的方法求解。</p><p>在NLP中，最常用的神经网络为循环神经网络（RNN，Recurrent Neural Network），它在处理变长输入和序列输入问题中有着巨大的优势。LSTM为RNN变种的一种，在一定程度上解决了RNN在训练过程中梯度消失和梯度爆炸的问题。双向（Bidirectional）循环神经网络分别从句子的开头和结尾开始对输入进行处理，将上下文信息进行编码，提升预测效果。</p><h2 id="2-词、字符频率统计"><a href="#2-词、字符频率统计" class="headerlink" title="2. 词、字符频率统计"></a>2. 词、字符频率统计</h2><p>统计一篇文章中单词出现的次数，首先应该知道该文章中，有多少个单词（去重后），然后再统计单词在文章中的出现频率。这里使用最简单的方式来实现该功能。</p><pre><code class="lang-python">def statistics():    path = ...    with open(path, &#39;r&#39;, encoding=&#39;UTF-8&#39;) as text:        print(string.punctuation)        words = [raw_word.strip(string.punctuation).lower() for raw_word in text.read().split()]        words_index = set(words)        counts_dict = &#123;index: words.count(index) for index in words_index&#125;    for word in sorted(counts_dict, key=lambda x: counts_dict[x], reverse=True):        print(&#39;&#123;&#125;--&#123;&#125; times&#39;.format(word, counts_dict[word]))</code></pre><h2 id="3-语言模型中unigram、bigram、trigram的概念"><a href="#3-语言模型中unigram、bigram、trigram的概念" class="headerlink" title="3. 语言模型中unigram、bigram、trigram的概念"></a>3. 语言模型中unigram、bigram、trigram的概念</h2><p>简单地说，语言模型就是用来计算一个句子的概率的模型。为了解决參数空间过大的问题。引入了马尔科夫假设：随意一个词出现的概率只与它前面出现的有限的一个或者几个词有关。</p><p>如果一个词的出现与它周围的词是独立的，那么我们就称之为unigram也就是一元语言模型：</p><p><img src="/2019/06/24/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/2019-06-24-20-55-14.png" alt></p><p>如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为bigram：</p><p><img src="/2019/06/24/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/2019-06-24-20-54-56.png" alt></p><p>同理，trigram：</p><p><img src="/2019/06/24/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/2019-06-24-20-55-41.png" alt></p><p>一般来说，N元模型就是假设当前词的出现概率只与它前面的N-1个词有关。在实践中用的最多的就是bigram和trigram了。</p><h2 id="4-文本矩阵化"><a href="#4-文本矩阵化" class="headerlink" title="4. 文本矩阵化"></a>4. 文本矩阵化</h2><pre><code class="lang-python">a =&quot;自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学&quot;b = &quot;因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。&quot;c =&quot;因而它是计算机科学的一部分。自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。&quot;import jiebaall_list= [&#39;  &#39;.join(jieba.cut(s,cut_all = False)) for s in [a,b,c]]print(all_list)</code></pre><pre><code class="lang-python">#从文件导入停用词表stpwrdpath =&quot;C:\\Users\\Administrator\Desktop\lect09_codes\lect09_proj\stop_words\\中文停用词库.txt&quot;with open(stpwrdpath, &#39;rb&#39;) as fp:    stopword = fp.read().decode(&#39;utf-8&#39;)  # 提用词提取#将停用词表转换为list  stpwrdlst = stopword.splitlines()# 从sklearn.feature_extraction.text里导入CountVectorizerfrom sklearn.feature_extraction.text import CountVectorizer# 对CountVectorizer进行初始化（去除中文停用词）count_vec=CountVectorizer(stop_words=stpwrdlst) #创建词袋数据结构X_count_train = count_vec.fit_transform(all_list[:2])  #&lt;class &#39;scipy.sparse.csr.csr_matrix&#39;&gt;# 将原始训练和测试文本转化为特征向量X_count_train= X_count_train.toarray()X_count_test = count_vec.transform(all_list[2]).toarray()print(X_count_train)#词汇表print(&#39;\nvocabulary list:\n\n&#39;,count_vec.get_feature_names())print( &#39;\nvocabulary dic :\n\n&#39;,count_vec.vocabulary_)print (&#39;vocabulary:\n\n&#39;)for key,value in count_vec.vocabulary_.items():    print(key,value)</code></pre>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP学习笔记1</title>
    <link href="/2019/06/21/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/"/>
    <url>/2019/06/21/NLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/</url>
    
    <content type="html"><![CDATA[<h1 id="IMDB数据集探索"><a href="#IMDB数据集探索" class="headerlink" title="IMDB数据集探索"></a>IMDB数据集探索</h1><p>实验是在Google Colab上面做的，机器也是用的谷歌云。</p><pre><code class="lang-python"># keras.datasets.imdb is broken in 1.13 and 1.14, by np 1.16.3!pip install tf_nightly</code></pre><p>安装tensorflow</p><pre><code class="lang-python">from __future__ import absolute_import, division, print_function, unicode_literalsimport tensorflow as tffrom tensorflow import kerasimport numpy as npprint(tf.__version__)</code></pre><p>导入相关的包</p><pre><code class="lang-python">imdb = keras.datasets.imdb(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)</code></pre><p>导入IMDB数据集，将其分成四部分，分别是训练集、训练集答案、测试集、测试集答案。</p><pre><code class="lang-python">print(&quot;Training entries: &#123;&#125;, labels: &#123;&#125;&quot;.format(len(train_data), len(train_labels)))print(train_data[0])len(train_data[0]), len(train_data[1])</code></pre><p>探索数据集。可以发现，训练集的每一个训练样本是一个由数字组成的列表。我还纳闷呢，这不应该是文本序列吗？</p><p>后来我发现，每个数字对应着不同的单词。比如1对应的是The，4对应的是film。之后只要根据字典mapping一下就好。</p><pre><code class="lang-python"># A dictionary mapping words to an integer indexword_index = imdb.get_word_index()# The first indices are reservedword_index = &#123;k:(v+3) for k,v in word_index.items()&#125;word_index[&quot;&lt;PAD&gt;&quot;] = 0word_index[&quot;&lt;START&gt;&quot;] = 1word_index[&quot;&lt;UNK&gt;&quot;] = 2  # unknownword_index[&quot;&lt;UNUSED&gt;&quot;] = 3reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])def decode_review(text):    return &#39; &#39;.join([reverse_word_index.get(i, &#39;?&#39;) for i in text])decode_review(train_data[0])</code></pre><p>提取字典。并尝试利用字典还原一个样本的本来面貌。</p><pre><code class="lang-python">train_data = keras.preprocessing.sequence.pad_sequences(train_data,                                                        value=word_index[&quot;&lt;PAD&gt;&quot;],                                                        padding=&#39;post&#39;,                                                        maxlen=256)test_data = keras.preprocessing.sequence.pad_sequences(test_data,                                                       value=word_index[&quot;&lt;PAD&gt;&quot;],                                                       padding=&#39;post&#39;,                                                       maxlen=256)len(train_data[0]), len(train_data[1])print(train_data[0])</code></pre><p>简化训练集和测试集，每个样本之提取最多256个单次，不够的就以0来凑。</p><pre><code class="lang-python"># input shape is the vocabulary count used for the movie reviews (10,000 words)vocab_size = 10000model = keras.Sequential()model.add(keras.layers.Embedding(vocab_size, 16))model.add(keras.layers.GlobalAveragePooling1D())model.add(keras.layers.Dense(16, activation=tf.nn.relu))model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))model.summary()</code></pre><p>安排网络，输入层、池化层、全连接、softmax层。</p><pre><code class="lang-python">model.compile(optimizer=&#39;adam&#39;,              loss=&#39;binary_crossentropy&#39;,              metrics=[&#39;acc&#39;])</code></pre><pre><code class="lang-python">x_val = train_data[:10000]partial_x_train = train_data[10000:]y_val = train_labels[:10000]partial_y_train = train_labels[10000:]history = model.fit(partial_x_train,                    partial_y_train,                    epochs=40,                    batch_size=512,                    validation_data=(x_val, y_val),                    verbose=1)</code></pre><p>训练！</p><pre><code class="lang-python">results = model.evaluate(test_data, test_labels)print(results)</code></pre><p>测试结果是准确率87%。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模型评估与选择——周志华《机器学习》CH2</title>
    <link href="/2019/06/21/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9%E2%80%94%E2%80%94%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8BCH2/"/>
    <url>/2019/06/21/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9%E2%80%94%E2%80%94%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8BCH2/</url>
    
    <content type="html"><![CDATA[<p>模型如何评估，选择标准是什么？<br>先让我们了解一下常见的衡量标准</p><p>错误率+精度=1</p><h3 id="误差："><a href="#误差：" class="headerlink" title="误差："></a>误差：</h3><p><strong>训练误差/经验误差</strong> training/empirical error<br><strong>泛化误差</strong> generalization error</p><p>训练误差低，泛化误差不一定低。这其中牵扯到过拟合和欠拟合的问题。</p><p><strong>过拟合</strong>：过分学习，将训练样本中不属于规律的的噪声也一并学习的现象。<br>防止过拟合，一般采用将数据集分成训练集和测试集，利用训练集训练模型，利用测试集拟合泛化误差的办法。</p><p><strong>测试误差</strong> testing error</p><h3 id="划分数据集的方法"><a href="#划分数据集的方法" class="headerlink" title="划分数据集的方法"></a>划分数据集的方法</h3><h4 id="样本划分之留出法-hold-out"><a href="#样本划分之留出法-hold-out" class="headerlink" title="样本划分之留出法 hold-out"></a>样本划分之留出法 hold-out</h4><p>将样本分成互斥的两部分S,T<br>用S训练，用T测试。分割比例自己确定。<br>需要注意的是，必须保证S、T同分布，建议采用<strong>分层采样</strong>stratified sampling，即数据集中的每个类别雨露均沾。<br>另外可以随机划分若干次，防止单次划分出现极端采样结果。随机次数越高，结果的<strong>保真性</strong>fidelity越高。</p><h4 id="样本划分之交叉验证-cross-validation"><a href="#样本划分之交叉验证-cross-validation" class="headerlink" title="样本划分之交叉验证 cross validation"></a>样本划分之交叉验证 cross validation</h4><p>将数据集采用分层划分，分成若干个互斥的小数据集。每个小数据集都当一次测试集，其他数据集组成新训练集。如果分割成k个小数据集，则这种验证方法会做k个不同的划分。因此又称为k-fold 交叉验证。</p><p>极端情况是留一法 leave one out，即k=|D|。</p><h4 id="样本划分之自助法-bootstrapping"><a href="#样本划分之自助法-bootstrapping" class="headerlink" title="样本划分之自助法 bootstrapping"></a>样本划分之自助法 bootstrapping</h4><p>从D中进行m次<strong>放回抽样</strong>，形成新的小数据集D’,理所应当地，新数据集内可能有重复元素，即<script type="math/tex">|unique(D')|\leq m</script></p><p>大数据集中的一个元素x不被选中的概率是<script type="math/tex">\mathbb{P}\{x\in \mathit{D}\cap x\notin \mathit{D}'\}=(1-\frac{1}{m})^m</script>，此时如果m取得越多，概率就越趋近于<script type="math/tex">\lim_{m\rightarrow\infty}(1-\frac{1}{m})^m=\frac{1}{e}</script>。</p><p>最后可以利用D’训练，用D/D’测试。此种抽样方法又称为<strong>包外估计</strong> out of bag estimate，适用于|D|很小的情况。</p><h3 id="如何判断模型的好坏？"><a href="#如何判断模型的好坏？" class="headerlink" title="如何判断模型的好坏？"></a>如何判断模型的好坏？</h3><p>为防止专有名词混淆，此处主要采用英文术语。<br>对于二分类模型，有以下评价模型的标准：</p><p>accuracy：模型结果与真实值相同的比率。中文称之为<strong>精度</strong>。</p><p>precision：模型所得结果中正例比率。中文称之为<strong>准确率</strong>。若想准确率提升，直观的方法是只挑选自己十分确定的样本。所谓不打无准备之仗。不过这样肯定会放过很多原本是正例的样本。</p><p>recall：正例中模型结果占比。中文称为<strong>查全率</strong>、<strong>召回率</strong>。想提高查全率，就要把所有疑似样本全都收集进来，所谓宁杀一千不放一个。这样显然也会提高误杀率。</p><p><strong>PR曲线</strong>：即准确率-查全率曲线。对于预测模型来说，对未知样本的预测，准确率和查全率往往不可兼得。呈现一个这种曲线：</p><p><img src="/2019/06/21/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9%E2%80%94%E2%80%94%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8BCH2/2019-06-21-11-58-57.png" alt></p><p>模型对每个样本会给出自己的判断，并且还会有自己的置信度。我们可以按照置信度排序，就可以做出PR曲线。</p><p>只有PR曲线，我们可以说模型C最差，因为这条曲线完全被A或B模型的曲线所包围。但是不好判断A红线与B黑线的性能，因为二者有交叉。这种情况有三种度量：</p><p>求<strong>曲线下面积</strong>是一种思路，不过有比较高的计算成本，更喜欢采用的是<strong>平衡点</strong>Break-Even point，可以看到我们挑了三个小红点。靠外的模型好。另外可以采用F1度量。计算公式是<script type="math/tex">F_1=\frac{2PR}{P+R}</script>这个公式就是准确率和查全率的调和平均<script type="math/tex">\frac{1}{F_1}=\frac{1}{2}(\frac{1}{P}+\frac{1}{R})</script>。之所以取调和平均，是因为调和平均在四种平均中最小，因此更重视较小值。</p><p><img src="/2019/06/21/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9%E2%80%94%E2%80%94%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8BCH2/2019-06-21-12-46-48.png" alt="四大基本不等式"></p><p>对于实际问题，准确率和查全率的意义不一样。超市小偷识别系统更害怕冤枉好人，因此可能更注重准确率；而地铁检查系统可能抱着“宁查一千不放一个”的态度，更追求查全率。因此对F1评价稍作修改，我们就得到了F<em>beta度量指标：$$F</em>{\beta}=\frac{(1+\beta^2)PR}{\beta^2P+R}<script type="math/tex">，其实这个公式就是加了权重后的调和平均</script>\frac{1}{F_{\beta}}=\frac{1}{1+\beta^2}(\frac{1}{P}+\frac{\beta^2}{R})$$。$\beta&gt;1$则更注重查全率R，$\beta&lt;1$则更注重查准率P。</p><p><strong>ROC曲线</strong>是另外一个思路的评价标准，横坐标是假正例，纵坐标是真正例。其绘制方法也是将样本按照置信度排序，如果样本是真正例，则垂直向y轴正方向绘制一个单位；如果样本是假正例，则水平向x轴正方向绘制一个单位。</p><p>理想状态下，模型将所有正例排在反例前面，因此曲线应该一直往上升，升到m个正例穷尽之后，再水平到|D|-m个反例。但是往往模型会判断失误，于是就出现了这种图像：</p><p><img src="/2019/06/21/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9%E2%80%94%E2%80%94%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8BCH2/2019-06-21-12-59-40.png" alt></p><p>左图是无限样例下，才可能达到的光滑ROC曲线。右图是实际可能的ROC曲线。ROC曲线围成的面积称之为<strong>AUC</strong>。ROC越丰满，AUC越大，模型的判别效果越好。</p><p><strong>AUC</strong>同样也可判断模型的好坏。而且AUC实际上可以通过数值方法来近似求解，有如下公式：<script type="math/tex">AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)(y_i+y_{i+1})</script></p><p>若预测正误代价不同，可参考西瓜书“<strong>代价曲线</strong>”，此处不再赘述。</p><p><strong>假设检验</strong>模块，数学味道太浓，写成博客实用度不高。而且西瓜书上讲的也不甚明了，真正想了解假设检验的朋友，可参考《统计推断》一书或其他的数理统计教材。</p><h3 id="偏差方差分解-bias-variance-decomposition"><a href="#偏差方差分解-bias-variance-decomposition" class="headerlink" title="偏差方差分解 bias variance decomposition"></a>偏差方差分解 bias variance decomposition</h3><p>这种分解可以解释泛化性能为什么会下降到一定程度后上升，越训练越差。</p><p>偏差 bias<br>方差 var</p><p><script type="math/tex">E(f) = bias^2(x)+var(x)+\epsilon^2</script>，我们想令Ef最小。事实上训练越充分，偏差bias越小，但是方差var会提高。所以并不是训练越多越好。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
      <tag>model evaluation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>三星笔记本升级硬盘实录</title>
    <link href="/2019/06/16/%E4%B8%89%E6%98%9F%E7%AC%94%E8%AE%B0%E6%9C%AC%E5%8D%87%E7%BA%A7%E7%A1%AC%E7%9B%98%E5%AE%9E%E5%BD%95/"/>
    <url>/2019/06/16/%E4%B8%89%E6%98%9F%E7%AC%94%E8%AE%B0%E6%9C%AC%E5%8D%87%E7%BA%A7%E7%A1%AC%E7%9B%98%E5%AE%9E%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<p>这是一篇装机实录，主要内容有：</p><ul><li>给老式（2014年左右）无光驱笔记本电脑安装固态硬盘、拆卸机械硬盘；</li><li>重装系统的坑</li><li>固态硬盘体验</li></ul><h1 id="旧本盼望新生"><a href="#旧本盼望新生" class="headerlink" title="旧本盼望新生"></a>旧本盼望新生</h1><p>很早之前就想为自己的古董笔记本提升一下性能了。我的笔记本型号是NP370R5V-S02CN，属于2014年那会儿产的机型。i5的CPU、8G的内存（其中我额外购置了4G内存条），再加上5年的使用习惯，让我对这台机器还比较满意。限制笔记本电脑性能的主要瓶颈就是硬盘了。</p><p>我是一个等等党，平时也不会在自己的电脑上运行特别复杂的程序，大部分都在服务器或者公有云上跑了。所以对笔记本电脑的需求不是那么急切。固态2块钱1G的时候我没有心动，1块钱1G的时候我还是没有心动，但是我看到西数蓝盘500G的固态只卖379元，而且5年质保时，我真的憋不住了。</p><p>之前我一直用的是windows 7的操作系统，一直没敢上windows 10，因为听说windows 10挺吃性能的。现在我打算把原来笔记本电脑上的数据备份一下之后，直接上windows 10。因此我不需要进行系统迁移，直接重装系统。</p><h1 id="部件难堪重负"><a href="#部件难堪重负" class="headerlink" title="部件难堪重负"></a>部件难堪重负</h1><h2 id="1-制作启动U盘"><a href="#1-制作启动U盘" class="headerlink" title="1) 制作启动U盘"></a>1) 制作启动U盘</h2><p>的过程，我完全按照网上说的，先到校园网下载win10专业版镜像，利用我手头上的16G空U盘和Rufus 3.5软件，将ISO镜像拷入U盘，格式化为启动盘。</p><p>此处U盘的大小一般8G以上为宜。现在是2019年，市面上8G的U盘算容量不大的了。不知道以后会不会淘汰掉。Rufus是免费软件，相同功能的软件还有UltraISO等。</p><p>需要注意的是，不要使用老毛桃等PE，虽然傻瓜式安装，但是系统不纯净。而且装系统的过程并不复杂，多踩踩坑就熟悉了。</p><h2 id="2-拆卸更换硬盘"><a href="#2-拆卸更换硬盘" class="headerlink" title="2) 拆卸更换硬盘"></a>2) 拆卸更换硬盘</h2><p>我的笔记本似乎在设计的时候就料到了用户会添加内存条和更换硬盘，因此包裹硬盘和内存区域的外壳与主板是分开的，只需拆卸一颗螺丝，即可将硬盘位和内存位暴露出来。这给我的拆卸过程减少了很多麻烦。</p><p>关机，拆后盖，连电池都不用拆卸，直接把硬盘固定位的四颗螺丝拧下来，换上固态，美滋滋。</p><p>将拆卸下来的机械硬盘放在我买的硬盘盒里面（25元），大小刚刚合适（2.5寸）。沉甸甸的，里面存了不少数据【doge】。</p><p>开始重装系统。现在的固态硬盘是空的，里面没有系统。插入U盘，按下电源键后按F2进入BIOS模式。这里开始就有坑了。</p><h2 id="3-更新操作系统"><a href="#3-更新操作系统" class="headerlink" title="3) 更新操作系统"></a>3) 更新操作系统</h2><p>首先我准备安装的是win10，原来安装的是win7。win7的磁盘引导是MBR方式，而最新的引导方式是UEFI，这两种磁盘格式化方式是不一样的，也只有当磁盘里没有任何数据的一开始，我敢将磁盘的引导方式修改一下，换作原来我真的害怕稍微操作数据就没了。所以我进行了如下设置：</p><ul><li>将Security Boot关闭</li><li>将AHCI引导换成UEFI启动</li><li>Boot Mode换成UEFI only<br>一顿操作，win10安装完成了，哈哈哈。令我没想到的事情来了！</li></ul><h2 id="4-电脑受不了Windows-10"><a href="#4-电脑受不了Windows-10" class="headerlink" title="4) 电脑受不了Windows 10"></a>4) 电脑受不了Windows 10</h2><p>安装win10后，固态硬盘的好处显现出来了，任何东西都是秒开，延迟大大的降低了。就在我享受各种操作的时候，突然电脑卡住了。</p><p>就是卡死在一瞬间，鼠标都不能动了。等了好久也没用，我长按电源键强制关机后再启动，没用，过一会儿还是死机。</p><p>我怀疑是不是我的电脑其他部件太过古老了？当时的风扇吹出来的风都烫手。但是当我放凉之后再次开机，还是会毫无征兆地死机。</p><p>我怀疑是不是我安装的软件里面有不兼容的驱动？有可能是显卡驱动之类的。因为三星官网关于我这个机型的驱动，最多支持到win7。</p><p>各种原因都查了个遍，一下午过去了，一晚上过去了，我还是没能解决这个问题。总不能是固态的问题吧？</p><p>我认真的思考了一下，西部数据蓝盘，打折出售，也有可能是卖给我了劣质盘。但是我现在没办法安装磁盘检测软件，一开电脑就死机。</p><h2 id="5-悲伤地换回windows-7"><a href="#5-悲伤地换回windows-7" class="headerlink" title="5) 悲伤地换回windows 7"></a>5) 悲伤地换回windows 7</h2><p>于是我决定，重新安装回win7，如果能用就不折腾了。</p><p>下载win7镜像，使用rufus制作启动盘，此时的分区格式就选择mbr好了。我妥协了！不折腾了。</p><p>一如既往地修改BIOS，BootMode改成Legacy。win7一会儿就安装好了。没有其他的问题。</p><p>真是邪了门了，而且用win7发热的现象也缓解了很多！</p><p>看来真的是，给老电脑更换一个新固态，好比是给老年人更换年轻人的心脏，心有余而力不足啊！</p><h1 id="体验固态硬盘"><a href="#体验固态硬盘" class="headerlink" title="体验固态硬盘"></a>体验固态硬盘</h1><p>上次有这种性能大幅度稳定提升一个档次的感觉的时候，是这台电脑安装新的内存条之后。读写速度真的是限制目前性能的一大瓶颈。</p><p>由于我的内部接口是SATA 2，所以无法发挥SATA 3接口的西数蓝盘的全部威力。不过这也在我的计算之中了，因为相对于HDD，SSD带来的提升实在是天壤之别。</p><p><img src="/2019/06/16/%E4%B8%89%E6%98%9F%E7%AC%94%E8%AE%B0%E6%9C%AC%E5%8D%87%E7%BA%A7%E7%A1%AC%E7%9B%98%E5%AE%9E%E5%BD%95/2019-06-19-19-01-03.png" alt></p><p>等到老娘今后有钱了，这个硬盘还能拆下来重复利用，岂不是美滋滋。</p><p>本来寻思给这篇文章加点图，算啦，反正也不是什么成功经验。</p>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hardware</tag>
      
      <tag>SSD</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高级操作系统——分布式系统——课程设计与实现</title>
    <link href="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"/>
    <url>/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="实验二"><a href="#实验二" class="headerlink" title="实验二"></a>实验二</h1><h2 id="一、实验目的"><a href="#一、实验目的" class="headerlink" title="一、实验目的"></a>一、实验目的</h2><p>尝试实现一个无连接的数据报Socket进程间通信（UDP）</p><h2 id="二、实验内容"><a href="#二、实验内容" class="headerlink" title="二、实验内容"></a>二、实验内容</h2><ol><li>创建两个进程，使用无链接的数据报Socket实现交换一个字符串。一个叫做Sender.java ，用于向一个名为Receiver.java的进程发送一个字符串。</li><li>Sender.java需要一个命令行参数，用于表示传递消息的端口号.</li><li>Receiver.java 需要一个命令行参数用于表示接受消息的端口号。</li><li>接受方需要阻塞直到从发送方收到一个消息。如果发送放在接受运行之前发送出消息，消息会丢失。这种情况在此实验中是允许的。</li><li>消息缓冲可以是定长的。如果发送的消息比消息缓冲长，接受方就看不到完整的消息。这种情况在此实验中是允许的。例如，消息缓冲的长度是5，而发送方发送一个消息 “123456789”，则接受方只能看到“12345”。</li></ol><h2 id="三、实验设计"><a href="#三、实验设计" class="headerlink" title="三、实验设计"></a>三、实验设计</h2><h3 id="实验背景："><a href="#实验背景：" class="headerlink" title="实验背景："></a>实验背景：</h3><p><strong>UDP简介</strong></p><p><a href="https://blog.csdn.net/qq_23473123/article/details/51464272">JAVA Socket 实现 UDP 编程</a></p><p>UDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。在OSI模型中，在第四层——传输层，处于IP协议的上一层。UDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。UDP用来支持那些需要在计算机之间传输数据的网络应用。包括网络视频会议系统在内的众多的客户/服务器模式的网络应用都需要使用UDP协议。</p><p>UDP协议的主要作用是将网络数据流量压缩成数据包的形式。一个典型的数据包就是一个二进制数据的传输单位。每一个数据包的前8个字节用来包含报头信息，剩余字节则用来包含具体的传输数据。</p><p><strong>什么时候应该使用UDP：</strong><br>当对网络通讯质量要求不高的时候，要求网络通讯速度能尽量的快，这时就可以使用UDP。 </p><p>比如，日常生活中，常见使用UDP协议的应用如下：</p><p>QQ语音、QQ视频……</p><p><strong>Java中的UDP</strong></p><p>在Java中，实现UDP连接和数据传输的类主要是DatagramPacket和DatagramSocket。</p><p>DatagramSocket类表示用来发送和接收数据报包的套接字。</p><p>数据报套接字是包投递服务的发送或接收点。每个在数据报套接字上发送或接收的包都是单独编址和路由的。从一台机器发送到另一台机器的多个包可能选择不同的路由，也可能按不同的顺序到达。</p><p>在 DatagramSocket 上总是启用 UDP 广播发送。在接收端（Receiver），DatagramSocket只需输入空闲端口即可初始化对象。在消息发送端（Sender），DatagramSocket不需要任何参数初始化，直接新建对象。</p><p>DatagramSocket对象拥有send()和receive()方法，参数是DatagramPacket对象，即发送和接收数据包。</p><p>DatagramPacket类表示数据报包。</p><p>数据报包用来实现无连接包投递服务。每条报文仅根据该包中包含的信息从一台机器路由到另一台机器。从一台机器发送到另一台机器的多个包可能选择不同的路由，也可能按不同的顺序到达。不对包投递做出保证。</p><p>发送方（sender）要负责在数据报包上面贴好标签，注明收信人（地址和端口），这样才能够准确地被收信人（receiver）收到。</p><p>在DatagramPacket包中的函数 int getLength()返回实际接受的字节数，<br>byte[] getData()返回接受到的数据。</p><p>要想接受端给发送端回信息，就需要知道发送端的IP地址InetAddress getAddress()和发送端进程所绑定的端口号int getPort()。</p><p>数据报套接字发送成功之后，就相当于建立了一个虚连接，双方可以发送数据。</p><h3 id="实验环境："><a href="#实验环境：" class="headerlink" title="实验环境："></a>实验环境：</h3><p>Intellij IDEA, JDK 1.8, Windows 10</p><h3 id="实验思路："><a href="#实验思路：" class="headerlink" title="实验思路："></a>实验思路：</h3><ol><li>新建两个类，Sender和Receiver。由于逻辑简单，具体代码可直接在main中实现；</li><li>Sender和Receiver类需要接收命令行参数，可利用Intellij IDEA的类运行设置来输入参数，利用main函数的args参数来传递命令行参数，避免了操作命令行界面的不便；<br><img src="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/2019-06-15-16-36-58.png" alt><br><img src="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/2019-06-15-16-37-07.png" alt></li><li>Sender：定义地址、端口号、数据；创建数据报包，包含发送的数据信息；创建DatagramSocket对象；向服务器发送数据报包。</li><li>Receiver：创建服务器端DatagramSocket，指定端口；创建数据报，用于接收客户端发送的数据；接收客户端发送的数据；读取数据。</li></ol><h3 id="相关代码："><a href="#相关代码：" class="headerlink" title="相关代码："></a>相关代码：</h3><p><code>Sender.java</code></p><pre><code class="lang-java">public class Sender &#123;    public static void main(String[] args) throws IOException &#123;        /*         * 向Receiver发送数据         * java Sender 192.168.1.101 12 hello         */        // 1.定义服务器的地址、端口号、数据        if (args.length != 3) &#123;            System.out.println(&quot;参数个数不正确！&quot; + args.length);            return;        &#125;        System.out.println(args[0]);        System.out.println(args[1]);        System.out.println(args[2]);        InetAddress address = InetAddress.getByName(args[0]);        //InetAddress address = InetAddress.getByName(&quot;localhost&quot;);        int port = Integer.parseInt(args[1]);        byte[] data = args[2].getBytes();        // 2.创建数据报，包含发送的数据信息        DatagramPacket packet = new DatagramPacket(data, data.length, address, port);        // 3.创建DatagramSocket对象        DatagramSocket socket = new DatagramSocket();        // 4.向服务器端发送数据报        socket.send(packet);    &#125;&#125;</code></pre><p><code>Receiver.java</code></p><pre><code class="lang-java">public class Receiver &#123;    /*     * 接收Sender发送的数据     * java Receiver 12     */    public static void main(String[] args) throws IOException &#123;        // 设置缓冲区大小        int bufferLength = 5;        // 设置参数格式        if (args.length != 1) &#123;            System.out.println(&quot;参数个数不正确！&quot; + args.length);            return;        &#125;        // 1.创建服务器端DatagramSocket，指定端口        DatagramSocket socket = new DatagramSocket(Integer.parseInt(args[0]));        // 2.创建数据报，用于接收客户端发送的数据        byte[] data = new byte[bufferLength];        DatagramPacket packet = new DatagramPacket(data, data.length);        // 3.接收客户端发送的数据        System.out.println(&quot;****服务器端已经启动，等待客户端发送数据&quot;);        socket.receive(packet);// 此方法在接收到数据报之前会一直阻塞        // 4.读取数据        String info = new String(data, 0, packet.getLength());        System.out.println(&quot;我是服务器，客户端说：&quot; + info);    &#125;&#125;</code></pre><h2 id="四、实验结果与分析"><a href="#四、实验结果与分析" class="headerlink" title="四、实验结果与分析"></a>四、实验结果与分析</h2><p>编译过程略去。</p><ol><li>运行Reciever<br><img src="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/2019-06-15-16-53-22.png" alt></li><li>运行Sender<br><img src="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/2019-06-15-16-54-43.png" alt></li><li>此时的Receiver<br><img src="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/2019-06-15-16-55-10.png" alt></li></ol><h3 id="实验分析："><a href="#实验分析：" class="headerlink" title="实验分析："></a>实验分析：</h3><p>缓冲区大小可以修改。修改后的缓冲区大小为5，可以看到，Sender中的消息<code>Hello!</code>发送给Receiver后，尾部的感叹号被削去。</p><p>通过本次实验，我了解了UDP连接的原理与优缺点，掌握了在Java中建立UDP连接的方法。</p><hr><h1 id="实验三"><a href="#实验三" class="headerlink" title="实验三"></a>实验三</h1><h2 id="一、实验目的-1"><a href="#一、实验目的-1" class="headerlink" title="一、实验目的"></a>一、实验目的</h2><p>尝试通过面向流模式的socket实现通信。</p><h2 id="二、实验内容-1"><a href="#二、实验内容-1" class="headerlink" title="二、实验内容"></a>二、实验内容</h2><ol><li>创建一个名为Acceptor.java的程序。此程序可以接受一个连接并用流模式socket接受一个消息。创建一个名为 Requestor.java 的程序。此程序可以请求一个连接，并使用流模式socket。</li><li>Acceptor.java 有2个命令行参数，分别用于表示本进程使用的服务器socket的端口号，以及要发送的消息。</li><li>Requestor.java 有2个命令行参数，分别表示连接acceptor的主机名和连接acceptor的端口号。</li></ol><h2 id="三、实验设计-1"><a href="#三、实验设计-1" class="headerlink" title="三、实验设计"></a>三、实验设计</h2><h3 id="实验背景：-1"><a href="#实验背景：-1" class="headerlink" title="实验背景："></a>实验背景：</h3><p><a href="https://blog.csdn.net/qq_23473123/article/details/51461894">Java 通过 Socket 实现 TCP 编程</a></p><p><strong>TCP简介</strong></p><p>TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。</p><p><strong>Java Socket简介</strong></p><p>所谓socket 通常也称作”套接字“，用于描述IP地址和端口，是一个通信链的句柄。应用程序通常通过”套接字”向网络发出请求或者应答网络请求。</p><p>ServerSocket用于服务器端，Socket是建立网络连接时使用的。在连接成功时，应用程序两端都会产生一个Socket实例，操作这个实例，完成所需的会话。对于一个网络连接来说，套接字是平等的，并没有差别，不因为在服务器端或在客户端而产生不同级别。不管是Socket还是ServerSocket它们的工作都是通过SocketImpl类及其子类完成的。</p><p><strong>Java Socket常用方法</strong></p><p>. Accept方法用于产生”阻塞”，直到接受到一个连接，并且返回一个客户端的Socket对象实例。”阻塞”是一个术语，它使程序运行暂时”停留”在这个地方，直到一个会话产生，然后程序继续；通常”阻塞”是由循环产生的。</p><p>. getInputStream方法获得网络连接输入，同时返回一个InputStream对象实例。<br>. getOutputStream方法连接的另一端将得到输入，同时返回一个OutputStream对象实例。</p><p>注意：其中getInputStream和getOutputStream方法均会产生一个IOException，它必须被捕获，因为它们返回的流对象，通常都会被另一个流对象使用。</p><p><strong>Java 操作流对象</strong></p><p>建立Socket连接后，两台机器之间便以流模式进行通信。在Java API中，可以从其中读入一个字节序列的对象称做输入流，而可以向其中写入一个字节序列的对象称做输出流。这些字节序列的来源地和目的地可以是文件，而且通常都是文件，但是也可以是网络连接，甚至是内存块。抽象类InputStream和OutputStream构成了输入/输出（I/O)类层次结构的基础。</p><p>本次实验我选择利用Scanner作为读取流的手段，用PrintWriter作为写入流的手段。</p><h3 id="实验环境：-1"><a href="#实验环境：-1" class="headerlink" title="实验环境："></a>实验环境：</h3><p>Intellij IDEA, JDK 1.8, Windows 10</p><h3 id="实验思路：-1"><a href="#实验思路：-1" class="headerlink" title="实验思路："></a>实验思路：</h3><ol><li>新建两个类，ConnectionAcceptor和ConnectionRequestor。由于逻辑简单，具体代码可直接在main中实现；</li><li>ConnectionAcceptor和ConnectionRequestor类需要接收命令行参数，可利用Intellij IDEA的类运行设置来输入参数，利用main函数的args参数来传递命令行参数，避免了操作命令行界面的不便；</li><li>ConnectionRequestor：建立连接；接收数据；发送数据；关闭连接。</li><li>ConnectionAcceptor：接收请求；发送数据；接收数据；断开连接。</li></ol><h3 id="相关代码：-1"><a href="#相关代码：-1" class="headerlink" title="相关代码："></a>相关代码：</h3><p><code>ConnectionRequestor.java</code></p><pre><code class="lang-java">public class ConnectionRequestor &#123;    /*     * java ConnectionRequestor 192.168.1.101 12     */    public static void main(String[] args) throws IOException &#123;        // 检查参数个数        if (args.length != 2) &#123;            System.out.println(&quot;Wrong parameters!&quot;);            return;        &#125;        // 1. 建立连接        String host = args[0];        int port = Integer.parseInt(args[1]);        Socket socket = new Socket(host, port);        // 2. 接收数据        InputStream inputStream = socket.getInputStream();        Scanner scanner = new Scanner(inputStream);        String line = scanner.nextLine();        System.out.println(&quot;收到来自发送方的消息：&quot; + line);        // 3. 发送数据        OutputStream outputStream = socket.getOutputStream();        PrintWriter printWriter = new PrintWriter(outputStream, true);        printWriter.println(&quot;我已收到消息，内容是&quot; + line);        // 4. 关闭连接        socket.close();    &#125;&#125;</code></pre><p><code>ConnectionAcceptor.java</code></p><pre><code class="lang-java">public class ConnectionAcceptor &#123;    /*     * java ConnectionAcceptor 12 Hello     */    public static void main(String[] args) throws IOException &#123;        // 检查参数个数        if (args.length != 2) &#123;            System.out.println(&quot;Wrong parameters!&quot;);            return;        &#125;        // 1. 接受请求        int port = Integer.parseInt(args[0]);        ServerSocket serverSocket = new ServerSocket(port);        Socket socket = serverSocket.accept();        // 2. 发送数据        OutputStream outputStream = socket.getOutputStream();        PrintWriter printWriter = new PrintWriter(outputStream, true);        printWriter.println(args[1]);        // 3. 接收数据        InputStream inputStream = socket.getInputStream();        Scanner scanner = new Scanner(inputStream);        String line = scanner.nextLine();        System.out.println(&quot;服务器应答：&quot; + line);        // 4. 断开连接        socket.close();    &#125;&#125;</code></pre><h2 id="四、实验结果与分析-1"><a href="#四、实验结果与分析-1" class="headerlink" title="四、实验结果与分析"></a>四、实验结果与分析</h2><p>编译过程略去。</p><ol><li>运行ConnectionAcceptor<br><img src="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/2019-06-15-17-27-34.png" alt></li><li>运行ConnectionRequestor<br><img src="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/2019-06-15-17-28-48.png" alt></li><li>此时的ConnectionAcceptor<br><img src="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/2019-06-15-17-29-13.png" alt></li></ol><h3 id="实验分析：-1"><a href="#实验分析：-1" class="headerlink" title="实验分析："></a>实验分析：</h3><p>通过本次实验，我了解了TCP连接的原理与优缺点，掌握了TCP和UDP的主要不同之处，掌握了在Java中建立TCP Socket连接的方法，掌握了流处理的基本方法。</p><hr><h1 id="实验四"><a href="#实验四" class="headerlink" title="实验四"></a>实验四</h1><h2 id="一、实验目的-2"><a href="#一、实验目的-2" class="headerlink" title="一、实验目的"></a>一、实验目的</h2><p>创建进程之间的多播。</p><h2 id="二、实验内容-2"><a href="#二、实验内容-2" class="headerlink" title="二、实验内容"></a>二、实验内容</h2><ol><li>MulticastSender.java用于发送多播消息给多播接收程序。多播IP地址是239.1.2.3 端口号为1234。</li><li>MulticastReceiver.java 用于接收多播消息并显示消息。</li><li>实验最终效果要求：至少开启两个以上的MulticastReceiver进程，MulticastSender发送的消息，均可被MulticastReceiver收到。</li></ol><h2 id="三、实验设计-2"><a href="#三、实验设计-2" class="headerlink" title="三、实验设计"></a>三、实验设计</h2><h3 id="实验背景：-2"><a href="#实验背景：-2" class="headerlink" title="实验背景："></a>实验背景：</h3><p><a href="https://blog.csdn.net/zhouzixin053/article/details/22823521">java————多播编程——-MulticastSocket</a></p><p><strong>单播：</strong></p><p>一个单个的发送者和一个接受者之间通过网络进行的通信。</p><p>1、服务器及时响应客户机的请求</p><p>2、服务器针对每个客户不同的请求发送不同的数据，容易实现个性化服务。</p><p><strong>多播：</strong></p><p>一个发送者和多个接受者之间的通信。</p><p>广播特点：主机之间“一对所有”的通讯模式，网络对其中每一台主机发出的信号都进行无条件复制并转发，所有主机都可以接收到所有信息（不管你是否需要）。</p><p>1、网络设备简单，维护简单，布网成本低廉。</p><p>2、由于服务器不用向每个客户机单独发送数据，所以服务器流量负载极低。</p><p>多播的地址是特定的，D类地址用于多播。D类IP地址就是多播IP地址，即224.0.0.0至239.255.255.255之间的IP地址。</p><p><strong>多播程序设计的框架</strong></p><p>要进行多播的编程，需要遵从一定的编程框架。多播程序框架主要包含套接字初始化、设置多播超时时间、加入多播组、发送数据、接收数据以及从多播组中离开几个方面。其步骤如下：</p><p>（1）建立一个socket。</p><p>（2）然后设置多播的参数，例如超时时间TTL、本地回环许可LOOP等。</p><p>（3）加入多播组。</p><p>（4）发送和接收数据。</p><p>（5）从多播组离开。</p><p><strong>Java MulticastSocket</strong></p><p>多播通过多播数据报套接MulticastSocket类来实现</p><p>重要的构造方法：</p><pre><code class="lang-java">MulticastSocket()//创建多播套接字MulticastSocket(int port)//创建多播套接字并将其绑定到特定端口MulticastSocket(SocketAddress bindaddr)//创建绑定到指定套接字地址的MulticastSocket</code></pre><p>常用的方法：</p><pre><code class="lang-java">void joinGroup(InetAddress meastaddr)//加入多播组void leaveGroup(InetAddress meastaddr)//离开多播组void send(DatagramPacket p)//从此套接字发送数据包public void receive(DatagramPacket p)//从此套接字接收数据包</code></pre><h3 id="实验环境：-2"><a href="#实验环境：-2" class="headerlink" title="实验环境："></a>实验环境：</h3><p>Intellij IDEA, JDK 1.8, Windows 10</p><h3 id="实验思路：-2"><a href="#实验思路：-2" class="headerlink" title="实验思路："></a>实验思路：</h3><ol><li>新建两个类，MulticastSender和MulticastReciever。由于逻辑简单，具体代码可直接在main中实现；</li><li>为了体现多播的广播特性，MulticastReciever的进程要多几个，随着MulticastSender的启动，全部收到信息才行。</li><li>MulticastSender：新建多播连接；构造数据包；广播数据；关闭连接。</li><li>MulticastReciever：新建多播连接；构造数据包；接收数据；关闭连接。</li></ol><h3 id="相关代码：-2"><a href="#相关代码：-2" class="headerlink" title="相关代码："></a>相关代码：</h3><p><code>MulticastSender.java</code></p><pre><code class="lang-java">public class MulticastSender &#123;    public static void main(String[] args) throws IOException &#123;        // 新建多播连接        int port = 1234;        String address = &quot;239.1.2.3&quot;;        MulticastSocket multicastSocket = new MulticastSocket(port);        InetAddress groupAddress = InetAddress.getByName(address);        multicastSocket.joinGroup(groupAddress);        // 构造数据包        byte[] message = &quot;Hello!&quot;.getBytes();        DatagramPacket datagramPacket = new DatagramPacket(message, message.length, InetAddress.getByName(address), port);        // 广播数据        multicastSocket.send(datagramPacket);        // 关闭连接        multicastSocket.close();    &#125;&#125;</code></pre><p><code>MulticastReciever.java</code></p><pre><code class="lang-java">public class MulticastReciever &#123;    public static void main(String[] args) throws IOException &#123;        // 新建多播连接        int port = 1234;        String address = &quot;239.1.2.3&quot;;        MulticastSocket multicastSocket = new MulticastSocket(port);        InetAddress groupAddress = InetAddress.getByName(address);        multicastSocket.joinGroup(groupAddress);        // 构造数据包        byte[] message = new byte[1024];        DatagramPacket datagramPacket = new DatagramPacket(message, message.length, InetAddress.getByName(address), port);        // 接收数据        multicastSocket.receive(datagramPacket);        System.out.println(&quot;接收到了广播消息：&quot; + new String(message));        // 关闭连接        multicastSocket.close();    &#125;&#125;</code></pre><h2 id="四、实验结果与分析-2"><a href="#四、实验结果与分析-2" class="headerlink" title="四、实验结果与分析"></a>四、实验结果与分析</h2><p>编译过程略去。</p><ol><li>运行MulticastReciever（6个）<br><img src="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/2019-06-15-17-45-12.png" alt></li><li>运行MulticastSender<br><img src="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/2019-06-15-17-46-17.png" alt></li><li>此时的MulticastReciever<br><img src="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/2019-06-15-17-46-34.png" alt><br><img src="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/2019-06-15-17-46-48.png" alt><br><img src="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/2019-06-15-17-47-02.png" alt><br>剩下三个略去不表，结果相同。</li></ol><h3 id="实验分析：-2"><a href="#实验分析：-2" class="headerlink" title="实验分析："></a>实验分析：</h3><p>通过本次实验，我了解了多播的原理和相对于单播的优点，掌握了在Java中建立多播MulticastSocket连接的方法。</p>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>distribute system</tag>
      
      <tag>OS</tag>
      
      <tag>java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高级操作系统——分布式系统——课程作业与解答</title>
    <link href="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A%E4%B8%8E%E8%A7%A3%E7%AD%94/"/>
    <url>/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A%E4%B8%8E%E8%A7%A3%E7%AD%94/</url>
    
    <content type="html"><![CDATA[<h2 id="第一章作业"><a href="#第一章作业" class="headerlink" title="第一章作业"></a>第一章作业</h2><h3 id="1-什么是分布式系统？请举例说明分布式系统的特点。"><a href="#1-什么是分布式系统？请举例说明分布式系统的特点。" class="headerlink" title="1. 什么是分布式系统？请举例说明分布式系统的特点。"></a>1. 什么是分布式系统？请举例说明分布式系统的特点。</h3><p>定义1：分布式系统是多个独立计算机的集合，该系统用户认为它是一个单独的一致的系统<br>定义2：由在通过消息进行通信和协作操作的网络计算机上的软件硬件部件组成的任何系统<br>分布式系统的特点：<br>（1）隐藏性，隐藏了计算机之间的不同，隐藏了计算机之间的通信过程。<br>（2）统一性和一致性，无论何时何地，用户都采用统一和一致的方法访问分布式系统。<br>（3）可扩展性，隐藏一个独立的计算机如何参与系统的运作；在系统的一部分不能工作的情况下整个系统仍然能持续提供服务；当系统的某些部件被替换或被修改或是系统提供了新的服务时，不应让用户注意到这些改变。<br>（4）并发性，在共享资源的情况下，协调不同的程序并发执行。<br>（5）没有全局时钟，很难在网络中精确地同步所有的时钟，更重要的是发现动作发生的先后顺序。<br>举例部分略。</p><h3 id="2-有一个简单的C-S系统，此系统只用一台服务器来响应所有客户的请求。"><a href="#2-有一个简单的C-S系统，此系统只用一台服务器来响应所有客户的请求。" class="headerlink" title="2. 有一个简单的C/S系统，此系统只用一台服务器来响应所有客户的请求。"></a>2. 有一个简单的C/S系统，此系统只用一台服务器来响应所有客户的请求。</h3><p>（1）请解释为什么在这种情况下不能对服务器的响应时间进行限制（例如，这个限制可能是要求服务器一定要在10ms内对客户的请求进行响应）；<br>因为服务器过于简单，处理能力差，对于多线程高并发情况，当服务器发生堵塞时造成的服务等待问题会造成服务器响应不及时的情况。并且一台服务器性能有限，不能保证服务端高效性和高的容错性。因此在这种情况下不能对服务器的响应时间进行限制。</p><p>（2）如果我们想限制服务器的响应时间，应该如何修改设计？<br>可以通过增加服务器的数量或提高服务器的处理能力。</p><p>（3）在现实世界中使用的系统中，这种限制是否是必须的？为什么？<br>是必须的。因为如果不加限制，那么就会导致用户使用系统的体验降低，从而放弃这个系统。</p><h3 id="3-当年12306订票系统在2011年第一次上线的时候出现了很多的问题。如果你是12306的设计师，请问应该注意哪几个方面的内容？"><a href="#3-当年12306订票系统在2011年第一次上线的时候出现了很多的问题。如果你是12306的设计师，请问应该注意哪几个方面的内容？" class="headerlink" title="3. 当年12306订票系统在2011年第一次上线的时候出现了很多的问题。如果你是12306的设计师，请问应该注意哪几个方面的内容？"></a>3. 当年12306订票系统在2011年第一次上线的时候出现了很多的问题。如果你是12306的设计师，请问应该注意哪几个方面的内容？</h3><p>（答案略）</p><h2 id="第二章作业"><a href="#第二章作业" class="headerlink" title="第二章作业"></a>第二章作业</h2><h3 id="1-网络协议分层有什么好处？"><a href="#1-网络协议分层有什么好处？" class="headerlink" title="1.    网络协议分层有什么好处？"></a>1.    网络协议分层有什么好处？</h3><p>每层协议可以单独设计，只依赖于底层协议工作，增加了灵活性。</p><h3 id="2-一个客户向服务器发出RPC。客户花5ms时间计算每个请求的参数，服务器花10ms处理每个请求。本地操作系统每次发送和接收的时间是5ms，网络传递或者应答消息的时间是3ms。编码、解码每个消息需要0-5ms。计算一个-RPC实现所需要的时间。"><a href="#2-一个客户向服务器发出RPC。客户花5ms时间计算每个请求的参数，服务器花10ms处理每个请求。本地操作系统每次发送和接收的时间是5ms，网络传递或者应答消息的时间是3ms。编码、解码每个消息需要0-5ms。计算一个-RPC实现所需要的时间。" class="headerlink" title="2.    一个客户向服务器发出RPC。客户花5ms时间计算每个请求的参数，服务器花10ms处理每个请求。本地操作系统每次发送和接收的时间是5ms，网络传递或者应答消息的时间是3ms。编码、解码每个消息需要0.5ms。计算一个 RPC实现所需要的时间。"></a>2.    一个客户向服务器发出RPC。客户花5ms时间计算每个请求的参数，服务器花10ms处理每个请求。本地操作系统每次发送和接收的时间是5ms，网络传递或者应答消息的时间是3ms。编码、解码每个消息需要0.5ms。计算一个 RPC实现所需要的时间。</h3><p>答：<br>5ms 准备参数<br>0.5ms 打包参数<br>5ms 发送<br>3ms 网络传送<br>5ms 接收<br>0.5ms 解包参数<br>10ms 处理请求<br>0.5ms 结果打包<br>5ms 发送<br>3ms 网络传送<br>5ms 接收<br>0.5ms 解包结果<br>共43ms</p><h3 id="3-举例说明RPC在异构系统中实现时进行参数传递会遇到什么问题？"><a href="#3-举例说明RPC在异构系统中实现时进行参数传递会遇到什么问题？" class="headerlink" title="3.    举例说明RPC在异构系统中实现时进行参数传递会遇到什么问题？"></a>3.    举例说明RPC在异构系统中实现时进行参数传递会遇到什么问题？</h3><p>（例子见PPT，略）</p><h3 id="4-暂时通信和永久通信有什么区别？"><a href="#4-暂时通信和永久通信有什么区别？" class="headerlink" title="4.    暂时通信和永久通信有什么区别？"></a>4.    暂时通信和永久通信有什么区别？</h3><p>(答案见PPT)</p><h3 id="5-什么是复杂流？如何实现复杂流同步？"><a href="#5-什么是复杂流？如何实现复杂流同步？" class="headerlink" title="5.    什么是复杂流？如何实现复杂流同步？"></a>5.    什么是复杂流？如何实现复杂流同步？</h3><p>(答案见PPT)</p><h2 id="第三章作业"><a href="#第三章作业" class="headerlink" title="第三章作业"></a>第三章作业</h2><h3 id="1-设计一个并发服务器，它为每个到来的请求创建一个服务器进程。请分析这种设计与多线程服务器之间的利弊。"><a href="#1-设计一个并发服务器，它为每个到来的请求创建一个服务器进程。请分析这种设计与多线程服务器之间的利弊。" class="headerlink" title="1. 设计一个并发服务器，它为每个到来的请求创建一个服务器进程。请分析这种设计与多线程服务器之间的利弊。"></a>1. 设计一个并发服务器，它为每个到来的请求创建一个服务器进程。请分析这种设计与多线程服务器之间的利弊。</h3><p>多线程服务器实现了并发性并可以充分利用服务器端的资源。虽然采用多进程的方式可以获得这些好处，但是进程的创建和消亡需要更大的开销，进程之间的通信也比一个进程内部多个线程之间的开销大。</p><h3 id="2-假设一个单线程服务器需要花6ms的时间接受一个请求，并使用20ms的时间计算出请求结果并将结果返回。请问，对于一个拥有10个CPU的服务器，若同时到来4个请求，则"><a href="#2-假设一个单线程服务器需要花6ms的时间接受一个请求，并使用20ms的时间计算出请求结果并将结果返回。请问，对于一个拥有10个CPU的服务器，若同时到来4个请求，则" class="headerlink" title="2. 假设一个单线程服务器需要花6ms的时间接受一个请求，并使用20ms的时间计算出请求结果并将结果返回。请问，对于一个拥有10个CPU的服务器，若同时到来4个请求，则"></a>2. 假设一个单线程服务器需要花6ms的时间接受一个请求，并使用20ms的时间计算出请求结果并将结果返回。请问，对于一个拥有10个CPU的服务器，若同时到来4个请求，则</h3><p>（1）如果使用单线程服务器，需要多少时间完成这4个请求？（2）如果设计一个dispatcher/worker形式的多线程服务器，需要dispatcher线程在接受请求后多耗费1ms进行任务的分配工作。使用此多线程服务器，一个CPU专门负责执行dispatcher线程，最短需要多少时间能够将这些请求处理完？（线程创建的开销和线程之间传递数据的时间忽略不计）<br>答:<br>(1)    因为只有一个线程，所以四个请求串行完成，一共需要（6ms+20ms）<em>4=26ms</em>4=104ms<br>(2)    因为有10个CPU，所以可以dispatcher占用一个CPU，余下的四个worker线程各占用一个CPU。所以一共需要：（6ms+1ms）*4+20ms=28ms+20ms=48ms</p><h3 id="3-迭代服务器和并发服务器有何区别？"><a href="#3-迭代服务器和并发服务器有何区别？" class="headerlink" title="3. 迭代服务器和并发服务器有何区别？"></a>3. 迭代服务器和并发服务器有何区别？</h3><p>迭代服务器负责处理请求并将结果返回给客户端。并发服务器本身并不处理客户端的请求，它将请求发送给一个线程或者进程，并等待下一个请求。</p><h3 id="4-如果要设计一个购物网站，在进行服务器设计的时候应该采用有状态服务器还是无状态服务器？说明你做出此种的选择的原因。"><a href="#4-如果要设计一个购物网站，在进行服务器设计的时候应该采用有状态服务器还是无状态服务器？说明你做出此种的选择的原因。" class="headerlink" title="4. 如果要设计一个购物网站，在进行服务器设计的时候应该采用有状态服务器还是无状态服务器？说明你做出此种的选择的原因。"></a>4. 如果要设计一个购物网站，在进行服务器设计的时候应该采用有状态服务器还是无状态服务器？说明你做出此种的选择的原因。</h3><p>采用有状态服务器。因为有状态服务器维护客户信息，如果服务器失败，其上的所有状态必须恢复。在购物时，服务器需要记录客户很多的相关信息。</p><h3 id="5-什么是软件代理？若要设计一个个人秘书系统，希望根据用户的需求搜集用户感兴趣的信息，可以选择怎样的软件代理实现什么样的功能（至少设计出3个功能）"><a href="#5-什么是软件代理？若要设计一个个人秘书系统，希望根据用户的需求搜集用户感兴趣的信息，可以选择怎样的软件代理实现什么样的功能（至少设计出3个功能）" class="headerlink" title="5. 什么是软件代理？若要设计一个个人秘书系统，希望根据用户的需求搜集用户感兴趣的信息，可以选择怎样的软件代理实现什么样的功能（至少设计出3个功能）"></a>5. 什么是软件代理？若要设计一个个人秘书系统，希望根据用户的需求搜集用户感兴趣的信息，可以选择怎样的软件代理实现什么样的功能（至少设计出3个功能）</h3><p>软件代理是一个自治的进程，能够在其所在环境中创建并初始化变化，并可以与用户或者其他代理进行协作。<br>可以选择的代理，比如接口代理，可以提供用户使用方便舒适的接口；或者选择数据代理，可以为用户提供数据的分类和过滤等等。</p><h3 id="6-8分-假设处理器P有一个新任务T，可以有下面的三个算法来实现处理器任务分配"><a href="#6-8分-假设处理器P有一个新任务T，可以有下面的三个算法来实现处理器任务分配" class="headerlink" title="6. (8分)假设处理器P有一个新任务T，可以有下面的三个算法来实现处理器任务分配"></a>6. (8分)假设处理器P有一个新任务T，可以有下面的三个算法来实现处理器任务分配</h3><p>算法A：<br>（i）搜集所有处理器（包括P自己）的负载信息；<br>（ii）选择负载最低的处理器，并将任务分配给负载最低的处理器；<br>（iii）算法结束。<br>算法B:<br>（i）随机选择一个处理器Q，询问其负载情况;<br>（ii）若Q欠载，则将T分配给Q，算法结束；<br>（iii）否则继续执行（i）<br>算法C:<br>（i）随机选择一个处理器Q，询问其负载情况;<br>（ii）若Q欠载，则将T分配给Q，算法结束；<br>（iii）否则，如果已经执行（i）K次，则将T留在本地执行，否则继续执行（i）<br>请问：<br>（1）（2分）上面三个算法哪个是确定性算法？哪个是启发式算法？<br>算法A是确定性算法，B和C是启发式算法。<br>（2）（4分）算法B和算法C很相似，请问哪个算法可行性更强？为什么？<br>算法B有可能不能在有限步骤内结束，因此是个错误的算法。而算法C则可以在有限步骤内结束，可行性强。<br>（3）（2分）算法C是否还可以优化？<br>可以有各种优化方式。比如，只有在本地机器过载的情况下才开始询问其他机器；或者用多播机制发送K个消息询问K台机器的负载情况，然后选择负载最小的机器执行进程。</p><h2 id="第四章作业"><a href="#第四章作业" class="headerlink" title="第四章作业"></a>第四章作业</h2><h3 id="1-在分布式系统中，扁平式命名方式（flat-naming）和名字空间的命名方式（name-space）的主要区别是什么？"><a href="#1-在分布式系统中，扁平式命名方式（flat-naming）和名字空间的命名方式（name-space）的主要区别是什么？" class="headerlink" title="1. 在分布式系统中，扁平式命名方式（flat naming）和名字空间的命名方式（name space）的主要区别是什么？"></a>1. 在分布式系统中，扁平式命名方式（flat naming）和名字空间的命名方式（name space）的主要区别是什么？</h3><p>扁平式命名方式使用ID唯一表示实体，没有结构，不包含任何关于如何定位实体访问点的任何信息。<br>名字空间是一个有标签的有向图，它是结构化的名字。</p><h3 id="2-在扁平式命名方式中使用的层次化方法来定位一个实体的时候，为什么其插入和查找可以体现局部性？"><a href="#2-在扁平式命名方式中使用的层次化方法来定位一个实体的时候，为什么其插入和查找可以体现局部性？" class="headerlink" title="2. 在扁平式命名方式中使用的层次化方法来定位一个实体的时候，为什么其插入和查找可以体现局部性？"></a>2. 在扁平式命名方式中使用的层次化方法来定位一个实体的时候，为什么其插入和查找可以体现局部性？</h3><p>因为在层次化方法中，网络被分为了若干个域，顶层与扩展整个网络，每个域切分为多个更小的域，每个域D中的每个实体E都在dir(D)中有位置记录。因此在修改或者查找的时候，所有的操作都可以在饱含了操作发起点和目标点的最小域里解决，因此体现了局部性。</p><h3 id="3-在图中给出的名字空间中，请为标记为A的实体给出一个绝对路径名和任意一个相对路径名。"><a href="#3-在图中给出的名字空间中，请为标记为A的实体给出一个绝对路径名和任意一个相对路径名。" class="headerlink" title="3. 在图中给出的名字空间中，请为标记为A的实体给出一个绝对路径名和任意一个相对路径名。"></a>3. 在图中给出的名字空间中，请为标记为A的实体给出一个绝对路径名和任意一个相对路径名。</h3><p><img src="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A%E4%B8%8E%E8%A7%A3%E7%AD%94/2019-06-21-13-31-47.png" alt><br>绝对路径名：k: /vu/home/steen/mbox<br>相对路径名：n0: /home/steen/mbox</p><h3 id="4-为什么缓存可以提升名字服务的可用性？"><a href="#4-为什么缓存可以提升名字服务的可用性？" class="headerlink" title="4. 为什么缓存可以提升名字服务的可用性？"></a>4. 为什么缓存可以提升名字服务的可用性？</h3><p>名字解析过程中，很多步骤的结果都是不变化的，因此使用缓存可以大大提升名字服务的可用性。</p><h3 id="5-为什么要删除不被引用的实体？请解释纯粹采用引用图的方式来解决这个问题时会遇到什么问题？"><a href="#5-为什么要删除不被引用的实体？请解释纯粹采用引用图的方式来解决这个问题时会遇到什么问题？" class="headerlink" title="5. 为什么要删除不被引用的实体？请解释纯粹采用引用图的方式来解决这个问题时会遇到什么问题？"></a>5. 为什么要删除不被引用的实体？请解释纯粹采用引用图的方式来解决这个问题时会遇到什么问题？</h3><p>因为删除无引用实体可以节省资源。<br>当纯粹采用引用图方式解决问题时，很难维护一个全局的引用图，并且需要确定根集，由根集负责发起算法，这些集中式的特点不适用于分布式系统的扩展性。</p><h3 id="6-什么是幂等操作？为什么在删除不被引用的实体的时候采用引用列表方式来可以利用到幂等操作的特性，而采用简单引用计数的方法就不可以？"><a href="#6-什么是幂等操作？为什么在删除不被引用的实体的时候采用引用列表方式来可以利用到幂等操作的特性，而采用简单引用计数的方法就不可以？" class="headerlink" title="6. 什么是幂等操作？为什么在删除不被引用的实体的时候采用引用列表方式来可以利用到幂等操作的特性，而采用简单引用计数的方法就不可以？"></a>6. 什么是幂等操作？为什么在删除不被引用的实体的时候采用引用列表方式来可以利用到幂等操作的特性，而采用简单引用计数的方法就不可以？</h3><p>一个操作多次执行的结果和一次执行的结果是一样的，不会因为多次执行而产生副作用。<br>使用简单计数法时计数增加和减少的操作都不是幂等操作；而采用引用列表时插入和删除操作都是幂等操作。因此在分布式系统中因为通信的不可靠性的存在，幂等操作的存在可以非常适应。</p><h3 id="7-在使用标记不可达对象的跟踪方案删除不被引用的实体时，可以使用两个方案：一个是从根集出发，标记所有可达实体，从而删除所有不可达实体；一个是采用组内跟踪方案，先在一个进程组中标记所有外部可达实体，删除垃圾，然后再扩大组的范围。请解释这两种方案各有何利弊。"><a href="#7-在使用标记不可达对象的跟踪方案删除不被引用的实体时，可以使用两个方案：一个是从根集出发，标记所有可达实体，从而删除所有不可达实体；一个是采用组内跟踪方案，先在一个进程组中标记所有外部可达实体，删除垃圾，然后再扩大组的范围。请解释这两种方案各有何利弊。" class="headerlink" title="7. 在使用标记不可达对象的跟踪方案删除不被引用的实体时，可以使用两个方案：一个是从根集出发，标记所有可达实体，从而删除所有不可达实体；一个是采用组内跟踪方案，先在一个进程组中标记所有外部可达实体，删除垃圾，然后再扩大组的范围。请解释这两种方案各有何利弊。"></a>7. 在使用标记不可达对象的跟踪方案删除不被引用的实体时，可以使用两个方案：一个是从根集出发，标记所有可达实体，从而删除所有不可达实体；一个是采用组内跟踪方案，先在一个进程组中标记所有外部可达实体，删除垃圾，然后再扩大组的范围。请解释这两种方案各有何利弊。</h3><p>第一种方法伸缩性很差，因为需要跟踪分布式系统中的所有对象，是个集中式算法。<br>第二种方法可以适应于分布式系统，可以解决伸缩性问题</p><h2 id="第五章作业"><a href="#第五章作业" class="headerlink" title="第五章作业"></a>第五章作业</h2><h3 id="1-Cristian算法和Berkeley算法哪一个算法可以保证系统中所有的时钟可以与UTC一致？"><a href="#1-Cristian算法和Berkeley算法哪一个算法可以保证系统中所有的时钟可以与UTC一致？" class="headerlink" title="1. Cristian算法和Berkeley算法哪一个算法可以保证系统中所有的时钟可以与UTC一致？"></a>1. Cristian算法和Berkeley算法哪一个算法可以保证系统中所有的时钟可以与UTC一致？</h3><p>答：只有Cristian算法维持了一个拥有WWV接收器的Time server，让所有其他的机器与Time server一致。而Berkeley算法只是维持所有机器时间一致。</p><h3 id="2-在下图中，有三个进程，每个进程上的圆点表示这个进程上发生的事件，事件之间的有箭头的线表示一个消息的发送和接收，请在括号中写出下面四对事件之间的关系【用a-gt-b表示事件a与事件b有happens-before关系，否则用a-x-b表示事件a和b之间没有happens-before关系】"><a href="#2-在下图中，有三个进程，每个进程上的圆点表示这个进程上发生的事件，事件之间的有箭头的线表示一个消息的发送和接收，请在括号中写出下面四对事件之间的关系【用a-gt-b表示事件a与事件b有happens-before关系，否则用a-x-b表示事件a和b之间没有happens-before关系】" class="headerlink" title="2. 在下图中，有三个进程，每个进程上的圆点表示这个进程上发生的事件，事件之间的有箭头的线表示一个消息的发送和接收，请在括号中写出下面四对事件之间的关系【用a -&gt; b表示事件a与事件b有happens before关系，否则用a x b表示事件a和b之间没有happens before关系】"></a>2. 在下图中，有三个进程，每个进程上的圆点表示这个进程上发生的事件，事件之间的有箭头的线表示一个消息的发送和接收，请在括号中写出下面四对事件之间的关系【用a -&gt; b表示事件a与事件b有happens before关系，否则用a x b表示事件a和b之间没有happens before关系】</h3><p><img src="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A%E4%B8%8E%E8%A7%A3%E7%AD%94/2019-06-21-13-33-55.png" alt><br>（1）P1和q1    【    p1 x q1           】<br>（2）p4和q4    【    q4 -&gt; p4           】<br>（3）p1和r4     【    p1 -&gt; r4           】<br>（4）P4和r1    【    p4 x r1           】</p><h3 id="3-（分）下面的两个全局快照的切口是否是一致的切口？"><a href="#3-（分）下面的两个全局快照的切口是否是一致的切口？" class="headerlink" title="3. （分）下面的两个全局快照的切口是否是一致的切口？"></a>3. （分）下面的两个全局快照的切口是否是一致的切口？</h3><p> <img src="/2019/06/15/%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A%E4%B8%8E%E8%A7%A3%E7%AD%94/2019-06-21-13-34-07.png" alt><br>               （a）                                            （b）<br>（a）是一致的切口<br>（m1）p3已发送给p1的消息，p1还未接收到；（m2）p2已发送给p3的消息，p3还未收到；（m3）代表的事件还未发生。<br>（b）是不一致的切口<br>（m1）p3已发送给p1的消息，p1还未接收到，此事件没有问题；（m2）p2已发送给p3的消息，p3还未接收到，此事件也没有问题；（m3）p2已接收到p1给其发送的消息，而p1还未发送该消息，不符合事情发展的顺序，因此图（b）不是一致的切口。</p><h3 id="4-（分）假设有一个事务用于在一个银行的两个账户A和B之间进行转账100元钱。写法如下："><a href="#4-（分）假设有一个事务用于在一个银行的两个账户A和B之间进行转账100元钱。写法如下：" class="headerlink" title="4. （分）假设有一个事务用于在一个银行的两个账户A和B之间进行转账100元钱。写法如下："></a>4. （分）假设有一个事务用于在一个银行的两个账户A和B之间进行转账100元钱。写法如下：</h3><p>Begin-Transaction<br>A=A-100;<br>B=B+100;<br>End-Transaction<br>请使用这个例子来说明一个事务的ACID特性。</p><p>事物的ACID特性为：原子性（A）、一致性（C）、独立性（I）、永久性（D）<br>（1）原子性：Begin-Transaction和End-Transaction之间的操作要么全部正常执行完毕，要么不执行，若中间该事物出现错误需要全部回滚到Begin-Transaction之前的状态<br>（2）一致性：一个事物不能破坏系统的一致性，事务发生之前、之后系统应该维持一个稳定的平衡状态，在此例中，银行的总资产可看作是这个稳定的平衡状态（系统的一致性），即A+B的和维持不变<br>（3）独立性：例如与此例一同并发的还有另一个事务：<br>Begin-Transaction<br>A=A+B<em>0.1;<br>C=C-B</em>0.1;<br>End-Transaction<br>两个事务并发，第一个事务的第二条语句执行之前和执行之后B的值是不一样的，这样就导致了最终第二个事务执行之前和之后A+C的值不一样。这就导致一个事务的执行影响了另一个事务的执行结果，独立性没有了。<br>（4）永久性：事务中的操作成功执行完毕后事务便会提交，提交所带来的变化是永久性的。</p><h3 id="5-为实现事务回滚，请分析Private-Workspace方法和Writeahead-Log方法分别应该在怎样的环境下使用。"><a href="#5-为实现事务回滚，请分析Private-Workspace方法和Writeahead-Log方法分别应该在怎样的环境下使用。" class="headerlink" title="5. 为实现事务回滚，请分析Private Workspace方法和Writeahead Log方法分别应该在怎样的环境下使用。"></a>5. 为实现事务回滚，请分析Private Workspace方法和Writeahead Log方法分别应该在怎样的环境下使用。</h3><p>Private workspace应该在事务冲突很多发生的时候使用，而writeahead log应该在事务冲突发生很少的时候使用。</p><h3 id="6-什么是串行性？"><a href="#6-什么是串行性？" class="headerlink" title="6. 什么是串行性？"></a>6. 什么是串行性？</h3><p>如果多个事务的并发执行结果与所有事务按照某个顺序串行执行是一样的，那么就是串行一致的。并发事务的调度要符合串行性就是正确的调度。</p><h3 id="7-请举例说明为何2PL方法会导致死锁？"><a href="#7-请举例说明为何2PL方法会导致死锁？" class="headerlink" title="7. 请举例说明为何2PL方法会导致死锁？"></a>7. 请举例说明为何2PL方法会导致死锁？</h3><p>如果有两个事务用相反的顺序申请两个共享数据A和B，采用2PL的方法，如果第一个事务先访问了A，于是给A加锁；然后第二个事务访问了B，于是给B加锁。再后来两个事务就都不能执行下去，因为都申请不到下一个数据的锁，导致死锁。</p><h3 id="8-为什么会出现假死锁？请举例说明。"><a href="#8-为什么会出现假死锁？请举例说明。" class="headerlink" title="8.为什么会出现假死锁？请举例说明。"></a>8.为什么会出现假死锁？请举例说明。</h3><p>在用一台机器（协调者）维护整个系统的资源图，来描述进程和资源关系时，协调者检测出循环则认为是死锁。机器之间的消息传递有延时，可能会造成先发送的消息后收到，如课件中的例子：首先B想要归还资源R，随后发生B申请资源T，但后一条消息首先到达Coordinator，Coordinator增加了一条B-&gt; T的边，此时本应该没有R-&gt;B，由于还未收到该消息，Coordinator判断资源图有循环，呈现假死锁状态。</p><h2 id="第六章作业"><a href="#第六章作业" class="headerlink" title="第六章作业"></a>第六章作业</h2><h3 id="1-在分布式系统中为什么要对一些实体建立副本？"><a href="#1-在分布式系统中为什么要对一些实体建立副本？" class="headerlink" title="1. 在分布式系统中为什么要对一些实体建立副本？"></a>1. 在分布式系统中为什么要对一些实体建立副本？</h3><p>可靠性：分布在各个机器上的备份可以防止其中某几台机器文件的损坏甚至宕机带来的影响；<br>性能：建立副本性能上得以提升，获取资源可以从地域近的、不繁忙的机器上获取需要的数据，减少通信的开销，实现负载均衡；<br>容错：由于其可大大降低单机文件损坏的影响，因此其容错性高</p><h3 id="2-为何严格的一致性模型在分布式系统中很难实现？"><a href="#2-为何严格的一致性模型在分布式系统中很难实现？" class="headerlink" title="2. 为何严格的一致性模型在分布式系统中很难实现？"></a>2. 为何严格的一致性模型在分布式系统中很难实现？</h3><p>由于严格一致性需要每个读操作都返回最新的写操作的结果，因此需要存在全局的时钟来确定哪个操作时最新的写操作。这在分布式系统中是很难实现的。</p><h3 id="3-假设在一个分布式系统中有下面四个进程："><a href="#3-假设在一个分布式系统中有下面四个进程：" class="headerlink" title="3. 假设在一个分布式系统中有下面四个进程："></a>3. 假设在一个分布式系统中有下面四个进程：</h3><p>P1：a=5；a=6；<br>P2：a=7；<br>P3：printf(a); printf(a); printf(a);<br>P4: printf(a); printf(a); printf(a);<br>最后P3和P4的运行结果都是6 5 7。<br>请问这个运行结果在顺序一致性模型中是可以接受的吗？为什么？<br>顺序一致性不能够接受这个结果，因为这个结果不符合P1的程序中语句的执行次序。</p><h3 id="4-请写出在维护副本一致性的时候进行修改传播的三种方法。"><a href="#4-请写出在维护副本一致性的时候进行修改传播的三种方法。" class="headerlink" title="4.请写出在维护副本一致性的时候进行修改传播的三种方法。"></a>4.请写出在维护副本一致性的时候进行修改传播的三种方法。</h3><p>三种方法是：<br>（1）传播无效性消息<br>（2）传播修改了的数据<br>（3）传播修改操作</p><h3 id="5-为了实现副本的一致性可以采用push的协议，也可以使用pull的协议。请问两种协议都适用于哪种类型的系统"><a href="#5-为了实现副本的一致性可以采用push的协议，也可以使用pull的协议。请问两种协议都适用于哪种类型的系统" class="headerlink" title="5.为了实现副本的一致性可以采用push的协议，也可以使用pull的协议。请问两种协议都适用于哪种类型的系统"></a>5.为了实现副本的一致性可以采用push的协议，也可以使用pull的协议。请问两种协议都适用于哪种类型的系统</h3><p>push的协议会把修改积极推送到各个副本上。在这种情况下，服务器需维护副本的信息，在读写频率高时性能较好，免去了多个副本不断询问的开销；<br>pull的协议副本向服务器轮询是否有修改，当读写频率低时使用较好，不用维护副本信息表，服务器开销小。</p><h3 id="6-（8分）假设在网络中为了提升数据的有效性，为一个数据保存了10个副本。若使用基于法定数量的数据一致性策略，学生A、B、C采用了不同的方案。学生A的方案为读法定数量为1，写法定数量为9；学生B的方案为读法定数量为6，写法定数量为5；学生C的方案为读法定数量为2，写法定数量为9。请回答：（1）这三个方案中哪个方案是正确的？"><a href="#6-（8分）假设在网络中为了提升数据的有效性，为一个数据保存了10个副本。若使用基于法定数量的数据一致性策略，学生A、B、C采用了不同的方案。学生A的方案为读法定数量为1，写法定数量为9；学生B的方案为读法定数量为6，写法定数量为5；学生C的方案为读法定数量为2，写法定数量为9。请回答：（1）这三个方案中哪个方案是正确的？" class="headerlink" title="6.（8分）假设在网络中为了提升数据的有效性，为一个数据保存了10个副本。若使用基于法定数量的数据一致性策略，学生A、B、C采用了不同的方案。学生A的方案为读法定数量为1，写法定数量为9；学生B的方案为读法定数量为6，写法定数量为5；学生C的方案为读法定数量为2，写法定数量为9。请回答：（1）这三个方案中哪个方案是正确的？"></a>6.（8分）假设在网络中为了提升数据的有效性，为一个数据保存了10个副本。若使用基于法定数量的数据一致性策略，学生A、B、C采用了不同的方案。学生A的方案为读法定数量为1，写法定数量为9；学生B的方案为读法定数量为6，写法定数量为5；学生C的方案为读法定数量为2，写法定数量为9。请回答：（1）这三个方案中哪个方案是正确的？</h3><p>（2）对于每种错误的方案给出一个例子说明错误发生的原因。<br>（1）三个方案中C的是正确的<br>（2）如果按照A的方案，读法定数量为1，写法定数量为9，读者会读到一个旧版本的数据；如果按照C的方案，读法定数量为6，写法定数量为5，读者永远读不到6个版本一致的数据。</p>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>distribute system</tag>
      
      <tag>OS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>贝叶斯分类</title>
    <link href="/2019/06/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB/"/>
    <url>/2019/06/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="朴素贝叶斯法（Naive-Bayes）"><a href="#朴素贝叶斯法（Naive-Bayes）" class="headerlink" title="朴素贝叶斯法（Naive Bayes）"></a>朴素贝叶斯法（Naive Bayes）</h2><p>在高斯判别分析（GDA）方法中，特征向量 $x$ 是连续的，值为实数的向量。下面我们要讲的是当 $x_i$ 是离散值的时候来使用的另外一种学习算法。</p><p>下面就来继续看一个之前见过的样例，来尝试建立一个邮件筛选器，使用机器学习的方法。这回咱们要来对邮件信息进行分类，来判断是否为商业广告邮件（就是垃圾邮件），还是非垃圾邮件。在学会了怎么实现之后，我们就可以让邮件阅读器能够自动对垃圾信息进行过滤，或者单独把这些垃圾邮件放进一个单独的文件夹中。对邮件进行分类是一个案例，属于文本分类这一更广泛问题集合。</p><p>假设我们有了一个训练集（也就是一堆已经标好了是否为垃圾邮件的邮件）。要构建垃圾邮件分选器，咱们先要开始确定用来描述一封邮件的特征$x_i$有哪些。</p><p>我们将用一个特征向量来表示一封邮件，这个向量的长度等于字典中单词的个数。如果邮件中包含了字典中的第 $i$ 个单词，那么就令 $x_i = 1$；反之则$x_i = 0$。例如下面这个向量：</p><script type="math/tex; mode=display">x=\begin{bmatrix}1\\0\\0\\\vdots \\1\\ \vdots \\0\end{bmatrix} \begin{matrix}\text{a}\\ \text{aardvark}\\ \text{aardwolf}\\ \vdots\\ \text{buy}\\ \vdots\\ \text{zygmurgy}\\ \end{matrix}</script><p>就用来表示一个邮件，其中包含了两个单词 “a” 和 “buy”，但没有单词 “aardvark”， “aardwolf” 或者 “zymurgy”  。这个单词集合编码整理成的特征向量也成为<strong>词汇表（vocabulary,），</strong> 所以特征向量 $x$ 的维度就等于词汇表的长度。</p><blockquote><p>注：实际应用中并不需要遍历整个英语词典来组成所有英语单词的列表，实践中更常用的方法是遍历一下训练集，然后把出现过一次以上的单词才编码成特征向量。这样做除了能够降低模型中单词表的长度之外，还能够降低运算量和空间占用，此外还有一个好处就是能够包含一些你的邮件中出现了而词典中没有的单词，比如本课程的缩写CS229。有时候（比如在作业里面），还要排除一些特别高频率的词汇，比如像冠词the，介词of 和and 等等；这些高频率但是没有具体意义的虚词也叫做stop words，因为很多文档中都要有这些词，用它们也基本不能用来判定一个邮件是否为垃圾邮件。</p></blockquote><p>选好了特征向量了，接下来就是建立一个生成模型（generative model）。所以我们必须对$p(x|y)$进行建模。但是，假如我们的单词有五万个词，则特征向量$x \in  {0, 1}^{50000}$ （即 $x$是一个 $50000$ 维的向量，其值是$0$或者$1$），如果我们要对这样的 $x$进行多项式分布的建模，那么就可能有$2^{50000}$ 种可能的输出，然后就要用一个 $(2^{50000}-1)$维的参数向量。这样参数明显太多了。</p><p>要给$p(x|y)$建模，先来做一个非常强的假设。我们<strong>假设特征向量$x_i$ 对于给定的 $y$ 是独立的。</strong> 这个假设也叫做<strong>朴素贝叶斯假设（Naive Bayes ，NB assumption），</strong> 基于此假设衍生的算法也就叫做<strong>朴素贝叶斯分类器（Naive Bayes classifier）。</strong> 例如，如果 $y = 1$ 意味着一个邮件是垃圾邮件；然后其中”buy” 是第$2087$个单词，而 “price”是第$39831$个单词；那么接下来我们就假设，如果我告诉你 $y = 1$，也就是说某一个特定的邮件是垃圾邮件，那么对于$x<em>{2087}$ （也就是单词 buy 是否出现在邮件里）的了解并不会影响你对$x</em>{39831}$ （单词price出现的位置）的采信值。更正规一点，可以写成 $p(x<em>{2087}|y) = p(x</em>{2087}|y, x<em>{39831})$。（要注意这个并不是说$x</em>{2087}$ 和 $x<em>{39831}$这两个特征是独立的，那样就变成了$p(x</em>{2087}) = p(x<em>{2087}|x</em>{39831})$，我们这里是说在给定了 $y$ 的这样一个条件下，二者才是有条件的独立。）</p><p>然后我们就得到了等式：</p><script type="math/tex; mode=display">\begin{aligned}p(x_1, ..., x_{50000}|y) & = p(x_1|y)p(x_2|y,x_1)p(x_3|y,x_1,x_2) ... p(x_{50000}|y,x_1,x_2,...,x_{49999})\\& = p(x_1|y)p(x_2|y)p(x_3|y) ... p(x_{50000}|y)\\& = \prod^n_{i=1}p(x_i|y)\\\end{aligned}</script><p>第一行的等式就是简单地来自概率的基本性质，第二个等式则使用了朴素贝叶斯假设。这里要注意，朴素贝叶斯假设也是一个很强的假设，产生的这个算法可以适用于很多种问题。</p><p>我们这个模型的参数为 $\phi<em>{i|y=1} = p (x_i = 1|y = 1), \phi</em>{i|y=0} = p (x_i = 1|y = 0)$, 而 $\phi_y = p (y = 1)$。和以往一样，给定一个训练集${(x^{(i)},y^{(i)}); i = 1, …, m}$，就可以写出下面的联合似然函数：</p><script type="math/tex; mode=display">\mathcal{L}(\phi_y,\phi_{j|y=0},\phi_{j|y=1})=\prod^m_{i=1}p(x^{(i)},y^{(i)})</script><p>找到使联合似然函数取得最大值的对应参数组合 $\phi<em>y , \phi</em>{i|y=0} 和 \phi_{i|y=1}$ 就给出了最大似然估计：</p><script type="math/tex; mode=display">\begin{aligned}\phi_{j|y=1} &=\frac{\sum^m_{i=1}1\{x_j^{(i)} =1 \wedge y^{(i)} =1\} }{\sum^m_{i=1}1\{y^{(i)} =1\}} \\\phi_{j|y=0} &= \frac{\sum^m_{i=1}1\{x_j^{(i)} =1 \wedge y^{(i)} =0\} }{\sum^m_{i=1}1\{y^{(i)} =0\}} \\\phi_{y} &= \frac{\sum^m_{i=1}1\{y^{(i)} =1\}}{m}\\\end{aligned}</script><p>在上面的等式中，”$\wedge$(and)”这个符号的意思是逻辑”和”。这些参数有一个非常自然的解释。例如 $\phi_{j|y=1}$ 正是单词 $j$ 出现的邮件中垃圾邮件所占 $(y = 1)$ 的比例。</p><p>拟合好了全部这些参数之后，要对一个新样本的特征向量 $x$ 进行预测，只要进行如下的简单地计算：</p><script type="math/tex; mode=display">\begin{aligned}p(y=1|x)&=  \frac{p(x|y=1)p(y=1)}{p(x)}\\&= \frac{(\prod^n_{i=1}p(x_i|y=1))p(y=1)}{(\prod^n_{i=1}p(x_i|y=1))p(y=1)+  (\prod^n_{i=1}p(x_i|y=0))p(y=0)}  \\\end{aligned}</script><p>然后选择有最高后验概率的概率。</p><p>最后我们要注意，刚刚我们对朴素贝叶斯算法的使用中，特征向量 $x_i$ 都是二值化的，其实特征向量也可以是多个离散值，比如${1, 2, …, k_i}$这样也都是可以的。这时候只需要把对$p(x_i|y)$ 的建模从伯努利分布改成多项式分布。实际上，即便一些原始的输入值是连续值（比如我们第一个案例中的房屋面积），也可以转换成一个小规模的离散值的集合，然后再使用朴素贝叶斯方法。例如，如果我们用特征向量 $x_i$ 来表示住房面积，那么就可以按照下面所示的方法来对这一变量进行离散化：</p><div class="table-container"><table><thead><tr><th style="text-align:center">居住面积</th><th style="text-align:center">$&lt;400$</th><th style="text-align:center">$400-800$</th><th style="text-align:center">$800-1200$</th><th style="text-align:center">$1200-1600$</th><th style="text-align:center">$&gt;1600$</th></tr></thead><tbody><tr><td style="text-align:center">离散值 $x_i$</td><td style="text-align:center">$1$</td><td style="text-align:center">$2$</td><td style="text-align:center">$3$</td><td style="text-align:center">$4$</td><td style="text-align:center">$5$</td></tr></tbody></table></div><p>这样，对于一个面积为 $890$ 平方英尺的房屋，就可以根据上面这个集合中对应的值来把特征向量的这一项的$x_i$值设置为$3$。然后就可以用朴素贝叶斯算法，并且将$p(x_i|y)$作为多项式分布来进行建模，就都跟前面讲过的内容一样了。当原生的连续值的属性不太容易用一个多元正态分布来进行建模的时候，将其特征向量离散化然后使用朴素贝叶斯法（NB）来替代高斯判别分析法（GDA），通常能形成一个更好的分类器。</p><h3 id="1-拉普拉斯平滑（Laplace-smoothing）"><a href="#1-拉普拉斯平滑（Laplace-smoothing）" class="headerlink" title="1 拉普拉斯平滑（Laplace smoothing）"></a>1 拉普拉斯平滑（Laplace smoothing）</h3><p>刚刚讲过的朴素贝叶斯算法能够解决很多问题了，但还能对这种方法进行一点小调整来进一步提高效果，尤其是应对文本分类的情况。我们来简要讨论一下一个算法当前状态的一个问题，然后在讲一下如何解决这个问题。</p><p>还是考虑垃圾邮件分类的过程，设想你学完了CS229的课程，然后做了很棒的研究项目，之后你决定在$2003$年$6$月把自己的作品投稿到NIPS会议，这个NIPS是机器学习领域的一个顶级会议，递交论文的截止日期一般是六月末到七月初。你通过邮件来对这个会议进行了讨论，然后你也开始收到带有 nips 四个字母的信息。但这个是你第一个NIPS论文，而在此之前，你从来没有接到过任何带有 nips 这个单词的邮件；尤其重要的是，nips 这个单词就从来都没有出现在你的垃圾/正常邮件训练集里面。加入这个 nips 是你字典中的第$35000$个单词那么你的朴素贝叶斯垃圾邮件筛选器就要对参数$\phi_{35000|y}$ 进行最大似然估计，如下所示：</p><script type="math/tex; mode=display">\begin{aligned}\phi_{35000|y=1} &=  \frac{\sum^m_{i=1}1\{x^{(i)}_{35000}=1 \wedge y^{(i)}=1  \}}{\sum^m_{i=1}1\{y^{(i)}=0\}}  &=0 \\\phi_{35000|y=0} &=  \frac{\sum^m_{i=1}1\{x^{(i)}_{35000}=1 \wedge y^{(i)}=0  \}}{\sum^m_{i=1}1\{y^{(i)}=0\}}  &=0 \\\end{aligned}</script><p>也就是说，因为之前程序从来没有在别的垃圾邮件或者正常邮件的训练样本中看到过 nips 这个词，所以它就认为看到这个词出现在这两种邮件中的概率都是$0$。因此当要决定一个包含 nips 这个单词的邮件是否为垃圾邮件的时候，他就检验这个类的后验概率，然后得到了：</p><script type="math/tex; mode=display">\begin{aligned}p(y=1|x) &= \frac{ \prod^n_{i=1} p(x_i|y=1)p(y=1) }   {\prod^n_{i=1} p(x_i|y=1)p(y=1) +\prod^n_{i=1} p(x_i|y=1)p(y=0)    }\\&= \frac00\\\end{aligned}</script><p>这是因为对于”  $\prod^n<em>{i=1} p(x_i|y)$”中包含了$p(x</em>{35000}|y) = 0$的都加了起来，也就还是$0$。所以我们的算法得到的就是 $\frac00$，也就是不知道该做出怎么样的预测了。</p><p>然后进一步拓展一下这个问题，统计学上来说，只因为你在自己以前的有限的训练数据集中没见到过一件事，就估计这个事件的概率为零，这明显不是个好主意。假设问题是估计一个多项式随机变量 $z$ ，其取值范围在${1,…, k}$之内。接下来就可以用$\phi_i = p (z = i)$ 来作为多项式参数。给定一个 $m$ 个独立观测${z^{(1)}, …, z^{(m)}}$ 组成的集合，然后最大似然估计的形式如下：</p><script type="math/tex; mode=display">\phi_j=\frac{\sum^m_{i=1}1\{z^{(i)}=j\}}m</script><p>正如咱们之前见到的，如果我们用这些最大似然估计，那么一些$\phi_j$可能最终就是零了，这就是个问题了。要避免这个情况，咱们就可以引入<strong>拉普拉斯平滑（Laplace smoothing），</strong> 这种方法把上面的估计替换成：</p><script type="math/tex; mode=display">\phi_j=\frac{\sum^m_{i=1}1\{z^{(i)}=j\}+1}{m+k}</script><p>这里首先是对分子加$1$，然后对分母加$k$，要注意$\sum^k_{j=1} \phi_j = 1$依然成立（自己检验一下），这是一个必须有的性质，因为$\phi_j$ 是对概率的估计，然后所有的概率加到一起必然等于$1$。另外对于所有的 $j$ 值，都有$\phi_j \neq 0$，这就解决了刚刚的概率估计为零的问题了。在某些特定的条件下（相当强的假设条件下，arguably quite strong），可以发现拉普拉斯平滑还真能给出对参数$\phi_j$ 的最佳估计（optimal estimator）。</p><p>回到我们的朴素贝叶斯分选器问题上，使用了拉普拉斯平滑之后，对参数的估计就写成了下面的形式：</p><script type="math/tex; mode=display">\begin{aligned}\phi_{j|y=1} & =\frac{\sum^m_{i=1}1\{x_j^{(i)}=1\wedge y ^{(i)}=1\}+1}{\sum^m_{i=1}1{\{y^{(i)}=1\}}+2}\\\phi_{j|y=0} & =\frac{\sum^m_{i=1}1\{x_j^{(i)}=1\wedge y ^{(i)}=10\}+1}{\sum^m_{i=1}1{\{y^{(i)}=0\}}+2}\\\end{aligned}</script><p>（在实际应用中，通常是否对$\phi_y$ 使用拉普拉斯并没有太大影响，因为通常我们会对每个垃圾邮件和非垃圾邮件都有一个合适的划分比例，所以$\phi_y$ 会是对$p(y = 1)$ 的一个合理估计，无论如何都会与零点有一定距离。）</p><h3 id="2-针对文本分类的事件模型（Event-models-for-text-classification）"><a href="#2-针对文本分类的事件模型（Event-models-for-text-classification）" class="headerlink" title="2 针对文本分类的事件模型（Event models for text classification）"></a>2 针对文本分类的事件模型（Event models for text classification）</h3><p>到这里就要给咱们关于生成学习算法的讨论进行一下收尾了，所以就接着讲一点关于文本分类方面的另一个模型。我们刚已经演示过的朴素贝叶斯方法能够解决很多分类问题了，不过还有另一个相关的算法，在针对文本的分类效果还要更好。</p><p>在针对文本进行分类的特定背景下，咱们上面讲的朴素贝叶斯方法使用的是一种叫做<strong>多元伯努利事件模型（Multi-Variate Bernoulli event model）。</strong> 在这个模型里面，我们假设邮件发送的方式，是随机确定的（根据先验类<em>class priors</em>， $p(y)$），无论是不是垃圾邮件发送者，他是否给你发下一封邮件都是随机决定的。那么发件人就会遍历词典，决定在邮件中是否包含某个单词 $i$，各个单词之间互相独立，并且服从概率分布$p(x<em>i=1|y)=\phi</em>{i|y}$。因此，一条消息的概率为：$p(y)\prod^n_{i-1}p(x_i|y)$</p><p> 然后还有另外一个模型，叫做<strong>多项式事件模型（Multinomial event model）。</strong> 要描述这个模型，我们需要使用一个不同的记号和特征集来表征各种邮件。设 $x_i$ 表示单词中的第$i$个单词。因此，$x_i$现在就是一个整数，取值范围为${1,…,|V|}$，这里的$|V|$是词汇列表（即字典）的长度。这样一个有 $n$ 个单词的邮件就可以表征为一个长度为 $n$ 的向量$(x_1,x_2,…,x_n)$；这里要注意，不同的邮件内容，$n$ 的取值可以是不同的。例如，如果一个邮件的开头是”A NIPS . . .” ，那么$x_1 = 1$ (“a” 是词典中的第一个)，而$x_2 = 35000$ (这是假设 “nips”是词典中的第35000个)。</p><p>在多项式事件模型中，我们假设邮件的生成是通过一个随机过程的，而是否为垃圾邮件是首先决定的（根据$p(y)$），这个和之前的模型假设一样。然后邮件的发送者写邮件首先是要生成  从对单词$(p(x<em>1|y))$ 的某种多项式分布中生成 $x_1$。然后第二步是独立于 $x_1$ 来生成 $x_2$，但也是从相同的多项式分布中来选取，然后是 $x_3$,$x_4$  等等，以此类推，直到生成了整个邮件中的所有的词。因此，一个邮件的总体概率就是$p(y)\prod^n</em>{i=1}p(x_i|y)$。要注意这个方程看着和我们之前那个多元伯努利事件模型里面的邮件概率很相似，但实际上这里面的意义完全不同了。尤其是这里的$x_i|y$现在是一个多项式分布了，而不是伯努利分布了。</p><p>我们新模型的参数还是$\phi<em>y = p(y)$，这个跟以前一样，然后还有$\phi</em>{k|y=1} = p(x<em>j =k|y=1)$ (对任何 $j$)以及 $\phi</em>{i|y=0} =p(x_j =k|y=0)$。要注意这里我们已经假设了对于任何$j$ 的值，$p(x_j|y)$这个概率都是相等的，也就是意味着在这个哪个词汇生成的这个分布不依赖这个词在邮件中的位置$j$。</p><p>如果给定一个训练集${(x^{(i)},y^{(i)}); i = 1, …, m}$，其中 $x^{(i)}  = ( x^{(i)}<em>{1} , x^{(i)}</em>{2} ,…, x^{(i)}_{n_i})$（这里的$n$是在第$i$个训练样本中的单词数目），那么这个数据的似然函数如下所示：</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}(\phi,\phi_{k|y=0},\phi_{k|y=1})& = \prod^m_{i=1}p( x^{(i)},y^{(i)})\\& = \prod^m_{i=1}(\prod^{n_i}_{j=1}p(x_j^{(i)}|y;\phi_{k|y=0},\phi_{k|y=1}))p( y^{(i)};\phi_y)\\\end{aligned}</script><p>让上面的这个函数最大化就可以产生对参数的最大似然估计：</p><script type="math/tex; mode=display">\begin{aligned}\phi_{k|y=1}&=  \frac{\sum^m_{i=1}\sum^{n_i}_{j=1}1\{x_j^{(i)}=k\wedge y^{(i)}=1\}}{\sum^m_{i=1}1\{y^{(i)}=1\}n_i} \\\phi_{k|y=0}&=  \frac{\sum^m_{i=1}\sum^{n_i}_{j=1}1\{x_j^{(i)}=k\wedge y^{(i)}=0\}}{\sum^m_{i=1}1\{y^{(i)}=0\}n_i} \\\phi_y&=   \frac{\sum^m_{i=1}1\{y^{(i)}=1\}}{m}\\\end{aligned}</script><p>如果使用拉普拉斯平滑（实践中会用这个方法来提高性能）来估计$\phi<em>{k|y=0}$ 和 $\phi</em>{k|y=1}$，就在分子上加1，然后分母上加$|V|$，就得到了下面的等式：</p><script type="math/tex; mode=display">\begin{aligned}\phi_{k|y=1}&=  \frac{\sum^m_{i=1}\sum^{n_i}_{j=1}1\{x_j^{(i)}=k\wedge y^{(i)}=1\}+1}{\sum^m_{i=1}1\{y^{(i)}=1\}n_i+|V|} \\\phi_{k|y=0}&=  \frac{\sum^m_{i=1}\sum^{n_i}_{j=1}1\{x_j^{(i)}=k\wedge y^{(i)}=0\}+1}{\sum^m_{i=1}1\{y^{(i)}=0\}n_i+|V|} \\\end{aligned}</script><p>当然了，这个并不见得就是一个最好的分类算法，不过朴素贝叶斯分选器通常用起来还都出乎意料地那么好。所以这个方法就是一个很好的”首发选择”，因为它很简单又很好实现。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
      <category>转载</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
      <tag>bayes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>牛奶可乐经济学note2</title>
    <link href="/2019/06/08/%E7%89%9B%E5%A5%B6%E5%8F%AF%E4%B9%90%E7%BB%8F%E6%B5%8E%E5%AD%A6note2/"/>
    <url>/2019/06/08/%E7%89%9B%E5%A5%B6%E5%8F%AF%E4%B9%90%E7%BB%8F%E6%B5%8E%E5%AD%A6note2/</url>
    
    <content type="html"><![CDATA[<p>回顾一下上一节的概念：</p><ol><li><strong>机会成本</strong>：</li></ol><p>一件事情的机会成本，是指为了从事这件事而放弃的其他事情的价值。</p><p>假设小金抽中了大众点评霸王餐，价值200元的A餐馆的晚饭，必须今晚吃。但是今晚小金最爱的林俊杰今晚开演唱会，演唱会门票400元。小金之前想的是演唱会门票超过500元就不去，但是霸王餐的消息突如其来，小金有点犹豫了。</p><p>那么请问小金去吃霸王餐的机会成本是多少？</p><p>答案是100元。吃霸王餐的价值与本题无关，因为小金放弃的是林俊杰的演唱会。林俊杰演唱会在小金心中值500，但他要花费400元买票，因此只要霸王餐在小金的心目中的价值超过100元，小金就应该去吃霸王餐。</p><p><strong>在经济学理论里，成本如果没有特殊说明就是指机会成本。</strong></p><ol><li><strong>成本效益原则</strong>：</li></ol><p>唯有当行动带来的收益比成本高时，你才应该这么做。</p><p>设想一下，马上618购物节了，你想买空调和衣服。已知你有一张打折优惠券。买空调价格2000元，打折后能省100块钱；买衣服200元，打折后能省90元。空调打折了衣服就不能打折，衣服打折了空调就不能打折。你应该在买什么的时候使用打折优惠券？</p><p>答案显而易见，该在买空调时用，多省10元。</p><ol><li><strong>有例外，才能证明规律存在</strong>：</li></ol><p>个人理解，就是找出命题的逆否命题，并证明逆否命题成立。</p><p>比如动物中很多雄性的体型比雌性大，这是因为体型大的雄性有机会与更多雌性交配。这算是规律吗？还是巧合呢？按照“有例外，才能证明规律存在”这一标准，我们只需找到雄性不会和很多雌性交配的情况，并看看他们是不是雄性体型小于等于雌性即可。</p><p>大多一夫一妻制的鸟类，雄鸟和雌鸟体型相似。很多昆虫中的雄性并不会通过体型来和其他雄性竞争交配权，所以昆虫的雄性甚至比雌性还小。</p><ol><li><strong>以叙述形式表达的信息最易被吸收</strong>：</li></ol><p>这就是为什么<em>码农有道</em>公众号的文章受很多程序员喜爱的原因。他每次科普都会讲一个故事，虽然十分老套但是效果很神奇，很简单就理解了（当然记不记得住、能不能应用就是另一回事了）。</p><ol><li><strong>学习最好的方式之一，就是把它写下来</strong>：</li></ol><p>这就是众多大佬，比如《暗时间》一书的作者刘未鹏提倡写博客的缘故吧。</p><hr><p>开始这一讲的话题：</p><h1 id="CH1-产品设计中的经济学"><a href="#CH1-产品设计中的经济学" class="headerlink" title="CH1 产品设计中的经济学"></a>CH1 产品设计中的经济学</h1><ol><li><strong>除非改动带来的收益大于改动的成本，否则生产商不会给产品研发新功能</strong></li></ol><p>商业环境中，产品设计、科学研发要符合成本效益原则。即产品设计既要包含最符合消费者心意的功能，又要满足卖方保持低价、便于竞争的需求。产品设计必须在两者之间保持平衡。</p><p>举个例子，为什么某些公司从“技工贸”转向“贸工技”（即从技术最重要转向贸易最重要）？这也许是公司领导层认为投入在技术研发上面的成本超过了所能带来的收入。</p><h1 id="CH2-供求关系实践"><a href="#CH2-供求关系实践" class="headerlink" title="CH2 供求关系实践"></a>CH2 供求关系实践</h1><ol><li><strong>商业公司的最大价值，不在于它能提高多少生产率，而在于它能创造多少利润</strong></li></ol><p>从长远角度看，新技术所借省下来的成本并不会给生产者带来更高的利润，而是降低了产品的价格，使消费者受惠。</p><ol><li><strong>没有免费的午餐</strong></li></ol><p>提防那些太过美好的机遇。赚取财富的唯一方式仍然是天赋、勤俭、幸运，再加上艰苦的劳动。</p><p>然而，成千上万的人似乎认为自己能够轻松致富。他们亲眼见别人这么做过：有人把手里的钱全买了一飞冲天的高科技股票，比如阿里巴巴、新浪等，就这样发了大财；还有人拼命借贷，买下大大超过个人负担能力的房地产，一夜暴富。</p><p>“没有免费午餐”定律成功解释了2008年的金融危机。不了解的朋友请看《大空头》这部电影，电影描述了几个经济学家在所有人都陷入房价狂欢的时候，率先意识到股市和房市会崩盘，因此大量做空房市，最后大赚一笔。电影中写了几个细节，说就连夜场舞女、地痞流氓等低信用人士都借贷购买了大量房产，一旦金融危机爆发，他们根本无力还款。所有人都陷入免费午餐中的时候，你就要警惕了。</p><ol><li><strong>一价定律</strong></li></ol><p>两座城市之间商品的价格，不会超过两地之间的运输成本。“一价定律”尤其适合于竞争激烈的市场。</p><p>“一价定律”指出，任何试图利用富人多花钱的想法的供应商，都会给竞争对手创造出直接获利的机会。</p><p>在沙漠里向马云和另外一个穷人卖水，马云可能会给出更高的价格。但是在竞争激烈的市场，水的价格都是统一的。这个价格就是人们心中认为它的价值。</p><ol><li><strong>边际成本</strong></li></ol><p>生产的商品数目越多，<strong>总成本</strong>越高，这是显然的。但是总成本与生产数目的关系并不是线性的。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>study method</tag>
      
      <tag>economics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络学习笔记note1</title>
    <link href="/2019/06/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0note1/"/>
    <url>/2019/06/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0note1/</url>
    
    <content type="html"><![CDATA[<h1 id="实现神经网络"><a href="#实现神经网络" class="headerlink" title="实现神经网络"></a>实现神经网络</h1><p>简单的神经网络实现。包含随机梯度下降算法、反向传播算法。</p><pre><code class="lang-python">class Network(object):    def __init__(self, sizes):        self.num_layers = len(sizes)        self.sizes = sizes        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]        self.weights = [np.random.randn(y, x)                        for x, y in zip(sizes[:-1], sizes[1:])]    def feedforward(self, a):        for b, w in zip(self.biases, self.weights):            a = sigmoid(np.dot(w, a)+b)        return a    def SGD(self, training_data, epochs, mini_batch_size, eta,            test_data=None):        training_data = list(training_data)        n = len(training_data)        if test_data:            test_data = list(test_data)            n_test = len(test_data)        for j in range(epochs):            random.shuffle(training_data)            mini_batches = [                training_data[k:k+mini_batch_size]                for k in range(0, n, mini_batch_size)]            for mini_batch in mini_batches:                self.update_mini_batch(mini_batch, eta)            if test_data:                print(&quot;Epoch &#123;&#125; : &#123;&#125; / &#123;&#125;&quot;.format(j,self.evaluate(test_data),n_test));            else:                print(&quot;Epoch &#123;&#125; complete&quot;.format(j))    def update_mini_batch(self, mini_batch, eta):        nabla_b = [np.zeros(b.shape) for b in self.biases]        nabla_w = [np.zeros(w.shape) for w in self.weights]        for x, y in mini_batch:            delta_nabla_b, delta_nabla_w = self.backprop(x, y)            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]        self.weights = [w-(eta/len(mini_batch))*nw                        for w, nw in zip(self.weights, nabla_w)]        self.biases = [b-(eta/len(mini_batch))*nb                       for b, nb in zip(self.biases, nabla_b)]    def backprop(self, x, y):        nabla_b = [np.zeros(b.shape) for b in self.biases]        nabla_w = [np.zeros(w.shape) for w in self.weights]        # feedforward        activation = x        activations = [x] # list to store all the activations, layer by layer        zs = [] # list to store all the z vectors, layer by layer        for b, w in zip(self.biases, self.weights):            z = np.dot(w, activation)+b            zs.append(z)            activation = sigmoid(z)            activations.append(activation)        # backward pass        delta = self.cost_derivative(activations[-1], y) * \            sigmoid_prime(zs[-1])        nabla_b[-1] = delta        nabla_w[-1] = np.dot(delta, activations[-2].transpose())        for l in range(2, self.num_layers):            z = zs[-l]            sp = sigmoid_prime(z)            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp            nabla_b[-l] = delta            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())        return (nabla_b, nabla_w)    def evaluate(self, test_data):        test_results = [(np.argmax(self.feedforward(x)), y)                        for (x, y) in test_data]        return sum(int(x == y) for (x, y) in test_results)    def cost_derivative(self, output_activations, y):        return (output_activations-y)#### Miscellaneous functionsdef sigmoid(z):    &quot;&quot;&quot;The sigmoid function.&quot;&quot;&quot;    return 1.0/(1.0+np.exp(-z))def sigmoid_prime(z):    &quot;&quot;&quot;Derivative of the sigmoid function.&quot;&quot;&quot;    return sigmoid(z)*(1-sigmoid(z))</code></pre>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
      <category>转载</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>machcine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>支持向量机笔记</title>
    <link href="/2019/05/30/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AC%94%E8%AE%B0/"/>
    <url>/2019/05/30/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h1><h2 id="1-思想"><a href="#1-思想" class="headerlink" title="1. 思想"></a>1. 思想</h2><p>SVM的思想，即对于平面上的二分类问题，找到一条直线，不但能够将两类数据恰好分开，而且要分的越开越好。即最大化不同类别的（关键）样本点之间的距离。</p><p><strong>我们的目的是找到一条河，让这条河的河岸恰恰经过正负样本的同时，尽可能宽。</strong></p><p><img src="/2019/05/30/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AC%94%E8%AE%B0/2019-05-30-20-27-32.png" alt></p><p>上图蓝线是最终确定的超平面，红线则是正例和反例的“楚河汉界”。这“楚河汉界”与超平面平行，并且距超平面相同距离。</p><p><strong>我们不妨定义这段距离为1。</strong></p><h2 id="2-最大化距离的方法"><a href="#2-最大化距离的方法" class="headerlink" title="2. 最大化距离的方法"></a>2. 最大化距离的方法</h2><p>假设已经找到了这样一条超平面，它的法向量为$\vec{w}$.</p><p>对于任意样本$\vec{u}$，如果$f(u)=w\cdot u+b &gt; 0$则把样本u归为正例，反之为负例。</p><p>现在想要找到一条河，让这条河的河岸恰恰经过正负样本的同时，尽可能宽。这样我们对正例和负例的划分做出了更高的要求：不仅仅大于0，更要大于河的宽度（的一半），即大于1。</p><script type="math/tex; mode=display">f\left(\mathbf{x}_{+}\right)=\mathbf{w} \cdot \mathbf{x}_{+}+b \geq 1</script><script type="math/tex; mode=display">f\left(\mathbf{x}_{-}\right)=\mathbf{w} \cdot \mathbf{x}_{-}+b \leq-1</script><p>设想一下，我们有两个样本，一个小红，一个小蓝，他们站在河的对岸相望。</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{w} \cdot \mathbf{x}_{1}+b &=+1 \\ \mathbf{w} \cdot \mathbf{x}_{2}+b &=-1 \end{aligned}</script><p><img src="/2019/05/30/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AC%94%E8%AE%B0/2019-05-30-20-45-50.png" alt></p><p>那我现在问你，求河的长度。WTF？这样就能求出河的长度了吗？是的，想一下，从小红指向小蓝的那一条向量$x_1-x_2$，向超平面的法向量$w$方向投影，然后得到的向量长度不就是河的长度了吗？</p><p>河的长度：$ \mathbf{w} \cdot\left(\mathbf{x}<em>{1}-\mathbf{x}</em>{2}\right)=2 $<br>。还记得两个向量投影就是求点积吗？蛤蛤。</p><p>忘了，法向量也是有长度的，为了把法向量标准化，我们需要除以法向量的长度：</p><script type="math/tex; mode=display">\frac{\mathbf{w}}{\|\mathbf{w}\|} \cdot\left(\mathbf{x}_{1}-\mathbf{x}_{2}\right)=\frac{2}{\|\mathbf{w}\|}</script><p>问题就转化为将$\frac{2}{||\mathbf{w}||}$求最大。这就是一个最优化问题，按理说利用拉格朗日乘子法，对分量挨个求导即可求出极值点。但是现在这副模样连求导都做不到。我们想办法变形一下：</p><p>常数项肯定就不要了，问题就变成了最小化$\frac{1}{2}|\mathbf{w}|^{2}$。这一切都是为了方便数学处理呀。</p><p>不要忘了，还有一个（其实是i个）限制条件：</p><script type="math/tex; mode=display">y_{i}\left(\mathbf{w} \cdot \mathbf{x}_{i}+b\right) \geq 1</script><p>y取1或-1，用来标记样本类别。</p><h2 id="3-二次优化问题"><a href="#3-二次优化问题" class="headerlink" title="3. 二次优化问题"></a>3. 二次优化问题</h2><p>所谓拉格朗日，就是在限制条件前面加个参数，然后附加在要求的优化方程式中，从而将问题转化成无约束优化问题。被拉格朗日之后，我们加了i个参数，组成参数向量a</p><script type="math/tex; mode=display">L=\frac{1}{2}\|\mathbf{w}\|^{2}-\sum_{i=1}^{l} a_{i}\left(y_{i}\left(\mathbf{x}_{i} \cdot \mathbf{w}+b\right)-1\right)</script><p>接下来要对向量求导了。害怕吗？有如下公式：$\frac{\partial|\mathbf{w}|^{2}}{\partial \mathbf{w}}=2 \mathbf{w}$和$\frac{\partial \mathbf{x} \cdot \mathbf{w}}{\partial \mathbf{w}}=\mathbf{x}$</p><p>我们分别对w和b求导，看看拉格朗日函数对w和b的变化分别有什么反应：</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial L}{\partial \mathbf{w}} &=\mathbf{w}-\sum_{i=1}^{l} a_{i} y_{i} \mathbf{x}_{i}=0 \\ \frac{\partial L}{\partial b} &=\sum_{i=1}^{l} a_{i} y_{i}=0 \end{aligned}</script><p>现在合适的 $ w^*=\sum<em>{i=1}^{l} a</em>{i} y<em>{i} \mathbf{x}</em>{i} $ 也都找到了，将他们回代到拉格朗日函数中去，看看他的最小点长什么样子。</p><script type="math/tex; mode=display">L=\sum_{i=1}^{l} a_{i}-\frac{1}{2} \sum_{i, j=1}^{l} a_{i} a_{j} y_{i} y_{j} \mathbf{x}_{i} \cdot \mathbf{x}_{j}</script><p>我们可以看到啊，这个L的大小，取决于样本之间的点积，也就是我们选的两个小红和小蓝，他们站的位置。他们的位置要是不好，那距离就会不够大，那我们的拉格朗日同学就要重新选人。因此呢，最终被选择的样本，那一定是具有代表性的，我们称它们为<strong>支持向量</strong>。这就是支持向量机一词的由来。</p><h2 id="4-核方法的运用"><a href="#4-核方法的运用" class="headerlink" title="4. 核方法的运用"></a>4. 核方法的运用</h2><p>有时候我们会碰到一些样本，他们本来就不是线性可分的，比如</p><p><img src="/2019/05/30/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AC%94%E8%AE%B0/2019-05-30-21-14-02.png" alt></p><p>但我们还是将他们利用支持向量机分开了。怎么做到的？怎么“以直为曲”？那是因为有一个道理，<strong>高维空间比低维空间更广阔的</strong>，更难出现线性不可分的状况。所以很自然的，我们人为添加一些分量，比如将二维点添加z轴分量到三维啊，甚至无限维。之后我们可以找到一个超平面，将他们分开就是轻而易举的事情啦。</p><p>我们需要一种神秘变换$\Phi$，接受一个样本向量，输出提升维度之后的样本向量。</p><p><img src="/2019/05/30/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AC%94%E8%AE%B0/2019-05-30-21-17-55.png" alt></p><p>$\Phi$很难找，但是我们不必找。因为根据前面的结论，拉格朗日函数根我们选取的关键向量的<strong>内积</strong>有关，直接表示出$ \Phi\left(\mathbf{x}<em>{1}\right) \cdot \Phi\left(\mathbf{x}</em>{2}\right) $，一步到位哦。</p><p>正好有这种专门表示两个向量运算的函数，叫做核函数。</p><script type="math/tex; mode=display">\Phi\left(\mathbf{x}_{1}\right) \cdot \Phi\left(\mathbf{x}_{2}\right)=K\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right)</script><p>举个例子，线性核函数：</p><script type="math/tex; mode=display">K(x, y) = x\cdot y</script><p>多项式核函数：</p><script type="math/tex; mode=display">K\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right)=\left(\mathbf{x}_{1} \cdot \mathbf{x}_{2}+1\right)^{n}</script><p>高斯核函数：</p><script type="math/tex; mode=display">K\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right)=e^{\frac{-|| x_{1}+x_{2} ||^{2}}{2 \sigma^{2}}}</script><p>有的时候我们会碰到一些样本，他们很难缠，甚至根本线性不可分。类似于红军打入了蓝军内部，根本无法找到一条直线将他们分开。这时候我们就需要放低要求。</p><p>引入松弛变量，出现的错误样本在一定程度范围内越小越好。</p><script type="math/tex; mode=display">\xi_i =  1-y_{i}\left(w^{T} x_{i}+b\right)</script><p>则有下式</p><script type="math/tex; mode=display">y_{i}\left(w^{T} x_{i}+b\right) \geq 1-\xi_{i}, \quad i=1, \dots, n</script><p>最终的优化问题就变成了</p><script type="math/tex; mode=display">\begin{array}{l}{\min \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{n} \xi_{i}} \\ {\text {s.t.}, y_{i}\left(w^{T} x_{i}+b\right) \geq 1-\xi_{i}, i=1, \ldots, n} \\ {\xi_{i} \geq 0, i=1, \ldots, n}\end{array}</script><p>C为经验参数，调参侠必备。您想让误差少点，C就大点。</p><p>接下来的套路就和之前一样了，拉格朗日啊之类的，略去不表。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
      <tag>SVM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>华为的命运与中国的反应</title>
    <link href="/2019/05/25/%E5%8D%8E%E4%B8%BA%E7%9A%84%E5%91%BD%E8%BF%90%E4%B8%8E%E4%B8%AD%E5%9B%BD%E7%9A%84%E5%8F%8D%E5%BA%94/"/>
    <url>/2019/05/25/%E5%8D%8E%E4%B8%BA%E7%9A%84%E5%91%BD%E8%BF%90%E4%B8%8E%E4%B8%AD%E5%9B%BD%E7%9A%84%E5%8F%8D%E5%BA%94/</url>
    
    <content type="html"><![CDATA[<h1 id="美国将华为列入了黑名单，实施一系列与针对中兴类似的、外科手术式的精准打击。华为在劫难逃。那么华为或中国可以做什么来应对呢？中国会采取哪些手段？"><a href="#美国将华为列入了黑名单，实施一系列与针对中兴类似的、外科手术式的精准打击。华为在劫难逃。那么华为或中国可以做什么来应对呢？中国会采取哪些手段？" class="headerlink" title="美国将华为列入了黑名单，实施一系列与针对中兴类似的、外科手术式的精准打击。华为在劫难逃。那么华为或中国可以做什么来应对呢？中国会采取哪些手段？"></a>美国将华为列入了黑名单，实施一系列与针对中兴类似的、外科手术式的精准打击。华为在劫难逃。那么华为或中国可以做什么来应对呢？中国会采取哪些手段？</h1><p>华为被列为美国“实体清单”无疑是当下最受关注的话题。显然，美国是想采取一系列精准打击，妄图将华为推入和中兴一样的境地。因此，无论是国内还是国外，线上还是线下，华为和中国的反应无疑是最受人关注的。鉴于国内新闻平台罕有客观分析的文章，我通过Quora论坛（又称作美国版知乎）摘录并翻译了Janus Dongye Qimeng对于该问题的精彩回答。他回答于5月22日，截至目前（25日）收获了5.9k个赞同。让我们来欣赏Janus对于该问题的精彩回答吧。</p><p>链接：<a href="https://www.quora.com/US-blacklisted-Huawei-in-what-is-evidently-ZTE-style-surgical-strike-It-appears-Huawei-is-pretty-much-dead-What-can-Huawei-or-China-do-to-overcome-this-or-to-retaliate-What-will-China-do">US blacklisted Huawei in what is evidently ZTE style surgical strike. It appears Huawei is pretty much dead. What can Huawei or China do to overcome this or to retaliate? What will China do?</a></p><p>Janus Dongye Qimeng：</p><p>不谈观点，只说事实。</p><p>下面是华为2019年最畅销的P30手机的供应链：</p><p><img src="/2019/05/25/%E5%8D%8E%E4%B8%BA%E7%9A%84%E5%91%BD%E8%BF%90%E4%B8%8E%E4%B8%AD%E5%9B%BD%E7%9A%84%E5%8F%8D%E5%BA%94/芯片.png" alt></p><p>华为P30手机的“大脑”是由海思公司设计的麒麟980系统芯片（System-on-Chip，SOC）。而海思是华为旗下的子公司。为什么称它为系统芯片？因为该芯片集成了许多由世界其他地方设计的组件。</p><p>该系统芯片背后是什么？</p><p><strong>指令集架构</strong>（Instruction set architecture）：海思购买了<strong>英国</strong>剑桥ARM的CPU和GPU架构许可证。通过许可，海思可以使用ARM指令集（armv8）并开发自己的64位CPU架构。AMBA等总线标准也是ARM授权的。</p><p><strong>CPU，GPU</strong>：海思在中国深圳拥有数百名员工，负责设计CPU内核、加速器和IP组件。为了设计自己的CPU，他们需要使用Synopsis，Cadence和Xilinx的电子设计自动化（EDA）工具。这些EDA公司都是<strong>美国</strong>加利福尼亚州的公司。海思需要支付许可费才能使用他们的工具来设计和模拟自己的CPU。</p><blockquote><p>译者注：电子设计自动化（英语：Electronic design automation，缩写：EDA）是指利用计算机辅助设计（CAD）软件，来完成超大规模集成电路（VLSI）芯片的功能设计、综合、验证、物理设计（包括布局、布线、版图、设计规则检查等）等流程的设计方式。</p></blockquote><p>同时，海思还可以集成ARM设计的现有软核，如强大的核心Cortex A76和高效的核心Cortex A55。 两者都在同一芯片中。大核心在<strong>美国</strong>德克萨斯州奥斯汀市设计，小核心在<strong>英国</strong>剑桥设计。一些低端CPU核心也从中国台湾联发科购买。同时，海思还可以从ARM购买其他知识产权，包括Mali T830 GPU和互连子系统。Mali GPU设计在<strong>英国</strong>剑桥的ARM总部。</p><p><strong>内存</strong>：海思在存储器控制器和SRAM系统中设计了自己的逻辑电路。SRAM和DRAM单元由<strong>韩国</strong>三星授权。未来的7nm 3D堆叠RAM也将由三星设计，但在中国大连制造。</p><p><strong>DSP和相机</strong>：海思购买了德国徕卡相机的相机镜头设计知识产权和控制系统，其中大部分系统都是在<strong>德国</strong>韦茨拉尔设计的。实际镜头由中国台湾的大立光电（Largan Precision）和中国大陆的舜宇光学科技（Sunny Optical Technology）制造。用于驱动相机改变焦点的电动马达由三美电机（Mitsumi）在日本Tsurumaki制造。为了将光转换为信号，光敏胶片由中国深圳的欧菲光（O-film）设计（也是iPhone X的供应商）。海思从<strong>美国</strong>亚利桑那州凤凰城的安森美（ON Semiconductors）购买了用于自动聚焦和图像稳定的硬件解决方案。高清视频处理芯片由<strong>日本</strong>索尼授权。海思设计了自己的图像处理硬件加速器（ISP），从<strong>美国</strong>加利福尼亚州的CEVA购买了许多DSP专利，并从中国北京购买了来自寒武纪科技（Cambricon Technologies）的AI芯片。</p><p><strong>通信模块</strong>（Baseband）：海思购买了许可证，使用来自<strong>美国</strong>加利福尼亚州圣何塞的博通（Broadcom）的WIFI、GPS和蓝牙。对于3G支持，海思必须向<strong>美国</strong>加利福尼亚州圣地亚哥高通公司持有的专利支付许可使用费。对于后来的4G LTE和5G，海思拥有自己的专利和通信模块处理器，称为Balong，由中国数百人设计。海思还从中国科学院购买了北斗导航系统。请注意，一些芯片验证任务由<strong>印度</strong>海得拉巴的工程师执行。</p><p><strong>射频模块</strong>（Radio frequency，缩写为RF）：要在各种通信信号之间进行多路复用并将模拟信号放大到不同的无线频率，它们需要射频集成电路（RFIC）。RFIC的大多数专利都是由<strong>美国</strong>北卡罗来纳州的RF Micro Devices公司持有，现在在与TriQuint合并后成为Qorvo。RFIC芯片需要一些功率放大器，这由<strong>日本</strong>京都的Murata Manufacturing制造的高端电容器提供。 还需要TST台湾和Microgate在深圳设计和制造的表面声波（SAW）传感器。还需要一些由<strong>美国</strong>Skyworks Solutions设计并由Skyworks在中国制造的绝缘硅绝缘体开关。对于天线组件，它们由深圳的Sunway公司和位于中国上海的Rosenberger（<strong>美国</strong>）工厂设计和制造。在5G时代，华为模拟设备也必须使用来自<strong>美国</strong>，<strong>日本</strong>和中国的这些设备。</p><p><strong>NFC和触控设备</strong>：<strong>荷兰</strong>的恩智浦半导体为华为提供NFC解决方案。该芯片由英飞凌在<strong>德国</strong>西门子开发。深圳Goodix Co提供指纹传感器。USB Type-C解决方案由深圳Everwin Precision提供。</p><p><strong>芯片装配</strong>：在海思将所有知识产权和零件集成到一个系统芯片SOC之后，该设计被送到台湾半导体制造公司（TSMC）进行物理布局和制造。SOC芯片的制造过程是一项非常复杂的任务。对于最重要的步骤，TSMC需要导入由<strong>荷兰</strong>ASML设计的掩模对齐系统（MAS）。他们还需要使用来自<strong>日本</strong>的Shin-Etsu，来自<strong>德国</strong>的Siltronic AG和来自<strong>日本</strong>Minato的SUMCO Corporation的大量晶圆化学品。</p><p><strong>源材料</strong>：但是，大多数化学产品和半成品都是从中国进口的。最具代表性的是中国的稀土金属。对于包括玻璃和钢材在内的其他材料，比亚迪在深圳负责制造手机梯度框架和高密度眼镜。盛意电子为手机生产所有PCB板。</p><p><strong>屏幕</strong>：华为P30采用三星OLED硬屏，但P30 Pro采用京东方科技在中国设计的OLED软屏。有些屏幕也由<strong>韩国</strong>LG在中国广州制造。现在，韩国和中国公司都在屏幕市场占据主导地位。</p><p><strong>组装</strong>：华为随后从每个服务提供商处订购所有组件，并将组件运送到中国郑州的富士康公司。富士康的工人将所有组件组装成一个完整的手机。</p><p><img src="/2019/05/25/%E5%8D%8E%E4%B8%BA%E7%9A%84%E5%91%BD%E8%BF%90%E4%B8%8E%E4%B8%AD%E5%9B%BD%E7%9A%84%E5%8F%8D%E5%BA%94/p30.jpg" alt></p><p>这是华为单个手机的供应链。华为的主要产品不只是手机，但他们的手机仍然成功击败苹果，成为未进入美国市场的第二大智能手机公司。华为的主要优势在于其通信基础设施和解决方案。因为这方面我并不熟悉，所以我并不打算写。</p><p>现在请计算这些供应商来自美国，中国，日本和韩国的数量。对于上面列出的每家公司，请访问他们自己的网站，查看他们的产品实际销售到华为或中国市场的份额，以及他们的材料从中国进口的数量。你会惊讶地发现，华为通常是他们最大的客户，他们再也不能离开中国了。</p><p>这意味着如果你杀死了华为，那么大多数供应商也会受到很大的伤害。有些公司会陪葬。他们中的大多数是韩国和日本唯一的高价值公司。他们会遭受40％及以上的市场损失。这将对韩国和日本经济造成巨大打击。</p><p>很显然，特朗普背后的智囊并不了解半导体行业的现状。我想Quora的大多数人都不知道。</p><p><img src="/2019/05/25/%E5%8D%8E%E4%B8%BA%E7%9A%84%E5%91%BD%E8%BF%90%E4%B8%8E%E4%B8%AD%E5%9B%BD%E7%9A%84%E5%8F%8D%E5%BA%94/trump.jpg" alt></p><p>华为已死？</p><p>当然没有。十年前，华为已经启动了美国政府的各种情景备胎计划。他们甚至有针对当整个中国被阻止使用x86指令集场景的极端备份计划。</p><h3 id="中国会采取什么手段？"><a href="#中国会采取什么手段？" class="headerlink" title="中国会采取什么手段？"></a>中国会采取什么手段？</h3><p>让我们来看看中国采取的最新的应对措施：</p><p><a href="https://www.scmp.com/tech/enterprises/article/2139699/china-cuts-taxes-chip-makers-promote-industry-development">由于贸易紧张局势加剧，中国削减了芯片制造商的税收</a></p><p>中国宣布所有国内半导体供应商公司将获得五年免税。他们五年不需要纳税！这意味着他们可以降低运营成本并击败其外国竞争对手。这是美国一直在抱怨中国的地方。现在中国一直在做，不管美国有什么话要说。</p><p>如果华为找不到美国的供应商，那么他们会找到替代品，主要是中国的国内供应商。这些供应商在中国免税。这将为国内公司带来巨大的推动力，因为它们可以同时降低成本和获得客户。</p><p>我一直在与中国半导体行业不同领域的许多中国学者交谈。他们说，华为之所以从这么多来源购买知识产权，并不是说华为没有这项技术，更重要的是，对于他们最重要的领域，他们并不打算<strong>重新发明轮子</strong>（reinvent the wheels），他们希望与世界共享利益。</p><p>确实有一些中国仍然落后的关键技术，如制造工艺和射频芯片。但我们应该明白。中国能达到目前为止的技术水平，正是由于巴黎多边出口管制协调委员会的技术封锁和制裁，中国被禁止使用所有高端技术。多亏了他们，中国有机会自己“重新发明轮子”。</p><p>同时，由于采用了华为技术，北京的地铁已经安装并覆盖了5G信号。</p><p><img src="/2019/05/25/%E5%8D%8E%E4%B8%BA%E7%9A%84%E5%91%BD%E8%BF%90%E4%B8%8E%E4%B8%AD%E5%9B%BD%E7%9A%84%E5%8F%8D%E5%BA%94/5g.jpg" alt></p><p>但现在，我坐在伦敦的地铁上。我的手机没有任何信号。因此，我必须离线阅读关于一篇精彩的英国脱欧文章的帖子。我周围的人都在看华为手机上的离线小说。</p><p>有关软件方面“供应链”的分析，请参考我的答案：</p><p><a href="https://www.quora.com/How-badly-will-Huaweis-smartphone-business-be-affected-by-Googles-response-to-US-placing-Huawei-on-Entity-list-Huawei-loses-access-to-Google-proprietary-apps-and-services-but-is-still-be-able-to-run-the-Android-Open/answer/Janus-Dongye-Qimeng">Janus Dongye Qimeng’s answer to How badly will Huawei’s smartphone business be affected by Google’s response to US placing Huawei on “Entity” list? (Huawei loses access to Google proprietary apps and services but is still be able to run the Android Open Source license (AOSP).</a></p><p>我希望你能通过本文了解更多关于现状的信息。</p><blockquote><p>Talk is cheap. Show me the code.<br>— Linus Torvalds<br>空谈误国，实干兴邦。</p></blockquote><p>本文由<strong>Superlova</strong>翻译自Quora平台Janus Dongye Qimeng对<a href="https://www.quora.com/US-blacklisted-Huawei-in-what-is-evidently-ZTE-style-surgical-strike-It-appears-Huawei-is-pretty-much-dead-What-can-Huawei-or-China-do-to-overcome-this-or-to-retaliate-What-will-China-do">US blacklisted Huawei in what is evidently ZTE style surgical strike. It appears Huawei is pretty much dead. What can Huawei or China do to overcome this or to retaliate? What will China do?</a>的回答，翻译并传播的初心是用作英语翻译习作和思想交流。郑重声明：本文的观点并不代表译者的观点。转载本译文请注明本译文出处！</p>]]></content>
    
    
    <categories>
      
      <category>translation</category>
      
      <category>转载</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】机器学习之决策树算法</title>
    <link href="/2019/05/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95/"/>
    <url>/2019/05/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>决策树算法是一类经典的监督学习算法，它的实现简单、可解释性强、能适应多种类型的输入数据，并且多颗决策树可以集成为强大的模型。常用于复杂问题的 baseline 解决方案，以及分析特征重要程度。</p><!--more---><h2 id="1-树的划分流程：原理与思想"><a href="#1-树的划分流程：原理与思想" class="headerlink" title="1. 树的划分流程：原理与思想"></a>1. 树的划分流程：原理与思想</h2><h3 id="1-1-决策树的工作原理"><a href="#1-1-决策树的工作原理" class="headerlink" title="1.1 决策树的工作原理"></a>1.1 决策树的工作原理</h3><p>决策树是广泛用于分类和回归任务的模型。本质上，它从一层层的if/else 问题中进行学习，并得出结论。</p><p><img src="/2019/05/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95/2019-05-22-21-00-49.png" alt="区分几种动物的决策树"></p><p>在这张图中，树的每个结点代表一个问题或一个包含答案的终结点（也叫叶结点）。树的边将问题的答案与将问的下一个问题连接起来。</p><p>用机器学习的语言来说就是，为了区分四类动物（鹰、企鹅、海豚和熊），我们利用三个特征（“有没有羽毛”“会不会飞”和“有没有鳍”）来构建一个模型。我们可以利用监督学习从数据中学习模型，而无需人为构建模型。</p><h3 id="1-2-决策树的思想"><a href="#1-2-决策树的思想" class="headerlink" title="1.2 决策树的思想"></a>1.2 决策树的思想</h3><p>原则上讲，对于给定的属性集，可以构造的决策树的数目达指数级。尽管某些决策树比其他决策树更准确，但是由于搜索空间是指数规模的，遍历找出最佳决策树在计算上是不可行的。</p><p>尽管如此，人们还是开发出了一写有效的算法，这些算法通常基于贪心策略，在选择划分数据的属性时，采取一系列局部最优决策来构造决策树。</p><p>一种朴素的构建决策树的思路如下：</p><ol><li>如果某节点中所有记录都为同一类别，则无需分类，该节点为叶子结点；</li><li>如果某节点中包含多个类别的记录，则按照某个特征的大小为参考，令节点分裂为多个子节点；比如该特征大于某值则令记录进入子节点 1，否则进入子节点 2；</li><li>对所有子节点递归调用该算法，直到该节点内的所有记录都属于同一个类别，或者所有记录的选定特征都相同。</li></ol><p>该算法会引出两个关键问题：</p><ol><li><strong>如何选择特征进行分裂</strong>？需要建立一种评估方法，评估不同特征对该节点的分类能力，并优先选择分类效果好的特征；</li><li><strong>如何停止分裂</strong>？数据本身存在噪声，递归分裂过深会导致过拟合，如何剪枝来保证模型的泛化能力？</li></ol><h2 id="2-如何选择特征进行分裂？"><a href="#2-如何选择特征进行分裂？" class="headerlink" title="2. 如何选择特征进行分裂？"></a>2. 如何选择特征进行分裂？</h2><p>决策树最关键的是选择合适的划分方法，来使得问题的规模最大程度的简化。</p><p>一般来说，我们希望优先选择那些能够最大程度简化问题的特征。对于二分类任务，如果存在某个特征，大于某值的记录全都是类别 1，小于某值的记录全是类别 2，则该特征就能最大程度简化问题。</p><p>举例来说，男女性别分类任务，”有没有喉结“就是一个好特征，”头发长度“、”身高“则相对分类能力不那么强。</p><h3 id="2-1-信息熵"><a href="#2-1-信息熵" class="headerlink" title="2.1 信息熵"></a>2.1 信息熵</h3><p>为此，我们需要一个物理量，来度量某节点内的所有记录的类别单纯程度。如果一个节点中的记录全都是同一类别，则我们认为该节点十分纯洁。</p><p>然后判断特征的优劣，只需求目前的纯度，然后根据该特征进行分裂，再计算一遍纯度，则纯度下降的值，就可以用来度量该特征的分类能力了。</p><p><strong>纯度（purity）</strong> 的定义有很多方法，比如信息熵（information entropy）。假设 $Y$ 为类别集合，$D$ 为节点中的样本集合，$p_k$ 为当前集合 $D$ 中第 $k$ 类样本占比，则信息熵的定义为：</p><script type="math/tex; mode=display">\text{Entropy}(D)=-\sum_{k=1}^{|Y|} p_k\ln{p_k}</script><p>假设叶子结点有 10 个样本，它们的类别分别为： {1,2,2,3,3,3,3,3,3,3}，则此时该特征对应的信息熵为：</p><script type="math/tex; mode=display">\text{Entropy}=-(\frac{1}{10}\ln{\frac{1}{10}}+\frac{2}{10}\ln{\frac{2}{10}}+\frac{7}{10}\ln{\frac{7}{10}})\approx0.8</script><p>对于样本集合 {1,1,1,2,2,2,3,3,3,3}，则此时的信息熵为：</p><script type="math/tex; mode=display">\text{Entropy}=-(\frac{3}{10}\ln{\frac{3}{10}}+\frac{3}{10}\ln{\frac{3}{10}}+\frac{4}{10}\ln{\frac{4}{10}})\approx1.09</script><p>比较上述两个样本集合，集合 1 大部分样本是类别 3，集合 2 的类别分布比较均匀，则集合 1 的信息熵比集合 2 的要小，这说明集合 1 更纯。</p><h3 id="2-2-信息增益"><a href="#2-2-信息增益" class="headerlink" title="2.2 信息增益"></a>2.2 信息增益</h3><p>接下来我们需要选定特征，比如身高这个特征，可能产出三种分支节点：[0m, 1m), [1m, 2m], [2m, +inf] ，每个分支节点内部都对应了一些人。则身高这个特征划分节点后的信息熵之和，与未划分时的信息熵之差，就是<strong>信息增益（ information gain）</strong>。</p><p>假定特征 a 有 V 种取值，则对应 V 种叶子结点，每个节点包含的样本集合为 $D_v$ 。则 $D_v$ 的信息熵为：$\text{Entropy}(D_v)$，此时全部样本按照特征 a 进行分裂的信息增益为:</p><script type="math/tex; mode=display">\text{Gain}(D, a)=\text{Entropy}(D)-\sum^{V}_{v=1}\frac{|D_v|}{|D|}\text{Entropy}(D_v)</script><p>信息增益越大，则意味着使用属性 a 来进行划分所获得的”纯度提升“越大。通过遍历所有特征，取当前节点对应的信息增益最大的特征进行分裂，这种做法就是 ID3 决策树生成算法 所采用的。</p><h3 id="2-3-信息增益率"><a href="#2-3-信息增益率" class="headerlink" title="2.3 信息增益率"></a>2.3 信息增益率</h3><p>在实际使用中，我们发现单纯使用信息增益进行分裂，则决策树总是试图先使用那些取值数目较丰富的特征。假如样本存在编号，并把编号当做分类特征（这个特征当然没有意义），那么 ID3 将总会选择 编号特征作为叶片的分裂特征。因此需要惩罚那些具有过多取值的特征。</p><p>具体地，对每个特征的信息增益，除以固有值（intrinsic value），就得到了<strong>信息增益率</strong>（gain ratio）:</p><script type="math/tex; mode=display">\text{Gain Ratio}(D, a)=\frac{\text{Gain}(D, a)}{\text{IV}(a)}</script><p>其中</p><script type="math/tex; mode=display">\text{IV}(a)=-\sum^{V}_{v=1}\frac{|D_v|}{|D|}\log{\frac{|D_v|}{|D|}}</script><p>C4.5 决策树生成算法采用了该思想，并做了一定优化：先从候选特征集合中，选择那些信息增益高于平均水平的特征，再从中选择信息增益率最高的。这是为了防止一上来就选择信息增益率最大的特征，导致算法滑向另一个极端：总是选取可取值数目较少的那些特征。</p><h3 id="2-4-基尼系数"><a href="#2-4-基尼系数" class="headerlink" title="2.4 基尼系数"></a>2.4 基尼系数</h3><p>除了使用信息熵作为衡量节点纯度的方法，我们还可以使用基尼值来衡量。数据集的<strong>基尼系数（Gini）</strong>可以定义为：</p><script type="math/tex; mode=display">\begin{align*}\text{Gini}(D) &= \sum^{|Y|}_{k=1}\sum_{k'\neq k}p_k p_{k'} \\               &= 1-\sum^{|Y|}_{k=1}p_k^{2}\end{align*}</script><p>直观来说，$Gini(D)$ 反映了从数据集 D 中随机抽取两个样本，其类别标签不一致的概率。$Gini(D)$ 越小，数据集的纯度越高。</p><p>则选择特征时，只需遍历特征并选择能够令节点的基尼系数最小的特征作为分裂特征即可。</p><script type="math/tex; mode=display">\text{Gini Index}(D, a)=\sum_{v=1}^{V}\frac{|D_v|}{|D|}\text{Gini}(D_v)</script><p>该思想被应用在 CART 算法中，CART决策树的全称为Classification and Regression Tree，可以应用于分类和回归。</p><h2 id="3-如何停止分裂？"><a href="#3-如何停止分裂？" class="headerlink" title="3. 如何停止分裂？"></a>3. 如何停止分裂？</h2><p>随着节点划分递归进行，决策树分支变得过多，甚至每个样本对应 1 个叶子。如果不对分裂过程加以干预，可能导致训练样本准确率 100%，但是实际预测时效果极差，这就是过拟合（overfitting）现象。</p><p>为了规避过拟合现象，我们可以提前终止分裂，或者在生成完毕的树结构上修剪部分分支。</p><h3 id="3-1-预剪枝（提前终于规则）："><a href="#3-1-预剪枝（提前终于规则）：" class="headerlink" title="3.1 预剪枝（提前终于规则）："></a>3.1 预剪枝（提前终于规则）：</h3><p>停止生长的方法有很多，比如可以当观察到的不纯性度量的增益低于某个确定的阈值时就停止扩展叶结点；当分裂后的准确率下降时停止分裂；或者干脆限制树的高度等。</p><p>通过这种方法，限制产出过于复杂的子树，减少了训练时间。在 Sklearn 中有具体的实现。</p><blockquote><p><strong>Sklearn 中的预剪枝操作</strong></p></blockquote><p>Sklearn 是 python 的一个机器学习工具包，可以很方便地实现各种机器学习算法。在 Sklearn 中，决策树有如下参数，控制预剪枝过程：</p><ol><li>max_depth， 最常用，限制树的最大深度。在样本量较少时很有用。</li><li>min_samples_leaf， 每个结点包含的最少样本量。如果分裂后，存在一个子叶，样本量过少，则没必要分裂。</li><li>min_samples_split， 每个节点包含的最少样本量。如果分裂前，节点包含过少的样本，则没必要分裂。和上个参数的含义相同。</li><li>max_features， 限制分裂时考虑的特征个数，超过限制个数的特征都会被舍弃。该参数比较暴力，使用频率较低。</li><li>min_impurity_decrease， 限制信息增益的大小，信息增益小于设定数值的分裂不会发生。</li></ol><h3 id="3-2-后剪枝："><a href="#3-2-后剪枝：" class="headerlink" title="3.2 后剪枝："></a>3.2 后剪枝：</h3><p>初始决策树按照最大规模生长，然后进行剪枝的步骤，按照自底向上的方式修剪完全增长然后修剪。</p><p>修剪有两种做法：</p><ol><li>用新的叶子节点替换子树，该叶结点的类标号由子树下记录中的多数类决定。子树替换</li><li>用子树中最常使用的分支代替子树。当模型不能再改进时终止剪枝步骤。子树提升</li></ol><p>后剪枝的泛化性能往往强于只使用预剪枝的决策树，但是后剪枝需要先产出完整的树再剪枝，时间和空间成本要更高。另外，Sklearn 中没有专门针对后剪枝的实现方法。</p><h2 id="4-如何处理连续、缺失值"><a href="#4-如何处理连续、缺失值" class="headerlink" title="4. 如何处理连续、缺失值"></a>4. 如何处理连续、缺失值</h2><p>决策树的一大优势是可以不加预处理地应对离散和连续两种输入，并能够灵活应对数据集中部分样本的特征缺失情况，这令决策树算法的适用场景十分宽广。</p><p>需要提前声明，不同决策树生成算法会采用不同的处理策略，下面举的例子如果没有单独声明，都是 C4.5 算法的处理策略。</p><h3 id="4-1-连续值如何划分？"><a href="#4-1-连续值如何划分？" class="headerlink" title="4.1 连续值如何划分？"></a>4.1 连续值如何划分？</h3><p>如果我们要以”身高“为特征进行分裂，但是“身高”是一个连续值，无法枚举所有情况，如何分裂呢？</p><p>转换一下思路，虽然“身高”无法枚举，但是数据集是可以遍历的。将该节点内的所有数据的身高取出来，按照大小排序，然后遍历所有潜在的分裂点，依次计算纯度即可将连续值划分为两段不连续的区间，取最高的纯度为区间分割点，进而分裂为两个不同的节点。</p><h3 id="4-2-如何处理特征缺失？"><a href="#4-2-如何处理特征缺失？" class="headerlink" title="4.2 如何处理特征缺失？"></a>4.2 如何处理特征缺失？</h3><p><img src="/2019/05/22/%E3%80%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95/数据缺失.png" alt></p><p>数据的特征缺失现象很普遍，常见的缺失数据处理方法有：</p><p><strong>1）抛弃缺失值</strong></p><p>抛弃极少量的缺失值的样本对决策树的创建影响不是太大。但是如果属性缺失值较多或是关键属性值缺失，创建的决策树将是不完全的，同时可能给用户造成知识上的大量错误信息，所以抛弃缺失值一般不采用。只有在数据库具有极少量的缺失值同时缺失值不是关键的属性值时，且为了加快创建决策树的速度，才采用抛弃属性缺失值的方式创建决策树。</p><p><strong>2）补充缺失值</strong></p><p>缺失值较少时按照我们上面的补充规则是可行的。但如果数据库的数据较大,缺失值较多(当然,这样获取的数据库在现实中使用的意义已不大,同时在信息获取方面基本不会出现这样的数据库),这样根据填充后的数据库创建的决策树可能和根据正确值创建的决策树有很大变化。</p><p><strong>3）概率化缺失值</strong></p><p>对缺失值的样本赋予该属性所有属性值的概率分布，即将缺失值按照其所在属性已知值的相对概率分布来创建决策树。</p><p>用系数F进行合理的修正计算的信息量，</p><p>F=数据库中缺失值所在的属性值样本数量去掉缺失值样本数量/数据库中样本数量的总和，</p><p>即F表示所给属性具有已知值样本的概率。</p><p>上述的缺失值处理算法是 C4.5 算法所采纳的，除了该方法之外，还有很多其他的方法：</p><ul><li>插值法（Imputation）： QUEST，CRUISE</li><li>替代法（Alternate/Surrogate Splits）：CART，CRUISE</li><li>缺失值单独分支（Missing value branch）：CHAID，GUIDE</li><li>概率权重（Probability weights）： C4.5</li></ul>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
      <tag>decision tree</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>牛奶可乐经济学读书笔记（不定期更新）</title>
    <link href="/2019/05/18/%E7%89%9B%E5%A5%B6%E5%8F%AF%E4%B9%90%E7%BB%8F%E6%B5%8E%E5%AD%A6%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8D%E5%AE%9A%E6%9C%9F%E6%9B%B4%E6%96%B0%EF%BC%89/"/>
    <url>/2019/05/18/%E7%89%9B%E5%A5%B6%E5%8F%AF%E4%B9%90%E7%BB%8F%E6%B5%8E%E5%AD%A6%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8D%E5%AE%9A%E6%9C%9F%E6%9B%B4%E6%96%B0%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>2019年5月16日</p><p>我对经济学比较感兴趣，毕竟我本身就是个穷人，我也希望能够高效利用我身上有限的资源。学习经济学能够对我的思维方式有很大改善。</p><p>我认为阅读一本能够给自己带来很多知识的书时，应当一步一步不要贪多。每次阅读都是美妙的经历，就好像在和读者对话一样。</p><p>这本《牛奶可乐经济学》一开始让我感到有点啰嗦，作者一直在讲他的授课经历，以及学生的反映。似乎欧美的书籍都会有这样的通病。我误以为这又是一本充满废话和鸡汤的科普书籍，便一页一页翻过去，翻到后来却感觉什么也没看进去。于是我从头沉下心来，一点一点阅读。</p><p>终于在睡前阅读完毕了第一章。我强忍着想要阅读第二章的好奇心，闭上眼睛总结学到的东西。我一直有这样的读书习惯。</p><p>对我来说，第一章教给我的并不是“沉没成本”，而是以下两点：</p><ol><li><p>学习一门知识，如果能把知识点穿成串，最好形成绘声绘色的故事，辅以生动的图画，那么这样的知识算是“易消化吸收”的知识。也就是说，知识叙述化、图像化，更利于知识的吸收（和分享）。由此可见，初学者就不要费力去找什么机器学习精华笔记、思维导图了同理，大牛们也别指望将自己的所谓极简笔记造福新手了，你的笔记越简短，越难被别人理解。如果在这个领域没有基础，根本不能被其他人吸收。</p></li><li><p>学习一个领域的知识，要靠不断地、创新性地应用于各种场景。重复是重要的，但是机械化的重复是没必要的，只有创新性的（即换一种形式）应用，才能代表你（现在）真正的掌握了。当然现在掌握了将来也要不断重复，要不就忘记了。</p></li></ol><p>突然对欧美的行文风格产生好感了:)</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>study method</tag>
      
      <tag>economics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python3中enumerate/zip等函数使用方法</title>
    <link href="/2019/05/18/Python3%E4%B8%ADenumerate-zip%E7%AD%89%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <url>/2019/05/18/Python3%E4%B8%ADenumerate-zip%E7%AD%89%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>最近正在研读《Python机器学习基础教程》（Introduction to Machine Learning with Python）这本书。书中的Python3代码、对于numpy、pandas、matplotlib以及scikit-learn库的使用都让人叹为观止。作为Python初学者，这本书不仅可以让人入门机器学习，更可以让人的Python技巧得到提升。</p><p>下面的代码使用sklearn自带数据集moon以及sklearn的随机森林模型构建由5棵树组成的随机森林，并利用matplotlib库可视化。</p><pre><code class="lang-python">import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport mglearn # 需要额外下载from sklearn.ensemble import RandomForestClassifierfrom sklearn.datasets import make_moonsfrom sklearn.model_selection import train_test_splitX, y = make_moons(n_samples=100, noise=0.25, random_state=3)X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,                                                     random_state=42)# 创建5棵树组成的随机森林forest = RandomForestClassifier(n_estimators=5, random_state=2)# 对训练集进行拟合forest.fit(X_train, y_train)# 生成两行三列的六张图，宽20高10fig, axes = plt.subplots(2, 3, figsize=(20, 10))</code></pre><p><code>forest.estimators_</code>是一个列表，保存五棵树的信息</p><pre><code class="lang-python">print(len(forest.estimators_))print(type(forest.estimators_[0]))5&lt;class &#39;sklearn.tree.tree.DecisionTreeClassifier&#39;&gt;</code></pre><p>下面的for遍历用法是我之前很少接触的，尤其是对于enumerate与zip的使用，在此记录下来。</p><pre><code class="lang-python">for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):    ax.set_title(&quot;Tree &#123;&#125;&quot;.format(i))    mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)mglearn.plots.plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1],                               alpha=.4)axes[-1, -1].set_title(&quot;Random Forest&quot;)mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)</code></pre><p>首先zip函数就像大型相亲现场，接受两个可迭代集合：一波男性和一波女性，将前一个集合中的元素与后一个集合中的元素一一配对，返回两两结合的对象，即返回一大群元素组成的集合，集合中元素都是一男一女配对。配对方法就是粗暴的第i个男生-第i个女生，如果有男生or女生多了咋办？zip不管配不上对的元素，只挑选配对成功的组合。</p><p>经过zip函数处理，axes里面的六张图配上了五棵树，最后一张图我们留到最后处理。</p><p>接下来是enumerate，enumerate接受可迭代对象，不仅仅输出对象元素，还附带输出该元素所在的位置。enumerate本身就是“枚举”的意思嘛。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>矩阵求导总结</title>
    <link href="/2019/05/15/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E6%80%BB%E7%BB%93/"/>
    <url>/2019/05/15/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>我实在是不会把hexo中的latex开关打开了，大家如果想看公式，可以使用markdown编辑器，复制本文后在本地查看。本文是由本人汇编自网上的资料整理而成，为的是记忆和分享。</p><p>然后来建立运算法则。回想遇到较复杂的一元函数如$f = \log(2+\sin x)e^{\sqrt{x}}$，我们是如何求导的呢？通常不是从定义开始求极限，而是先建立了初等函数求导和四则运算、复合等法则，再来运用这些法则。故而，我们来创立常用的矩阵微分的运算法则：</p><p>加减法：$d(X\pm Y) = dX \pm dY$；矩阵乘法：$d(XY) = (dX)Y + X dY $；转置：$d(X^T) = (dX)^T$；迹：$d\text{tr}(X) = \text{tr}(dX)$。<br>逆：$dX^{-1} = -X^{-1}dX X^{-1}$。此式可在$XX^{-1}=I$两侧求微分来证明。<br>行列式：$d|X| = \text{tr}(X^$表示X的伴随矩阵，在X可逆时又可以写作$d|X|= |X|\text{tr}(X^{-1}dX)$。此式可用Laplace展开来证明，详见张贤达《矩阵分析与应用》第279页。<br>逐元素乘法：$d(X\odot Y) = dX\odot Y + X\odot dY$，$\odot$表示尺寸相同的矩阵X,Y逐元素相乘。<br>逐元素函数：$d\sigma(X) = \sigma’(X)\odot dX$ ，$\sigma(X) = \left[\sigma(X<em>{ij})\right]$是逐元素标量函数运算，$ \sigma’(X)=[\sigma’(X</em>{ij})]$是逐元素求导数。举个例子，<script type="math/tex">X=\left[\begin{matrix}x_{11} & x_{12} \\ x_{21} & x_{22}\end{matrix}\right], d \sin(X) = \left[\begin{matrix}\cos x_{11} dx_{11} & \cos x_{12} d x_{12}\\ \cos x_{21} d x_{21}& \cos x_{22} dx_{22}\end{matrix}\right] = \cos(X)\odot dX</script>。</p><p>我们试图利用矩阵导数与微分的联系$df = \text{tr}\left(\frac{\partial f}{\partial X}^T dX\right)$ ，在求出左侧的微分df后，该如何写成右侧的形式并得到导数呢？这需要一些迹技巧(trace trick)：</p><p>标量套上迹：$a = \text{tr}(a)$<br>转置：$\mathrm{tr}(A^T) = \mathrm{tr}(A)$。<br>线性：$\text{tr}(A\pm B) = \text{tr}(A)\pm \text{tr}(B)$。<br>矩阵乘法交换：$\text{tr}(AB) = \text{tr}(BA)$，其中$A$与$B^T$尺寸相同。两侧都等于$\sum<em>{i,j}A</em>{ij}B<em>{ji}$。<br>矩阵乘法/逐元素乘法交换：$\text{tr}(A^T(B\odot C)) = \text{tr}((A\odot B)^TC)$，其中$A, B, C$尺寸相同。两侧都等于$\sum</em>{i,j}A<em>{ij}B</em>{ij}C_{ij}$。</p><p>观察一下可以断言，若标量函数f是矩阵X经加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对f求微分，再使用迹技巧给df套上迹并将其它项交换至dX左侧，对照导数与微分的联系$df = \text{tr}\left(\frac{\partial f}{\partial X}^T dX\right)$，即能得到导数。</p><p>特别地，若矩阵退化为向量，对照导数与微分的联系$df = \frac{\partial f}{\partial \boldsymbol{x}}^T d\boldsymbol{x}$ ，即能得到导数。</p><p>在建立法则的最后，来谈一谈复合：假设已求得$\frac{\partial f}{\partial Y}$，而Y是X的函数，如何求$\frac{\partial f}{\partial X}$呢？在微积分中有标量求导的链式法则$\frac{\partial f}{\partial x} = \frac{\partial f}{\partial y} \frac{\partial y}{\partial x}$，但这里我们不能随意沿用标量的链式法则，因为矩阵对矩阵的导数$\frac{\partial Y}{\partial X}$截至目前仍是未定义的。于是我们继续追本溯源，链式法则是从何而来？源头仍然是微分。我们直接从微分入手建立复合法则：先写出$df = \text{tr}\left(\frac{\partial f}{\partial Y}^T dY\right)$，再将dY用dX表示出来代入，并使用迹技巧将其他项交换至dX左侧，即可得到$\frac{\partial f}{\partial X}$。</p><p>最常见的情形是$Y = AXB$，此时 <script type="math/tex">df = \text{tr}\left(\frac{\partial f}{\partial Y}^T dY\right) = \text{tr}\left(\frac{\partial f}{\partial Y}^T AdXB\right) =  \text{tr}\left(B\frac{\partial f}{\partial Y}^T AdX\right) = \text{tr}\left((A^T\frac{\partial f}{\partial Y}B^T)^T dX\right)</script> ，可得到<script type="math/tex">\frac{\partial f}{\partial X}=A^T\frac{\partial f}{\partial Y}B^T</script>。注意这里$dY = (dA)XB + AdXB + AXdB = AdXB$，由于A,B是常量，$dA=0,dB=0$，以及我们使用矩阵乘法交换的迹技巧交换了$\frac{\partial f}{\partial Y}^T AdX$与$B$。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
      <category>转载</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>math</tag>
      
      <tag>matrix analysis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229-note-4-生成学习算法</title>
    <link href="/2019/05/06/CS229-note-4-%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    <url>/2019/05/06/CS229-note-4-%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p><a href="https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/Markdown/cs229-notes2.md">转载自@飞龙</a></p><div class="table-container"><table><thead><tr><th>相关链接</th></tr></thead><tbody><tr><td><a href="https://github.com/Kivy-CN/Stanford-CS-229-CN">Github 地址</a></td></tr><tr><td><a href="https://zhuanlan.zhihu.com/MachineLearn">知乎专栏</a></td></tr><tr><td><a href="http://cs229.stanford.edu/">斯坦福大学 CS229 课程网站</a></td></tr><tr><td><a href="http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html">网易公开课中文字幕视频</a></td></tr></tbody></table></div><h3 id="第四部分-生成学习算法-Generative-Learning-algorithms"><a href="#第四部分-生成学习算法-Generative-Learning-algorithms" class="headerlink" title="第四部分 生成学习算法(Generative Learning algorithms)"></a>第四部分 生成学习算法(Generative Learning algorithms)</h3><p>目前为止，我们讲过的学习算法的模型都是$p (y|x;\theta)$，也就是给定 $x$ 下 $y$ 的条件分布，以  $\theta$  为参数。例如，逻辑回归中就是以 $h_\theta(x) = g(\theta^T x)$ 作为 $p (y|x;\theta)$ 的模型，这里的 $g$ 是一个 $S$型函数（sigmoid function）。接下来，咱们要讲一下一种不同类型的学习算法。</p><p>设想有这样一种分类问题，我们要学习基于一个动物的某个特征来辨别它是大象$(y=1)$还是小狗$(y=0)$。给定一个训练集，用逻辑回归或者基础版的<strong>感知器算法（perceptron algorithm）</strong> 这样的一个算法能找到一条直线，作为区分开大象和小狗的边界。接下来，要辨别一个新的动物是大象还是小狗，程序就要检查这个新动物的值落到了划分出来的哪个区域中，然后根据所落到的区域来给出预测。</p><p>还有另外一种方法。首先，观察大象，然后我们针对大象的样子来进行建模。然后，再观察小狗，针对小狗的样子另外建立一个模型。最后要判断一种新动物归属哪一类，我们可以把新动物分别用大象和小狗的模型来进比对，看看新动物更接近哪个训练集中已有的模型。</p><p>例如逻辑回归之类的直接试图建立 $p(y|x)$的算法，以及感知器算法（perceptron algorithm）等直接用投图（mappings directly）的思路来判断对应 $X$ 的值落到了 ${0, 1}$ 中哪个区域的算法，这些都叫<strong>判别式学习算法（discriminative learning algorithms）。</strong> 和之前的这些判别式算法不同，下面我们要讲的新算法是对 $p(x|y)$ 和 $p(y)$来进行建模。这类算法叫<strong>做生成学习算法（generative learning algorithms）</strong>。例如如果 $y$ 用来表示一个样例是  小狗 $(0)$ 或者  大象 $(1)$，那么$p(x|y = 0)$就是对小狗特征分布的建模，而$p(x|y = 1)$就是对大象特征分布的建模。</p><p>对 $p(y)$ (通常称为<strong>class priors</strong><code>译者注：这里没有找到合适的词进行翻译</code>) 和$p(x|y)$ 进行建模之后，我们的算法就是用<strong>贝叶斯规则（Bayes rule）</strong> 来推导对应给定 $x$ 下 $y$ 的<strong>后验分布（posterior distribution）</strong>：</p><script type="math/tex; mode=display">p(y|x)=\frac{p(x|y)p(y)}{p(x)}</script><p>这里的<strong>分母（denominator）</strong> 为：$p(x) = p(x|y = 1)p(y = 1) + p(x|y = 0)p(y = 0)$（这个等式关系可以根据概率的标准性质来推导验证<code>译者注：其实就是条件概率</code>），这样接下来就可以把它表示成我们熟悉的 $p(x|y)$和 $p(y)$ 的形式了。实际上如果我们计算$p(y|x)$ 来进行预测，那就并不需要去计算这个分母，因为有下面的等式关系：</p><script type="math/tex; mode=display">\begin{aligned}\arg \max_y p(y|x) & =\arg \max_y \frac{p(x|y)p(y)}{p(x)}\\&= \arg \max_y p(x|y)p(y)\end{aligned}</script><h4 id="1-高斯判别分析（Gaussian-discriminant-analysis）"><a href="#1-高斯判别分析（Gaussian-discriminant-analysis）" class="headerlink" title="1 高斯判别分析（Gaussian discriminant analysis）"></a>1 高斯判别分析（Gaussian discriminant analysis）</h4><p>咱们要学的第一个生成学习算法就是<strong>高斯判别分析（Gaussian discriminant analysis</strong> ，缩写为GDA。<code>译者注：高斯真棒！</code>）在这个模型里面，我们<strong>假设 $p(x|y)$是一个多元正态分布。</strong> 所以首先咱们简单讲一下多元正态分布的一些特点，然后再继续讲 GDA 高斯判别分析模型。</p><h5 id="1-1-多元正态分布（multivariate-normal-distribution）"><a href="#1-1-多元正态分布（multivariate-normal-distribution）" class="headerlink" title="1.1 多元正态分布（multivariate normal distribution）"></a>1.1 多元正态分布（multivariate normal distribution）</h5><p>$n$维多元正态分布，也叫做多变量高斯分布，参数为一个$n$维 <strong>均值向量</strong> $\mu \in  R^n $，以及一个 <strong>协方差矩阵</strong> $\Sigma \in  R^{n\times n}$，其中$\Sigma \geq 0$ 是一个对称（symmetric）的半正定（positive semi-definite）矩阵。当然也可以写成”$N (\mu, \Sigma)$” 的分布形式，密度（density）函数为：</p><script type="math/tex; mode=display">p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))</script><p>在上面的等式中，”$|\Sigma|$”的意思是矩阵$\Sigma$的行列式（determinant）。对于一个在 $N(\mu,\Sigma)$分布中的随机变量 $X$ ，其平均值（跟正态分布里面差不多，所以并不意外）就是 $\mu$ 了：</p><script type="math/tex; mode=display">E[X]=\int_x xp(x;\mu,\Sigma)dx=\mu</script><p>随机变量$Z$是一个有值的向量（vector-valued random variable），$Z$ 的 <strong>协方差（covariance）</strong> 的定义是：$Cov(Z) = E[(Z-E[Z])(Z-E[Z])^T ]$。这是对实数随机变量的方差（variance）这一概念的泛化扩展。这个协方差还可以定义成$Cov(Z) = E[ZZ^T]-(E[Z])(E[Z])^T$（你可以自己证明一下这两个定义实际上是等价的。）如果 $X$ 是一个多变量正态分布，即 $X \sim N (\mu, \Sigma)$，则有：</p><script type="math/tex; mode=display">Cov(X)=\Sigma</script><p>下面这些样例是一些高斯分布的密度图，如下图所示：</p><p><img src="https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f1.png" alt></p><p>最左边的图，展示的是一个均值为$0$（实际上是一个$2\times 1$ 的零向量）的高斯分布，协方差矩阵就是$\Sigma = I$ （一个 $2\times 2$的单位矩阵，identity matrix）。这种均值为$0$ 并且协方差矩阵为单位矩阵的高斯分布也叫做<strong>标准正态分布。</strong> 中间的图中展示的是均值为$0$而协方差矩阵是$0.6I$ 的高斯分布的概率密度函数；最右边的展示的是协方差矩阵$\Sigma = 2I$的高斯分布的概率密度函数。从这几个图可以看出，随着协方差矩阵$\Sigma$变大，高斯分布的形态就变得更宽平（spread-out），而如果协方差矩阵$\Sigma$变小，分布就会更加集中（compressed）。</p><p>来看一下更多的样例：</p><p><img src="https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f2.png" alt></p><p>上面这几个图展示的是均值为$0$，但协方差矩阵各不相同的高斯分布，其中的协方差矩阵依次如下所示：</p><script type="math/tex; mode=display">\Sigma =\begin{bmatrix} 1 & 0 \\ 0 & 1 \\ \end{bmatrix};\Sigma =\begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \\ \end{bmatrix};\Sigma =\begin{bmatrix} 1 & 0.8 \\ 0.8 & 1 \\ \end{bmatrix}</script><p>第一幅图还跟之前的标准正态分布的样子很相似，然后我们发现随着增大协方差矩阵$\Sigma$ 的反对角线（off-diagonal）的值，密度图像开始朝着  45° 方向 (也就是 $x_1 = x_2$ 所在的方向)逐渐压缩（compressed）。  看一下三个同样分布密度图的轮廓图（contours）能看得更明显：</p><p><img src="https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f3.png" alt></p><p>下面的是另外一组样例，调整了协方差矩阵$\Sigma$:</p><script type="math/tex; mode=display">\Sigma =\begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \\ \end{bmatrix};\Sigma =\begin{bmatrix} 1 & 0.8 \\ 0.8 & 1 \\ \end{bmatrix}\Sigma =\begin{bmatrix} 3 & 0.8 \\ 0.8 & 1 \\ \end{bmatrix};</script><p>上面这三个图像对应的协方差矩阵分别如下所示：</p><p><img src="https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f4.png" alt></p><p>从最左边的到中间<code>译者注：注意，左边和中间的这两个协方差矩阵中，右上和左下的元素都是负值！</code>很明显随着协方差矩阵中右上左下这个对角线方向元素的值的降低，图像还是又被压扁了（compressed），只是方向是反方向的。最后，随着我们修改参数，通常生成的轮廓图（contours）都是椭圆（最右边的图就是一个例子）。</p><p>再举一些例子，固定协方差矩阵为单位矩阵，即$\Sigma = I$，然后调整均值$\mu$，我们就可以让密度图像随着均值而移动：</p><p><img src="https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f5.png" alt></p><p>上面的图像中协方差矩阵都是单位矩阵，即 $\Sigma = I$，对应的均值$\mu$如下所示：</p><script type="math/tex; mode=display">\mu =\begin{bmatrix} 1 \\ 0 \\ \end{bmatrix};\mu =\begin{bmatrix} -0.5 \\ 0 \\ \end{bmatrix};\mu =\begin{bmatrix} -1 \\ -1.5 \\ \end{bmatrix};</script><h5 id="1-2-高斯判别分析模型（Gaussian-Discriminant-Analysis-model）"><a href="#1-2-高斯判别分析模型（Gaussian-Discriminant-Analysis-model）" class="headerlink" title="1.2 高斯判别分析模型（Gaussian Discriminant Analysis model）"></a>1.2 高斯判别分析模型（Gaussian Discriminant Analysis model）</h5><p>假如我们有一个分类问题，其中输入特征 $x$ 是一系列的连续随机变量（continuous-valued random variables），那就可以使用高斯判别分析（Gaussian Discriminant Analysis ，缩写为 GDA）模型，其中对 $p(x|y)$用多元正态分布来进行建模。这个模型为：</p><script type="math/tex; mode=display">\begin{aligned}y & \sim Bernoulli(\phi)\\x|y = 0 & \sim N(\mu_o,\Sigma)\\x|y = 1 & \sim N(\mu_1,\Sigma)\\\end{aligned}</script><p>分布写出来的具体形式如下：</p><script type="math/tex; mode=display">\begin{aligned}p(y) & =\phi^y (1-\phi)^{1-y}\\p(x|y=0) & = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} exp ( - \frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)  )\\p(x|y=1) & = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} exp ( - \frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)  )\\\end{aligned}</script><p>在上面的等式中，模型的参数包括$\phi, \Sigma, \mu_0 和 \mu_1$。（要注意，虽然这里有两个不同方向的均值向量$\mu_0$ 和 $\mu_1$，针对这个模型还是一般只是用一个协方差矩阵$\Sigma$。）取对数的似然函数（log-likelihood）如下所示：</p><script type="math/tex; mode=display">\begin{aligned}l(\phi,\mu_0,\mu_1,\Sigma) &= \log \prod^m_{i=1}p(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma)\\&= \log \prod^m_{i=1}p(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)\\\end{aligned}</script><p>通过使 $l$ 取得最大值，找到对应的参数组合，然后就能找到该参数组合对应的最大似然估计，如下所示（参考习题集1）：</p><script type="math/tex; mode=display">\begin{aligned}\phi & = \frac {1}{m} \sum^m_{i=1}1\{y^{(i)}=1\}\\\mu_0 & = \frac{\sum^m_{i=1}1\{y^{(i)}=0\}x^{(i)}}{\sum^m_{i=1}1\{y^{(i)}=0\}}\\\mu_1 & = \frac{\sum^m_{i=1}1\{y^{(i)}=1\}x^{(i)}}{\sum^m_{i=1}1\{y^{(i)}=1\}}\\\Sigma & = \frac{1}{m}\sum^m_{i=1}(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T\\\end{aligned}</script><p>用图形化的方式来表述，这个算法可以按照下面的图示所表示：</p><p><img src="https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f6.png" alt></p><p>图中展示的点就是训练数据集，图中的两个高斯分布就是针对两类数据各自进行的拟合。要注意这两个高斯分布的轮廓图有同样的形状和拉伸方向，这是因为他们都有同样的协方差矩阵$\Sigma$，但他们有不同的均值$\mu_0$ 和 $\mu_1$ 。此外，图中的直线给出了$p (y = 1|x) = 0.5$ 这条边界线。在这条边界的一侧，我们预测 $y = 1$是最可能的结果，而另一侧，就预测 $y = 0$是最可能的结果。</p><h5 id="1-3-讨论：高斯判别分析（GDA）与逻辑回归（logistic-regression）"><a href="#1-3-讨论：高斯判别分析（GDA）与逻辑回归（logistic-regression）" class="headerlink" title="1.3 讨论：高斯判别分析（GDA）与逻辑回归（logistic regression）"></a>1.3 讨论：高斯判别分析（GDA）与逻辑回归（logistic regression）</h5><p>高斯判别分析模型与逻辑回归有很有趣的相关性。如果我们把变量（quantity）$p (y = 1|x; \phi, \mu_0, \mu_1, \Sigma)$ 作为一个 $x$ 的函数，就会发现可以用如下的形式来表达：</p><script type="math/tex; mode=display">p(y=1|x;\phi,\Sigma,\mu_0,\mu_1)=\frac 1 {1+exp(-\theta^Tx)}</script><p>其中的  $\theta$  是对$\phi$, $\Sigma$, $\mu_0$, $\mu_1$的某种函数。这就是逻辑回归（也是一种判别分析算法）用来对$p (y = 1|x)$ 建模的形式。</p><blockquote><p>注：上面这里用到了一种转换，就是重新对$x^{(i)}$向量进行了定义，在右手侧（right-hand-side）增加了一个额外的坐标$x_0^{(i)} = 1$，然后使之成为了一个 $n+1$维的向量；具体内容参考习题集1。</p></blockquote><p>这两个模型中什么时候该选哪一个呢？一般来说，高斯判别分析（GDA）和逻辑回归，对同一个训练集，可能给出的判别曲线是不一样的。哪一个更好些呢？</p><p>我们刚刚已经表明，如果$p(x|y)$是一个多变量的高斯分布（且具有一个共享的协方差矩阵$\Sigma$），那么$p(y|x)$则必然符合一个逻辑函数（logistic function）。然而，反过来，这个命题是不成立的。例如假如$p(y|x)$是一个逻辑函数，这并不能保证$p(x|y)$一定是一个多变量的高斯分布。这就表明<strong>高斯判别模型能比逻辑回归对数据进行更强的建模和假设（stronger modeling assumptions）。</strong> 这也就意味着，<strong>在这两种模型假设都可用的时候，高斯判别分析法去拟合数据是更好的，是一个更好的模型。</strong> 尤其当$p(x|y)$已经确定是一个高斯分布（有共享的协方差矩阵$\Sigma$），那么高斯判别分析是<strong>渐进有效的（asymptotically efficient）。</strong> 实际上，这也意味着，在面对非常大的训练集（训练样本规模 $m $特别大）的时候，严格来说，可能就没有什么别的算法能比高斯判别分析更好（比如考虑到对 $p(y|x)$估计的准确度等等）。所以在这种情况下就表明，高斯判别分析（GDA）是一个比逻辑回归更好的算法；再扩展一下，即便对于小规模的训练集，我们最终也会发现高斯判别分析（GDA）是更好的。</p><p>奈何事有正反，由于逻辑回归做出的假设要明显更弱一些（significantly weaker），所以因此逻辑回归给出的判断鲁棒性（robust）也更强，同时也对错误的建模假设不那么敏感。有很多不同的假设集合都能够将$p(y|x)$引向逻辑回归函数。例如，如果$x|y = 0\sim Poisson(\lambda_0)$ 是一个泊松分布，而$x|y = 1\sim Poisson(\lambda_1)$也是一个泊松分布，那么$p(y|x)$也将是适合逻辑回归的（logistic）。逻辑回归也适用于这类的泊松分布的数据。但对这样的数据，如果我们强行使用高斯判别分析（GDA），然后用高斯分布来拟合这些非高斯数据，那么结果的可预测性就会降低，而且GDA这种方法也许可行，也有可能是不能用。</p><p>总结一下也就是：高斯判别分析方法（GDA）能够建立更强的模型假设，并且在数据利用上更加有效（比如说，需要更少的训练集就能有”还不错的”效果），当然前提是模型假设争取或者至少接近正确。逻辑回归建立的假设更弱，因此对于偏离的模型假设来说更加鲁棒（robust）。然而，如果训练集数据的确是非高斯分布的（non-Gaussian），而且是有限的大规模数据（in the limit of large datasets），那么逻辑回归几乎总是比GDA要更好的。因此，在实际中，逻辑回归的使用频率要比GDA高得多。（关于判别和生成模型的对比的相关讨论也适用于我们下面要讲的朴素贝叶斯算法（Naive Bayes），但朴素贝叶斯算法还是被认为是一个非常优秀也非常流行的分类算法。）</p><h4 id="2-朴素贝叶斯法（Naive-Bayes）"><a href="#2-朴素贝叶斯法（Naive-Bayes）" class="headerlink" title="2 朴素贝叶斯法（Naive Bayes）"></a>2 朴素贝叶斯法（Naive Bayes）</h4><p>在高斯判别分析（GDA）方法中，特征向量 $x$ 是连续的，值为实数的向量。下面我们要讲的是当 $x_i$ 是离散值的时候来使用的另外一种学习算法。</p><p>下面就来继续看一个之前见过的样例，来尝试建立一个邮件筛选器，使用机器学习的方法。这回咱们要来对邮件信息进行分类，来判断是否为商业广告邮件（就是垃圾邮件），还是非垃圾邮件。在学会了怎么实现之后，我们就可以让邮件阅读器能够自动对垃圾信息进行过滤，或者单独把这些垃圾邮件放进一个单独的文件夹中。对邮件进行分类是一个案例，属于文本分类这一更广泛问题集合。</p><p>假设我们有了一个训练集（也就是一堆已经标好了是否为垃圾邮件的邮件）。要构建垃圾邮件分选器，咱们先要开始确定用来描述一封邮件的特征$x_i$有哪些。</p><p>我们将用一个特征向量来表示一封邮件，这个向量的长度等于字典中单词的个数。如果邮件中包含了字典中的第 $i$ 个单词，那么就令 $x_i = 1$；反之则$x_i = 0$。例如下面这个向量：</p><script type="math/tex; mode=display">x=\begin{bmatrix}1\\0\\0\\\vdots \\1\\ \vdots \\0\end{bmatrix} \begin{matrix}\text{a}\\ \text{aardvark}\\ \text{aardwolf}\\ \vdots\\ \text{buy}\\ \vdots\\ \text{zygmurgy}\\ \end{matrix}</script><p>就用来表示一个邮件，其中包含了两个单词 “a” 和 “buy”，但没有单词 “aardvark”， “aardwolf” 或者 “zymurgy”  。这个单词集合编码整理成的特征向量也成为<strong>词汇表（vocabulary,），</strong> 所以特征向量 $x$ 的维度就等于词汇表的长度。</p><blockquote><p>注：实际应用中并不需要遍历整个英语词典来组成所有英语单词的列表，实践中更常用的方法是遍历一下训练集，然后把出现过一次以上的单词才编码成特征向量。这样做除了能够降低模型中单词表的长度之外，还能够降低运算量和空间占用，此外还有一个好处就是能够包含一些你的邮件中出现了而词典中没有的单词，比如本课程的缩写CS229。有时候（比如在作业里面），还要排除一些特别高频率的词汇，比如像冠词the，介词of 和and 等等；这些高频率但是没有具体意义的虚词也叫做stop words，因为很多文档中都要有这些词，用它们也基本不能用来判定一个邮件是否为垃圾邮件。</p></blockquote><p>选好了特征向量了，接下来就是建立一个生成模型（generative model）。所以我们必须对$p(x|y)$进行建模。但是，假如我们的单词有五万个词，则特征向量$x \in  {0, 1}^{50000}$ （即 $x$是一个 $50000$ 维的向量，其值是$0$或者$1$），如果我们要对这样的 $x$进行多项式分布的建模，那么就可能有$2^{50000}$ 种可能的输出，然后就要用一个 $(2^{50000}-1)$维的参数向量。这样参数明显太多了。</p><p>要给$p(x|y)$建模，先来做一个非常强的假设。我们<strong>假设特征向量$x_i$ 对于给定的 $y$ 是独立的。</strong> 这个假设也叫做<strong>朴素贝叶斯假设（Naive Bayes ，NB assumption），</strong> 基于此假设衍生的算法也就叫做<strong>朴素贝叶斯分类器（Naive Bayes classifier）。</strong> 例如，如果 $y = 1$ 意味着一个邮件是垃圾邮件；然后其中”buy” 是第$2087$个单词，而 “price”是第$39831$个单词；那么接下来我们就假设，如果我告诉你 $y = 1$，也就是说某一个特定的邮件是垃圾邮件，那么对于$x<em>{2087}$ （也就是单词 buy 是否出现在邮件里）的了解并不会影响你对$x</em>{39831}$ （单词price出现的位置）的采信值。更正规一点，可以写成 $p(x<em>{2087}|y) = p(x</em>{2087}|y, x<em>{39831})$。（要注意这个并不是说$x</em>{2087}$ 和 $x<em>{39831}$这两个特征是独立的，那样就变成了$p(x</em>{2087}) = p(x<em>{2087}|x</em>{39831})$，我们这里是说在给定了 $y$ 的这样一个条件下，二者才是有条件的独立。）</p><p>然后我们就得到了等式：</p><script type="math/tex; mode=display">\begin{aligned}p(x_1, ..., x_{50000}|y) & = p(x_1|y)p(x_2|y,x_1)p(x_3|y,x_1,x_2) ... p(x_{50000}|y,x_1,x_2,...,x_{49999})\\& = p(x_1|y)p(x_2|y)p(x_3|y) ... p(x_{50000}|y)\\& = \prod^n_{i=1}p(x_i|y)\\\end{aligned}</script><p>第一行的等式就是简单地来自概率的基本性质，第二个等式则使用了朴素贝叶斯假设。这里要注意，朴素贝叶斯假设也是一个很强的假设，产生的这个算法可以适用于很多种问题。</p><p>我们这个模型的参数为 $\phi<em>{i|y=1} = p (x_i = 1|y = 1), \phi</em>{i|y=0} = p (x_i = 1|y = 0)$, 而 $\phi_y = p (y = 1)$。和以往一样，给定一个训练集${(x^{(i)},y^{(i)}); i = 1, …, m}$，就可以写出下面的联合似然函数：</p><script type="math/tex; mode=display">\mathcal{L}(\phi_y,\phi_{j|y=0},\phi_{j|y=1})=\prod^m_{i=1}p(x^{(i)},y^{(i)})</script><p>找到使联合似然函数取得最大值的对应参数组合 $\phi<em>y , \phi</em>{i|y=0} 和 \phi_{i|y=1}$ 就给出了最大似然估计：</p><script type="math/tex; mode=display">\begin{aligned}\phi_{j|y=1} &=\frac{\sum^m_{i=1}1\{x_j^{(i)} =1 \wedge y^{(i)} =1\} }{\sum^m_{i=1}1\{y^{(i)} =1\}} \\\phi_{j|y=0} &= \frac{\sum^m_{i=1}1\{x_j^{(i)} =1 \wedge y^{(i)} =0\} }{\sum^m_{i=1}1\{y^{(i)} =0\}} \\\phi_{y} &= \frac{\sum^m_{i=1}1\{y^{(i)} =1\}}{m}\\\end{aligned}</script><p>在上面的等式中，”$\wedge$(and)”这个符号的意思是逻辑”和”。这些参数有一个非常自然的解释。例如 $\phi_{j|y=1}$ 正是单词 $j$ 出现的邮件中垃圾邮件所占 $(y = 1)$ 的比例。</p><p>拟合好了全部这些参数之后，要对一个新样本的特征向量 $x$ 进行预测，只要进行如下的简单地计算：</p><script type="math/tex; mode=display">\begin{aligned}p(y=1|x)&=  \frac{p(x|y=1)p(y=1)}{p(x)}\\&= \frac{(\prod^n_{i=1}p(x_i|y=1))p(y=1)}{(\prod^n_{i=1}p(x_i|y=1))p(y=1)+  (\prod^n_{i=1}p(x_i|y=0))p(y=0)}  \\\end{aligned}</script><p>然后选择有最高后验概率的概率。</p><p>最后我们要注意，刚刚我们对朴素贝叶斯算法的使用中，特征向量 $x_i$ 都是二值化的，其实特征向量也可以是多个离散值，比如${1, 2, …, k_i}$这样也都是可以的。这时候只需要把对$p(x_i|y)$ 的建模从伯努利分布改成多项式分布。实际上，即便一些原始的输入值是连续值（比如我们第一个案例中的房屋面积），也可以转换成一个小规模的离散值的集合，然后再使用朴素贝叶斯方法。例如，如果我们用特征向量 $x_i$ 来表示住房面积，那么就可以按照下面所示的方法来对这一变量进行离散化：</p><div class="table-container"><table><thead><tr><th style="text-align:center">居住面积</th><th style="text-align:center">$&lt;400$</th><th style="text-align:center">$400-800$</th><th style="text-align:center">$800-1200$</th><th style="text-align:center">$1200-1600$</th><th style="text-align:center">$&gt;1600$</th></tr></thead><tbody><tr><td style="text-align:center">离散值 $x_i$</td><td style="text-align:center">$1$</td><td style="text-align:center">$2$</td><td style="text-align:center">$3$</td><td style="text-align:center">$4$</td><td style="text-align:center">$5$</td></tr></tbody></table></div><p>这样，对于一个面积为 $890$ 平方英尺的房屋，就可以根据上面这个集合中对应的值来把特征向量的这一项的$x_i$值设置为$3$。然后就可以用朴素贝叶斯算法，并且将$p(x_i|y)$作为多项式分布来进行建模，就都跟前面讲过的内容一样了。当原生的连续值的属性不太容易用一个多元正态分布来进行建模的时候，将其特征向量离散化然后使用朴素贝叶斯法（NB）来替代高斯判别分析法（GDA），通常能形成一个更好的分类器。</p><h5 id="2-1-拉普拉斯平滑（Laplace-smoothing）"><a href="#2-1-拉普拉斯平滑（Laplace-smoothing）" class="headerlink" title="2.1 拉普拉斯平滑（Laplace smoothing）"></a>2.1 拉普拉斯平滑（Laplace smoothing）</h5><p>刚刚讲过的朴素贝叶斯算法能够解决很多问题了，但还能对这种方法进行一点小调整来进一步提高效果，尤其是应对文本分类的情况。我们来简要讨论一下一个算法当前状态的一个问题，然后在讲一下如何解决这个问题。</p><p>还是考虑垃圾邮件分类的过程，设想你学完了CS229的课程，然后做了很棒的研究项目，之后你决定在$2003$年$6$月<code>译者注：作者这讲义一定写得很早</code>把自己的作品投稿到NIPS会议，这个NIPS是机器学习领域的一个顶级会议，递交论文的截止日期一般是六月末到七月初。你通过邮件来对这个会议进行了讨论，然后你也开始收到带有 nips 四个字母的信息。但这个是你第一个NIPS论文，而在此之前，你从来没有接到过任何带有 nips 这个单词的邮件；尤其重要的是，nips 这个单词就从来都没有出现在你的垃圾/正常邮件训练集里面。加入这个 nips 是你字典中的第$35000$个单词那么你的朴素贝叶斯垃圾邮件筛选器就要对参数$\phi_{35000|y}$ 进行最大似然估计，如下所示：</p><script type="math/tex; mode=display">\begin{aligned}\phi_{35000|y=1} &=  \frac{\sum^m_{i=1}1\{x^{(i)}_{35000}=1 \wedge y^{(i)}=1  \}}{\sum^m_{i=1}1\{y^{(i)}=0\}}  &=0 \\\phi_{35000|y=0} &=  \frac{\sum^m_{i=1}1\{x^{(i)}_{35000}=1 \wedge y^{(i)}=0  \}}{\sum^m_{i=1}1\{y^{(i)}=0\}}  &=0 \\\end{aligned}</script><p>也就是说，因为之前程序从来没有在别的垃圾邮件或者正常邮件的训练样本中看到过 nips 这个词，所以它就认为看到这个词出现在这两种邮件中的概率都是$0$。因此当要决定一个包含 nips 这个单词的邮件是否为垃圾邮件的时候，他就检验这个类的后验概率，然后得到了：</p><script type="math/tex; mode=display">\begin{aligned}p(y=1|x) &= \frac{ \prod^n_{i=1} p(x_i|y=1)p(y=1) }   {\prod^n_{i=1} p(x_i|y=1)p(y=1) +\prod^n_{i=1} p(x_i|y=1)p(y=0)    }\\&= \frac00\\\end{aligned}</script><p>这是因为对于”  $\prod^n<em>{i=1} p(x_i|y)$”中包含了$p(x</em>{35000}|y) = 0$的都加了起来，也就还是$0$。所以我们的算法得到的就是 $\frac00$，也就是不知道该做出怎么样的预测了。</p><p>然后进一步拓展一下这个问题，统计学上来说，只因为你在自己以前的有限的训练数据集中没见到过一件事，就估计这个事件的概率为零，这明显不是个好主意。假设问题是估计一个多项式随机变量 $z$ ，其取值范围在${1,…, k}$之内。接下来就可以用$\phi_i = p (z = i)$ 来作为多项式参数。给定一个 $m$ 个独立观测${z^{(1)}, …, z^{(m)}}$ 组成的集合，然后最大似然估计的形式如下：</p><script type="math/tex; mode=display">\phi_j=\frac{\sum^m_{i=1}1\{z^{(i)}=j\}}m</script><p>正如咱们之前见到的，如果我们用这些最大似然估计，那么一些$\phi_j$可能最终就是零了，这就是个问题了。要避免这个情况，咱们就可以引入<strong>拉普拉斯平滑（Laplace smoothing），</strong> 这种方法把上面的估计替换成：</p><script type="math/tex; mode=display">\phi_j=\frac{\sum^m_{i=1}1\{z^{(i)}=j\}+1}{m+k}</script><p>这里首先是对分子加$1$，然后对分母加$k$，要注意$\sum^k_{j=1} \phi_j = 1$依然成立（自己检验一下），这是一个必须有的性质，因为$\phi_j$ 是对概率的估计，然后所有的概率加到一起必然等于$1$。另外对于所有的 $j$ 值，都有$\phi_j \neq 0$，这就解决了刚刚的概率估计为零的问题了。在某些特定的条件下（相当强的假设条件下，arguably quite strong），可以发现拉普拉斯平滑还真能给出对参数$\phi_j$ 的最佳估计（optimal estimator）。</p><p>回到我们的朴素贝叶斯分选器问题上，使用了拉普拉斯平滑之后，对参数的估计就写成了下面的形式：</p><script type="math/tex; mode=display">\begin{aligned}\phi_{j|y=1} & =\frac{\sum^m_{i=1}1\{x_j^{(i)}=1\wedge y ^{(i)}=1\}+1}{\sum^m_{i=1}1{\{y^{(i)}=1\}}+2}\\\phi_{j|y=0} & =\frac{\sum^m_{i=1}1\{x_j^{(i)}=1\wedge y ^{(i)}=10\}+1}{\sum^m_{i=1}1{\{y^{(i)}=0\}}+2}\\\end{aligned}</script><p>（在实际应用中，通常是否对$\phi_y$ 使用拉普拉斯并没有太大影响，因为通常我们会对每个垃圾邮件和非垃圾邮件都有一个合适的划分比例，所以$\phi_y$ 会是对$p(y = 1)$ 的一个合理估计，无论如何都会与零点有一定距离。）</p><h5 id="2-2-针对文本分类的事件模型（Event-models-for-text-classification）"><a href="#2-2-针对文本分类的事件模型（Event-models-for-text-classification）" class="headerlink" title="2.2 针对文本分类的事件模型（Event models for text classification）"></a>2.2 针对文本分类的事件模型（Event models for text classification）</h5><p>到这里就要给咱们关于生成学习算法的讨论进行一下收尾了，所以就接着讲一点关于文本分类方面的另一个模型。我们刚已经演示过的朴素贝叶斯方法能够解决很多分类问题了，不过还有另一个相关的算法，在针对文本的分类效果还要更好。</p><p>在针对文本进行分类的特定背景下，咱们上面讲的朴素贝叶斯方法使用的是一种叫做<strong>多元伯努利事件模型（Multi-Variate Bernoulli event model）。</strong> 在这个模型里面，我们假设邮件发送的方式，是随机确定的（根据先验类<em>class priors</em>， $p(y)$），无论是不是垃圾邮件发送者，他是否给你发下一封邮件都是随机决定的。那么发件人就会遍历词典，决定在邮件中是否包含某个单词 $i$，各个单词之间互相独立，并且服从概率分布$p(x<em>i=1|y)=\phi</em>{i|y}$。因此，一条消息的概率为：$p(y)\prod^n_{i-1}p(x_i|y)$</p><p> 然后还有另外一个模型，叫做<strong>多项式事件模型（Multinomial event model）。</strong> 要描述这个模型，我们需要使用一个不同的记号和特征集来表征各种邮件。设 $x_i$ 表示单词中的第$i$个单词。因此，$x_i$现在就是一个整数，取值范围为${1,…,|V|}$，这里的$|V|$是词汇列表（即字典）的长度。这样一个有 $n$ 个单词的邮件就可以表征为一个长度为 $n$ 的向量$(x_1,x_2,…,x_n)$；这里要注意，不同的邮件内容，$n$ 的取值可以是不同的。例如，如果一个邮件的开头是”A NIPS . . .” ，那么$x_1 = 1$ (“a” 是词典中的第一个)，而$x_2 = 35000$ (这是假设 “nips”是词典中的第35000个)。</p><p>在多项式事件模型中，我们假设邮件的生成是通过一个随机过程的，而是否为垃圾邮件是首先决定的（根据$p(y)$），这个和之前的模型假设一样。然后邮件的发送者写邮件首先是要生成  从对单词$(p(x<em>1|y))$ 的某种多项式分布中生成 $x_1$。然后第二步是独立于 $x_1$ 来生成 $x_2$，但也是从相同的多项式分布中来选取，然后是 $x_3$,$x_4$  等等，以此类推，直到生成了整个邮件中的所有的词。因此，一个邮件的总体概率就是$p(y)\prod^n</em>{i=1}p(x_i|y)$。要注意这个方程看着和我们之前那个多元伯努利事件模型里面的邮件概率很相似，但实际上这里面的意义完全不同了。尤其是这里的$x_i|y$现在是一个多项式分布了，而不是伯努利分布了。</p><p>我们新模型的参数还是$\phi<em>y = p(y)$，这个跟以前一样，然后还有$\phi</em>{k|y=1} = p(x<em>j =k|y=1)$ (对任何 $j$)以及 $\phi</em>{i|y=0} =p(x_j =k|y=0)$。要注意这里我们已经假设了对于任何$j$ 的值，$p(x_j|y)$这个概率都是相等的，也就是意味着在这个哪个词汇生成的这个分布不依赖这个词在邮件中的位置$j$。</p><p>如果给定一个训练集${(x^{(i)},y^{(i)}); i = 1, …, m}$，其中 $x^{(i)}  = ( x^{(i)}<em>{1} , x^{(i)}</em>{2} ,…, x^{(i)}_{n_i})$（这里的$n$是在第$i$个训练样本中的单词数目），那么这个数据的似然函数如下所示：</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}(\phi,\phi_{k|y=0},\phi_{k|y=1})& = \prod^m_{i=1}p( x^{(i)},y^{(i)})\\& = \prod^m_{i=1}(\prod^{n_i}_{j=1}p(x_j^{(i)}|y;\phi_{k|y=0},\phi_{k|y=1}))p( y^{(i)};\phi_y)\\\end{aligned}</script><p>让上面的这个函数最大化就可以产生对参数的最大似然估计：</p><script type="math/tex; mode=display">\begin{aligned}\phi_{k|y=1}&=  \frac{\sum^m_{i=1}\sum^{n_i}_{j=1}1\{x_j^{(i)}=k\wedge y^{(i)}=1\}}{\sum^m_{i=1}1\{y^{(i)}=1\}n_i} \\\phi_{k|y=0}&=  \frac{\sum^m_{i=1}\sum^{n_i}_{j=1}1\{x_j^{(i)}=k\wedge y^{(i)}=0\}}{\sum^m_{i=1}1\{y^{(i)}=0\}n_i} \\\phi_y&=   \frac{\sum^m_{i=1}1\{y^{(i)}=1\}}{m}\\\end{aligned}</script><p>如果使用拉普拉斯平滑（实践中会用这个方法来提高性能）来估计$\phi<em>{k|y=0}$ 和 $\phi</em>{k|y=1}$，就在分子上加1，然后分母上加$|V|$，就得到了下面的等式：</p><script type="math/tex; mode=display">\begin{aligned}\phi_{k|y=1}&=  \frac{\sum^m_{i=1}\sum^{n_i}_{j=1}1\{x_j^{(i)}=k\wedge y^{(i)}=1\}+1}{\sum^m_{i=1}1\{y^{(i)}=1\}n_i+|V|} \\\phi_{k|y=0}&=  \frac{\sum^m_{i=1}\sum^{n_i}_{j=1}1\{x_j^{(i)}=k\wedge y^{(i)}=0\}+1}{\sum^m_{i=1}1\{y^{(i)}=0\}n_i+|V|} \\\end{aligned}</script><p>当然了，这个并不见得就是一个最好的分类算法，不过朴素贝叶斯分选器通常用起来还都出乎意料地那么好。所以这个方法就是一个很好的”首发选择”，因为它很简单又很好实现。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
      <category>转载</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229-note-3-广义线性模型</title>
    <link href="/2019/04/26/CS229-note-3-%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <url>/2019/04/26/CS229-note-3-%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p>在前面的学习中，我们主要讨论回归和分类的问题，在回归问题中我们默认的分布模型为正态分布$y | x ; \theta \sim \mathcal{N}\left(\mu, \sigma^{2}\right)$，在分类问题中的模型为伯努利二项分布$y | x ; \theta \sim$ Bernoulli $(\phi)$。这里的$\mu$与$\phi$是$x$与$\theta$的函数</p><p>我们接下来要介绍广义线性模型（generalized linear models，简称 GLM），之前的两种模型都是广义线性模型的特例，除此之外广义线性模型还可以导出其他不同的模型，适用于其他类型的分类或回归问题。</p><h2 id="1-指数族（The-Exponential-Family）"><a href="#1-指数族（The-Exponential-Family）" class="headerlink" title="1. 指数族（The Exponential Family）"></a>1. 指数族（The Exponential Family）</h2><p>在介绍广义线性模型之前，要先介绍指数族分布。如果一个分布的概率密度函数可以写成：</p><script type="math/tex; mode=display">p(y ; \eta)=b(y) \exp \left(\eta^{T} T(y)-a(\eta)\right)</script><p>则称该分布属于指数族。这里的$\eta$称为该分部的natural parameter。T(y)称之为充分统计量(sufficient statistic)，在我们讨论的例子中，T(y)=y。$a(\eta)$是一个log partition函数。$e^{-a(\eta)}$是一个用来让整个函数正规化（normalization）的常数，也就是让整个$p(y;\eta)$函数对y积分或加和之后等于一。</p><p>选定T、a、b之后，就会得到一个参数为$\eta$的分布族，当我们改变$\eta$的值，就可以得到该分布族的一个分布。</p><p>接下来将证明Bernoulli 与 Gaussian（正态）分布其实也都是指数族分布的一种，也就是说上面的T、a、b经过适当的选择之后，就可以得到这两个分布的概率密度函数。</p><p>Bernoulli(φ)的概率密度函数为</p><script type="math/tex; mode=display">p(y ; \phi)=\left\{\begin{array}{ll}{\phi,} & {\text { if } \mathrm{y}=1} \\ {1-\phi,} & {\text { if } \mathrm{y}=0}\end{array}\right.</script><p>当我们改变φ的时候，就可以得到不同的Bernoulli分布。上面这个式子可以改写成：</p><script type="math/tex; mode=display">\begin{aligned} p(y, \phi) &=\phi^{y}(1-\phi)^{1-y} \\ &=\exp (y \log \phi+(1-y) \log (1-\phi)) \\ &=\exp \left(\left(\log \left(\frac{\phi}{1-\phi}\right)\right) y+\log (1-\phi)\right) \end{aligned}</script><p>这样就可以看出来，natural parameter为$\eta=\log{\frac{\phi}{1-\phi}}$，此时</p><script type="math/tex; mode=display">\begin{aligned} T(y) &=y \\ a(\eta) &=-\log (1-\phi) \\ &=\log \left(1+e^{\eta}\right) \\ b(y) &=1 \end{aligned}</script><p>同理，如果设置正态分布的方差为1，则正态分布的概率密度函数也可以写成：</p><script type="math/tex; mode=display">\begin{aligned} p(y ; \mu) &=\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{1}{2}(y-\mu)^{2}\right) \\ &=\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{1}{2} y^{2}\right) \cdot \exp \left(\mu y-\frac{1}{2} \mu^{2}\right) \end{aligned}</script><p>其中</p><script type="math/tex; mode=display">\begin{aligned} \eta &=\mu \\ T(y) &=y \\ a(\eta) &=\mu^{2} / 2 \\ &=\eta^{2} / 2 \\ b(y) &=(1 / \sqrt{2 \pi}) \exp \left(-y^{2} / 2\right) \end{aligned}</script><p>事实上，很多分布都属于指数组分布，包括多项式分布、泊松分布、指数分布、gamma分布、beta分布、狄利克雷分布等等</p><h2 id="2-构造广义线性模型（GLMs）"><a href="#2-构造广义线性模型（GLMs）" class="headerlink" title="2. 构造广义线性模型（GLMs）"></a>2. 构造广义线性模型（GLMs）</h2><p>以下部分摘自@飞龙翻译的CS229笔记。<br>设想你要构建一个模型，来估计在给定的某个小时内来到你商店的顾客人数（或者是你的网站的页面访问次数），基于某些确定的特征 $x$ ，例如商店的促销、最近的广告、天气、今天周几啊等等。我们已经知道泊松分布（Poisson distribution）通常能适合用来对访客数目进行建模。知道了这个之后，怎么来建立一个模型来解决咱们这个具体问题呢？非常幸运的是，泊松分布是属于指数分布族的一个分布，所以我们可以对该问题使用广义线性模型（Generalized Linear Model，缩写为 GLM）。在本节，我们讲一种对刚刚这类问题构建广义线性模型的方法。</p><p>进一步泛化，设想一个分类或者回归问题，要预测一些随机变量 $y$ 的值，作为 $x$ 的一个函数。要导出适用于这个问题的广义线性模型，就要对我们的模型、给定 $x$ 下 $y$ 的条件分布来做出以下三个假设：</p><p>$y | x; \theta ∼ Exponential Family(\eta)$，即给定 $x$ 和 $\theta, y$ 的分布属于指数分布族，是一个参数为 $\eta$ 的指数分布。——<strong>假设1</strong></p><p>给定 $x$，目的是要预测对应这个给定 $x$ 的 $T(y)$ 的期望值。咱们的例子中绝大部分情况都是 $T(y) = y$，这也就意味着我们的学习假设 $h$ 输出的预测值 $h(x)$ 要满足 $h(x) = E[y|x]$。 （注意，这个假设通过对 $h<em>\theta(x)$ 的选择而满足，在逻辑回归和线性回归中都是如此。例如在逻辑回归中， $h</em>\theta (x) = [p (y = 1|x; \theta)] =[ 0 \cdot p (y = 0|x; \theta)+1\cdot p(y = 1|x;\theta)] = E[y|x;\theta]$。译者注：这里的$E[y|x$]应该就是对给定$x$时的$y$值的期望的意思。）——<strong>假设2</strong></p><p>自然参数 $\eta$ 和输入值 $x$ 是线性相关的，$\eta = \theta^T x$，或者如果 $\eta$ 是有值的向量，则有$\eta_i = \theta_i^T x$。——<strong>假设3</strong></p><p>上面的几个假设中，第三个可能看上去证明得最差，所以也更适合把这第三个假设看作是一个我们在设计广义线性模型时候的一种 “设计选择 design choice”，而不是一个假设。那么这三个假设/设计，就可以用来推导出一个非常合适的学习算法类别，也就是广义线性模型 GLMs，这个模型有很多特别友好又理想的性质，比如很容易学习。此外，这类模型对一些关于 $y$ 的分布的不同类型建模来说通常效率都很高；例如，我们下面就将要简单介绍一些逻辑回归以及普通最小二乘法这两者如何作为广义线性模型来推出。</p><h3 id="2-1-普通最小二乘法（Ordinary-Least-Squares）"><a href="#2-1-普通最小二乘法（Ordinary-Least-Squares）" class="headerlink" title="2.1 普通最小二乘法（Ordinary Least Squares）"></a>2.1 普通最小二乘法（Ordinary Least Squares）</h3><p>我们这一节要讲的是普通最小二乘法实际上是广义线性模型中的一种特例，设想如下的背景设置：目标变量 $y$（在广义线性模型的术语也叫做响应变量response variable）是连续的，然后我们将给定 $x$ 的 $y$ 的分布以高斯分布 $N(\mu, \sigma^2)$ 来建模，其中 $\mu$ 可以是依赖 $x$ 的一个函数。这样，我们就让上面的$ExponentialFamily(\eta)$分布成为了一个高斯分布。在前面内容中我们提到过，在把高斯分布写成指数分布族的分布的时候，有$\mu = \eta$。所以就能得到下面的等式：</p><script type="math/tex; mode=display">\begin{aligned} h_\theta(x)& = E[y|x;\theta] \ & = \mu \ & = \eta \ & = \theta^Tx\ \end{aligned}</script><p>第一行的等式是基于<strong>假设2</strong>；第二个等式是基于定理当 $y|x; \theta ∼ N (\mu, \sigma ^2)$，则 $y$ 的期望就是 $\mu$ ；第三个等式是基于<strong>假设1</strong>，以及之前我们此前将高斯分布写成指数族分布的时候推导出来的性质 $\mu = \eta$；最后一个等式就是基于<strong>假设3</strong>。</p><h3 id="2-2-逻辑回归（Logistic-Regression）"><a href="#2-2-逻辑回归（Logistic-Regression）" class="headerlink" title="2.2 逻辑回归（Logistic Regression）"></a>2.2 逻辑回归（Logistic Regression）</h3><p>接下来咱们再来看看逻辑回归。这里咱们还是看看二值化分类问题，也就是 $y \in {0, 1}$。给定了$y$ 是一个二选一的值，那么很自然就选择伯努利分布（Bernoulli distribution）来对给定 $x$ 的 $y$ 的分布进行建模了。在我们把伯努利分布写成一种指数族分布的时候，有 $\phi = 1/ (1 + e^{−\eta})$。另外还要注意的是，如果有 $y|x; \theta ∼ Bernoulli(\phi)$，那么 $E [y|x; \theta] = \phi$。所以就跟刚刚推导普通最小二乘法的过程类似，有以下等式：</p><script type="math/tex; mode=display">\begin{aligned} h_\theta(x)& = E[y|x;\theta] \ & = \phi \ & = 1/(1+ e^{-\eta}) \ & = 1/(1+ e^{-\theta^Tx})\ \end{aligned}</script><p>所以，上面的等式就给了给了假设函数的形式：$h_\theta(x) = 1/ (1 + e^{−\theta^T x})$。如果你之前好奇咱们是怎么想出来逻辑回归的函数为$1/ (1 + e^{−z} )$，这个就是一种解答：一旦我们假设以 $x$ 为条件的 $y$ 的分布是伯努利分布，那么根据广义线性模型和指数分布族的定义，就会得出这个式子。</p><p>再解释一点术语，这里给出分布均值的函数 $g$ 是一个关于自然参数的函数，$g(\eta) = E[T(y); \eta]$，这个函数也叫做规范响应函数（canonical response function）， 它的反函数 $g^{−1}$ 叫做规范链接函数（canonical link function）。 因此，对于高斯分布来说，它的规范响应函数正好就是识别函数（identify function）；而对于伯努利分布来说，它的规范响应函数则是逻辑函数（logistic function）。$^*$注</p><ul><li>很多教科书用 $g$ 表示链接函数，而用反函数$g^{−1}$ 来表示响应函数；但是咱们这里用的是反过来的，这是继承了早期的机器学习中的用法，我们这样使用和后续的其他课程能够更好地衔接起来。</li></ul><h3 id="2-3-Softmax-回归"><a href="#2-3-Softmax-回归" class="headerlink" title="2.3 Softmax 回归"></a>2.3 Softmax 回归</h3><p>咱们再来看一个广义线性模型的例子吧。设想有这样的一个分类问题，其中响应变量 $y$ 的取值可以是 $k$ 个值当中的任意一个，也就是 $y \in {1, 2, …, k}$。例如，我们这次要进行的分类就比把邮件分成垃圾邮件和正常邮件两类这种二值化分类要更加复杂一些，比如可能是要分成三类，例如垃圾邮件、个人邮件、工作相关邮件。这样响应变量依然还是离散的，但取值就不只有两个了。因此咱们就用多项式分布（multinomial distribution）来进行建模。</p><p>下面咱们就通过这种多项式分布来推出一个广义线性模型。要实现这一目的，首先还是要把多项式分布也用指数族分布来进行描述。</p><p>要对一个可能有 $k$ 个不同输出值的多项式进行参数化，就可以用 $k$ 个参数 $\phi<em>1,…,\phi</em> k$ 来对应各自输出值的概率。不过这么多参数可能太多了，形式上也太麻烦，他们也未必都是互相独立的（比如对于任意一个$\phi<em> i$中的值来说，只要知道其他的 $k-1$ 个值，就能知道这最后一个了，因为总和等于$1$，也就是$\sum^k</em>{i=1} \phi<em>i = 1$）。所以咱们就去掉一个参数，只用 $k-1$ 个：$\phi_1,…,\phi</em> {k-1}$ 来对多项式进行参数化，其中$\phi<em>i = p (y = i; \phi)，p (y = k; \phi) = 1 −\sum ^{k−1}{i=1}\phi i$。为了表述起来方便，我们还要设 $\phi_k = 1 − \sum</em>{i=1}^{k−1} \phi_i$，但一定要注意，这个并不是一个参数，而是完全由其他的 $k-1$ 个参数来确定的。</p><p>要把一个多项式表达成为指数组分布，还要按照下面的方式定义一个 $T (y) \in R^{k−1}$:</p><script type="math/tex; mode=display">T(1)=\left[ \begin{array}{c}{1} \\ {0} \\ {0} \\ {\vdots} \\ {0}\end{array}\right], T(2)=\left[ \begin{array}{c}{0} \\ {1} \\ {0} \\ {\vdots} \\ {0}\end{array}\right], T(3)=\left[ \begin{array}{c}{0} \\ {0} \\ {1} \\ {\vdots} \\ {0}\end{array}\right], \cdots, T(k-1)=\left[ \begin{array}{c}{0} \\ {0} \\ {0} \\ {\vdots} \\ {1}\end{array}\right], T(k)=\left[ \begin{array}{c}{0} \\ {0} \\ {0} \\ {\vdots} \\ {0}\end{array}\right]</script><p>这次和之前的样例都不一样了，就是不再有 $T(y) = y$；然后，$T(y)$ 现在是一个 $k – 1$ 维的向量，而不是一个实数了。向量 $T(y)$ 中的第 $i$ 个元素写成$(T(y))_i$ 。</p><p>现在介绍一种非常有用的记号。指示函数（indicator function）$1{\cdot }$，如果参数为真，则等于$1$；反之则等于$0$（$1{True} = 1, 1{False} = 0$）。例如$1{2 = 3} = 0$, 而$1{3 = 5 − 2} = 1$。所以我们可以把$T(y)$ 和 $y$ 的关系写成 $(T(y))_i = 1{y = i}$。（往下继续阅读之前，一定要确保你理解了这里的表达式为真！）在此基础上，就有了$E[(T(y))_i] = P (y = i) = \phi_i$。</p><p>现在一切就绪，可以把多项式写成指数族分布了。写出来如下所示：</p><script type="math/tex; mode=display">\begin{aligned} p(y ; \phi) &=\phi_{1}^{1\{y=1\}} \phi_{2}^{1\{y=2\}} \ldots \phi_{k}^{1\{y=k\}} \\ &=\phi_{1}^{1\{y=1\}} \phi_{2}^{1\{y=2\}} \cdots \phi_{k}^{1-\sum_{i=1}^{k-1} 1\{y=i\}} \\ &=\phi_{1}^{(T(y))_{1}} \phi_{2}^{(T(y))_{2}} \ldots \phi_{k}^{1-\sum_{i=1}^{k-1}(T(y))_{i}} \\ &=\exp \left((T(y))_{1} \log \left(\phi_{1}\right)+(T(y))_{2} \log \left(\phi_{2}\right)\right) \\ &=\exp \left((T(y))_{1} \log \left(\phi_{1} / \phi_{k}\right)+(T(y))_{2} \log \left(\phi_{2} / \phi_{k}\right)\right) \\ &=b(y) \exp \left(\eta^{T} T(y)-a(\eta)\right) \end{aligned}</script><p>其中：</p><script type="math/tex; mode=display">\begin{aligned} \eta &= \begin{bmatrix} \log (\phi _1/\phi _k)\ \log (\phi _2/\phi _k)\ \vdots \ \log (\phi _{k-1}/\phi _k)\ \end{bmatrix}, \ a(\eta) &= -\log (\phi _k)\ b(y) &= 1\ \end{aligned}</script><p>这样咱们就把多项式方程作为一个指数族分布来写了出来。</p><p>与 $i (for\quad i = 1, …, k)$对应的链接函数为：</p><script type="math/tex; mode=display">\eta_i =\log \frac {\phi_i}{\phi_k}</script><p>为了方便起见，我们再定义 $\eta_k = \log (\phi_k/\phi_k) = 0$。对链接函数取反函数然后推导出响应函数，就得到了下面的等式：</p><script type="math/tex; mode=display">\begin{aligned} e^{\eta_i} &= \frac {\phi_i}{\phi_k}\ \phi_k e^{\eta_i} &= \phi_i \qquad\text{(7)}\ \phi_k \sum^k_{i=1} e^{\eta_i}&= \sum^k_{i=1}\phi_i= 1\ \end{aligned}</script><p>这就说明了$\phi<em>k = \frac 1 {\sum^k</em>{i=1} e^{\eta_i}}$，然后可以把这个关系代入回到等式$(7)$，这样就得到了响应函数：</p><script type="math/tex; mode=display">\phi_i = \frac { e^{\eta_i} }{ \sum^k_{j=1} e^{\eta_j}}</script><p>上面这个函数从$\eta$ 映射到了$\phi$，称为 Softmax 函数。</p><p>要完成我们的建模，还要用到前文提到的假设3，也就是 $\eta<em>i$ 是一个 $x$ 的线性函数。所以就有了 $\eta_i= \theta_i^Tx (for\quad i = 1, …, k − 1)$，其中的 $\theta_1, …, \theta</em>{k−1} \in R^{n+1}$ 就是我们建模的参数。为了表述方便，我们这里还是定义$\theta_k = 0$，这样就有 $\eta_k = \theta_k^T x = 0$，跟前文提到的相符。因此，我们的模型假设了给定 $x$ 的 $y$ 的条件分布为：</p><script type="math/tex; mode=display">\begin{aligned} p(y=i|x;\theta) &= \phi_i \ &= \frac {e^{\eta_i}}{\sum^k_{j=1}e^{\eta_j}}\ &=\frac {e^{\theta_i^Tx}}{\sum^k_{j=1}e^{\theta_j^Tx}}\qquad\text{(8)}\ \end{aligned}</script><p>这个适用于解决 $y \in{1, …, k}$ 的分类问题的模型，就叫做 Softmax 回归。 这种回归是对逻辑回归的一种扩展泛化。</p><p>假设（hypothesis） $h$ 则如下所示:</p><script type="math/tex; mode=display">h_{\theta}(x)=\mathrm{E}[T(y) | x ; \theta]\\=\mathrm{E} \left.\left[ \begin{array}{c}{1\{y=1\}} \\ {1\{y=2\}} \\ {\vdots} \\ {1\{y=k-1\}}\end{array}\right| x ; \theta \right]\\=\left[ \begin{array}{c}{\phi_{1}} \\ {\phi_{2}} \\ {\vdots} \\ {\phi_{k-1}}\end{array}\right]\\=\left[ \begin{array}{c}{\frac{\exp \left(\theta_{1}^{T} x\right)}{\sum_{j=1}^{k} \exp \left(\theta_{j}^{T} x\right)}} \\ {\frac{\exp \left(\theta_{2}^{T} x\right)}{\sum_{j=1}^{k} \exp \left(\theta_{j}^{T} x\right)}} \\ {\vdots} \\ {\frac{\exp \left(\theta_{k-1}^{T} x\right)}{\sum_{j=1}^{k} \exp \left(\theta_{j}^{T} x\right)}}\end{array}\right]</script><p>也就是说，我们的假设函数会对每一个 $i = 1,…,k$ ，给出 $p (y = i|x; \theta)$ 概率的估计值。（虽然咱们在前面假设的这个 $h<em>\theta(x)$ 只有 $k-1$ 维，但很明显 $p (y = k|x; \theta)$ 可以通过用 $1$ 减去其他所有项目概率的和来得到，即$1− \sum^{k-1}</em>{i=1}\phi_i$。）</p><p>最后，咱们再来讲一下参数拟合。和我们之前对普通最小二乘线性回归和逻辑回归的原始推导类似，如果咱们有一个有 $m$ 个训练样本的训练集 ${(x^{(i)}, y^{(i)}); i = 1, …, m}$，然后要研究这个模型的参数 $\theta_i$ ，我们可以先写出其似然函数的对数：</p><script type="math/tex; mode=display">\begin{aligned} l(\theta)& =\sum^m_{i=1} \log p(y^{(i)}|x^{(i)};\theta)\ &= \sum^m_{i=1}log\prod ^k_{l=1}(\frac {e^{\theta_l^Tx^{(i)}}}{\sum^k_{j=1} e^{\theta_j^T x^{(i)}}})^{1(y^{(i)}=l)}\ \end{aligned}</script><p>要得到上面等式的第二行，要用到等式$(8)$中的设定 $p(y|x; \theta)$。现在就可以通过对 $l(\theta)$ 取最大值得到的 $\theta$ 而得到对参数的最大似然估计，使用的方法就可以用梯度上升法或者牛顿法了。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
      <category>转载</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229-note-2-分类问题与逻辑回归</title>
    <link href="/2019/04/26/CS229-note-2-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <url>/2019/04/26/CS229-note-2-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<h2 id="1-分类问题"><a href="#1-分类问题" class="headerlink" title="1. 分类问题"></a>1. 分类问题</h2><p>区别于<a href="./CS229-note-1-Introduction.md">CS229 note 1: Introduction</a>提到的回归问题，我们这里要研究的问题是分类，即输出变量是离散的，非黑即白、非一即二，取值范围不再是实数。</p><p>对于2-分类问题，我们倾向于找到一个函数h(x)，输入特征x后，给出0或者1的结果。看起来就是一个简化版的回归问题，毕竟结果不需要精确到数字，只需要给出一个类别就可以了。可不可以利用回归问题的解决思路来解决分类问题？答案是也可以，但会出现一系列的问题，相当于用一个复杂的模型去拟合一个简单的数据。</p><p>直观来讲，我们只需要设置某个阈值，高于此阈值的为1，低于此阈值的为0即可。没错，这是一种分类方法。</p><script type="math/tex; mode=display">g(z)=\left\{\begin{array}{ll}{1} & {\text { if } z \geq 0} \\ {0} & {\text { if } z<0}\end{array}\right.</script><p>还有其他的分类方法，比如Sigmoid函数，该函数具有良好的性质，比如光滑可求导，值域在(0,1)内等等。<br><img src="/2019/04/26/CS229-note-2-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/2019-04-26-20-12-09.png" alt></p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><p>我们利用Sigmoid函数作为$h_{\theta}(x)$，调整θ，使得分类器效果最好，这就是逻辑斯蒂回归模型。</p><script type="math/tex; mode=display">h_{\theta}(x)=g\left(\theta^{T} x\right)=\frac{1}{1+e^{-\theta^{T} x}}</script><p>请注意，虽然是回归模型，但逻辑斯蒂回归做的事情其实是分类。问题又切换到如何构建一个合理的、以θ为变量的函数，对其优化、找到最低点？</p><p>让我们以抛硬币为例，阐述二元分类问题。抛硬币事件符合0-1分布，即事情发生是1，不发生是0。如果连续抛很多次硬币，问有多少次正面，多少次反面的概率，那这就是伯努利分布，即二项分布。</p><p>二分类问题也是如此，已知样本x，在参数为θ的情况下，y=1即正面的概率，即为h(x)，y=0即反面的概率为1-h(x)。</p><script type="math/tex; mode=display">\begin{array}{l}{P(y=1 | x ; \theta)=h_{\theta}(x)} \\ {P(y=0 | x ; \theta)=1-h_{\theta}(x)}\end{array}</script><p>将两个式子通过一些数学技巧结合起来，方便数学讨论：</p><script type="math/tex; mode=display">p(y | x ; \theta)=\left(h_{\theta}(x)\right)^{y}\left(1-h_{\theta}(x)\right)^{1-y}</script><p>推广到全部数据：在参数为θ时，已知输入数据集为X，则输出为$\vec y$的概率可以用下面的函数来表示：</p><script type="math/tex; mode=display">\begin{aligned} L(\theta) &=p(\vec{y} | X ; \theta) \\ &=\prod_{i=1}^{m} p\left(y^{(i)} | x^{(i)} ; \theta\right) \\ &=\prod_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)\right)^{y^{(i)}}\left(1-h_{\theta}\left(x^{(i)}\right)\right)^{1-y^{(i)}} \end{aligned}\\\begin{aligned} \ell(\theta) &=\log L(\theta) \\ &=\sum_{i=1}^{m} y^{(i)} \log h\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h\left(x^{(i)}\right)\right) \end{aligned}</script><p>又变成了我们熟悉的最大似然估计问题，即求θ，使得$\ell(\theta)$最大。我们既可以求导数，也可以采用随机梯度下降法。</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial}{\partial \theta_{j}} \ell(\theta) &=\left(y \frac{1}{g\left(\theta^{T} x\right)}-(1-y) \frac{1}{1-g\left(\theta^{T} x\right)}\right) \frac{\partial}{\partial \theta_{j}} g\left(\theta^{T} x\right) \\ &=\left(y \frac{1}{g\left(\theta^{T} x\right)}-(1-y) \frac{1}{1-g\left(\theta^{T} x\right)}\right) g\left(\theta^{T} x\right)\left(1-g\left(\theta^{T} x\right)\frac{\partial}{\partial \theta_{j}} \theta^{T} x\right.\\ &=\left(y\left(1-g\left(\theta^{T} x\right)\right)-(1-y) g\left(\theta^{T} x\right)\right) x_{j} \\ &=\left(y-h_{\theta}(x)\right) x_{j} \end{aligned}\\\begin{aligned}\theta_{j} :=\theta_{j}+\alpha\left(y^{(i)}-h_{\theta}\left(x^{(i)}\right)\right) x_{j}^{(i)}\end{aligned}</script><p>有没有感觉求偏导数后的形式与回归问题很相似？这背后又有着怎样的共同点？</p><p>还有更巧的呢。记得我们之前说的$<br>g(z)=\left{\begin{array}{ll}{1} &amp; {\text { if } z \geq 0} \ {0} &amp; {\text { if } z&lt;0}\end{array}\right.<br>$吗？采用该分类函数作为分类器进行训练的模型，叫做感知器(perceptron learning algorithm)模型。它的梯度下降算法也是这种形式：$\theta<em>{j} :=\theta</em>{j}+\alpha\left(y^{(i)}-h<em>{\theta}\left(x^{(i)}\right)\right) x</em>{j}^{(i)}$</p><p>这背后的原因将留在我之后的更新中解答。</p><h2 id="2-其他优化似然函数的方法：牛顿法"><a href="#2-其他优化似然函数的方法：牛顿法" class="headerlink" title="2.其他优化似然函数的方法：牛顿法"></a>2.其他优化似然函数的方法：牛顿法</h2><p>除了前面说的梯度下降法，牛顿法也是机器学习中用的比较多的一种优化算法。牛顿法遵循这样的优化规则：</p><script type="math/tex; mode=display">\theta :=\theta-\frac{f(\theta)}{f^{\prime}(\theta)}</script><p><img src="/2019/04/26/CS229-note-2-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/2019-04-26-20-52-14.png" alt><br>首先选择一点，从该点作函数的切线，交x轴于新的点x2，x2再作切线，交x轴于新的点x3，如上图所示。</p><p>最终牛顿法会找到f(x)最小时的x值，整个过程会很快，比梯度下降要快。<br>如果我们想要优化的函数是$\ell^{\prime}(\theta)$，则牛顿法优化规则为：</p><script type="math/tex; mode=display">\theta :=\theta-\frac{\ell^{\prime}(\theta)}{\ell^{\prime \prime}(\theta)}</script><p>上面我们假设θ是一个实变量。如果θ是一个向量，则牛顿法变成：</p><script type="math/tex; mode=display">\theta :=\theta-H^{-1} \nabla_{\theta} \ell(\theta)</script><p>其中：</p><script type="math/tex; mode=display">H_{i j}=\frac{\partial^{2} \ell(\theta)}{\partial \theta_{i} \partial \theta_{j}}</script><p>像这样将牛顿法应用于逻辑斯蒂回归的似然函数优化问题上，叫做fisher’s scoring。</p><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h2><p>我们主要讨论了分类问题的概念，二分类问题的处理方法：感知器、逻辑斯蒂回归，以及将牛顿法应用于逻辑回归。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229 note 1: Introduction</title>
    <link href="/2019/04/26/CS229-note-1-Introduction/"/>
    <url>/2019/04/26/CS229-note-1-Introduction/</url>
    
    <content type="html"><![CDATA[<h2 id="1-Supervised-learning"><a href="#1-Supervised-learning" class="headerlink" title="1. Supervised learning"></a>1. Supervised learning</h2><p>机器学习，机器从数据中学习固定的问题-答案模式，形成固定的问答模型的过程。</p><p>机器学习的过程可用下图表示：</p><p><img src="/2019/04/26/CS229-note-1-Introduction/2019-04-26-12-57-21.png" alt></p><p>机器通过特定的学习方法Learning Algorithm学习已知的问题-答案数据集Training Set，Learning Algorithm最终会得到一个与真实模型相差无几的假说模型hypothesis。得到该模型之后，我们就可以利用它，对全新的问题的答案作出预测。</p><p>机器通过问题-答案训练模型的过程，称之为监督学习过程。问题是由一组已知数据构成的输入，比如房屋的面积、照射率、卧室数目等等；答案则是一个确切的结论，比如房屋的价格。刚才举的例子就是依照房屋的面积和其他信息来预测房屋的价格，输出值是一个实数（连续变量）。这在统计学上称之为回归问题。假设房屋的价格和面积成正比，则称上个问题为线性回归问题。</p><p>好了，那么机器怎么实现学习的呢？我们可以以预测房屋价格为例，构建一个学习模型。</p><p>首先，回归问题的本质就是通过给定一系列的参数，拟合出一个函数h(x)，这个函数应该与真实函数y(x)越接近越好。h(x)大概长成这样：</p><script type="math/tex; mode=display">h_{\vec\theta}(\vec x)=\theta_0+\theta_1x_1+\theta_2x_2+\cdots\\=\vec\theta^T\vec x</script><p>今后为了方便起见，$\vec x$和$\vec \theta$等等就不加上箭头了，他们都表示由许多分量的向量。</p><p>可以看到，学习效果的好坏，取决于$\theta$选择的好坏，选择的好了$h_{\theta}(x)$就更逼近真实的y。学习的过程，就是不断修改$\theta$的过程。</p><p>那我们怎么改$\theta$呢？回想我们的数学建模过程，我们可以构造一个函数，自变量为$\theta$，检验y(x)与h(x)的距离（即差值），距离越小则说明$\theta$越完美。这不就转化成了一个最优化问题了吗？如果构造出来的函数还能够求导的话，求一次导，找到最低点，岂不是美滋滋？因此我们构造出来了以下函数：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{2}\Sigma_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})</script><p>其中m是样本个数，以上就是<strong>线性回归</strong>问题的<strong>损失函数</strong>，损失函数越小，训练得越好。</p><h2 id="2-求解损失函数最小值的迭代思路"><a href="#2-求解损失函数最小值的迭代思路" class="headerlink" title="2. 求解损失函数最小值的迭代思路"></a>2. 求解损失函数最小值的迭代思路</h2><p>按理说，我们直接对损失函数求导，马上就知道二次函数的最低点了。求这个函数的导数可不太容易，首先我们以非解析方法来迭代求解该函数。我们采用梯度下降法，逐步逼近全局最优点：</p><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)</script><p>$\theta$维度太多，求导是个困难的事情。如果对$\theta$求梯度，得到某一点处下降速度最快的方向，即可逐步实现优化。下面是对$J(\theta)$求偏导数的过程：</p><script type="math/tex; mode=display">\frac{\partial J(\theta)}{\partial \theta_j}=\frac{1}{2}\Sigma_{i=i}^{m}\frac{\partial(\theta^Tx^{(i)}-y^{(i)})^2}{\partial\theta_j}\\=\Sigma_{i=i}^{m}(\theta^Tx^{(i)}-y^{(i)})\cdot x_j^{(i)}</script><p>因此我们得到了以下学习算法：</p><p><img src="/2019/04/26/CS229-note-1-Introduction/2019-04-26-14-57-21.png" alt></p><p>该算法具有“离终点越近，收敛速度越慢”的特点。该算法还有一个特点，即每计算一个$\theta$的分量$\theta_j$，算法都需要遍历一遍整个数据集。效率是O(n*n*m*k),n是维度，m是数据集大小，k是直到收敛为止循环的次数。</p><p>因为每判断一次，都需要整批数据的支持，因此这种梯度下降方式又称之为<strong>批梯度下降</strong>(batch gradient descent)。</p><p>做一个不恰当的比喻：批梯度下降好比每做一次选择，总是要询问周边所有的人的意见，之后给出一个让所有人都比较满意的方向。</p><p>与之相对应，如果我只问一个人的意见，就给出结论，这不就节约了遍历整个数据集的时间了吗？这种方法叫做<strong>随机梯度下降</strong>(stochastic gradient descent)。</p><p>算法表示如下：</p><p><img src="/2019/04/26/CS229-note-1-Introduction/2019-04-26-15-17-07.png" alt></p><p>该算法的效率是O(n<em>\n\</em>k)。虽然性能得到了提升，但是牺牲了一部分准确性。事实证明随机梯度下降能在一定程度上代替批梯度下降，我们更多地使用小批量梯度下降法，即结合二者的优势，做决定之前问问一部分人的意见即可。</p><h2 id="3-求解损失函数最小值的代数证明"><a href="#3-求解损失函数最小值的代数证明" class="headerlink" title="3. 求解损失函数最小值的代数证明"></a>3. 求解损失函数最小值的代数证明</h2><p>前文说过，对$J(\theta)$求导不是不行，而是因为如果$\theta$维度变高，求导相对困难。下面我们就来硬肛这个方法。</p><p>先要做一些数学知识铺垫，即矩阵的求导运算：</p><ol><li>函数对矩阵求导<br>对于一个函数$f:\mathbb{R}^{m\times n}\mapsto\mathbb{R}$，即输入一个矩阵，输出一个值的此类函数，定义f对矩阵A求导(the derivative of f with respect to A)为：<script type="math/tex; mode=display">\nabla_{A} f(A)=\left[ \begin{array}{ccc}{\frac{\partial f}{\partial A_{11}}} & {\cdots} & {\frac{\partial f}{\partial A_{1 n}}} \\ {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial f}{\partial A_{m 1}}} & {\cdots} & {\frac{\partial f}{\partial A_{m n}}}\end{array}\right]</script></li></ol><p>举个例子，若定义$A=\left[ \begin{array}{ll}{A<em>{11}} &amp; {A</em>{12}} \ {A<em>{21}} &amp; {A</em>{22}}\end{array}\right]$，$f(A)=\frac{3}{2} A<em>{11}+5 A</em>{12}^{2}+A<em>{21} A</em>{22}$，那么有$\nabla<em>{A} f(A)=\left[ \begin{array}{cc}{\frac{3}{2}} &amp; {10 A</em>{12}} \ {A<em>{22}} &amp; {A</em>{21}}\end{array}\right]$。</p><ol><li>矩阵的迹(trace, tr)<br>矩阵的迹特指方阵的迹。矩阵的迹就是上文所定义的函数f的一个代表。定义如下：</li></ol><script type="math/tex; mode=display">\operatorname{tr} A=\sum_{i=1}^{n} A_{i i}</script><p>矩阵的迹有以下性质：</p><p>$tr(AB)=tr(BA)\<br>tr(ABC)=tr(CAB)=tr(BCA)\<br>tr(ABCD)=tr(DABC)=tr(CDAB)=tr(BCDA)\<br>trA=trA^T\<br>tr(A+B)=trA+trB\<br>traA=atrA$</p><p>矩阵的转置的性质：</p><p>$(AB)^T= B^TA^T \<br>(A+B)^T=A^T+B^T$</p><p>对迹求导的几个结论：<br>$\begin{aligned} \nabla<em>{A} \operatorname{tr} A B &amp;=B^{T} \ \nabla</em>{A^{T}} f(A) &amp;=\left(\nabla<em>{A} f(A)\right)^{T} \ \nabla</em>{A} \operatorname{tr} A B A^{T} C &amp;=C A B+C^{T} A B^{T} \ \nabla_{A}|A| &amp;=|A|\left(A^{-1}\right)^{T} \end{aligned}$</p><ol><li>一些符号的定义<br>如果X是由样本构成的矩阵，<script type="math/tex; mode=display">X=\left[ \begin{array}{c}{-\left(x^{(1)}\right)^{T}-} \\ {-\left(x^{(2)}\right)^{T}-} \\ {\vdots} \\ {-\left(x^{(m)}\right)^{T}-}\end{array}\right]</script>y是由真实结果组成的向量，<script type="math/tex; mode=display">\vec{y}=\left[ \begin{array}{c}{y^{(1)}} \\ {y^{(2)}} \\ {\vdots} \\ {y^{(m)}}\end{array}\right]</script></li></ol><p>由于$h_{\theta}\left(x^{(i)}\right)=\left(x^{(i)}\right)^{T} \theta$，所以我们可以得到下式：</p><script type="math/tex; mode=display">X \theta-\vec{y}=\left[ \begin{array}{c}{\left(x^{(1)}\right)^{T} \theta} \\ {\vdots} \\ {\left(x^{(m)}\right)^{T} \theta}\end{array}\right]-\left[ \begin{array}{c}{y^{(1)}} \\ {\vdots} \\ {y^{(m)}}\end{array}\right]$$</script><p>=\left[ \begin{array}{c}{h<em>{\theta}\left(x^{(1)}\right)-y^{(1)}} \ {\vdots} \ {h</em>{\theta}\left(x^{(m)}\right)-y^{(m)}}\end{array}\right]</p><script type="math/tex; mode=display">则我们的损失函数可以化成：</script><p>\begin{aligned} \frac{1}{2}(X \theta-\vec{y})^{T}(X \theta-\vec{y}) &amp;=\frac{1}{2} \sum<em>{i=1}^{m}\left(h</em>{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2} \ &amp;=J(\theta) \end{aligned}</p><script type="math/tex; mode=display">4. 开始推导根据上面的结论，让损失函数对$\theta$求导，得到：</script><p>\begin{aligned} \nabla<em>{\theta} J(\theta) &amp;=\nabla</em>{\theta} \frac{1}{2}(X \theta-\vec{y})^{T}(X \theta-\vec{y}) \ &amp;=\frac{1}{2} \nabla<em>{\theta}\left(\theta^{T} X^{T} X \theta-\theta^{T} X^{T} \vec{y}-\vec{y}^{T} X \theta+\vec{y}^{T} \vec{y}\right) \ &amp;=\frac{1}{2} \nabla</em>{\theta} \operatorname{tr}\left(\theta^{T} X^{T} X \theta-\theta^{T} X^{T} \vec{y}-\vec{y}^{T} X \theta+\vec{y}^{T} \vec{y}\right) \ &amp;=\frac{1}{2} \nabla_{\theta}\left(\operatorname{tr} \theta^{T} X^{T} X \theta-2 \operatorname{tr} \vec{y}^{T} X \theta\right) \ &amp;=\frac{1}{2}\left(X^{T} X \theta+X^{T} X \theta-2 X^{T} \vec{y}\right) \ &amp;=X^{T} X \theta-X^{T} \vec{y} \end{aligned}</p><script type="math/tex; mode=display">令导数为零，求得零点的$\theta$值为：</script><p>\theta=\left(X^{T} X\right)^{-1} X^{T} \vec{y}</p><script type="math/tex; mode=display"># 4. 为什么使用最小二乘作为损失函数？除了易于数学处理之外，还有更深刻的道理。我们将从概率的角度解释为什么使用最小二乘法作为损失函数。我们说，现实生活中的大多数模型，甚至是全部模型，其输入都含有一定的误差$\epsilon$。这些误差各自符合不同的分布。</script><p>y^{(i)}=\theta^{T} x^{(i)}+\epsilon^{(i)}</p><script type="math/tex; mode=display">根据大数定律，多个随机变量的结果趋近于高斯分布（正态分布），即$\epsilon^{(i)}\sim\mathcal{N}\left(0, \sigma^{2}\right)$。则$\epsilon^{(i)}$的概率是：</script><p>p\left(\epsilon^{(i)}\right)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(\epsilon^{(i)}\right)^{2}}{2 \sigma^{2}}\right)</p><script type="math/tex; mode=display">将$y^{(i)}=\theta^{T} x^{(i)}+\epsilon^{(i)}$带入得：</script><p>p\left(y^{(i)} | x^{(i)} ; \theta\right)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right)</p><script type="math/tex; mode=display">上式左边是在第i个样本为x、参数为θ的条件下，得到y的概率。上式完全是一个与θ有关的函数，因为样本x和y都是固定的。别忘了，我们需要求的是θ。问题转化成了**我们已知事件发生的概率，想要反推具有最大可能性时，θ的值**。这边就要稍微插一句了，机器学习学术界有两种学派，一派是*频率学派*，另一派是*贝叶斯学派*。二者各自有用武之地，都能解决上面提出的问题。下面我们利用频率学派的方法：**最大似然估计**解决这个问题。首先构造下面的函数：</script><p>L(\theta)=L(\theta ; X, \vec{y})=p(\vec{y} | X ; \theta)</p><script type="math/tex; mode=display">我们的样本扩大到整个样本集，则整体的概率分布就如上面的公式。因为我们假设每个样本集中的样本独立同分布（IID），所以根据概率的乘法定理，我们可以把每个事件概率相乘，得到整体的概率。</script><p>\begin{aligned} L(\theta) &amp;=\prod<em>{i=1}^{m} p\left(y^{(i)} | x^{(i)} ; \theta\right) \ &amp;=\prod</em>{i=1}^{m} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right) \end{aligned}</p><script type="math/tex; mode=display">又回到了最优化问题的思路来了，这是一个关于θ的函数，那我们对它求导，就有希望得到极值。连乘$\prod$不好求导的话，我们不妨对整个似然函数L(θ)取对数，因为对数在其定义域上是单调增的，所以不改变函数的单调性，我们可以利用对数函数的性质。</script><p>\begin{aligned} \ell(\theta) &amp;=\log L(\theta) \ &amp;=\log \prod<em>{i=1}^{m} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right) \ &amp;=\sum</em>{i=1}^{m} \log \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right) \ &amp;=m \log \frac{1}{\sqrt{2 \pi} \sigma}-\frac{1}{\sigma^{2}} \cdot \frac{1}{2} \sum_{i=1}^{m}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2} \end{aligned}</p><script type="math/tex; mode=display">看到了吗？式子最右端，出现了，我们的最小二乘优化项：</script><p>\frac{1}{2} \sum_{i=1}^{m}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}</p><script type="math/tex; mode=display">## 5. 局部加权回归局部加权回归的思想很简单，它可以解决欠拟合(underfitting)与过拟合(overfitting)问题。![](CS229-note-1-Introduction/2019-04-26-18-52-01.png)左图对数据的预测效果不是很好，数据明显的趋势被忽略了；右图好的过分了，模型考虑了过多的噪音和误差，导致拟合产生的函数不能良好的预测未知数据。对于现实世界的复杂模型，很难说一定能找到一个完美的函数，恰好拟合所有的数据，同时对未知数据又具有良好的预测能力。局部加权回归的思想就是，在咨询别人意见时，对于我周围的意见着重考虑，对于离我很远的意见我略微考虑，即根据离自己的距离，来决定权值大小。则优化目标就从$\sum_{i}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}$变成了$\sum_{i} w^{(i)}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}$。其中$\omega$定义为$$w^{(i)}=\exp \left(-\frac{\left(x^{(i)}-x\right)^{2}}{2 \tau^{2}}\right)</script><p>其中$\tau$定义为带宽，我将会于近期更新文章解释局部加权回归。</p><h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h2><p>我们介绍了机器学习中监督学习的概念，并引入了回归问题的解决方案，使用了最小二乘法对回归问题进行机器学习。对于最小二乘项，有两种方法可以对其优化，一种是逐步求精的梯度下降法，另一种则是一步到位的数学推导法。梯度下降适用于工程实现，而数学推导则是归纳总结背后的原理。</p><p>我们还从概率角度讨论了为什么回归问题要采用最小二乘法，并引入了欠拟合与过拟合的概念，最后采用局部加权回归方法尽可能避免拟合不良的问题。</p>]]></content>
    
    
    <categories>
      
      <category>notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hexo图片加载失败解决方案</title>
    <link href="/2019/04/25/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <url>/2019/04/25/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
    
    <content type="html"><![CDATA[<h2 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h2><p>我平时写blog使用的编辑器是Typora，采用hexo部署。某篇文章编辑的时候需要插入图片，在Typora上面，插入图片的语法是</p><p><code>![图片标题](图片地址)</code><br>比如<code>![](C:\Folder\picture.png)</code></p><p>其中图片地址是绝对路径，即盘符开头的那个路径。但是如果要上传到GitHub Pages上面，路径毫无疑问会发生变化，所以这里只能用相对路径。假设你的图片和当前的博客文件在同一目录下，则直接这样：</p><p><code>![](picture.png)</code><br><code>![](.\picture.png)</code><br><code>![](./picture.png)</code></p><p>用过unix文件系统的人都知道，其实那个<code>.</code>的作用就是指明“当前文件夹下”。但如果图片和博客文件放在一个文件夹下，肯定会导致文件夹越来越乱。所以我们有其他方式插入图片。</p><ol><li>在你的博客根目录下，打开git bash，输入以下命令：</li></ol><p><code>npm install hexo-asset-image --save</code></p><p><img src="/2019/04/25/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/npm安装图片插件.png" alt="npm安装图片插件"></p><ol><li><p>打开博客根目录下的_config.yml<br>添加一行 <code>post_asset_folder: true</code> 或者将该项设成true</p></li><li><p>以后插图片就这么插：<br><code>![随便写写](picture.png)</code></p></li></ol><p>但是你看！我还是失败了，这是搞毛呢？</p><p><img src="/2019/04/25/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/插入图片失败示例.png" alt="插入图片失败示例"></p><p>经查，很多人有我这样的问题，出错方法却千奇百怪。<br>有的人是生成目录出错的：<br><img src="/2019/04/25/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/生成绝对路径不完整.png" alt="生成绝对路径不完整"><br>有人生成目录日期格式不对的<br><img src="/2019/04/25/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/有生成目录日期格式不对的.png" alt="生成目录日期格式不对"><br>还有人回退版本妥协~<br><img src="/2019/04/25/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/还有回退版本的.png" alt="还有回退版本的"><br>我还是从这位老哥的回答受到了启发<br><img src="/2019/04/25/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/这位老哥还解决了问题.png" alt="这位老哥还解决了问题"><br>其实答案就在其中<br><img src="/2019/04/25/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/但是其实人家答案已经很清楚了.png" alt="但是其实人家答案已经很清楚了"><br>那就是打开_config.yml（我发现我的所有错误都可以从该文件中得到解决），按下图修改。<br><img src="/2019/04/25/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/不作死就不会死.png" alt="不作死就不会死"><br>我如此疯狂地插入图片就是为了练习插入图片的方法，怕是有生之年不会忘记了。</p>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>github pages</tag>
      
      <tag>hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>写在某次期末课程作业之后</title>
    <link href="/2019/04/25/%E5%86%99%E5%9C%A8%E6%9F%90%E6%AC%A1%E6%9C%9F%E6%9C%AB%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A%E4%B9%8B%E5%90%8E/"/>
    <url>/2019/04/25/%E5%86%99%E5%9C%A8%E6%9F%90%E6%AC%A1%E6%9C%9F%E6%9C%AB%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A%E4%B9%8B%E5%90%8E/</url>
    
    <content type="html"><![CDATA[<h1 id="写在某次期末课程作业之后"><a href="#写在某次期末课程作业之后" class="headerlink" title="写在某次期末课程作业之后"></a>写在某次期末课程作业之后</h1><p>当任课老师在群里说：“Superlova，请尽快将你的作业发给我，电子版和纸质版。”的时候，我还在看“POP子与PIPI美”的第二遍。</p><p>还好我的不看微信的习惯此时并没有让我继续坠入深渊，不过当我阅读完整段消息，一股过电的感觉从心口开始向四肢扩散。我的心脏仿佛停止了跳动，我想大叫、砸东西，我脑子里反复出现以下字眼：</p><blockquote><p>你在干什么呢小S，都什么时候了你还在看动漫？<br>我的天啊就剩我一个人没交了，本来不是说好了三天之前就做完吗？<br>我真是个废物……<br>我该怎么用一个小时不到的时间把所有的工作做完？？？<br>……</p></blockquote><p>1个小时候，我把论文写完，放弃和妥协了一些没能完成的目标，发到老师的邮箱中，并打印了出来，送到老师的办公室。老师看着我的论文，说：“你的作业是我最满意的。”</p><p>从办公室出来的那一刻，我心中除了如释重负、开心愉悦之外，还不断地思考着，是什么使得我将本来能用1小时做完的任务拖延到3天？有什么内在的原因，又有什么解决方案？</p><p>我以我的亲身实践为教训，得到了以下结论。</p><h2 id="1-拖延症的核心：完美主义"><a href="#1-拖延症的核心：完美主义" class="headerlink" title="1. 拖延症的核心：完美主义"></a>1. 拖延症的核心：完美主义</h2><p>其实那篇论文的内容我在第一天就写完了，剩下的就是排版和引用参考文献、制作图表等细节上的工作。这也是为什么最后我经过一个小时的突击就能够写完论文的客观原因。不过，正是因为我在最后对目标的一些trade off，才使得我恢复到了一个正常的、正在工作的状态。</p><p>首先我放弃了参考文献中对图表的引用。其次我放弃了对个人信息的增删改查，而是把个人信息放于文件名和邮件之中。还有一些小细节。这些细节的确能使我的论文登上大雅之堂，但是对当时的我来说无疑是负重前行。我对自己的要求很高，凡是希望自己做到最好，正是这种心态使得我做什么都慢，不到最后不罢休。</p><h2 id="2-拖延症的解决方法：设置Deadline"><a href="#2-拖延症的解决方法：设置Deadline" class="headerlink" title="2. 拖延症的解决方法：设置Deadline"></a>2. 拖延症的解决方法：设置Deadline</h2><p>老师的一席话，让我产生了浑身过电的感觉。在之后的1个小时，我仿佛进入了一种奇妙的“入定”状态，大脑中的所有资源都集中于手头的任务，这种感觉也存在于高中做题的时候。就是越写越兴奋，越兴奋越思如泉涌，越写越多的状态。还有一个特点，那就是这种状态结束后，自己会很饿，可能是大脑消耗资源太多所导致的。<br>回想自己高中的时候，进入这种状态的最快方法就是自信，并且在考试开始的时候全做简单题，一旦那种势如破竹的感觉上身，就很容易进入“入定”状态。就怕卡壳，入定状态会随着时间的流逝逐渐消失。</p><p>现在的我没有题海要做，只有等待我完成的任务。为了不让我受外界的干扰，我尝试过番茄工作法、任务导向工作法等等，效果时好时坏，没有绝对管用的效果。也就是说采取哪个方法与学习效率是否提高的相关性不是很强。我认为只有设置Deadline，而且是完不成就真的会Dead的line，才能给自己足够大的压力，让自己心无旁骛的投入到任务中去。</p><h2 id="3-拖延症的解决方法：设置子任务"><a href="#3-拖延症的解决方法：设置子任务" class="headerlink" title="3. 拖延症的解决方法：设置子任务"></a>3. 拖延症的解决方法：设置子任务</h2><p>如果设置了Deadline，那么接下来的情况大概是这样子：</p><p><img src="/2019/04/25/%E5%86%99%E5%9C%A8%E6%9F%90%E6%AC%A1%E6%9C%9F%E6%9C%AB%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A%E4%B9%8B%E5%90%8E/写在某次期末课程作业之后\任务完成度曲线.png" alt="任务完成度曲线"></p><p>也就是说之后最后的那段时间，完成了绝大多数的工作。这样也不行，因为人的工作效率是有限的，过分逼迫自己做自己明显做不完的事情，不但会给自己过大的心理负担，而且会导致完不成任务的自己产生负面情绪，加重拖延。</p><p>解决方法那就是将大任务分解成一口可以吃掉的小任务，并且为了小人物设置Deadline。</p><p>现在很火的“流利阅读”啊，“水滴阅读”啊等等让你先付费再坚持习惯的方法，原理也是一样的。如果你没完成自己的任务，那么你之前交的钱就不可能给你退费了，只有当自己完成了自己的目标才能够退还费用。</p><p>并且每天需要完成的任务量也不是很大，人家已经帮你把任务分割完毕了。</p><p><img src="/2019/04/25/%E5%86%99%E5%9C%A8%E6%9F%90%E6%AC%A1%E6%9C%9F%E6%9C%AB%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A%E4%B9%8B%E5%90%8E/写在某次期末课程作业之后\任务完成度曲线四个Deadline.png" alt="任务完成度曲线四个Deadline"></p><p>虽说以上制度有庞氏骗局之嫌，并且在朋友圈形成了一股精神污染的风气，但是的的确确让很多人养成了阅读的习惯。</p><h2 id="4-拖延症的解决方法：形成正反馈"><a href="#4-拖延症的解决方法：形成正反馈" class="headerlink" title="4. 拖延症的解决方法：形成正反馈"></a>4. 拖延症的解决方法：形成正反馈</h2><p>每次自己完成子任务，都必须要给自己一点点正反馈，这个反馈必然能让自己产生舒适的情绪而不必拘泥于形式。而且不能过于沉迷导致无法完成接下来的子任务。</p><p>我大学的一位同学，是不折不扣的学神，成绩超好，人缘超棒。每天早上出门学习之后，不像其他人，他晚上8点多就回到宿舍了，开始打游戏，看纪录片，看电影，弹吉他，和别人视频聊天。</p><p>我问他你怎么不继续学习了，他说我已经把一天的任务完成了，接下来的时间就是休息时间。</p><p>这样做是有道理的，如果自己是一个目标导向的人，当自己的目标完成之后，就应该给自己奖励。一天一次，有何不可？</p><p>同理，番茄工作法也是每25分钟奖励自己一次。</p><h2 id="5-提升效率的前提：选择合适的目标和选择合理的任务"><a href="#5-提升效率的前提：选择合适的目标和选择合理的任务" class="headerlink" title="5.提升效率的前提：选择合适的目标和选择合理的任务"></a>5.提升效率的前提：选择合适的目标和选择合理的任务</h2><p>在高中，甚至在大学，我们只需要将课本上的内容、课程的习题熟练掌握，就能够得到自己满意的、中等偏上的成绩。</p><p>但是如果对自己要求高一点，就需要对自己进行深入的剖析，甚至对未来做出规划，以10年、20年为长远的目标。</p><p>虽然很多人鼓吹“技多不压身”、“现在学到的将来早晚都会用到，所以应该不求甚解的终身学习”，但是我们应当注意学习的效率。同样的时间，如果坚持十件事，未必能做到尽如人意；但是只做一件事，就可以让自己出名，让别人敬佩，最不济也让自己充满自信。</p><p>坦白来说，这个第五点恰恰是我最应该思考的一点。</p><hr><p>|: 接下来的日子我会坚持实践，争取让自己的效率能够得到提升。:|</p><hr><p>目录：</p><ul><li>完美主义</li></ul><ul><li>设置Deadline</li><li>分割子任务</li><li>正反馈</li><li>设置合理的目标和选择合适的任务</li></ul>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>procrastination</tag>
      
      <tag>study method</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Coreutils重新编译方法</title>
    <link href="/2019/04/14/Coreutils%E9%87%8D%E6%96%B0%E7%BC%96%E8%AF%91%E6%96%B9%E6%B3%95/"/>
    <url>/2019/04/14/Coreutils%E9%87%8D%E6%96%B0%E7%BC%96%E8%AF%91%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="Coreutils重新编译方法"><a href="#Coreutils重新编译方法" class="headerlink" title="Coreutils重新编译方法"></a>Coreutils重新编译方法</h1><ol><li>下载coreutils，在Linux系统下解压</li></ol><p><img src="/2019/04/14/Coreutils%E9%87%8D%E6%96%B0%E7%BC%96%E8%AF%91%E6%96%B9%E6%B3%95/下载coreutils解压.png" alt="下载coreutils解压"></p><ol><li>运行指令 <code>./configure</code></li></ol><p><img src="/2019/04/14/Coreutils%E9%87%8D%E6%96%B0%E7%BC%96%E8%AF%91%E6%96%B9%E6%B3%95/运行指令.png" alt="运行指令"></p><ol><li><p>运行 <code>make</code></p></li><li><p>进入src文件夹，挑选您要修改的文件，我以pwd.c为例，将其复制到我的个人文件夹</p></li><li><p>修改pwd.c，将其内部所有带“VERSION”的行全部注释掉</p></li><li><p>运行指令1</p><pre><code>$ gcc -E -I ~/MyCode/coreutils-8.30/lib/ -I ~/MyCode/coreutils-8.30/ -I ~/MyCode/coreutils-8.30/src pwd.c -o pwd.i</code></pre></li><li><p>运行指令2</p><pre><code>$ gcc -c pwd.i -o pwd.o</code></pre></li><li><p>运行指令3</p><pre><code>$ gcc -L ~/MyCode/coreutils-8.30/lib/ -L /usr/lib/ pwd.o -o pwd -lcoreutils -lcrypt</code></pre></li><li><p>执行<code>./pwd</code><br>其他文件同理</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>compiler</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>配置hexo+GitHub Pages纪实</title>
    <link href="/2019/04/14/%E9%85%8D%E7%BD%AEhexo+GitHub%20Pages%E7%BA%AA%E5%AE%9E/"/>
    <url>/2019/04/14/%E9%85%8D%E7%BD%AEhexo+GitHub%20Pages%E7%BA%AA%E5%AE%9E/</url>
    
    <content type="html"><![CDATA[<h1 id="配置hexo-GitHub-Pages纪实"><a href="#配置hexo-GitHub-Pages纪实" class="headerlink" title="配置hexo+GitHub Pages纪实"></a>配置hexo+GitHub Pages纪实</h1><p>想发博客不容易，折腾了大半天总算打通了一条路。</p><p>我一直有将自己匮乏的知识付诸于纸面的冲动。首先我在与FTK同学交流的过程中了解到同性交友网站GitHub可以搭建个人博客，于是今天试着探索了一下子。怎么说呢，没想到坑这么多？？？</p><p>本文旨在记录我从零开始建立个人博客的过程，还是有一定门槛的，如果对大家能有一点点帮助那就太好了。</p><h1 id="获得个人网站域名方法"><a href="#获得个人网站域名方法" class="headerlink" title="获得个人网站域名方法"></a>获得个人网站域名方法</h1><p>注册github即可，官网<code>www.github.com</code>，之后新建repository，名字为“你的用户名.github.io”。所以说用户名很重要，你要起个qq号做github名字那我估计没人能记住你的博客名，毕竟个人域名只能和用户名一样。记住项目一定要设置成公有public啊，博客是给大家看的嘛，孤芳自赏就不好玩了。</p><p>刚开始你的项目（repository）里面除了README.md都没有，但是不用怕。</p><h1 id="安装nodejs、git、hexo方法"><a href="#安装nodejs、git、hexo方法" class="headerlink" title="安装nodejs、git、hexo方法"></a>安装nodejs、git、hexo方法</h1><p>先下载Git，Git是什么？Git是代码托管工具。Git的特点是能够让多人一起开发一款软件。具体怎么实现的略去不表，总之Git的思想就是大家一起贡献代码，通过合理的组织方式保证代码提交的有序和质量的稳定，同时保存历史的代码记录防止现在的版本崩坏。但是Git的用途早已不限于程序员圈子了，起码在我看来Git用来写书、翻译、写文档等等都是极佳的，任何需要多人远程协作的项目都应该尝试使用Git。目前GitHub是全网最大的代码托管平台。自从GitHub背后的金主微软爸爸（一点也不微，一点也不软）财大气粗的宣布可以免费托管私人项目后，托管平台的隐私性得到了保证，我们更没有理由不试一试GitHub啦！</p><p>   OS: Windows 10 ~ 7均可</p><p>   <a href="https://hexo.io/zh-tw/">Hexo</a>: 3.3.1</p><p>   <a href="https://nodejs.org/en/">Node.js</a>: 6.10.2 LTS</p><p>   <a href="https://desktop.github.com/">Github Desktop</a></p><p>   <a href="https://git-scm.com/download/win">Git</a>选择64位的安装包，下载后安装</p><p>   新增一个仓库(Repositories)，仓库名称为 <code>yourname.github.io</code> [yourname是你的账号]。<br>   开启刚安装好的 Github Desktop ，并将刚创好的仓库存到本地端。<br>   然后右键刚拉下来的仓库，选取 Open in Git Shell 打开 Git bash(option可选)，执行指令将 Github 上的仓库拉到本地端。</p><pre><code>$ git pull origin master</code></pre><p>Github Desktop 右上的 Sync 按钮具有 pull/push 功能，不想打指令可以多尝试。<br>使用 Github Desktop 的好处是不必像其他教学一样，不需要配置 SSH Key 和 设定 origin 位置路径，省了两个步骤。<br>安装好 Node.js 后，就能使用 npm 安装 hexo。</p><pre><code>$ npm install -g hexo-cli</code></pre><p>输入以下指令可查看版本。</p><pre><code>$ hexo version</code></pre><p>接下来，依序输入以下指令，初始化我们的 Blog。</p><pre><code># git bash 上面的路径大概长这样You-PC@You  /e/Documents/GitHub/yourname.github.io (master)$ hexo init        # 初始化 blog# 输入上面那个指令后 hexo 会产生新的 .git盖掉旧的 .git$ git init              # 所以就重新产生一个 .git$ npm install        # 安装相关套件$ hexo g        # 产生静态页面$ hexo s        # 启动本地服务器</code></pre><p>网址列输入 <code>http://localhost:4000</code> ，就能观看 Blog 了。<br>预设会有一个 Hello World 文章。<br>到我们本地 yourname.github.io 的文件夹中能找到 _config.yml 文件。<br>这是 Hexo 的全局配置文件。</p><pre><code># Deploymentdeploy:  type: git  repository: git@github.com:yourname/yourname.github.io.git  branch: master</code></pre><p>这边是 YAML 语法，冒号后面记得空一格，照上面的设定输入，仓库的 SSH 地址如下图可获得。<br>执行以下命令安装 hexo-deployer-git，没安装套件前输入 hexo d 会出现 Error。</p><pre><code>$ npm install hexo-deployer-git --save</code></pre><p>产生静态页面后，部署到 Github。</p><pre><code>$ hexo d -g     # 等同输入 hexo g 和 hexo d 指令</code></pre><p>再来就可以上 <code>https://yourname.github.io/</code> 查看 Blog 了。</p><h2 id="常用-Hexo-指令"><a href="#常用-Hexo-指令" class="headerlink" title="常用 Hexo 指令"></a>常用 Hexo 指令</h2><p>写新文章</p><pre><code>$ hexo new &quot;postName&quot;         # 产生新的文章$ hexo new page &quot;pageName&quot;    # 产生新的页面</code></pre><p>Hexo提供了常用命令的简写</p><pre><code>$ hexo n == hexo new           # 产生新的 post/page/draft$ hexo g == hexo generate      # 产生静态页面$ hexo s == hexo server        # 启动本地浏览$ hexo d == hexo deploy        # 部署文件至 Github 上</code></pre><p>指令组合</p><pre><code>$ hexo d -g    # 产生静态文件后，部署 blog$ hexo s -g    # 产生静态文件后，预览 blog</code></pre><h1 id="各种坑的处理方法"><a href="#各种坑的处理方法" class="headerlink" title="各种坑的处理方法"></a>各种坑的处理方法</h1><h2 id="Hexo-无法生成-index-html"><a href="#Hexo-无法生成-index-html" class="headerlink" title="Hexo 无法生成 index.html"></a>Hexo 无法生成 index.html</h2><p>在刚初始化一个项目后， 你运行 <code>hexo g</code>，有时候 hexo 并不会生成 <code>index.html</code> 和其他一些静态文件。 这一般是没有初始化完全的原因, 有些插件没有安装</p><ol><li>查看 npm 插件缺失情况</li></ol><pre><code>$ npm ls --depth 0</code></pre><p>这时一般会提醒你有插件没有装。</p><pre><code>npm ERROR! missing xxx</code></pre><ol><li>安装缺失插件</li></ol><p>如果你的插件都在 <code>packages.json</code> 里， 可以简单通过如下命令安装</p><pre><code>$ npm install --save</code></pre><p>要是没有， 就依次将所有缺失的插件安装上</p><pre><code>$ npm install --save jquery jsdom [xxx ...]</code></pre><ol><li>重新生成静态文件</li></ol><p>安装好后，执行 <code>hexo g</code> 命令应该就可以正常生成完整博客了。</p><h2 id="hexo-deploy不上去，明明本地能看但是联网后死活404"><a href="#hexo-deploy不上去，明明本地能看但是联网后死活404" class="headerlink" title="hexo deploy不上去，明明本地能看但是联网后死活404"></a>hexo deploy不上去，明明本地能看但是联网后死活404</h2><p><img src="/2019/04/14/%E9%85%8D%E7%BD%AEhexo+GitHub%20Pages%E7%BA%AA%E5%AE%9E/github-page-404.png" alt="github-page-404"></p><ol><li><p>首先有可能是环境没安装完全。</p></li><li><p>其次，可能是config.yml中deploy不完全，应为https链接，此时删除根目录下.deploy_git文件夹后重新hexo deploy即可<br><img src="/2019/04/14/%E9%85%8D%E7%BD%AEhexo+GitHub%20Pages%E7%BA%AA%E5%AE%9E/config-yml-deploy-not-ready.png" alt="config-yml-deploy-not-ready"><br><img src="/2019/04/14/%E9%85%8D%E7%BD%AEhexo+GitHub%20Pages%E7%BA%AA%E5%AE%9E/config-yml-deploy-ready.png" alt="config-yml-deploy-ready"></p></li><li><p>再次，未使用git的bash，会提示权限不够</p></li><li><p>最后，没有CNAME，CNAME应位于source文件夹内，新建文本文件CNAME，内容为你的域名，username.github.io即可。CNAME不应该有后缀名。其实这一步应该是多余的，因为域名都会自动跳转。</p></li><li><p>另外，目前GitHub还没被墙，但不排除将来被隔离的可能。</p><h2 id="hexo-next主题下出现德文"><a href="#hexo-next主题下出现德文" class="headerlink" title="hexo next主题下出现德文"></a>hexo next主题下出现德文</h2><p>根目录theme文件夹，你正在使用的主题内的config.yml内language词条加属性zh-CN，不知道哪个language就全加。</p><h2 id="登录自己的域名，发现优先以文本方式显示README-md"><a href="#登录自己的域名，发现优先以文本方式显示README-md" class="headerlink" title="登录自己的域名，发现优先以文本方式显示README.md"></a>登录自己的域名，发现优先以文本方式显示README.md</h2><p>README.md应该位于source内，并且根目录config.yml中skip_render为README.md</p></li></ol><p><img src="/2019/04/14/%E9%85%8D%E7%BD%AEhexo+GitHub%20Pages%E7%BA%AA%E5%AE%9E/list-of-hexo-on-github.png" alt="list-of-hexo-on-github"></p><h1 id="需要的环境："><a href="#需要的环境：" class="headerlink" title="需要的环境："></a>需要的环境：</h1><ol><li>GitHub账号</li><li>安装nodejs环境</li><li>安装Git环境</li><li>windows10 or 7</li></ol><h1 id="2020年1月11日更新："><a href="#2020年1月11日更新：" class="headerlink" title="2020年1月11日更新："></a>2020年1月11日更新：</h1><p>许久不写博客，今日发现重装系统后的新电脑无法正常发布和更新博客内容。事实上我的内容都上传到我的repository上了，我只需要调整好下载的分支就可以了。具体请看<a href="https://www.zhihu.com/question/21193762">参考文献</a>。具体流程如下：</p><p>首先确认你的哪个分支是最全的。我的是hexo分支保存所有博文，master分支只适用于网页渲染。因此我迁移电脑需要下载hexo分支。</p><p>调整到hexo分支的本地目录，在git bash下执行<br><code>npm install hexo</code><br><code>npm install</code> # 下载和更新需要的库<br><code>npm install hexo-deployer-git</code><br>不需要执行hexo init。</p><p>然后下载，输入<br><code>git pull origin hexo</code></p><p>如果提示报错，显示你本地已经有修改，则输入下列命令放弃修改：<br><code>git reset --hard</code><br><code>git pull origin hexo</code></p><p>现在你的本地hexo分支内应该下载了已经上传过的所有博客。</p><h1 id="2020年10月23日更新："><a href="#2020年10月23日更新：" class="headerlink" title="2020年10月23日更新："></a>2020年10月23日更新：</h1><p>npm下载慢的话，可以先<br><code>npm install cnpm</code><br>然后将所有npm指令替代为cnpm</p>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>github pages</tag>
      
      <tag>hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的第一篇博文</title>
    <link href="/2019/04/13/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E6%96%87%E5%97%B7%E5%85%84%E5%BC%9F%E7%9B%9F/"/>
    <url>/2019/04/13/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E6%96%87%E5%97%B7%E5%85%84%E5%BC%9F%E7%9B%9F/</url>
    
    <content type="html"><![CDATA[<p>第一次写博客。不知道写啥好。就先Hello GitHub吧！<br>我的目标是每周写一篇。内容不限。Flag已经高高立起来了！<br>本周计划：</p><ol><li>看完论文并翻译，实现论文的方法；</li><li>搞定吴恩达机器学习作业1；</li><li>将在GitHub上面部署blog的过程整理发布。</li></ol>]]></content>
    
    
    <categories>
      
      <category>record</category>
      
    </categories>
    
    
    <tags>
      
      <tag>github pages</tag>
      
      <tag>hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
