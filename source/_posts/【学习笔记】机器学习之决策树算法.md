---
title: 【学习笔记】机器学习之决策树算法
date: 2019-05-22 15:16:55
index_img: /img/algorithm.png
tags: ['machine learning', 'decision tree']
categories:
  - notes
mathjax: true
math: true
---

决策树算法是一类经典的监督学习算法，它的实现简单、可解释性强、能适应多种类型的输入数据，并且多颗决策树可以集成为强大的模型。常用于复杂问题的 baseline 解决方案，以及分析特征重要程度。

<!--more--->

## 1. 树的划分流程：原理与思想

### 1.1 决策树的工作原理

决策树是广泛用于分类和回归任务的模型。本质上，它从一层层的if/else 问题中进行学习，并得出结论。

![区分几种动物的决策树](【学习笔记】机器学习之决策树算法/2019-05-22-21-00-49.png)

在这张图中，树的每个结点代表一个问题或一个包含答案的终结点（也叫叶结点）。树的边将问题的答案与将问的下一个问题连接起来。

用机器学习的语言来说就是，为了区分四类动物（鹰、企鹅、海豚和熊），我们利用三个特征（“有没有羽毛”“会不会飞”和“有没有鳍”）来构建一个模型。我们可以利用监督学习从数据中学习模型，而无需人为构建模型。

### 1.2 决策树的思想

原则上讲，对于给定的属性集，可以构造的决策树的数目达指数级。尽管某些决策树比其他决策树更准确，但是由于搜索空间是指数规模的，遍历找出最佳决策树在计算上是不可行的。

尽管如此，人们还是开发出了一写有效的算法，这些算法通常基于贪心策略，在选择划分数据的属性时，采取一系列局部最优决策来构造决策树。

一种朴素的构建决策树的思路如下：

1. 如果某节点中所有记录都为同一类别，则无需分类，该节点为叶子结点；
2. 如果某节点中包含多个类别的记录，则按照某个特征的大小为参考，令节点分裂为多个子节点；比如该特征大于某值则令记录进入子节点 1，否则进入子节点 2；
3. 对所有子节点递归调用该算法，直到该节点内的所有记录都属于同一个类别，或者所有记录的选定特征都相同。

该算法会引出两个关键问题：

1. **如何选择特征进行分裂**？需要建立一种评估方法，评估不同特征对该节点的分类能力，并优先选择分类效果好的特征；
2. **如何停止分裂**？数据本身存在噪声，递归分裂过深会导致过拟合，如何剪枝来保证模型的泛化能力？

## 2. 如何选择特征进行分裂？

决策树最关键的是选择合适的划分方法，来使得问题的规模最大程度的简化。

一般来说，我们希望优先选择那些能够最大程度简化问题的特征。对于二分类任务，如果存在某个特征，大于某值的记录全都是类别 1，小于某值的记录全是类别 2，则该特征就能最大程度简化问题。

举例来说，男女性别分类任务，”有没有喉结“就是一个好特征，”头发长度“、”身高“则相对分类能力不那么强。

### 2.1 信息熵

为此，我们需要一个物理量，来度量某节点内的所有记录的类别单纯程度。如果一个节点中的记录全都是同一类别，则我们认为该节点十分纯洁。

然后判断特征的优劣，只需求目前的纯度，然后根据该特征进行分裂，再计算一遍纯度，则纯度下降的值，就可以用来度量该特征的分类能力了。

**纯度（purity）** 的定义有很多方法，比如信息熵（information entropy）。假设 $Y$ 为类别集合，$D$ 为节点中的样本集合，$p_k$ 为当前集合 $D$ 中第 $k$ 类样本占比，则信息熵的定义为：

$$\text{Entropy}(D)=-\sum_{k=1}^{|Y|} p_k\ln{p_k}$$

假设叶子结点有 10 个样本，它们的类别分别为： {1,2,2,3,3,3,3,3,3,3}，则此时该特征对应的信息熵为：

$$
\text{Entropy}=-(\frac{1}{10}\ln{\frac{1}{10}}+\frac{2}{10}\ln{\frac{2}{10}}+\frac{7}{10}\ln{\frac{7}{10}})\approx0.8
$$

对于样本集合 {1,1,1,2,2,2,3,3,3,3}，则此时的信息熵为：

$$
\text{Entropy}=-(\frac{3}{10}\ln{\frac{3}{10}}+\frac{3}{10}\ln{\frac{3}{10}}+\frac{4}{10}\ln{\frac{4}{10}})\approx1.09
$$

比较上述两个样本集合，集合 1 大部分样本是类别 3，集合 2 的类别分布比较均匀，则集合 1 的信息熵比集合 2 的要小，这说明集合 1 更纯。

### 2.2 信息增益

接下来我们需要选定特征，比如身高这个特征，可能产出三种分支节点：[0m, 1m), [1m, 2m], [2m, +inf] ，每个分支节点内部都对应了一些人。则身高这个特征划分节点后的信息熵之和，与未划分时的信息熵之差，就是**信息增益（ information gain）**。

假定特征 a 有 V 种取值，则对应 V 种叶子结点，每个节点包含的样本集合为 $D_v$ 。则 $D_v$ 的信息熵为：$\text{Entropy}(D_v)$，此时全部样本按照特征 a 进行分裂的信息增益为:

$$
\text{Gain}(D, a)=\text{Entropy}(D)-\sum^{V}_{v=1}\frac{|D_v|}{|D|}\text{Entropy}(D_v)
$$

信息增益越大，则意味着使用属性 a 来进行划分所获得的”纯度提升“越大。通过遍历所有特征，取当前节点对应的信息增益最大的特征进行分裂，这种做法就是 ID3 决策树生成算法 所采用的。

### 2.3 信息增益率

在实际使用中，我们发现单纯使用信息增益进行分裂，则决策树总是试图先使用那些取值数目较丰富的特征。假如样本存在编号，并把编号当做分类特征（这个特征当然没有意义），那么 ID3 将总会选择 编号特征作为叶片的分裂特征。因此需要惩罚那些具有过多取值的特征。

具体地，对每个特征的信息增益，除以固有值（intrinsic value），就得到了**信息增益率**（gain ratio）:

$$
\text{Gain Ratio}(D, a)=\frac{\text{Gain}(D, a)}{\text{IV}(a)}
$$

其中

$$
\text{IV}(a)=-\sum^{V}_{v=1}\frac{|D_v|}{|D|}\log{\frac{|D_v|}{|D|}}
$$

C4.5 决策树生成算法采用了该思想，并做了一定优化：先从候选特征集合中，选择那些信息增益高于平均水平的特征，再从中选择信息增益率最高的。这是为了防止一上来就选择信息增益率最大的特征，导致算法滑向另一个极端：总是选取可取值数目较少的那些特征。

### 2.4 基尼系数

除了使用信息熵作为衡量节点纯度的方法，我们还可以使用基尼值来衡量。数据集的**基尼系数（Gini）**可以定义为：

$$
\begin{align*}
\text{Gini}(D) &= \sum^{|Y|}_{k=1}\sum_{k'\neq k}p_k p_{k'} \\
               &= 1-\sum^{|Y|}_{k=1}p_k^{2}
\end{align*}
$$

直观来说，$Gini(D)$ 反映了从数据集 D 中随机抽取两个样本，其类别标签不一致的概率。$Gini(D)$ 越小，数据集的纯度越高。

则选择特征时，只需遍历特征并选择能够令节点的基尼系数最小的特征作为分裂特征即可。

$$
\text{Gini Index}(D, a)=\sum_{v=1}^{V}\frac{|D_v|}{|D|}\text{Gini}(D_v)
$$

该思想被应用在 CART 算法中，CART决策树的全称为Classification and Regression Tree，可以应用于分类和回归。

## 3. 如何停止分裂？

随着节点划分递归进行，决策树分支变得过多，甚至每个样本对应 1 个叶子。如果不对分裂过程加以干预，可能导致训练样本准确率 100%，但是实际预测时效果极差，这就是过拟合（overfitting）现象。

为了规避过拟合现象，我们可以提前终止分裂，或者在生成完毕的树结构上修剪部分分支。

### 3.1 预剪枝（提前终于规则）：

停止生长的方法有很多，比如可以当观察到的不纯性度量的增益低于某个确定的阈值时就停止扩展叶结点；当分裂后的准确率下降时停止分裂；或者干脆限制树的高度等。

通过这种方法，限制产出过于复杂的子树，减少了训练时间。在 Sklearn 中有具体的实现。

> **Sklearn 中的预剪枝操作**

Sklearn 是 python 的一个机器学习工具包，可以很方便地实现各种机器学习算法。在 Sklearn 中，决策树有如下参数，控制预剪枝过程：

1. max_depth， 最常用，限制树的最大深度。在样本量较少时很有用。
2. min_samples_leaf， 每个结点包含的最少样本量。如果分裂后，存在一个子叶，样本量过少，则没必要分裂。
3. min_samples_split， 每个节点包含的最少样本量。如果分裂前，节点包含过少的样本，则没必要分裂。和上个参数的含义相同。
4. max_features， 限制分裂时考虑的特征个数，超过限制个数的特征都会被舍弃。该参数比较暴力，使用频率较低。
5. min_impurity_decrease， 限制信息增益的大小，信息增益小于设定数值的分裂不会发生。

### 3.2 后剪枝：

初始决策树按照最大规模生长，然后进行剪枝的步骤，按照自底向上的方式修剪完全增长然后修剪。

修剪有两种做法：

1. 用新的叶子节点替换子树，该叶结点的类标号由子树下记录中的多数类决定。子树替换
2. 用子树中最常使用的分支代替子树。当模型不能再改进时终止剪枝步骤。子树提升

后剪枝的泛化性能往往强于只使用预剪枝的决策树，但是后剪枝需要先产出完整的树再剪枝，时间和空间成本要更高。另外，Sklearn 中没有专门针对后剪枝的实现方法。

## 4. 如何处理连续、缺失值

决策树的一大优势是可以不加预处理地应对离散和连续两种输入，并能够灵活应对数据集中部分样本的特征缺失情况，这令决策树算法的适用场景十分宽广。

需要提前声明，不同决策树生成算法会采用不同的处理策略，下面举的例子如果没有单独声明，都是 C4.5 算法的处理策略。

### 4.1 连续值如何划分？

如果我们要以”身高“为特征进行分裂，但是“身高”是一个连续值，无法枚举所有情况，如何分裂呢？

转换一下思路，虽然“身高”无法枚举，但是数据集是可以遍历的。将该节点内的所有数据的身高取出来，按照大小排序，然后遍历所有潜在的分裂点，依次计算纯度即可将连续值划分为两段不连续的区间，取最高的纯度为区间分割点，进而分裂为两个不同的节点。

### 4.2 如何处理特征缺失？

![](【学习笔记】机器学习之决策树算法/数据缺失.png)

数据的特征缺失现象很普遍，常见的缺失数据处理方法有：

**1）抛弃缺失值**

抛弃极少量的缺失值的样本对决策树的创建影响不是太大。但是如果属性缺失值较多或是关键属性值缺失，创建的决策树将是不完全的，同时可能给用户造成知识上的大量错误信息，所以抛弃缺失值一般不采用。只有在数据库具有极少量的缺失值同时缺失值不是关键的属性值时，且为了加快创建决策树的速度，才采用抛弃属性缺失值的方式创建决策树。

**2）补充缺失值**

缺失值较少时按照我们上面的补充规则是可行的。但如果数据库的数据较大,缺失值较多(当然,这样获取的数据库在现实中使用的意义已不大,同时在信息获取方面基本不会出现这样的数据库),这样根据填充后的数据库创建的决策树可能和根据正确值创建的决策树有很大变化。

**3）概率化缺失值**

对缺失值的样本赋予该属性所有属性值的概率分布，即将缺失值按照其所在属性已知值的相对概率分布来创建决策树。

用系数F进行合理的修正计算的信息量，

F=数据库中缺失值所在的属性值样本数量去掉缺失值样本数量/数据库中样本数量的总和，

即F表示所给属性具有已知值样本的概率。


上述的缺失值处理算法是 C4.5 算法所采纳的，除了该方法之外，还有很多其他的方法：

- 插值法（Imputation）： QUEST，CRUISE
- 替代法（Alternate/Surrogate Splits）：CART，CRUISE
- 缺失值单独分支（Missing value branch）：CHAID，GUIDE
- 概率权重（Probability weights）： C4.5
